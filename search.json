[
  {
    "objectID": "core/slides/slides-dplyr.html#tables-examples",
    "href": "core/slides/slides-dplyr.html#tables-examples",
    "title": "Tables manipulation with dplyr",
    "section": "Tables (examples)",
    "text": "Tables (examples)\n\nSpeadsheets (Excel)\n Relational tables\n\nDataframes in datascience frameworks\n\n\n: data.frame, tibble, …\n\n: pandas.dataframe\n\n\nspark: dataframe\n\n\nDask: dataframe\n\nand many others"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tables-why",
    "href": "core/slides/slides-dplyr.html#tables-why",
    "title": "Tables manipulation with dplyr",
    "section": "Tables (Why ?)",
    "text": "Tables (Why ?)\nIn Data Science, each framework comes with its own flavor(s) of table(s)\n Tables from relational databases serve as inspiration\nIn  legacy dataframes shape the life of statisticians and data scientists\nThe purpose of this session is\n\ndescribe dataframes from an end-user viewpoint (we leave aside implementations)\n\npresenting tools for\n\naccessing information within dataframes (querying)\nsummarizing information (aggregation queries)\ncleaning/cleaning dataframes (tidying)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#loading-tables-and-packages",
    "href": "core/slides/slides-dplyr.html#loading-tables-and-packages",
    "title": "Tables manipulation with dplyr",
    "section": "Loading tables and packages",
    "text": "Loading tables and packages\n\nrequire(\"tidyverse\")      # All we need is there\nrequire(\"nycflights13\")    # for flight data\nrequire(\"gt\")\nrequire(\"kableExtra\")\n# \ndata(flights)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#about-loaded-packages",
    "href": "core/slides/slides-dplyr.html#about-loaded-packages",
    "title": "Tables manipulation with dplyr",
    "section": "About loaded packages",
    "text": "About loaded packages\n\nMetapackage tidyverse provides tools to create, query, tidy dataframes as well as tools to load data from various sources and save them in persistent storage\nnycflights13 provides the dataframes we play with\ngt for tayloring table displays"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#the-flights-table",
    "href": "core/slides/slides-dplyr.html#the-flights-table",
    "title": "Tables manipulation with dplyr",
    "section": "The flights table",
    "text": "The flights table\n\n\n\nhead(flights) |&gt;\n  glimpse(width = 30) \n\nRows: 6\nColumns: 19\n$ year           &lt;int&gt; 2013,…\n$ month          &lt;int&gt; 1, 1,…\n$ day            &lt;int&gt; 1, 1,…\n$ dep_time       &lt;int&gt; 517, …\n$ sched_dep_time &lt;int&gt; 515, …\n$ dep_delay      &lt;dbl&gt; 2, 4,…\n$ arr_time       &lt;int&gt; 830, …\n$ sched_arr_time &lt;int&gt; 819, …\n$ arr_delay      &lt;dbl&gt; 11, 2…\n$ carrier        &lt;chr&gt; \"UA\",…\n$ flight         &lt;int&gt; 1545,…\n$ tailnum        &lt;chr&gt; \"N142…\n$ origin         &lt;chr&gt; \"EWR\"…\n$ dest           &lt;chr&gt; \"IAH\"…\n$ air_time       &lt;dbl&gt; 227, …\n$ distance       &lt;dbl&gt; 1400,…\n$ hour           &lt;dbl&gt; 5, 5,…\n$ minute         &lt;dbl&gt; 15, 2…\n$ time_hour      &lt;dttm&gt; 2013…\n\n\n\n\nA dataframe is a two-ways (two-dimensional) table\nhead(df) displays the first 6 rows of its first argument\nThe vectors making a dataframe may have different types/classes (a dataframe is not a matrix)\nCompare str(), glimpse(), head()"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#table-schema",
    "href": "core/slides/slides-dplyr.html#table-schema",
    "title": "Tables manipulation with dplyr",
    "section": "Table schema",
    "text": "Table schema\nA table is a list of columns\nEach column has\n\n\nname and\n\ntype (class in \n\n\n\n\nglimpse(flights,   #&lt;&lt;\n        width=50)\n\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12,…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 46…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, …\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 7…\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0,…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 201…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section",
    "href": "core/slides/slides-dplyr.html#section",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "flights has 19 columns\nEach column is a sequence (vector) of items with the same type/class\nAll columns have the same length\n\nflights has 336776 rows\nIn  parlance, a row is (often) called a tuple\n\nIn  parlance, a column is (often) called a variable"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#column-types",
    "href": "core/slides/slides-dplyr.html#column-types",
    "title": "Tables manipulation with dplyr",
    "section": "Column types",
    "text": "Column types\n\n\n\n\n\n\n\n\nclass\ncolumns\n\n\n\ninteger\n‘year’ ‘month’ ‘day’ ‘dep_time’ ‘sched_dep_time’ ‘arr_time’ ‘sched_arr_time’ ‘flight’\n\n\nnumeric\n‘dep_delay’ ‘arr_delay’ ‘air_time’ ‘distance’ ‘hour’ ‘minute’\n\n\ncharacter\n‘carrier’ ‘tailnum’ ‘origin’ ‘dest’\n\n\nPOSIXct\n‘time_hour’\n\n\nPOSIXt\n‘time_hour’\n\n\n\n\nA column, as a vector, may be belong to different classes\nOther classes: factor for categorical variables\nColumns dest, origin carrier could be coerced as factors\nShould columns dest and origin be coerced to the same factor?"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#nycflights13",
    "href": "core/slides/slides-dplyr.html#nycflights13",
    "title": "Tables manipulation with dplyr",
    "section": "nycflights13",
    "text": "nycflights13"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#columns-specification",
    "href": "core/slides/slides-dplyr.html#columns-specification",
    "title": "Tables manipulation with dplyr",
    "section": "Columns specification",
    "text": "Columns specification\n\n\n\nas.col_spec(flights)\n\n\n\ncols(\n  year = col_integer(),\n  month = col_integer(),\n  day = col_integer(),\n  dep_time = col_integer(),\n  sched_dep_time = col_integer(),\n  dep_delay = col_double(),\n  arr_time = col_integer(),\n  sched_arr_time = col_integer(),\n  arr_delay = col_double(),\n  carrier = col_character(),\n  flight = col_integer(),\n  tailnum = col_character(),\n  origin = col_character(),\n  dest = col_character(),\n  air_time = col_double(),\n  distance = col_double(),\n  hour = col_double(),\n  minute = col_double(),\n  time_hour = col_datetime(format = \"\")\n)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-1",
    "href": "core/slides/slides-dplyr.html#section-1",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "\\(\\approx\\) table schema in relational databases\nColumn specifications are useful when loading dataframes from structured text files like .csv files\n.csv files do not contain typing information\nFile loaders from package readr can be tipped about column classes using column specifications"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-2",
    "href": "core/slides/slides-dplyr.html#section-2",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "SQL stands for structured/simple Query Language\nA query language elaborated during the 1970’s at IBM by E. Codd\nGeared towards exploitation of collections of relational tables\nLess powerful but simpler to use than a programming language\ndplyr is a principled -friendly implementation of SQL ideas (and other things)\n\nAt the core of SQL lies the idea of a table calculus called relational algebra"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#relational-algebra-basics",
    "href": "core/slides/slides-dplyr.html#relational-algebra-basics",
    "title": "Tables manipulation with dplyr",
    "section": "Relational algebra (basics)",
    "text": "Relational algebra (basics)\nConvention: \\(R\\) is a table with columns \\(A_1, \\ldots, A_k\\)\n\n\n\n\n\n\nProjection (picking columns)\n\n\n\\(\\pi(R, A_1, A_3)\\)\n\n\n\n\n\n\n\n\n\nSelection/Filtering (picking rows)\n\n\n\\(\\sigma(R, {\\text{condition}})\\)\n\n\n\n\n\n\n\n\n\nJoin (mulitple tables operation)\n\n\n\\(\\bowtie(R,S, {\\text{condition}})\\)\n\n\n\n Any operation produces a table\n The schema of the derived table depends on the operation (but does not depend on the content/value of the operands)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-3",
    "href": "core/slides/slides-dplyr.html#section-3",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Table calculus relies on a small set of basic operations \\(\\pi, \\sigma, \\bowtie\\)\nEach operation has one or two table operands and produce a table\n There is more to SQL than relational algebra"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#projection-pi",
    "href": "core/slides/slides-dplyr.html#projection-pi",
    "title": "Tables manipulation with dplyr",
    "section": "Projection \\(\\pi\\)\n",
    "text": "Projection \\(\\pi\\)\n\n\\(\\pi(R, {A_1, A_3})\\)\nA projection \\(\\pi(\\cdot, {A_1, A_3})\\) is defined by a set of column names, say \\(A_1, A_3\\)\nIf \\(R\\) has columns with given names, the result is a table with names \\(A_1, A_3\\) and one row per row of \\(R\\)\nA projection is parametrized by a list of column names"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#package-dplyr",
    "href": "core/slides/slides-dplyr.html#package-dplyr",
    "title": "Tables manipulation with dplyr",
    "section": "\n Package dplyr\n",
    "text": "Package dplyr\n\n\n\n\nTranformation chapter in R4DS\nCheat sheet I\nCheat sheet II\n\n\n\n\n\n\nhttps://dplyr.tidyverse.org"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-4",
    "href": "core/slides/slides-dplyr.html#section-4",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Base  provides tools to perform relational algebra operations\nBut:\n\nBase  does not provide a consistent API\nThe lack of a consistent API makes operation chaining tricky"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#dplyr-verbs",
    "href": "core/slides/slides-dplyr.html#dplyr-verbs",
    "title": "Tables manipulation with dplyr",
    "section": "\ndplyr verbs",
    "text": "dplyr verbs\nFive basic verbs:\n\nPick observations/rows by their values (filter()) σ(…)\nPick variables by their names (select()) π(…)\nReorder the rows (arrange())\nCreate new variables with functions of existing variables (mutate())\nCollapse many values down to a single summary (summarise())\n\n\nAnd\n\n\ngroup_by() changes the scope of each function from operating on the entire dataset to operating on it group-by-group"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tidyverse",
    "href": "core/slides/slides-dplyr.html#tidyverse",
    "title": "Tables manipulation with dplyr",
    "section": "\n tidyverse",
    "text": "tidyverse\n\n\n\nAll verbs work similarly:\n\n\nThe first argument is a data frame (table).\n\n\nThe subsequent arguments describe what to do with the data frame, using the variable/column names (without quotes)\n\n\nThe result is a new data frame (table)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#dplyrselect-as-a-projection-operator-π",
    "href": "core/slides/slides-dplyr.html#dplyrselect-as-a-projection-operator-π",
    "title": "Tables manipulation with dplyr",
    "section": "\ndplyr::select() as a projection operator (π)",
    "text": "dplyr::select() as a projection operator (π)\n\\(\\pi(R, \\underbrace{A_1, \\ldots, A_3}_{\\text{column names}})\\)\nselect(R, A1, A3) #&lt;&lt;\nor, equivalently\nR |&gt; select(A1, A3) #&lt;&lt;\n |&gt; is the pipe operator\n x |&gt; f(y, z) is translated to f(x, y, z) and then evaluated"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#dplyrselect",
    "href": "core/slides/slides-dplyr.html#dplyrselect",
    "title": "Tables manipulation with dplyr",
    "section": "dplyr::select()",
    "text": "dplyr::select()\n\nFunction select has a variable number of arguments\nFunction select has a variable number of arguments\nFunction select allows to pick column by names (and much more)\nNote that in the current environment, there are no objects called A1, A3\nThe consistent API allows to use the pipe operator\n\n\n\n\n\n\n\nCaution\n\n\nThere is also a select() function in base R"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-tables",
    "href": "core/slides/slides-dplyr.html#toy-tables",
    "title": "Tables manipulation with dplyr",
    "section": "Toy tables",
    "text": "Toy tables\n\n\n\nspam &lt;- set.seed(42)\n\nR &lt;-  tibble(A1=seq(2, 10, 2),\n             A2=sample(letters, 5),\n             A3=seq(from=date(\"2021-10-21\"),\n                    to=date(\"2021-11-20\"),\n                    by=7),\n             D=sample(letters, 5))\n\nS &lt;- tibble(E=c(3,4,6,9, 10),\n            F=sample(letters, 5),\n            G=seq(from=date(\"2021-10-21\"),\n                   to=date(\"2021-10-21\")+4, by=1),\n            D=sample(letters,5)\n          )\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#projecting-toy-tables",
    "href": "core/slides/slides-dplyr.html#projecting-toy-tables",
    "title": "Tables manipulation with dplyr",
    "section": "Projecting toy tables",
    "text": "Projecting toy tables\n\n\n\nR |&gt; \n  dplyr::select(A2,D) |&gt; \n  knitr::kable(caption=\"Projecting R\")\n\n\nProjecting R\n\nA2\nD\n\n\n\nq\nr\n\n\ne\nq\n\n\na\no\n\n\nj\ng\n\n\nd\nd\n\n\n\n\n\n\n\nR |&gt; \n  dplyr::select(- where(is.character)) |&gt; \n  knitr::kable(caption=\"Projecting R, all but character columns\")\n\n\nProjecting R, all but character columns\n\nA1\nA3\n\n\n\n2\n2021-10-21\n\n\n4\n2021-10-28\n\n\n6\n2021-11-04\n\n\n8\n2021-11-11\n\n\n10\n2021-11-18"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#projecting-flights-on-origin-and-dest",
    "href": "core/slides/slides-dplyr.html#projecting-flights-on-origin-and-dest",
    "title": "Tables manipulation with dplyr",
    "section": "Projecting flights on origin and dest\n",
    "text": "Projecting flights on origin and dest\n\n\n\nflights |&gt;\n  select(origin, dest) |&gt;  #&lt;&lt;\n  head()\n\n\n# A tibble: 6 × 2\n  origin dest \n  &lt;chr&gt;  &lt;chr&gt;\n1 EWR    IAH  \n2 LGA    IAH  \n3 JFK    MIA  \n4 JFK    BQN  \n5 LGA    ATL  \n6 EWR    ORD  \n\n\nA more readable equivalent of\nhead(select(flights, origin, dest), 10)\nor\nSELECT \n  origin, dest\nFROM \n  flights\nLIMIT 6;"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#sigmar-textcondition",
    "href": "core/slides/slides-dplyr.html#sigmar-textcondition",
    "title": "Tables manipulation with dplyr",
    "section": "\\(\\sigma(R, \\text{condition})\\)",
    "text": "\\(\\sigma(R, \\text{condition})\\)\n\nA selection/filtering operation is defined by a condition that can be checked on the rows of tables with convenient schema\n\\(\\sigma(R, \\text{condition})\\) returns a table with the same schema as \\(R\\)\nThe resulting table contains the rows/tuples of \\(R\\) that satisfy \\(\\text{condition}\\)\n\\(\\sigma(R, \\text{FALSE})\\) returns an empty table with the same schema as \\(R\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#chaining-filtering-and-projecting",
    "href": "core/slides/slides-dplyr.html#chaining-filtering-and-projecting",
    "title": "Tables manipulation with dplyr",
    "section": "Chaining filtering and projecting",
    "text": "Chaining filtering and projecting\n\n\nstart &lt;- date(\"2021-10-27\")\nend &lt;- start + 21\n\nR |&gt;\n#  filter(A2 &gt; \"n\") |&gt;  #&lt;&lt;\n  filter(between(A3, start, end)) |&gt;\n  select(A1, A3) #&lt;&lt;\n\n\n# A tibble: 3 × 2\n     A1 A3        \n  &lt;dbl&gt; &lt;date&gt;    \n1     4 2021-10-28\n2     6 2021-11-04\n3     8 2021-11-11\n\n\n\n\nFiltering dropped one row\nProjecting dropped two columns"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#selecting-flights-based-on-origin-and-dest",
    "href": "core/slides/slides-dplyr.html#selecting-flights-based-on-origin-and-dest",
    "title": "Tables manipulation with dplyr",
    "section": "Selecting flights based on origin and dest\n",
    "text": "Selecting flights based on origin and dest\n\nand then projecting on dest, time_hour, carrier\n\n\nflights |&gt;\n  filter(dest %in% c('ATL', 'LAX'), #&lt;&lt;\n         origin == 'JFK') |&gt;\n  select(dest, time_hour, carrier) |&gt; #&lt;&lt;\n  head()\n\n\n# A tibble: 6 × 3\n  dest  time_hour           carrier\n  &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt;  \n1 LAX   2013-01-01 06:00:00 UA     \n2 ATL   2013-01-01 06:00:00 DL     \n3 LAX   2013-01-01 07:00:00 VX     \n4 LAX   2013-01-01 07:00:00 B6     \n5 LAX   2013-01-01 07:00:00 AA     \n6 ATL   2013-01-01 08:00:00 DL     \n\n\nIn SQL ( parlance:\nSELECT \n  dest, time_hour, carrier\nFROM \n  flights\nWHERE \n  dest IN ('ATL', 'LAX') AND\n  origin = 'JFK'\nLIMIT 6"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#logical-operations",
    "href": "core/slides/slides-dplyr.html#logical-operations",
    "title": "Tables manipulation with dplyr",
    "section": "Logical operations",
    "text": "Logical operations\n\nfilter(R, condition_1, condition_2) is meant to return the rows of R that satisfy condition_1 and condition_2\nfilter(R, condition_1 & condition_2) is an equivalent formulation\nfilter(R, condition_1 | condition_2) is meant to return the rows of R that satisfy condition_1 or condition_2 (possibly both)\nfilter(R, xor(condition_1,condition_2)) is meant to return the rows of R that satisfy either condition_1 or condition_2 (just one of them)\nfilter(R, ! condition_1) is meant to return the rows of R that do not satisfy condition_1"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#missing-values",
    "href": "core/slides/slides-dplyr.html#missing-values",
    "title": "Tables manipulation with dplyr",
    "section": "\n Missing values!",
    "text": "Missing values!\nNumerical column dep_time contains many NA's (missing values)\n\n# flights |&gt; pull(dep_time) |&gt; summary()\nsummary(flights$dep_time)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      1     907    1401    1349    1744    2400    8255 \n\n\n Missing values (NA and variants) should be handled with care\nNA & TRUE\n[1] NA\nNA | TRUE\n[1] TRUE"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#truth-tables-for-three-valued-logic",
    "href": "core/slides/slides-dplyr.html#truth-tables-for-three-valued-logic",
    "title": "Tables manipulation with dplyr",
    "section": "Truth tables for three-valued logic",
    "text": "Truth tables for three-valued logic\n\n\n  uses three-valued logic\n Generate complete truth tables for and, or, xor\n\nv &lt;- c(TRUE, FALSE, NA) # truth values\n\nlist_tt &lt;- map(c(`&`, `|`, xor),  #&lt;&lt;\n               ~ outer(v, v, .x)) #&lt;&lt;\n\nfor (i in seq_along(list_tt)){\n  colnames(list_tt[[i]]) &lt;- v\n  rownames(list_tt[[i]]) &lt;- v\n}\n\nnames(list_tt) &lt;- c('& AND',\n                    'OR',\n                    'XOR')\n\n\n\n\n\n\n& AND\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\n\nTRUE\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\nFALSE\n\n\nFALSE\n\n\nFALSE\n\n\nFALSE\n\n\n\n\nNA\n\n\nNA\n\n\nFALSE\n\n\nNA\n\n\n\n\n\n\n\n\nOR\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\n\nTRUE\n\n\nTRUE\n\n\nTRUE\n\n\nTRUE\n\n\n\n\nFALSE\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\nNA\n\n\nTRUE\n\n\nNA\n\n\nNA\n\n\n\n\n\n\n\n\nXOR\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nTRUE\n\n\nNA\n\n\n\n\nFALSE\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#slice-choosing-rows-based-on-location",
    "href": "core/slides/slides-dplyr.html#slice-choosing-rows-based-on-location",
    "title": "Tables manipulation with dplyr",
    "section": "\nslice(): choosing rows based on location",
    "text": "slice(): choosing rows based on location\n\n\nIn base  dataframe cells can be addressed by indices\nflights[5000:5010,seq(1, 19, by=5)] returns rows 5000:5010 and columns 1, 6, 11 from dataframe flights\nThis can be done in a (verbose) dplyr way using slice() and select()\n\n\nflights |&gt;\n  slice(5001:5005) |&gt;  #&lt;&lt;\n  select(seq(1, 19, by=5))\n\n# A tibble: 5 × 4\n   year dep_delay flight distance\n  &lt;int&gt;     &lt;dbl&gt;  &lt;int&gt;    &lt;dbl&gt;\n1  2013         3   4437      602\n2  2013        43   1016      187\n3  2013        -2   2190     1089\n4  2013        -1     91     2576\n5  2013         5   2131      502\n\n\n\n combined with aggregation (group_by()) variants of slice_ may be used to perform windowing operations\n Useful variant slice_sample()"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-5",
    "href": "core/slides/slides-dplyr.html#section-5",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Note\n\n\n\\(\\bowtie(R,S, {\\text{condition}})\\)\nstands for\n\njoin rows/tuples of \\(R\\) and rows/tuples of \\(S\\) that satisfy \\(\\text{condition}\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#nycflights-tables",
    "href": "core/slides/slides-dplyr.html#nycflights-tables",
    "title": "Tables manipulation with dplyr",
    "section": "\nnycflights tables",
    "text": "nycflights tables\n\n\nThe nycflights13 package offers five related tables:\n\n\nFact tables:\n\nflights\n\nweather (hourly weather conditions at different locations)\n\n\n\nDimension tables:\n\n\nairports (airports full names, location, …)\n\nplanes (model, manufacturer, year, …)\n\nairlines (full names)\n\n\n\nThis is an instance of a Star Schema"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#about-star-schemas",
    "href": "core/slides/slides-dplyr.html#about-star-schemas",
    "title": "Tables manipulation with dplyr",
    "section": "About Star schemas\n",
    "text": "About Star schemas\n\n\nFact tables record measurements for a specific event\nFact tables generally consist of numeric values, and foreign keys to dimensional data where descriptive information is kept\nDimension tables record informations about entities involved in events recorded in Fact tables\n\n\n\nFrom Wikipedia]"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#weather-conditions",
    "href": "core/slides/slides-dplyr.html#weather-conditions",
    "title": "Tables manipulation with dplyr",
    "section": "\n weather conditions",
    "text": "weather conditions\n\n\nweather |&gt;\n  glimpse(width = 50)\n\n\nRows: 26,115\nColumns: 15\n$ origin     &lt;chr&gt; \"EWR\", \"EWR\", \"EWR\", \"EWR\", \"…\n$ year       &lt;int&gt; 2013, 2013, 2013, 2013, 2013,…\n$ month      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ day        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ hour       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10…\n$ temp       &lt;dbl&gt; 39.02, 39.02, 39.02, 39.92, 3…\n$ dewp       &lt;dbl&gt; 26.06, 26.96, 28.04, 28.04, 2…\n$ humid      &lt;dbl&gt; 59.37, 61.63, 64.43, 62.21, 6…\n$ wind_dir   &lt;dbl&gt; 270, 250, 240, 250, 260, 240,…\n$ wind_speed &lt;dbl&gt; 10.35702, 8.05546, 11.50780, …\n$ wind_gust  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, N…\n$ precip     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pressure   &lt;dbl&gt; 1012.0, 1012.3, 1012.5, 1012.…\n$ visib      &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 1…\n$ time_hour  &lt;dttm&gt; 2013-01-01 01:00:00, 2013-01…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#connecting-flights-and-weather",
    "href": "core/slides/slides-dplyr.html#connecting-flights-and-weather",
    "title": "Tables manipulation with dplyr",
    "section": "Connecting flights and weather\n",
    "text": "Connecting flights and weather\n\nWe want to complement information in flights using data weather\nMotivation: we would like to relate delays (arr_delay) and weather conditions\n\ncan we explain (justify) delays using weather data?\ncan we predict delays using weather data?"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-6",
    "href": "core/slides/slides-dplyr.html#section-6",
    "title": "Tables manipulation with dplyr",
    "section": "\n ⋈ \n",
    "text": "⋈ \n\nFor each flight (row in flights)\n\nyear, month, day, hour (computed from time_hour) indicate the approaximate time of departure\norigin indicates the airport where the plane takes off\n\nEach row of weather contains corresponding information\n for each row of flights we look for rows of weather with matching values in year, month, day, hour and origin\n NATURAL INNER JOIN between the tables"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#inner_join-natural-join",
    "href": "core/slides/slides-dplyr.html#inner_join-natural-join",
    "title": "Tables manipulation with dplyr",
    "section": "\ninner_join: natural join",
    "text": "inner_join: natural join\n\n\nf_w &lt;- flights |&gt;\n  inner_join(weather) #&lt;&lt;\n\nf_w |&gt; \n  select(seq(1, \n             ncol(f_w),\n             by=2)) |&gt; \n  glimpse(width=50)\n\n\nRows: 335,220\nColumns: 14\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12,…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 46…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 201…\n$ dewp           &lt;dbl&gt; 28.04, 24.98, 26.96, 26.9…\n$ wind_dir       &lt;dbl&gt; 260, 250, 260, 260, 260, …\n$ wind_gust      &lt;dbl&gt; NA, 21.86482, NA, NA, 23.…\n$ pressure       &lt;dbl&gt; 1011.9, 1011.4, 1012.1, 1…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#join-schema",
    "href": "core/slides/slides-dplyr.html#join-schema",
    "title": "Tables manipulation with dplyr",
    "section": "Join schema",
    "text": "Join schema\n\n\nRows: 335,220\nColumns: 28\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12,…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 46…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, …\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 7…\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0,…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 201…\n$ temp           &lt;dbl&gt; 39.02, 39.92, 39.02, 39.0…\n$ dewp           &lt;dbl&gt; 28.04, 24.98, 26.96, 26.9…\n$ humid          &lt;dbl&gt; 64.43, 54.81, 61.63, 61.6…\n$ wind_dir       &lt;dbl&gt; 260, 250, 260, 260, 260, …\n$ wind_speed     &lt;dbl&gt; 12.65858, 14.96014, 14.96…\n$ wind_gust      &lt;dbl&gt; NA, 21.86482, NA, NA, 23.…\n$ precip         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pressure       &lt;dbl&gt; 1011.9, 1011.4, 1012.1, 1…\n$ visib          &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 1…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-7",
    "href": "core/slides/slides-dplyr.html#section-7",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "The schema of the result is the union of the schemas of the operands\nA tuple from flights matches a tuple from weather if the tuple have the same values in the common columns:\n\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\"      \"temp\"          \n[21] \"dewp\"           \"humid\"          \"wind_dir\"       \"wind_speed\"    \n[25] \"wind_gust\"      \"precip\"         \"pressure\"       \"visib\""
  },
  {
    "objectID": "core/slides/slides-dplyr.html#which-columns-are-used-when-joining-tables-r-and-s",
    "href": "core/slides/slides-dplyr.html#which-columns-are-used-when-joining-tables-r-and-s",
    "title": "Tables manipulation with dplyr",
    "section": "Which columns are used when joining tables \\(R\\) and \\(S\\)?",
    "text": "Which columns are used when joining tables \\(R\\) and \\(S\\)?\n\ndefault behavior of inner_join: all columns shared by \\(R\\) and \\(S\\). Common columns have the same name in both schema. They are expected to have the same class\nmanual definition: in many settings, we want to overrule the default behavior. We specify manually which column from \\(R\\) should match which column from \\(S\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#natural-join-of-flights-and-weather",
    "href": "core/slides/slides-dplyr.html#natural-join-of-flights-and-weather",
    "title": "Tables manipulation with dplyr",
    "section": "Natural join of flights and weather:",
    "text": "Natural join of flights and weather:\n\ncommon_names &lt;- base::intersect(names(weather),\n                                names(flights))\n\nsetequal(\n  inner_join(flights, weather),\n  inner_join(flights,\n             weather,\n             by=common_names)\n)\n\n[1] TRUE"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#are-you-surprised-by-the-next-chunk",
    "href": "core/slides/slides-dplyr.html#are-you-surprised-by-the-next-chunk",
    "title": "Tables manipulation with dplyr",
    "section": "\n Are you surprised by the next chunk?",
    "text": "Are you surprised by the next chunk?\n\ndtu  &lt;- inner_join(flights,\n           weather,\n           by=c(\"year\", \"month\", \"day\", \"origin\", \"hour\"))\n\ndtv &lt;- inner_join(flights,\n           weather,\n           by=c(\"origin\", \"time_hour\"))\n\n# setequal(dtu, dtv)\n\nRecall that columns year, month day hour can be computed from time_hour\n\nflights |&gt;\n  filter(year!=year(time_hour) |\n         month!=month(time_hour) |\n         day!=day(time_hour) |\n         hour!=hour(time_hour)) |&gt;\n  nrow()\n\n[1] 0"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#the-two-results-do-not-have-the-same-schema",
    "href": "core/slides/slides-dplyr.html#the-two-results-do-not-have-the-same-schema",
    "title": "Tables manipulation with dplyr",
    "section": "The two results do not have the same schema!",
    "text": "The two results do not have the same schema!\n\nsetdiff(colnames(dtv), colnames(dtu))\n\n[1] \"year.x\"    \"month.x\"   \"day.x\"     \"hour.x\"    \"time_hour\" \"year.y\"   \n[7] \"month.y\"   \"day.y\"     \"hour.y\"   \n\nsetdiff(colnames(dtu), colnames(dtv))\n\n[1] \"year\"        \"month\"       \"day\"         \"hour\"        \"time_hour.x\"\n[6] \"time_hour.y\""
  },
  {
    "objectID": "core/slides/slides-dplyr.html#fixing",
    "href": "core/slides/slides-dplyr.html#fixing",
    "title": "Tables manipulation with dplyr",
    "section": "Fixing",
    "text": "Fixing\n\ndtu  &lt;- inner_join(flights,\n           weather,\n           by=c(\"year\", \"month\", \"day\", \"origin\", \"hour\"),\n           suffix= c(\"\", \".y\")) |&gt;  #&lt;&lt;\n           select(-ends_with(\".y\"))  #&lt;&lt;\n\ndtv &lt;- inner_join(flights,\n           weather,\n           by=c(\"origin\", \"time_hour\"),\n           suffix= c(\"\", \".y\")) |&gt;  #&lt;&lt;\n           select(-ends_with(\".y\"))  #&lt;&lt;\n\nsetequal(dtu, dtv)\n\n[1] TRUE"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#about-inner_join",
    "href": "core/slides/slides-dplyr.html#about-inner_join",
    "title": "Tables manipulation with dplyr",
    "section": "About inner_join\n",
    "text": "About inner_join\n\n\n\ninner_join(\n  x, y,\n  by = NULL,      #&lt;&lt;\n  copy = FALSE,\n  suffix = c(\".x\", \".y\"), #&lt;&lt;\n  ...,\n  keep = FALSE,  #&lt;&lt;\n  na_matches = \"na\")  #&lt;&lt;\n\n\n\nby:\n\n\nby=c(\"A1\", \"A3\", \"A7\") row r from R and s from S match if r.A1 == s.A1, r.A3 == s.A3, r.A7 == s.A7\n\n\nby=c(\"A1\"=\"B\", \"A3\"=\"C\", \"A7\"=\"D\") row r from R and s from S match if r.A1 == s.B, r.A3 == s.C, r.A7 == s.D\n\n\n\nsuffix: If there are non-joined duplicate variables in x and y, these suffixes will be added to the output to disambiguate them.\nkeep: Should the join keys from both x and y be preserved in the output?\nna_matches: Should NA and NaN values match one another?\n\n\n\n\nFrom online documentation"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#join-flavors",
    "href": "core/slides/slides-dplyr.html#join-flavors",
    "title": "Tables manipulation with dplyr",
    "section": "Join flavors",
    "text": "Join flavors\nDifferent flavors of join can be used to join one table to columns from another, matching values with the rows that they correspond to\nEach join retains a different combination of values from the tables\n\n\nleft_join(x, y, by = NULL, suffix = c(\".x\", \".y\"), ...) Join matching values from y to x. Retain all rows of x padding missing values from y by NA\nsemi_join …\nanti_join …"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-examples-inner_join",
    "href": "core/slides/slides-dplyr.html#toy-examples-inner_join",
    "title": "Tables manipulation with dplyr",
    "section": "Toy examples : inner_join\n",
    "text": "Toy examples : inner_join\n\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\ninner_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD.x\nA2\nA3\nD.y\n\n\n\n4\ne\n2021-10-22\nc\ne\n2021-10-28\nq\n\n\n6\nn\n2021-10-23\ni\na\n2021-11-04\no\n\n\n10\nr\n2021-10-25\ne\nd\n2021-11-18\nd"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-examples-left_join",
    "href": "core/slides/slides-dplyr.html#toy-examples-left_join",
    "title": "Tables manipulation with dplyr",
    "section": "Toy examples : left_join\n",
    "text": "Toy examples : left_join\n\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\nleft_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD.x\nA2\nA3\nD.y\n\n\n\n3\ny\n2021-10-21\no\nNA\nNA\nNA\n\n\n4\ne\n2021-10-22\nc\ne\n2021-10-28\nq\n\n\n6\nn\n2021-10-23\ni\na\n2021-11-04\no\n\n\n9\nt\n2021-10-24\nd\nNA\nNA\nNA\n\n\n10\nr\n2021-10-25\ne\nd\n2021-11-18\nd"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-examples-semi_join-anti_join",
    "href": "core/slides/slides-dplyr.html#toy-examples-semi_join-anti_join",
    "title": "Tables manipulation with dplyr",
    "section": "Toy examples : semi_join anti_join\n",
    "text": "Toy examples : semi_join anti_join\n\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\nsemi_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD\n\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\nanti_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n9\nt\n2021-10-24\nd"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#conditional-theta--join",
    "href": "core/slides/slides-dplyr.html#conditional-theta--join",
    "title": "Tables manipulation with dplyr",
    "section": "Conditional/ \\(\\theta\\) -join",
    "text": "Conditional/ \\(\\theta\\) -join\nIn relational databases, joins are not restricted to natural joins\n\n\\[U \\leftarrow R \\bowtie_{\\theta} S\\]\nreads as\n\\[\\begin{array}{rl} T & \\leftarrow R \\times S\\\\ U & \\leftarrow \\sigma(T, \\theta)\\end{array}\\]\nwhere\n\n\\(R \\times S\\) is the cartesian product of \\(R\\) and \\(S\\)\n\\(\\theta\\) is a boolean expression that can be evaluated on any tuple of \\(R \\times S\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins",
    "href": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins",
    "title": "Tables manipulation with dplyr",
    "section": "Do we need conditional/ \\(\\theta\\) -joins?",
    "text": "Do we need conditional/ \\(\\theta\\) -joins?\n\n\n\n\n\n\nNote\n\n\n: We can implement \\(\\theta\\)/conditional-joins by pipelining a cross product and a filtering\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n: Cross products are costly:\n\n\\(\\#\\text{rows}(R \\times S) = \\#\\text{rows}(R) \\times \\#\\text{rows}(S)\\)\n\\(\\#\\text{cols}(R \\times S) = \\#\\text{cols}(R) + \\#\\text{cols}(S)\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins-1",
    "href": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins-1",
    "title": "Tables manipulation with dplyr",
    "section": "Do we need conditional/ \\(\\theta\\) -joins?",
    "text": "Do we need conditional/ \\(\\theta\\) -joins?\n\n\n\n\n\n\nNote\n\n\n: RDBMS use query planning and optimization, indexing to circumvent the cross product bottleneck (when possible)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n: if we need to perform a \\(\\theta\\)-join\n\noutsource it to a RDBMS, or\ndesign an ad hoc pipeline\n\n\n\n\n\n\n\nAbout conditional join"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#a-conditional-join-between-flights-and-weather",
    "href": "core/slides/slides-dplyr.html#a-conditional-join-between-flights-and-weather",
    "title": "Tables manipulation with dplyr",
    "section": "A conditional join between flights and weather\n",
    "text": "A conditional join between flights and weather\n\n\nThe natural join between flights and weather we implemented can be regarded as an ad hoc conditional join between normalized versions of weather and flights \nTable flights and weather are redundant: year, month, day, hour can be computed from time_hour\nAssume flights and weather are trimmed so as to become irredundant\nThe conditional join is then based on truncations of variables time_hour\n\nSELECT \n  *\nFROM \n  flights AS f, weather AS w\nWHERE \n  date_trunc('hour', f.time_hour) = date_trunc('hour', w.time_hour)\n\nAdding redundant columns to flights and weather allows us to transform a tricky conditional join into a simple natural join \n\n\n\n\nPostgreSQL documentation"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-8",
    "href": "core/slides/slides-dplyr.html#section-8",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Creation of new columns may happen\n\non the fly\nwhen altering (enriching) the schema of a table\n\nIn databases, creation of new columns may be the result of a query or be the result of altering a table schema with ALTER TABLE ADD COLUMN ...\nIn tidyverse() we use verbs mutate or add_column to add columns to the input table"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#mutate",
    "href": "core/slides/slides-dplyr.html#mutate",
    "title": "Tables manipulation with dplyr",
    "section": "mutate",
    "text": "mutate\n\n\nmutate(   #&lt;&lt;\n  .data,\n  new_col= expression, #&lt;&lt;\n  ...,   #&lt;&lt;\n  .keep = c(\"all\", \"used\", \"unused\", \"none\"),\n  .before = NULL,\n  .after = NULL\n)\n\n.data: the input data frame\nnew_col= expression:\n\nnew_col is the name of a new column\nexpression is evaluated on each row of .data or it is a vector of length 1\nall is the default behavior, retains all columns from .data"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#creating-a-categorical-column-to-spot-large-delays",
    "href": "core/slides/slides-dplyr.html#creating-a-categorical-column-to-spot-large-delays",
    "title": "Tables manipulation with dplyr",
    "section": "Creating a categorical column to spot large delays",
    "text": "Creating a categorical column to spot large delays\n\n\nbreaks_delay &lt;- with(flights,\n  c(min(arr_delay, na.rm=TRUE),\n    0, 30,\n    max(arr_delay, na.rm=TRUE))\n)\n\nlevel_delay &lt;- c(\"None\",\n                 \"Moderate\",\n                 \"Large\")\n\nflights |&gt;\n  mutate(large_delay = cut(\n    arr_delay,  #&lt;&lt;\n    breaks=breaks_delay, #&lt;&lt;\n    labels=level_delay,  #&lt;&lt;\n    ordered_result=TRUE)) |&gt;   #&lt;&lt;\n  select(large_delay, arr_delay) |&gt;\n  sample_n(5)\n\n\n# A tibble: 5 × 2\n  large_delay arr_delay\n  &lt;ord&gt;           &lt;dbl&gt;\n1 Large             219\n2 Moderate           18\n3 None              -19\n4 None              -16\n5 None               -1"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-9",
    "href": "core/slides/slides-dplyr.html#section-9",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "flights |&gt;\n  mutate(foo = if_else(arr_time &gt; sched_arr_time,        #&lt;&lt;\n                              arr_time - sched_arr_time,\n                              0L,\n                              missing = NA_integer_)) |&gt;\n  group_by( (foo &gt;0) & abs(foo - arr_delay)  &gt; 100) |&gt;\n  summarise(N=n())\n\n\n# A tibble: 3 × 2\n  `(foo &gt; 0) & abs(foo - arr_delay) &gt; 100`      N\n  &lt;lgl&gt;                                     &lt;int&gt;\n1 FALSE                                    322281\n2 TRUE                                       5157\n3 NA                                         9338"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#changing-the-class-of-a-column",
    "href": "core/slides/slides-dplyr.html#changing-the-class-of-a-column",
    "title": "Tables manipulation with dplyr",
    "section": "Changing the class of a column",
    "text": "Changing the class of a column\n\n\nflights |&gt;\n  mutate(large_delay = cut(arr_delay,  #&lt;&lt;\n    breaks=breaks_delay,\n    labels=level_delay,\n    ordered_result=TRUE),\n    origin = as.factor(origin), #&lt;&lt;\n    dest = as.factor(dest)    #&lt;&lt;\n  ) |&gt;\n  select(\n    large_delay,\n    arr_delay,\n    origin,\n    dest) |&gt;\n  sample_n(5)\n\n\n# A tibble: 5 × 4\n  large_delay arr_delay origin dest \n  &lt;ord&gt;           &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;\n1 None              -44 LGA    CVG  \n2 None              -15 EWR    DAY  \n3 Large             136 EWR    DEN  \n4 None               -9 EWR    TPA  \n5 Moderate           14 LGA    TPA"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tidying-tables-is-part-of-data-cleaning",
    "href": "core/slides/slides-dplyr.html#tidying-tables-is-part-of-data-cleaning",
    "title": "Tables manipulation with dplyr",
    "section": "Tidying tables is part of data cleaning",
    "text": "Tidying tables is part of data cleaning\n\nA (tidy) dataset is a collection of values, usually either numbers (if quantitative) or strings (if qualitative)\n\n\nValues are organised in two ways\n\n\nEvery value belongs to a variable and an observation\n\n\nA variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units\n\n\nAn observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes\n\n\nThe principles of tidy data are tied to those of relational databases and Codd’s relational algebra\n\n\n\n The tidy data paper"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#codds-principles",
    "href": "core/slides/slides-dplyr.html#codds-principles",
    "title": "Tables manipulation with dplyr",
    "section": "Codd’s principles",
    "text": "Codd’s principles\n\nInformation is represented logically in tables\n\nData must be logically accessible by table, primary key, and column.\n\nNull values must be uniformly treated as “missing information,” not as empty strings, blanks, or zeros.\nMetadata (data about the database) must be stored in the database just as regular data is\nA single language must be able to define data, views, integrity constraints, authorization, transactions, and data manipulation\n\nViews must show the updates of their base tables and vice versa\nA single operation must be available to do each of the following operations: retrieve data, insert data, update data, or delete data\nBatch and end-user operations are logically separate from physical storage and access methods\nBatch and end-user operations can change the database schema without having to recreate it or the applications built upon it\n\nIntegrity constraints must be available and stored in the metadata, not in an application program\nThe data manipulation language of the relational system should not care where or how the physical data is distributed and should not require alteration if the physical data is centralized or distributed\nAny row processing done in the system must obey the same integrity rules and constraints that set-processing operations do"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-10",
    "href": "core/slides/slides-dplyr.html#section-10",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "dplyr functions expect and return tidy tables\nIn a tidy table\n\nEach variable is a column\nEach observation is a row\nEvery cell is a single value\n\n\n\n The tidy data paper]"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-11",
    "href": "core/slides/slides-dplyr.html#section-11",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "In order to tell whether a table is tidy, we need to know what is the population under investigation, what are the observations/individuals, which measures are performed on each individual, …"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#untidy-data",
    "href": "core/slides/slides-dplyr.html#untidy-data",
    "title": "Tables manipulation with dplyr",
    "section": "Untidy data",
    "text": "Untidy data\n\nColumn headers are values, not variable names.\n\n\nMultiple variables are stored in one column.\n\n\nVariables are stored in both rows and columns.\n\n\nMultiple types of observational units are stored in the same table.\n\n\nA single observational unit is stored in multiple tables.\n\n\n…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#functions-from-tidyr...",
    "href": "core/slides/slides-dplyr.html#functions-from-tidyr...",
    "title": "Tables manipulation with dplyr",
    "section": "Functions from tidyr::...\n",
    "text": "Functions from tidyr::...\n\n\npivot_wider and pivot_longer\nseparate and unite\nHandling missing values with complete, fill, …\n…\n\ntidyr website"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#pivot-longer",
    "href": "core/slides/slides-dplyr.html#pivot-longer",
    "title": "Tables manipulation with dplyr",
    "section": "Pivot longer",
    "text": "Pivot longer\n\n\npivot_longer() is commonly needed to tidy wild-caught datasets as they often optimise for ease of data entry or ease of comparison rather than ease of analysis.\n\n\n\nmessy &lt;- tibble::tribble(\n  ~row, ~a, ~b, ~c,\n  \"A\", 1, 4, 7,\n  \"B\", 2, 5, 8,\n  \"C\", 3, 6, 9,\n)\nmessy |&gt; kable()\n\n\n\n\nrow\na\nb\nc\n\n\n\nA\n1\n4\n7\n\n\nB\n2\n5\n8\n\n\nC\n3\n6\n9\n\n\n\n\n\n\n\nmessy_long &lt;- messy |&gt; \n  pivot_longer(\n    cols=c(-row),  #&lt;&lt;\n    names_to = \"name\",\n    values_to = \"value\")\n  \nmessy_long  |&gt; \n  kable()\n\n\n\n\nrow\nname\nvalue\n\n\n\nA\na\n1\n\n\nA\nb\n4\n\n\nA\nc\n7\n\n\nB\na\n2\n\n\nB\nb\n5\n\n\nB\nc\n8\n\n\nC\na\n3\n\n\nC\nb\n6\n\n\nC\nc\n9\n\n\n\n\n\n\n\n\n\npivot_longer() makes datasets longer by increasing the number of rows and decreasing the number of columns. I don’t believe it makes sense to describe a dataset as being in “long form”. Length is a relative term, and you can only say (e.g.) that dataset A is longer than dataset B."
  },
  {
    "objectID": "core/slides/slides-dplyr.html#pivot-wider",
    "href": "core/slides/slides-dplyr.html#pivot-wider",
    "title": "Tables manipulation with dplyr",
    "section": "Pivot wider",
    "text": "Pivot wider\n\n\npivot_wider(  #&lt;&lt;\n  data,\n  id_cols = NULL, #&lt;&lt;\n  names_from = name, #&lt;&lt;\n  names_prefix = \"\",\n  values_from = value, #&lt;&lt;\n  ...\n)\n some optional arguments are missing\n\nWhen reporting, we often use pivot_wider (explicitely or implicitely) to make results more readable, possibly to conform to a tradition\n\nLife tables in demography and actuarial science\nLongitudinal data\nSee slide How many flights per day of week per departure airport?"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#pivot_wider-in-action",
    "href": "core/slides/slides-dplyr.html#pivot_wider-in-action",
    "title": "Tables manipulation with dplyr",
    "section": "\npivot_wider() in action",
    "text": "pivot_wider() in action\n\n\nmessy_long |&gt;\n  pivot_wider( \n  id_cols = c(\"row\"), #&lt;&lt;\n  names_from = name, #&lt;&lt;\n  names_prefix = \"\",\n  values_from = value\n)\n\n\n# A tibble: 3 × 4\n  row       a     b     c\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         1     4     7\n2 B         2     5     8\n3 C         3     6     9"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#how-many-flights-per-carrier",
    "href": "core/slides/slides-dplyr.html#how-many-flights-per-carrier",
    "title": "Tables manipulation with dplyr",
    "section": "How many flights per carrier?",
    "text": "How many flights per carrier?\n\n\nflights |&gt;\n  group_by(carrier) |&gt;  #&lt;&lt;\n  summarise(count=n()) |&gt;  #&lt;&lt;\n  arrange(desc(count))\n\n\n# A tibble: 16 × 2\n   carrier count\n   &lt;chr&gt;   &lt;int&gt;\n 1 UA      58665\n 2 B6      54635\n 3 EV      54173\n 4 DL      48110\n 5 AA      32729\n 6 MQ      26397\n 7 US      20536\n 8 9E      18460\n 9 WN      12275\n10 VX       5162\n11 FL       3260\n12 AS        714\n13 F9        685\n14 YV        601\n15 HA        342\n16 OO         32\n\n\nSELECT \n  carrier, COUNT(*) AS n\nFROM \n  flights\nGROUP BY \n  carrier\nORDER BY \n  n DESCENDING"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#how-many-flights-per-day-of-week-per-departure-airport",
    "href": "core/slides/slides-dplyr.html#how-many-flights-per-day-of-week-per-departure-airport",
    "title": "Tables manipulation with dplyr",
    "section": "How many flights per day of week per departure airport?",
    "text": "How many flights per day of week per departure airport?\n\n\nflights |&gt;\n  group_by(origin,  wday(time_hour, abbr=T, label=T)) |&gt;  #&lt;&lt;\n  summarise(count=n(), .groups=\"drop\") |&gt;       #&lt;&lt;\n  rename(day_of_week=`wday(time_hour, abbr = T, label = T)`) |&gt;\n  pivot_wider(  #&lt;&lt;\n    id_cols=\"origin\",   #&lt;&lt;\n    names_from=\"day_of_week\", #&lt;&lt;\n    values_from=\"count\") |&gt;  #&lt;&lt;\n  kable(caption=\"Departures per day\")\n\n\n\nDepartures per day\n\norigin\ndim.\nlun.\nmar.\nmer.\njeu.\nven.\nsam.\n\n\n\nEWR\n16425\n18329\n18243\n18180\n18169\n18142\n13347\n\n\nJFK\n15966\n16104\n16017\n15841\n16087\n16176\n15088\n\n\nLGA\n13966\n16257\n16162\n16039\n15963\n15990\n10285"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#window-queries-1",
    "href": "core/slides/slides-dplyr.html#window-queries-1",
    "title": "Tables manipulation with dplyr",
    "section": "Window queries",
    "text": "Window queries\nAssume we want to answer the question: for each day of week (Monday, Tuesday, …), what are the five carriers that experience the largest average delay?\n\n\nflights |&gt;\n  group_by(weekdays(time_hour), carrier) |&gt;\n  summarise(avg_dep_delay=mean(dep_delay, na.rm=T)) |&gt;\n  slice_max(n=2, order_by=avg_dep_delay)\n\n\n# A tibble: 14 × 3\n# Groups:   weekdays(time_hour) [7]\n   `weekdays(time_hour)` carrier avg_dep_delay\n   &lt;chr&gt;                 &lt;chr&gt;           &lt;dbl&gt;\n 1 dimanche              F9               23.7\n 2 dimanche              VX               17.4\n 3 jeudi                 YV               29.7\n 4 jeudi                 F9               26.5\n 5 lundi                 FL               24.8\n 6 lundi                 EV               23.4\n 7 mardi                 YV               19.1\n 8 mardi                 FL               17.7\n 9 mercredi              OO               52  \n10 mercredi              HA               24.5\n11 samedi                OO               41  \n12 samedi                F9               15.8\n13 vendredi              OO               29  \n14 vendredi              F9               25.6"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#the-sql-way",
    "href": "core/slides/slides-dplyr.html#the-sql-way",
    "title": "Tables manipulation with dplyr",
    "section": "The SQL way",
    "text": "The SQL way\nWITH R AS (\n  SELECT \n    EXTRACT(dow FROM time_hour) AS day_of_week,\n    carrier,\n    AVG(dep_delay) AS avg_dep_delay\n  FROM \n    flights\n  GROUP BY \n    EXTRACT(dow FROM time_hour), carrier\n), S AS (\n  SELECT \n    day_of_week,\n    carrier,\n    rank() OVER (PARTITION by day_of_week ORDER BY avg_dep_delay DESC) AS rnk\n  FROM \n    R\n)\n\nSELECT \n  day_of_week, \n  carrier, \n  rnk\nFROM \n  S\nWHERE \n  rnk &lt;= 10 ;"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#sliding-windows-and-package-slider",
    "href": "core/slides/slides-dplyr.html#sliding-windows-and-package-slider",
    "title": "Tables manipulation with dplyr",
    "section": "Sliding windows and package slider\n",
    "text": "Sliding windows and package slider\n\nTODO"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#and-other-pipes",
    "href": "core/slides/slides-dplyr.html#and-other-pipes",
    "title": "Tables manipulation with dplyr",
    "section": "\n|>, %>% and other pipes",
    "text": "|&gt;, %&gt;% and other pipes\n\nAll dplyr functions take a table as the first argument\nRather than forcing the user to either save intermediate objects or nest functions, dplyr provides the |&gt; operator from magrittr\nx |&gt; f(y) turns into f(x, y)\nThe result from one step is piped into the next step\nUse |&gt; to rewrite multiple operations that you can read left-to-right/top-to-bottom\n\n\ng(f(x, y), z)\n\nx |&gt;\n  f(y) |&gt;\n  g(z)\n\n\n\nFrom dplyr vignette"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#magrittr",
    "href": "core/slides/slides-dplyr.html#magrittr",
    "title": "Tables manipulation with dplyr",
    "section": "Magrittr %>%\n",
    "text": "Magrittr %&gt;%\n\n\n\n\n\n%&gt;% is not tied to dplyr\n\n\n%&gt;% can be used with packages from tidyverse\n\n\n%&gt;% can be used outside tidyverse that is with functions which take a table (or something else) as a second, third or keyword argument\n\n Use pronoun . to denote the LHS of the pipe expression\n\nSecond argument of g has the same type as the result of f\n\ng(z, f(x, y))\n\nx %&gt;%\n  f(y) %&gt;%\n  g(z, .)   #&lt;&lt;\n\nx %&gt;% f(y) is a shorthand for x %&gt;% f(., y)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#standard-pipe-version-4.",
    "href": "core/slides/slides-dplyr.html#standard-pipe-version-4.",
    "title": "Tables manipulation with dplyr",
    "section": "Standard pipe |> (version > 4.)",
    "text": "Standard pipe |&gt; (version &gt; 4.)\nAs of version 4.1 (2021), base  offers a pipe operator denoted by |&gt;\n\n\nx |&gt; f(y) turns into f(x, y)\n\ng(f(x, y), z)\n\nx |&gt;\n  f(y) |&gt;\n  g(z)\n\n\n the standard pipe |&gt; has no pronoun/placeholder to denote the LHS of the pipe expression\nThe roundabout consists in using another new construct \\(x)\ng(z, w)\n\nx |&gt;\n  (\\(x) g(z, w=x))()\n\n\"une\" |&gt;\n  (\\(x) str_c(\"ceci n'est pas\", x, sep=\" \"))() |&gt;\n  str_c(\"pipe\", sep=\" \") |&gt;\n  cat()\n\nceci n'est pas une pipe\n\n\n\n\n\nSee Blog on the new standard pipe]"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#other-pipes",
    "href": "core/slides/slides-dplyr.html#other-pipes",
    "title": "Tables manipulation with dplyr",
    "section": "Other pipes",
    "text": "Other pipes\nMagrittr offers several variants of |&gt;\n\nTee operator %T&gt;%\n\nAssignement pipe %&lt;&gt;%\n\nExposition operator %$%\n\n…\n\nSee pipes for beginners\n Base  has a pipe() function to manipulate connections (Files, URLs, …)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#references",
    "href": "core/slides/slides-dplyr.html#references",
    "title": "Tables manipulation with dplyr",
    "section": "\n References",
    "text": "References\n\n\nR for Data Science\n\nData transformation\n\n\nRstudio cheat sheets\n\ndplyr\ntidyr\ndatatable\nreadr"
  },
  {
    "objectID": "labs-listings.html",
    "href": "labs-listings.html",
    "title": "Labs",
    "section": "",
    "text": "Note\n\n\n\nSessions are organized around labs. Feel free to look at the lab before sessions. Do not rush to solutions proposed here\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 15, 2025\n\n\nIntroduction and Visualization\n\n\nVisualization, Public Statistics\n\n\n\n\nJan 15, 2025\n\n\nBrush up your R\n\n\nR language, Tidyverse, IDE\n\n\n\n\nJan 22, 2025\n\n\nTable wranglig\n\n\nR language, dplyr, tabula data\n\n\n\n\nJan 29, 2025\n\n\nUnivariate categorical data\n\n\nUnivariate data, GSS\n\n\n\n\nJan 30, 2025\n\n\nUnivariate numeric data\n\n\nUnivariate data, GSS\n\n\n\n\nFeb 5, 2025\n\n\nBivariate data\n\n\nbivariate data, mosaicplots, scatterplots, simple linear regression\n\n\n\n\nFeb 19, 2025\n\n\nLinear Regression I\n\n\nLinear regression, OLS, lm\n\n\n\n\nMar 5, 2025\n\n\nLinear regression II\n\n\nLinear regression, OLS, lm, diagnostics, variable selection\n\n\n\n\nMar 12, 2025\n\n\nSVD and PCA\n\n\nSVD, PCA\n\n\n\n\nMar 19, 2025\n\n\nCorrespondence Analysis\n\n\nSVD, CA, Correspondence Analysis\n\n\n\n\nMar 26, 2025\n\n\nSVD, MCA, CCA\n\n\nSVD, PCA, CA, MCA, CCA\n\n\n\n\nApr 2, 2025\n\n\nClustering: kmeans\n\n\nClustering, k-means\n\n\n\n\nApr 7, 2025\n\n\nClustering: Hierarchical\n\n\nClustering, Hierarchical\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nTip\n\n\n\nBefore working out a lab, make sure the relevant packages are installed in your environment.",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "labs-solutions-listings.html",
    "href": "labs-solutions-listings.html",
    "title": "Labs Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 15, 2025\n\n\nIntroduction and Visualization\n\n\nVisualization, Public Statistics\n\n\n\n\nJan 15, 2025\n\n\nBrush up your R\n\n\nR language, Tidyverse, IDE\n\n\n\n\nJan 22, 2025\n\n\nTable wranglig\n\n\nR language, dplyr, tabula data\n\n\n\n\nJan 29, 2025\n\n\nUnivariate categorical data\n\n\nUnivariate data, GSS\n\n\n\n\nJan 30, 2025\n\n\nUnivariate numeric data\n\n\nUnivariate data, GSS\n\n\n\n\nFeb 5, 2025\n\n\nBivariate data\n\n\nbivariate data, mosaicplots, scatterplots, simple linear regression\n\n\n\n\nFeb 19, 2025\n\n\nLinear regression I\n\n\nLinear regression, OLS, lm\n\n\n\n\nMar 5, 2025\n\n\nLinear regression II\n\n\nLinear regression, OLS, lm, diagnostics, variable selection\n\n\n\n\nMar 12, 2025\n\n\nSVD and PCA\n\n\nSVD, PCA\n\n\n\n\nMar 19, 2025\n\n\nCorrespondence Analysis\n\n\nSVD, CA, Correspondence Analysis\n\n\n\n\nMar 26, 2025\n\n\nSVD, MCA, CCA\n\n\nSVD, PCA, CA, MCA, CCA\n\n\n\n\nApr 2, 2025\n\n\nClustering: kmeans\n\n\nClustering, k-means\n\n\n\n\nApr 7, 2025\n\n\nClustering: Hierarchical\n\n\nClustering, Hierarchical\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Solutions"
    ]
  },
  {
    "objectID": "projects-listings.html",
    "href": "projects-listings.html",
    "title": "Projects",
    "section": "",
    "text": "Note\n\n\n\nCourse evaluation is based on Projects \n\n Find a friend : all work done by pairs of students\n Create a single private GitHub repository for each project and each pair of students.\n Grant me access to these repositories\n All work is transmitted through your private repository and nowhere else\n No emails for project submission\n All projects deliverables consist of Quarto notebooks\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Description\n        \n         \n          Tags\n        \n         \n          Due date - Oldest\n        \n         \n          Due date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDue date\n\n\nTitle\n\n\nDescription\n\n\nTags\n\n\n\n\n\n\nFeb 7, 2025\n\n\nData wrangling and Visualization\n\n\nData extraction, wrangling, visualization, reproducible data science\n\n\nOECD, Visualization, Quarto, tidyverse, plotly\n\n\n\n\nApr 27, 2025\n\n\nRegression, Package Development\n\n\nStatistical summaries, Regression, Package, Generics, isotone\n\n\nRegression, Diagnostics, Selection, Quarto, tidyverse, isotone, tidymodels\n\n\n\n\nMay 28, 2025\n\n\nSVD methods and Voting data\n\n\nSVD, Clustering,\n\n\nPCA, Clustering, Quarto\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nEvaluation guidelines\n\n\n\nGrading criteria are given per project. They involve the following aspects:\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nNarrative, spelling and syntax\n\nEnglish/French \n\n\nPlots correction\n\nchoice of aesthetics, geom, scale … \n\n\nPlots style\n\nTitles, legends, labels, breaks … \n\n\nTable wrangling\n\nETL, SQL like manipulations \n\n\nComputing Statistics\n\nAggregations, LR, PCA, CA, … \n\n\nDRY compliance\n\nDRY principle at  Wikipedia\n\n\nReport organization\n\n\n\n\nCode organization",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "slides-listings.html",
    "href": "slides-listings.html",
    "title": "Slides",
    "section": "",
    "text": "Slides summarize the lectures. Feel free to watch them before and after the lectures.\n point to material to be developped on blackboard.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Tags\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 22, 2025\n\n\nHandling tables with dplyr\n\n\nSQL Tables, Relational Algebra, tidyverse\n\n\n\n\nApr 2, 2025\n\n\nClustering kmeans\n\n\nkmeans, clustering\n\n\n\n\nApr 4, 2025\n\n\nHierarchical clustering\n\n\nDendrogram, Hierarchical clustering\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nMode d’emploi\n\n\n\nSlides use libraries revealjs or remark from . They are displayed in your browser.\nTo get help, press",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "weeks-listings.html",
    "href": "weeks-listings.html",
    "title": "Journal",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 15, 2025\n\n\nWeek 1\n\n\nVisualization, Public Statistics, R language, Tidyverse\n\n\n\n\nJan 22, 2025\n\n\nWeek 2\n\n\nR language, Tidyverse, Table manipulations\n\n\n\n\nJan 29, 2025\n\n\nWeek 3\n\n\nUnivariate data\n\n\n\n\nFeb 5, 2025\n\n\nWeek 4\n\n\nBivariate data\n\n\n\n\nFeb 19, 2025\n\n\nWeek 6\n\n\nLinear regression, OLS, lm, Model diagnostics\n\n\n\n\nMar 5, 2025\n\n\nWeek 7\n\n\nLinear regression, OLS, lm, Model diagnostics\n\n\n\n\nMar 12, 2025\n\n\nWeek 8\n\n\nPCA, SVD\n\n\n\n\nMar 19, 2025\n\n\nWeek 9\n\n\nCA, SVD\n\n\n\n\nMar 26, 2025\n\n\nWeek 10\n\n\nCCA, MCA, SVD\n\n\n\n\nApr 2, 2025\n\n\nWeek 11\n\n\nClusterings\n\n\n\n\nApr 9, 2025\n\n\nWeek 12\n\n\nClusterings\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Journal"
    ]
  },
  {
    "objectID": "core/labs/lab-pack.html",
    "href": "core/labs/lab-pack.html",
    "title": "R programming: packages",
    "section": "",
    "text": "This lab is just an incentive to dig into R Packages (2e) by Wickham and Bryan"
  },
  {
    "objectID": "core/labs/lab-pack.html#introduction",
    "href": "core/labs/lab-pack.html#introduction",
    "title": "R programming: packages",
    "section": "Introduction",
    "text": "Introduction\nSharing/Storing/Organizing\nPackages come with conventions\nPackaging can be helpful to perform data analysis."
  },
  {
    "objectID": "core/labs/lab-pack.html#setup",
    "href": "core/labs/lab-pack.html#setup",
    "title": "R programming: packages",
    "section": "Setup",
    "text": "Setup\nAutomating package development is facilitated by a collection of packages.\n\nCodestopifnot(\n  require(\"devtools\"),\n  require(\"usethis\"),\n  require(\"testthat\"),\n  require(\"styler\"),\n  require(\"roxygen2\")\n)\n\n\ndevtools is the cornerstone of the collections\n\nwith rstudio (hands in hands)\nwith vs code\n\nwith positron"
  },
  {
    "objectID": "core/labs/lab-pack.html#package-organization",
    "href": "core/labs/lab-pack.html#package-organization",
    "title": "R programming: packages",
    "section": "Package organization",
    "text": "Package organization\nMake sure you understand the different states of a package : source, bundled, binary, installed, in-memory.\n\nAt first, we deal with a source package. Where is it hosted?\nHow is the source organized?\nWhat are metadata for?\nWhat is the purpose of file .Rbuildignore?\nWhy do we bundle packages?\n\n\nStructure and states of a package\nTo initialize a package development directory.\n\nCodepackpath &lt;- \"hmw2.ma7by020\"\n\nif (!fs::dir_exists(packpath)){\n\n  fs::dir_create(packpath)\n}\n\nusethis::create_package(packpath)\n\n\nWithin directory hmw2.ma7by020, you should have the next organization\nhmw2.ma7by020/\n├── DESCRIPTION\n├── .gitignore\n├── hmw2.ma7by020.Rproj\n├── NAMESPACE\n├── R\n└── .Rbuildignore\n\n2 directories, 5 files\n\n\n\n\n\n\nWhat is the difference between loading and attaching a package?\n\n\n\nYou will have to\n\npopulate the R subdirectory\nupdate DESCRIPTION"
  },
  {
    "objectID": "core/labs/lab-pack.html#reusing-scripts",
    "href": "core/labs/lab-pack.html#reusing-scripts",
    "title": "R programming: packages",
    "section": "Reusing scripts",
    "text": "Reusing scripts"
  },
  {
    "objectID": "core/labs/lab-pack.html#styling",
    "href": "core/labs/lab-pack.html#styling",
    "title": "R programming: packages",
    "section": "Styling",
    "text": "Styling"
  },
  {
    "objectID": "core/labs/lab-pack.html#testing",
    "href": "core/labs/lab-pack.html#testing",
    "title": "R programming: packages",
    "section": "Testing",
    "text": "Testing\nThe workhorse of the development process is\n\nCodedevtools::load_all()\n\n\n\nCode and/or fix bugs\nLoad the code\nTest the code"
  },
  {
    "objectID": "core/labs/lab-pack.html#checking-package-state",
    "href": "core/labs/lab-pack.html#checking-package-state",
    "title": "R programming: packages",
    "section": "Checking Package state",
    "text": "Checking Package state\n\nCodedevtools::check()"
  },
  {
    "objectID": "core/labs/lab-pack.html#namespace-and-dependencies",
    "href": "core/labs/lab-pack.html#namespace-and-dependencies",
    "title": "R programming: packages",
    "section": "\nNAMESPACE and dependencies",
    "text": "NAMESPACE and dependencies"
  },
  {
    "objectID": "core/labs/lab-pack.html#versioning-git",
    "href": "core/labs/lab-pack.html#versioning-git",
    "title": "R programming: packages",
    "section": "Versioning (git)",
    "text": "Versioning (git)"
  },
  {
    "objectID": "core/labs/lab-pack.html#references",
    "href": "core/labs/lab-pack.html#references",
    "title": "R programming: packages",
    "section": "References",
    "text": "References\nWriting R extensions"
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (103A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-9.html#blackboard",
    "href": "weeks/week-9.html#blackboard",
    "title": "Week 9",
    "section": " Blackboard",
    "text": "Blackboard\n\nExtended SVD"
  },
  {
    "objectID": "weeks/week-9.html#labs",
    "href": "weeks/week-9.html#labs",
    "title": "Week 9",
    "section": " Labs",
    "text": "Labs\n\nLab 9 Correspondence Analysis as an application of extended SVD\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\nLab 6 - Linear Regression I\nLab 6 - Linear Regression II\nLab 7 - Linear Regression II\nLab 8 Principal Component Analysis as an application of SVD"
  },
  {
    "objectID": "weeks/week-9.html#further-work",
    "href": "weeks/week-9.html#further-work",
    "title": "Week 9",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-9.html#further-reading",
    "href": "weeks/week-9.html#further-reading",
    "title": "Week 9",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes"
  },
  {
    "objectID": "weeks/week-9.html#logistics",
    "href": "weeks/week-9.html#logistics",
    "title": "Week 9",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (255) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-7.html#blackboard",
    "href": "weeks/week-7.html#blackboard",
    "title": "Week 7",
    "section": " Blackboard",
    "text": "Blackboard\n\nOrdinary Least Squares\nQR factorizatio\nRidge regression (Regularized Least Squares)\nPseudo-Inversion\nLinear regrssion with R"
  },
  {
    "objectID": "weeks/week-7.html#labs",
    "href": "weeks/week-7.html#labs",
    "title": "Week 7",
    "section": " Labs",
    "text": "Labs\n\nLab 6 - Linear Regression I\nLab 7 - Linear Regression II\n –&gt;\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables"
  },
  {
    "objectID": "weeks/week-7.html#further-work",
    "href": "weeks/week-7.html#further-work",
    "title": "Week 7",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-7.html#further-reading",
    "href": "weeks/week-7.html#further-reading",
    "title": "Week 7",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes\n\nCours de Statistique Fondamentale\n\nModèles linéaires\nSherman–Morrison–Woodbury formula\n\nSourav Chatterjee: A new correlation coefficient"
  },
  {
    "objectID": "weeks/week-7.html#logistics",
    "href": "weeks/week-7.html#logistics",
    "title": "Week 7",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (RH04A) Wednesday 13h30-15h\nSession Ibis Sophie Germain (014) Thursday 10h45-12h45\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-3.html#blackboard",
    "href": "weeks/week-3.html#blackboard",
    "title": "Week 3",
    "section": " Blackboard",
    "text": "Blackboard\nWe reviewed the toolkit of univariate analysis\n\nLexicon\nCategorical samples\n\nCounts, Contingency tables, barplots, colplots\n\nNumeric samples\n\nNumerical summaries for location and scale\nCumulative distribution functions and Quantiles functions\nQuantile-quantile plots\nHistograms"
  },
  {
    "objectID": "weeks/week-3.html#labs",
    "href": "weeks/week-3.html#labs",
    "title": "Week 3",
    "section": " Labs",
    "text": "Labs\nWe went (briefly) through the two labs\n\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\n\nWe relied on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr"
  },
  {
    "objectID": "weeks/week-3.html#further-work",
    "href": "weeks/week-3.html#further-work",
    "title": "Week 3",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-3.html#further-reading",
    "href": "weeks/week-3.html#further-reading",
    "title": "Week 3",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\nCours de Statistique Fondamentale\n\nChap Estimation de densité\nChap Test de Kolmogorov-Smirnov"
  },
  {
    "objectID": "weeks/week-3.html#logistics",
    "href": "weeks/week-3.html#logistics",
    "title": "Week 3",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (130) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-11.html#labs",
    "href": "weeks/week-11.html#labs",
    "title": "Week 11",
    "section": " Labs",
    "text": "Labs\n\nLab 11 Clustering with \\(k\\)-means\nLab 12 Hierarchical clustering\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\nLab 6 - Linear Regression I\nLab 6 - Linear Regression II\nLab 7 - Linear Regression II\nLab 8 Principal Component Analysis as an application of SVD\nLab 9 Correspondence Analysis as an application of extended SVD\nLab 10 Multiple Correspondence Analysis and Canonical Correspondence Analysis as applications of extended SVD)"
  },
  {
    "objectID": "weeks/week-11.html#further-work",
    "href": "weeks/week-11.html#further-work",
    "title": "Week 11",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-11.html#further-reading",
    "href": "weeks/week-11.html#further-reading",
    "title": "Week 11",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes"
  },
  {
    "objectID": "weeks/week-11.html#logistics",
    "href": "weeks/week-11.html#logistics",
    "title": "Week 11",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (163) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-1.html#labs",
    "href": "weeks/week-1.html#labs",
    "title": "Week 1",
    "section": " Labs",
    "text": "Labs\nWe worked on the next two labs (solutions )\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\n\nWe used Rstudio, created and initialized a dedicated R project (without git and renv). We installed packages tidyverse, pak and gapminder.\nWe reconstructed the gapminder demo using ggplot2 and plotly.\nWe went through several aspects of R following Intro to R. This is an incentive to install packages lobstr and rlang. They can be very hepful when visualizing the data structures that underpin vectors, matrices, lists, and data frames."
  },
  {
    "objectID": "weeks/week-1.html#further-work",
    "href": "weeks/week-1.html#further-work",
    "title": "Week 1",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-1.html#further-reading",
    "href": "weeks/week-1.html#further-reading",
    "title": "Week 1",
    "section": " Further reading",
    "text": "Further reading\n\nThe ggplot book\nR for data science\n\nData visualization\nTransform\n\nLogical vectors\nNumbers\nFactors\nMissing values\nFunctions\nIteration\nA field guide to base R\n\n\nAdvanced R\n\nNames and values\nVectors\nSubsetting"
  },
  {
    "objectID": "weeks/week-1.html#logistics",
    "href": "weeks/week-1.html#logistics",
    "title": "Week 1",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (103A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\nSession II bis Olympe de Gouges (130) Friday 15h45-17h15\n Calendar"
  },
  {
    "objectID": "weeks/week-10.html#labs",
    "href": "weeks/week-10.html#labs",
    "title": "Week 10",
    "section": " Labs",
    "text": "Labs\n\nLab 10 Multiple Correspondence Analysis as application of extended SVD\nLab 10 bis Canonical Correspondence Analysis\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\nLab 6 - Linear Regression I\nLab 6 - Linear Regression II\nLab 7 - Linear Regression II\nLab 8 Principal Component Analysis as an application of SVD\nLab 9 Correspondence Analysis as an application of extended SVD"
  },
  {
    "objectID": "weeks/week-10.html#further-work",
    "href": "weeks/week-10.html#further-work",
    "title": "Week 10",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-10.html#further-reading",
    "href": "weeks/week-10.html#further-reading",
    "title": "Week 10",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes"
  },
  {
    "objectID": "weeks/week-10.html#logistics",
    "href": "weeks/week-10.html#logistics",
    "title": "Week 10",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (130) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-12.html#labs",
    "href": "weeks/week-12.html#labs",
    "title": "Week 12",
    "section": " Labs",
    "text": "Labs\nWe will work on coding and package development, with following goals\n\ndevelopping methods for tidy, glance, methods for S3 classes FactomineR::CA, FactomineR::MCA.\ninvestigate problems posed by output of cancor\ndevelop autoplot methods of FactomineR::CA, FactomineR::MCA\nspecial emphasis on documentation and testing\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\nLab 6 - Linear Regression I\nLab 6 - Linear Regression II\nLab 7 - Linear Regression II\nLab 8 Principal Component Analysis as an application of SVD\nLab 9 Correspondence Analysis as an application of extended SVD\nLab 10 Multiple Correspondence Analysis and Canonical Correspondence Analysis as applications of extended SVD\nLab 11 Clustering with \\(k\\)-means\nLab 12 Hierarchical clustering"
  },
  {
    "objectID": "weeks/week-12.html#further-work",
    "href": "weeks/week-12.html#further-work",
    "title": "Week 12",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-12.html#further-reading",
    "href": "weeks/week-12.html#further-reading",
    "title": "Week 12",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes"
  },
  {
    "objectID": "weeks/week-12.html#logistics",
    "href": "weeks/week-12.html#logistics",
    "title": "Week 12",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (163) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-2.html#labs",
    "href": "weeks/week-2.html#labs",
    "title": "Week 2",
    "section": " Labs",
    "text": "Labs\nWe revisit the first two labs (solutions )\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\n\nWe use Rstudio, and our dedicated R project (without git and renv). We install packages gt, nycflights and slider.\nWe review our first slide deck:\n\nIntroduction to dplyr\n\nand dig into\n\nLab 3 - Working with dplyr"
  },
  {
    "objectID": "weeks/week-2.html#further-work",
    "href": "weeks/week-2.html#further-work",
    "title": "Week 2",
    "section": " Further work",
    "text": "Further work\n Review the content of the three labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-2.html#further-reading",
    "href": "weeks/week-2.html#further-reading",
    "title": "Week 2",
    "section": " Further reading",
    "text": "Further reading\n\nR for data science\nAdvanced R"
  },
  {
    "objectID": "weeks/week-2.html#logistics",
    "href": "weeks/week-2.html#logistics",
    "title": "Week 2",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (RH10A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-4.html#blackboard",
    "href": "weeks/week-4.html#blackboard",
    "title": "Week 4",
    "section": " Blackboard",
    "text": "Blackboard\n\nCategorical samples\n\n2-ways Contingency tables\nChi-square statistics\n\nNumeric samples\n\nCovariance\nCorrelation(s)"
  },
  {
    "objectID": "weeks/week-4.html#labs",
    "href": "weeks/week-4.html#labs",
    "title": "Week 4",
    "section": " Labs",
    "text": "Labs\n\nLab 6 - Introduction to bivariate analysis\n\nWe relied on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables"
  },
  {
    "objectID": "weeks/week-4.html#further-work",
    "href": "weeks/week-4.html#further-work",
    "title": "Week 4",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-4.html#further-reading",
    "href": "weeks/week-4.html#further-reading",
    "title": "Week 4",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\nCours de Statistique Fondamentale\n\nChap Tests du chi-deux\n\nSourav Chatterjee: A new correlation coefficient"
  },
  {
    "objectID": "weeks/week-4.html#logistics",
    "href": "weeks/week-4.html#logistics",
    "title": "Week 4",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (RH10A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-6.html#blackboard",
    "href": "weeks/week-6.html#blackboard",
    "title": "Week 6",
    "section": " Blackboard",
    "text": "Blackboard\n\nOrdinary Least Squares\nQR factorizatio\nRidge regression (Regularized Least Squares)\nPseudo-Inversion"
  },
  {
    "objectID": "weeks/week-6.html#labs",
    "href": "weeks/week-6.html#labs",
    "title": "Week 6",
    "section": " Labs",
    "text": "Labs\n\nLab 6 - Linear Regression I\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables"
  },
  {
    "objectID": "weeks/week-6.html#further-work",
    "href": "weeks/week-6.html#further-work",
    "title": "Week 6",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-6.html#further-reading",
    "href": "weeks/week-6.html#further-reading",
    "title": "Week 6",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\nCours de Statistique Fondamentale\n\nChap Tests du chi-deux\nModèles linéaires\n\nSourav Chatterjee: A new correlation coefficient"
  },
  {
    "objectID": "weeks/week-6.html#logistics",
    "href": "weeks/week-6.html#logistics",
    "title": "Week 6",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (103A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-8.html#blackboard",
    "href": "weeks/week-8.html#blackboard",
    "title": "Week 8",
    "section": " Blackboard",
    "text": "Blackboard\n\nSVD\n\nDefinition\nExistence\n\nMatrix norms\nSVD and low rank approximations\n\nEckart-Young-Mirsky theorem for operator norm\nEckart-Young-Mirsky theorem for Frobenius-Hilbert-Schmidt norm"
  },
  {
    "objectID": "weeks/week-8.html#labs",
    "href": "weeks/week-8.html#labs",
    "title": "Week 8",
    "section": " Labs",
    "text": "Labs\n\nLab 8 Principal Component Analysis as an application of SVD\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\nLab 6 - Linear Regression I\nLab 6 - Linear Regression II\nLab 7 - Linear Regression II"
  },
  {
    "objectID": "weeks/week-8.html#further-work",
    "href": "weeks/week-8.html#further-work",
    "title": "Week 8",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-8.html#further-reading",
    "href": "weeks/week-8.html#further-reading",
    "title": "Week 8",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes"
  },
  {
    "objectID": "weeks/week-8.html#logistics",
    "href": "weeks/week-8.html#logistics",
    "title": "Week 8",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html",
    "href": "core/labs-solutions/lab-mca.html",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "",
    "text": "Besides the usual packages (tidyverse, …), we shall require FactoMineR and related packages.\nCodestopifnot(\n  require(FactoMineR),\n  require(factoextra),\n  require(FactoInvestigate)\n)"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#the-gss-dataset",
    "href": "core/labs-solutions/lab-mca.html#the-gss-dataset",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The GSS dataset",
    "text": "The GSS dataset\nWe will use data coming from the General Social Survey. The General Social Survey data can be retrieved using the gssr package. If needed, install the gssr package and its companion package gssrdoc.\n\nCodestopifnot(\n  require(\"gssr\"),\n  require(\"gssrdoc\")\n)\n\nLoading required package: gssr\n\n\nPackage loaded. To attach the GSS data, type data(gss_all) at the console.\nFor the codebook, type data(gss_dict).\nFor the panel data and documentation, type e.g. data(gss_panel08_long) and data(gss_panel_doc).\nFor help on a specific GSS variable, type ?varname at the console.\n\n\nLoading required package: gssrdoc\n\n\nThe data we will use are panel data made available by the GSS. In order to explore them, it’s very useful toload gss_dict and gss_panel_doc.\n\nCodedata(gss_dict)\ndata(gss_panel_doc)\n\n\nCheck the online help.\n\nCode?gss_panel_doc\n\n\ngss_dict and gss_panel_doc are dataframes that can be queried:\n\nCodegss_panel_doc |&gt; \n  dplyr::sample_n(5) |&gt; \n  select(id, text)  |&gt; \n  gt::gt()\n\n\n\n\n\nid\ntext\n\n\n\nrelhhd8\n1622. What is (PERSON)'s relationship to (HEAD OF HOUSEHOLD)?\n\n\ndenom16\n115a. If Protestant: What specific denomination is that, if any?\n\n\nconjudge\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? j. U.S. Supreme Court.\n\n\nwordh\n1612. We would like to know something about how people go about guessing words they do not know. On this card are listed some words--you may know some of them, and you may not know quite a few of them. On each line the first word is in capital letters like BEAST. Then there are five other words. Tell me the number of the word that comes closest to the meaning of the word in capital letters. For example, if the word in capital letters is BEAST, you would say \"4\" since \"animal\" come closer to BEAST than any of the other words. If you wish, I will read the words to you. These words are difficult for almost everyone just give me your best guess if you are not sure of the answer.\n\n\ncompwage\n1087. Do you believe your fixed annual wages in calendar year 2005 were higher or lower than those of employees with similar experience and job descriptions in other companies in your region? Please answer on a 1 to 5 scale.\n\n\n\n\n\n\nIn the panel questionnaire, some questions have race in the field id. Check again the online help.\n\nCode?race\n\n\nThe answer is something like that:\nQuestion 24. What race do you consider yourself?\nAnd it contains a brief summary of the answers given through the years.\n\n\nYear\nBlack\nOther\nWhite\niap (NA)\nTotal\n\n\n\n2010\n311\n183\n1550\n-\n2044\n\n\n2012\n301\n196\n1477\n-\n1974\n\n\n2014\n386\n262\n1890\n-\n2538\n\n\n\n(this is just an excerpt)\n\nCodegss_panel_doc |&gt; \n  filter(str_detect(id, \"race\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  select(id, description, text) |&gt; \n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\n\n\n\nrace\nRACE\n24. What race do you consider yourself?\n\n\nracecen3\nRACECEN3\n1602. What is your race? Indicate one or more races that you consider yourself to be.\n\n\nnatrace\nNATRACE\n68. We are faced with many problems in this country, none of some of these problems, and for each one I'd like you to tell me whether you think we're spending too much money on it, too little money, or about the right amount. h. Improving the conditions of Blacks.\n\n\nnatracey\nNATRACEY\n69. We are faced with many problems in this country, none of some of these problems, and for each one I'd like you to tell me whether you think we're spending too much money on it, too little money, or about the right amount. h. Assistance to blacks.\n\n\nintrace3\nINTRACE3\nWhat is your race? Indicate one or more races that you consider yourself to be.\n\n\n\n\n\n\nThe data set we will use comes from year 2010 panel data.\n\nCodedata(\"gss_panel10_long\")\n\ngss_panel10_long \n\n# A tibble: 6,132 × 1,200\n   firstid  wave oversamp sampcode  sample      form         formwt vpsu  vstrat\n   &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+&gt;\n 1 1           1        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 2 1           2        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 3 1           3        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 4 2           1        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 5 2           2        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 6 2           3        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 7 3           1        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n 8 3           2        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n 9 3           3        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n10 4           1        1 501       9 [2000 FP] 2 [ALTERNAT…      1 2     2240  \n# ℹ 6,122 more rows\n# ℹ 1,191 more variables: samptype &lt;dbl+lbl&gt;, wtpan12 &lt;dbl+lbl&gt;,\n#   wtpan123 &lt;dbl+lbl&gt;, wtpannr12 &lt;dbl+lbl&gt;, wtpannr123 &lt;dbl+lbl&gt;,\n#   id &lt;dbl+lbl&gt;, mar1 &lt;dbl+lbl&gt;, mar2 &lt;dbl+lbl&gt;, mar3 &lt;dbl+lbl&gt;,\n#   mar4 &lt;dbl+lbl&gt;, mar5 &lt;dbl+lbl&gt;, mar6 &lt;dbl+lbl&gt;, mar7 &lt;dbl+lbl&gt;,\n#   mar8 &lt;dbl+lbl&gt;, mar9 &lt;dbl+lbl&gt;, mar10 &lt;dbl&gt;, mar11 &lt;dbl+lbl&gt;, mar12 &lt;dbl&gt;,\n#   mar13 &lt;dbl&gt;, mar14 &lt;dbl&gt;, abany &lt;dbl+lbl&gt;, abdefect &lt;dbl+lbl&gt;, …\n\n\nAt the beginning, the panel was made of roughly 2000 respondents. And the respondents were interviewed in 2010, 2012 and 2014 (the three waves).\n\nCodegss_panel10_long  |&gt;  \n  filter(wave==3, !is.na(id)) |&gt;\n  select(firstid, wave, id, sex)\n\n# A tibble: 1,304 × 4\n   firstid  wave id        sex       \n   &lt;fct&gt;   &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; \n 1 1           3 10001     1 [MALE]  \n 2 2           3 10002     2 [FEMALE]\n 3 3           3 10003     2 [FEMALE]\n 4 6           3 10004     1 [MALE]  \n 5 7           3 10005     2 [FEMALE]\n 6 9           3 10006     2 [FEMALE]\n 7 10          3 10007     2 [FEMALE]\n 8 11          3 10008     2 [FEMALE]\n 9 12          3 10009     1 [MALE]  \n10 13          3 10010     1 [MALE]  \n# ℹ 1,294 more rows\n\n\nSome respondents left the survey. Attrition can be monitored within the next query.\n\nCodegss_panel10_long |&gt; \n  select(wave, id) |&gt;\n  group_by(wave) |&gt;\n  summarize(observed = n_distinct(id),\n            missing = sum(is.na(id)))\n\n# A tibble: 3 × 3\n   wave observed missing\n  &lt;dbl&gt;    &lt;int&gt;   &lt;int&gt;\n1     1     2044       0\n2     2     1552     493\n3     3     1305     740"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#the-case-for-using-mca",
    "href": "core/labs-solutions/lab-mca.html#the-case-for-using-mca",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The case for using MCA",
    "text": "The case for using MCA\n\nIn order to construct a field of ideological and political attitudes, which will subsequently be used as a reference for locating the empirical typologies of response styles and survey compliance, we apply multiple correspondence analysis (MCA). MCA belongs to the family of techniques used in geometric data analysis (Le Roux and Rouanet 2004). It allows for the extraction of the most important dimensions in a set of categorical variables, and the graphical representation of variable categories and individuals relative to each other in a coordinate system. Distances between categories as well as individuals can be interpreted as a measure of (dis)similarity: If categories often co-appear in individual’s responses, they are located close together in the space produced by MCA. Rare co-appearances, accordingly, result in a larger distance between the respective categories. Furthermore, illustrative variables can passively be projected into the field, a technique that has been termed visual regression analysis (Lebart et al. 1984). Whereas the space is determined by the distances between the categories of active variables, passive variables do not alter the structure of the constructed field, but appear in their average and hence most likely position.\n\nFrom https://doi.org/10.1007/s11135-016-0458-3\nIn this lab, we won’t look at the field of ideological and political attitudes, but rather at the field of confidence level in a variety of institutions."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#us-field-of-ideological-dispositions",
    "href": "core/labs-solutions/lab-mca.html#us-field-of-ideological-dispositions",
    "title": "LAB: Multiple Correspondance Analysis",
    "section": "US field of ideological dispositions",
    "text": "US field of ideological dispositions\n\nCodepanel_doc &lt;- gssrdoc::gss_panel_doc"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#elementary-statistics-and-table-wrangling",
    "href": "core/labs-solutions/lab-mca.html#elementary-statistics-and-table-wrangling",
    "title": "LAB: Multiple Correspondance Analysis",
    "section": "Elementary statistics and table wrangling",
    "text": "Elementary statistics and table wrangling\nBefore proceeding to Multiple Correspondence Analysis (CA), let us select the set of active variables.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBuild a bar plot to display the importance of Causes of deaths in France in years 1979 and 2006\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nCounts have already been computed above."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#multiple-correspondance-analysis",
    "href": "core/labs-solutions/lab-mca.html#multiple-correspondance-analysis",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Multiple Correspondance Analysis",
    "text": "Multiple Correspondance Analysis\n\n\n\n\n\n\nMCA executive summary\n\n\n\nThe input of multiple correspondence analysis is a data frame \\(X\\) with \\(n\\) rows and \\(p\\) categorical columns. Multiple Correspondence Analysis starts by building the indicator matrix. The indicator matrix is built by one-hot encoding of each categorical variable.\n\nA categorical variable \\(V_j\\) (factor) with \\(q\\) levels is mapped to \\(q\\) \\(\\{0,1\\}\\) -valued variables \\(V_{j,r}\\) for \\(r \\leq q\\)\nIf levels are indexed by \\(\\{1, \\ldots, q\\}\\), if the value of the categorical variable \\(V_j\\) from row \\(i\\) is \\(k \\in \\{1, \\ldots, q\\}\\), the bina$ \\[k \\mapsto \\underbrace{0,\\ldots, 0}_{k-1}, 1, \\underbrace{0, \\ldots, 0}_{q-k}\\]\nThe indicator matrix has as many rows as the data matrix\nThe number of columns of the indicator matrix is the sum of the number of levels of the categorical variables/columns of the data matrix\nThe indicator matrix is a numerical matrix. It is suitable for factorial methodss\n\nRecall \\(X\\) is the data matrix with \\(n\\) rows (individuals) and \\(p\\) categorical columns (variables)\nFor \\(j \\in \\{1, \\ldots, p\\}\\), let \\(J_j\\) denote the number of levels(categories) of variable \\(j\\)\nLet \\(q = \\sum_{j\\leq p} J_j\\) be the sum of the number of levels throughout the variables\nLet \\(Z\\) be the incidence matrix with \\(n\\) rows and \\(q\\) columns\nFor \\(j\\leq p\\) and \\(k \\leq J_j\\), let \\(\\langle j, k\\rangle = \\sum_{j'&lt;j} J_{j'}+k\\)\nLet \\(N = n \\times p = \\sum_{i\\leq n} \\sum_{j \\leq p} X_{i,j}\\) and \\[P = \\frac{1}{N} Z\\]\n(the correspondence matrix for MCA)\n The row wise sums of correspondence matrix \\(P\\) are all equal to \\(1/n=p/N\\)\nThe column wise sum of the correspondence matrix \\(P\\) for the \\(k\\)th level of the \\(j\\)th variable of \\(X\\) ( \\(j \\leq p\\) ) is \\[N_{\\langle j,k\\rangle}/N = f_{\\langle j,k\\rangle}/p\\]\nwhere \\(f_{\\langle j,k\\rangle}\\) stands for the relative frequency of level \\(k\\) of the \\(j\\)th variable\n\\[D_r = \\frac{1}{n}\\text{Id}_n\\qquad D_c =\\text{diag}\\left(\\frac{f_{\\langle j,k\\rangle}}{p}\\right)_{j \\leq p, k\\leq J_j}\\]\nIn MCA, we compute the SVD \\(U \\times D \\times V^\\top\\) of the standardized residuals matrix:\n\\[S = D_r^{-1/2}\\times \\left(P - r\\times c^\\top\\right) \\times D_c^{-1/2} = \\sqrt{n}\\left(P - r\\times c^\\top\\right) \\times D_c^{-1/2}\\]\nCoefficient \\(i, \\langle j, k\\rangle\\) of \\(S\\) is \\[\\frac{\\mathbb{I}_{i, \\langle j, k\\rangle}- f_{\\langle j,k\\rangle}}{\\sqrt{n f_{\\langle j,k\\rangle}/p}}\\]\nMCA consists in computing the SVD of the standardized residuals matrix \\(S =  U  \\times D \\times V^\\top\\)\nFrom the SVD, we get\n\n\n\\(D_r^{-1/2} \\times U\\) standardized coordinates of rows\n\n\n\\(D_c^{-1/2} \\times V\\) standardized coordinates of columns\n\n\n\\(D_r^{-1/2} \\times U \\times D\\) principal coordinates of rows\n\n\n\\(D_c^{-1/2} \\times V \\times D\\) principal coordinates of columns\n\nSquared singular values: the principal inertia\n\n\nWhen calling svd(.), the argument should be \\[D_r^{1/2}\\times \\left(D_r^{-1} \\times P \\times D_c^{-1}- \\mathbf{I}\\times \\mathbf{I}^\\top  \\right)\\times D_c^{1/2}= D_r^{-1/2}\\times \\left( P - r \\times c^\\top  \\right)\\times D_c^{-1/2}\\]\n\n\n\n\n\n\n\n\nMCA and extended SVD\n\n\n\nAs\n\\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^\\top = (D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^\\top\\]\n\\[(D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^\\top\\]\nis the extended SVD of\n\\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^\\top\\]\nwith respect to \\(D_r\\) and \\(D_c\\)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPerform MCA on the indicator matrix.\n\n\n\n\n\n\n\n\nYou may use FactoMineR::MCA(). It is interesting to compute the correspondence analysis in your own way, by preparing the matrix that is handled to svd() and returning a named list containing all relevant information.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe lazyly rely on FactoMineR::MCA.\n\nCoderes.mca &lt;- confidence_data |&gt; \n  select(starts_with(\"con\")) |&gt; \n  FactoMineR::MCA(graph=F)\n\n\nThe result is an object of S3 class MCA.\n\n\ns\n\n\n\n\n\n\nQuestion\n\n\n\nIf you did use FactoMineR::MCA(), explain the organization of the result.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThe result of FactoMineR::MCA(...) is a named and nested list with five elements:\n\neig\n\na matrix/array containing enough information to build a screeplot.\n\ncall\n\na list of 9, containing the call to CA(), an object of type language, telling (in principle) the user how CA() was called. However, this is a quoted expression. Here we need to guess the value of y in the calling environment understand what’s going on.\n\n\nElement call also contains the table margin distributions marge.col and marge.row. The truncation rank ncp (number of components) can be assigned before computing the SVD (default value is 5). Element \\(X\\) stores the contingency table that was effectively used for computing Correpondence Analysis.\n\nind\n\nInformation gathered from SVD to facilitate individulals (respndents) analysis.\n\nvar\n\na list structured in the same way as element ind. Used for variables/categories profiles analysis\n\nsvd\n\na list of 3, just as the resuld of svd() containing the singular values, the left and right singular vectors of matrix \\(...\\)\n\n\n\n\n\n\n\n\n\n\nSolution, output of print(res.mca)\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n1\n$eig\n“eigenvalues”\n\n\n2\n$var\n“results for the variables (categories)”\n\n\n3\n$var$coord\n“coord. of the categories”\n\n\n4\n$var$cos2\n“cos2 for the categories”\n\n\n5\n$var$contrib\n“contributions of the categories”\n\n\n6\n$var$v.test\n“v-test for the categories”\n\n\n7\n$ind\n“results for the individuals”\n\n\n8\n$ind$coord\n“coord. for the individuals”\n\n\n9\n$ind$cos2\n“cos2 for the individuals”\n\n\n10\n$ind$contrib\n“contributions of the individuals”\n\n\n11\n$quali.sup\n“results for the supplementary categorical variables”\n\n\n12\n$quali.sup$coord\n“coord. for the supplementary categories”\n\n\n13\n$quali.sup$cos2\n“cos2 for the supplementary categories”\n\n\n14\n$quali.sup$v.test\n“v-test for the supplementary categories”\n\n\n15\n$call\n“intermediate results”\n\n\n16\n$call$marge.col\n“weights of columns”\n\n\n17\n$call$marge.li\n“weights of rows”\n\n\n\n res.mca$svd is not part of the output!\n\neig is computed from the singular values in res.mca$svd\nvar contains material for plotting information about categories and variables on factorial planes\nind conatins material for plotting information about individuals on on factorial planes\n\n\n\n\n\n\n\n\n\nIn principle, all relevant information can be gathered from components svd, call.marge.li, and call.marge.col."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#screeplots",
    "href": "core/labs-solutions/lab-mca.html#screeplots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Screeplots",
    "text": "Screeplots\n\n\n\n\n\n\nQuestion\n\n\n\nDraw screeplots. Why are they useful? Comment briefly.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe first define a tidying function for preparing the screeplot. It is designed along the lines of method tidy for objects of class prcomp. It consists in building a data frame from component eig of the MCA object.\n\nCodetidy_mca_scree &lt;- function(res_mca){\n  res_mca$eig |&gt; \n    as_tibble(rownames=\"Dim\") |&gt;\n    rename(\n      `% Inertia` = `percentage of variance`,\n      `Cumulative % Inertia` = `cumulative percentage of variance`\n    ) |&gt; \n    mutate(\n      PC = as.integer(str_match(Dim, 'dim ([0-9]*)')[,2])\n    )\n}\n\n\nWe can now build a column plot.\n\nCoderes.mca |&gt; \n  tidy_mca_scree() |&gt;\n  ggplot() +\n  aes(x=PC, y=eigenvalue, label=glue(\"{round(`% Inertia`,2)} %\")) +\n  geom_col() +\n  geom_text(angle=45, vjust=-1, hjust=-.1) \n\n\n\n\n\n\n\nNote that the share of cumulated inertia doesn’t grow very fast with the number of components. Situation is different from what we met when performing Correspondence Analysis or Principal Component Analysis. This tells us that we will hqve to look beyond the first two axis. This is commonplace with multiple correspondence analysis. Look at the reference below where most of the interesting information can be found on the 3rd and 4th axis."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#row-profiles-analysis",
    "href": "core/labs-solutions/lab-mca.html#row-profiles-analysis",
    "title": "LAB: Multiple Correspondance Analysis",
    "section": "Row profiles analysis",
    "text": "Row profiles analysis\n\n\n\n\n\n\nQuestion\n\n\n\nPerform row profiles analysis.\nWhat are the classical plots? How can you build them from the output of FactoMiner::CA?\nBuild the table of row contributions (the so-called \\(\\cos^2\\))\n\n\n\n\n\n\n\n\nSolution\n\n\n\nAttribute row of objects of class MCA (exported from FactoMineR) is the starting point of any row profiles analysis.\nAttribute row is a named list made of \\(4\\) components.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncoord\n\na matrix with named rows and columns. The number of rows of coord matches the number of rows of the contingency table (here, the number of possible death Causes). The number of columns matches the rank of the truncated SVD that underlies Correspondance Analysis. Here it is \\(5\\) which also the rank of the standardized contingency table.\n\n\n\nThe row principal coordinates are the principal coordinates of each row profile in terms of the principal component.\n\nThe columns of coord are pairwise orthogonal in the inner product space defined by diag(call$marge.row) (which embodies the marginal probabilities of the so-called Causes of deaths)\nWe can recover row$coord from the left singular vectors and the singular values:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ninertia\n\na numerical vector with length matching the number of rows of coord, contrib and cos2.\n\n\n\nInertia is the way CA measures variation between row profiles. Total inertia is the \\(\\chi^2\\) statistic divided by sample size.\n\nRow inertia can be obtained by multiplying the row marginal probability by the squared Euclidean norm of the row in the principal coordinate matrix.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncos2\n\nCoefficients of matrix cos2 are the share of row inertia from the corresponding cell in coord\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncontrib\n\n\n\n\nNot too surprisingly, coord, contrib, and cos2 share the same row names and column names.\nThe Row Profiles are the rows of matrix R below\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe can now display a scatterplot from component coord. This is called a Row Plot.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPerform column profiles analysis\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#symmetric-plots",
    "href": "core/labs-solutions/lab-mca.html#symmetric-plots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Symmetric plots",
    "text": "Symmetric plots\n\n\n\n\n\n\nQuestion\n\n\n\nBuild the symmetric plots (biplots) for multiple correspondence analysis.\n\n\nFrom the shelf\nWe resort to factoextra. In principle, the biplot can be constructed from the coord matrices in the var and the ind components of the MCA object.\n\nCodep &lt;- res.mca |&gt; \n  factoextra::fviz_mca_biplot(\n    label= \"var\",\n    alpha.ind=.5\n  )\n\np\n\n\n\n\n\n\nCode# p |&gt;  plotly::ggplotly()\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#mosaicplots",
    "href": "core/labs-solutions/lab-mca.html#mosaicplots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Mosaicplots",
    "text": "Mosaicplots\nMCA can be complemented by Mosaicplots, Double Decker plots, Chi-square tests, and Correspondence analyses between pair of variables.\n\n\n\n\n\n\nQuestion\n\n\n\nDraw a mosaic plot to visualize association between confidence levesl in Science and confidence level in Medicine.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeconfidence_data |&gt; \n  select(consci, conmedic) |&gt;\n  table() |&gt; \n  mosaicplot(color=T)\n\n\n\n\n\n\n\n\nCodeconfidence_data |&gt; \n  select(consci, conmedic) |&gt; \n  table() |&gt;\n  chisq.test() |&gt; \n  broom::glance() |&gt; \n  gt::gt() |&gt; \n  gt::tab_caption(\"Chi2 independence test for Confidence in Science and Confidence in Medicine\")\n\n\n\n\nChi2 independence test for Confidence in Science and Confidence in Medicine\n\nstatistic\np.value\nparameter\nmethod\n\n\n137.8878\n7.994493e-29\n4\nPearson's Chi-squared test\n\n\n\n\n\n\nCodeconfidence_data |&gt; \n  select(consci, conclerg) |&gt; \n  table() |&gt;\n  chisq.test() |&gt; \n  broom::glance() |&gt; \n  gt::gt() |&gt; \n  gt::tab_caption(\"Chi2 independence test for Confidence in Science and Confidence in Clergy\")\n\n\n\n\nChi2 independence test for Confidence in Science and Confidence in Clergy\n\nstatistic\np.value\nparameter\nmethod\n\n\n39.27785\n6.103953e-08\n4\nPearson's Chi-squared test\n\n\n\n\n\n\nCodet_sex_clerg_sci &lt;- confidence_data |&gt; \n  select(sex, conclerg, consci) |&gt; \n  table()\n  \nt_sex_clerg_sci  |&gt;\n  vcd::doubledecker( )\n\n\n\n\n\n\n\n\nCodet_sex_clerg_sci |&gt; \n  mosaicplot(color=T, main=\"Confidence in Clergy and Science\")"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#the-confidence-topic",
    "href": "core/labs-solutions/lab-mca.html#the-confidence-topic",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The confidence topic",
    "text": "The confidence topic\nDo GSS surveys are made of a huge number of questions. Not all questions were asked to the respondents. Indeed, each question was asked to two thirds of the respondents. Questions were related to demographic features (Age, Sex, Level of education, Employment and so on). Answers to these questions can be used to determine whether the panel sample is representative. Data can be compared with last census data (there is a census every ten years in the US).\nA number of questions are related to the confidence topic. Respondents were asked about the level of confidence they put in a wide array of institutions.\n\nCodegss_panel_doc |&gt; \n  filter(str_detect(text, \"confidence\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  select(id, description, text) |&gt; \n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\n\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\n\n\nconbus\nCONBUS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? b. Major companies.\n\n\nconclerg\nCONCLERG\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? c. Organized religion.\n\n\nconeduc\nCONEDUC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? d. Education.\n\n\nconfed\nCONFED\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? e. Executive branch of the federal government.\n\n\nconlabor\nCONLABOR\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? f. Organized labor.\n\n\nconpress\nCONPRESS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? g. Press.\n\n\nconmedic\nCONMEDIC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? h. Medicine.\n\n\ncontv\nCONTV\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? i. Television.\n\n\nconjudge\nCONJUDGE\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? j. U.S. Supreme Court.\n\n\nconsci\nCONSCI\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? k. Scientific community.\n\n\nconlegis\nCONLEGIS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? l. Congress\n\n\nconarmy\nCONARMY\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? m. Military\n\n\n\n\n\n\nFor institutions like Science or Medicine, or Executive branch of federal government, Respondents were asked whether they have: Great deal of confidence, Only some confidence, Hardly any confidence in the institution. The same Likert scale with 3 levels was used for all institutions.\n\n\n\n\n\n\nQuestion\n\n\n\nFrom the gss_panel10_long dataset, extract columns corresponding to questions from the confidence topic\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe first gather the id of the questions pertaining to the confidence topic.\n\nCodeconfidence_ids &lt;- gss_panel_doc |&gt; \n  filter(str_detect(text, \"confidence\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  pull(id)\n\n\nThen we protect the panel data on the questions pertaining to the confidence topic. We also keep the respondent key, sex, and the relevant wave. s\n\nCodegss_panel10_long |&gt; \n  filter(wave==1) |&gt;\n  select(firstid, wave, id, sex, contains(confidence_ids)) |&gt;\n  sample_n(10) |&gt;\n  gt::gt()\n\n\n\n\n\nfirstid\nwave\nid_1: id\nsex_1: RESPONDENTS SEX\nconfinan_1: CONFID IN BANKS & FINANCIAL INSTITUTIONS\nconbus_1: CONFIDENCE IN MAJOR COMPANIES\nconclerg_1: CONFIDENCE IN ORGANIZED RELIGION\nconeduc_1: CONFIDENCE IN EDUCATION\nconfed_1: CONFID. IN EXEC BRANCH OF FED GOVT\nconlabor_1: CONFIDENCE IN ORGANIZED LABOR\nconpress_1: CONFIDENCE IN PRESS\nconmedic_1: CONFIDENCE IN MEDICINE\ncontv_1: CONFIDENCE IN TELEVISION\nconjudge_1: CONFID. IN UNITED STATES SUPREME COURT\nconsci_1: CONFIDENCE IN SCIENTIFIC COMMUNITY\neconsci\nconlegis_1: CONFIDENCE IN CONGRESS\nconarmy_1: CONFIDENCE IN MILITARY\n\n\n\n1225\n1\n1225\n2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1511\n1\n1511\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1153\n1\n1153\n2\n3\n2\n2\n1\n2\n2\n2\n2\n1\n2\n2\nNA\n2\n1\n\n\n1585\n1\n1585\n2\n3\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\n1\n3\n\n\n940\n1\n940\n2\n3\n2\n3\n2\n3\n3\n2\n1\n2\n3\n2\nNA\n3\n2\n\n\n110\n1\n110\n2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n559\n1\n559\n2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1536\n1\n1536\n1\n3\n2\n2\n2\n2\n2\n3\n2\n3\n2\n2\nNA\n3\n1\n\n\n846\n1\n846\n2\n3\n2\n2\n3\n3\n2\n3\n1\n3\n2\n1\n4\n3\n2\n\n\n220\n1\n220\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nLet us go back to the documentation.\n\nCodegss_panel_doc |&gt;\n  filter(id %in% confidence_ids) |&gt; \n  select(-ends_with(\"2\"), -ends_with(\"3\")) |&gt;\n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\nproperties_1\nmarginals_1\n\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/265\"), c(\"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\")\nc(30.1, 55.2, 14.7, NA, NA, 100), c(\"395\", \"725\", \"193\", \"671\", \"16\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\")\n\n\nconbus\nCONBUS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? b. Major companies.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/247\"), c(\"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\")\nc(17.6, 62.3, 20.1, NA, NA, NA, 100), c(\"228\", \"806\", \"260\", \"671\", \"32\", \"3\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\")\n\n\nconclerg\nCONCLERG\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? c. Organized religion.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/250\"), c(\"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\")\nc(25.1, 52.5, 22.4, NA, NA, NA, 100), c(\"324\", \"678\", \"289\", \"671\", \"35\", \"3\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\")\n\n\nconeduc\nCONEDUC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? d. Education.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/259\"), c(\"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\")\nc(27.1, 56.5, 16.4, NA, NA, 100), c(\"359\", \"747\", \"217\", \"671\", \"6\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\")\n\n\nconfed\nCONFED\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? e. Executive branch of the federal government.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/262\"), c(\"CONFED_1\", \"CONFED_1\", \"CONFED_1\")\nc(13.8, 45.4, 40.8, NA, NA, NA, 100), c(\"179\", \"590\", \"531\", \"671\", \"26\", \"3\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONFED_1\", \"CONFED_1\", \"CONFED_1\", \"CONFED_1\", \"CONFED_1\", \"CONFED_1\", \"CONFED_1\")\n\n\nconlabor\nCONLABOR\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? f. Organized labor.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/289\"), c(\"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\")\nc(12.3, 58.1, 29.6, NA, NA, 100), c(\"155\", \"731\", \"372\", \"671\", \"71\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\")\n\n\nconpress\nCONPRESS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? g. Press.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/298\"), c(\"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\")\nc(10, 48.5, 41.5, NA, NA, NA, 100), c(\"132\", \"638\", \"546\", \"671\", \"12\", \"1\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\")\n\n\nconmedic\nCONMEDIC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? h. Medicine.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/295\"), c(\"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\")\nc(40.1, 49.3, 10.6, NA, NA, 100), c(\"530\", \"651\", \"140\", \"671\", \"8\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\")\n\n\ncontv\nCONTV\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? i. Television.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/328\"), c(\"CONTV_1\", \"CONTV_1\", \"CONTV_1\")\nc(9.4, 48.2, 42.3, NA, NA, NA, 100), c(\"124\", \"634\", \"556\", \"671\", \"14\", \"1\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONTV_1\", \"CONTV_1\", \"CONTV_1\", \"CONTV_1\", \"CONTV_1\", \"CONTV_1\", \"CONTV_1\")\n\n\nconjudge\nCONJUDGE\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? j. U.S. Supreme Court.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/286\"), c(\"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\")\nc(31.8, 50.3, 17.8, NA, NA, NA, 100), c(\"410\", \"649\", \"230\", \"671\", \"38\", \"2\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\")\n\n\nconsci\nCONSCI\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? k. Scientific community.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/319\"), c(\"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\")\nc(41.6, 51, 7.3, NA, NA, NA, 100), c(\"527\", \"646\", \"93\", \"671\", \"62\", \"1\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\")\n\n\nconlegis\nCONLEGIS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? l. Congress\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/292\"), c(\"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\")\nc(10.8, 51.2, 37.9, NA, NA, NA, 100), c(\"141\", \"666\", \"493\", \"671\", \"28\", \"1\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\")\n\n\nconarmy\nCONARMY\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? m. Military\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/244\"), c(\"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\")\nc(46.8, 39.8, 13.4, NA, NA, 100), c(\"613\", \"521\", \"175\", \"671\", \"20\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\")\n\n\n\n\n\n\nLevels labels for answers to confidence questions are:\n\n\nCodefoo &lt;- gss_panel_doc |&gt;\n   filter(id %in% confidence_ids) |&gt; \n   select(-ends_with(\"2\"), -ends_with(\"3\")) |&gt; \n   unnest(\n    cols=c(properties_1), names_repair = \"universal\") |&gt; \n   unnest(\n    cols=c(marginals_1), names_repair = \"universal\") |&gt; \n   head(6) \n\nNew names:\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...6`\n\nCodefoo |&gt; gt::gt()\n\n\n\n\n\nid...1\ndescription\ntext\nproperty\nvalue...5\nid...6\npercent\nn\nvalue...9\nlabel\nid...11\n\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\n30.1\n395\n1\nA GREAT DEAL\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\n55.2\n725\n2\nONLY SOME\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\n14.7\n193\n3\nHARDLY ANY\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\nNA\n671\n0\nIAP\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\nNA\n16\n8\nDK\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\n100.0\n2,000\nNA\nTotal\nCONFINAN_1\n\n\n\n\n\n\n\n\nIAP stands for respondent was not asked the question.\n\nDK stands for Don’t Know\n\n\n\nIf respondents were not given the opportunity to provide a response to a question because it was not included in the survey they were administered, then their responses are coded as “IAP.”\n\n\n\n\nCodepanel_doc &lt;- gssrdoc::gss_panel_doc"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#table-wrangling",
    "href": "core/labs-solutions/lab-mca.html#table-wrangling",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Table wrangling",
    "text": "Table wrangling\nBefore proceeding to Multiple Correspondence Analysis (CA), let us select the set of active variables.\n\n\n\n\n\n\nQuestion\n\n\n\nProject gss_panel10_long on firstid, wave, id, sex, and columns with names in\n  confinan conbus conclerg coneduc confed conlabor conpress conmedic contv conjudge consci conlegis conarmy\nFilter so as to keep only wave 1.\nTransform all relevant columns into factors.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeconfidence_panel10 &lt;- gss_panel10_long |&gt; \n  filter(wave==1) |&gt;\n  select(firstid, wave, id, sex, contains(confidence_ids)) |&gt;\n  mutate(across(starts_with(\"con\"), as_factor)) |&gt;\n  mutate(sex=as_factor(sex))\n\n\n\nCodeconfidence_10 &lt;- gss_panel_doc |&gt;\n   filter(id %in% confidence_ids) |&gt; \n   select(-ends_with(\"2\"), -ends_with(\"3\")) |&gt; \n   unnest(\n    cols=c(properties_1), names_repair = \"universal\") |&gt; \n   unnest(\n    cols=c(marginals_1), names_repair = \"universal\") |&gt;\n   mutate(text=str_squish(text)) |&gt;\n   mutate(text= str_match(text, \"\\\\? [\\\\w]\\\\.(.*)\")) |&gt; \n   mutate(text=text[,2]) |&gt; \n   filter(property == \"Missing-data codes\") |&gt;\n   select(-starts_with(\"id\"), -property, -`value...5`)\n\nNew names:\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...6`\n\n\n\nCodelevels_table &lt;- confidence_10 |&gt; \n  filter(label!='Total' | is.na(label)) |&gt;\n  distinct(`value...9`, label)"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#elementary-statistics",
    "href": "core/labs-solutions/lab-mca.html#elementary-statistics",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Elementary statistics",
    "text": "Elementary statistics\n\n\n\n\n\n\nQuestion\n\n\n\nUse skimmr to summarize your dataset.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeconfidence_panel10 |&gt;\n  skimr::skim(where(is.factor)) |&gt;\n  select(-factor.ordered, -skim_type)|&gt;\n  filter(str_starts(skim_variable,\"con\")) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(decimals=3) |&gt;\n  gt::tab_caption(\n    caption = \"GSS Panel data 2010 First wave, Topic confidence\"\n  )\n\n\n\n\nGSS Panel data 2010 First wave, Topic confidence\n\nskim_variable\nn_missing\ncomplete_rate\nfactor.n_unique\nfactor.top_counts\n\n\n\nconfinan\n680.000\n0.667\n3.000\nONL: 656, HAR: 564, A G: 144, IAP: 0\n\n\nconbus\n705.000\n0.655\n3.000\nONL: 848, HAR: 317, A G: 174, IAP: 0\n\n\nconclerg\n725.000\n0.645\n3.000\nONL: 720, HAR: 328, A G: 271, IAP: 0\n\n\nconeduc\n681.000\n0.667\n3.000\nONL: 793, A G: 367, HAR: 203, IAP: 0\n\n\nconfed\n698.000\n0.659\n3.000\nONL: 620, HAR: 499, A G: 227, IAP: 0\n\n\nconlabor\n744.000\n0.636\n3.000\nONL: 781, HAR: 373, A G: 146, IAP: 0\n\n\nconpress\n689.000\n0.663\n3.000\nONL: 621, HAR: 594, A G: 140, IAP: 0\n\n\nconmedic\n681.000\n0.667\n3.000\nONL: 647, A G: 558, HAR: 158, IAP: 0\n\n\ncontv\n681.000\n0.667\n3.000\nONL: 677, HAR: 521, A G: 165, IAP: 0\n\n\nconjudge\n709.000\n0.653\n3.000\nONL: 706, A G: 400, HAR: 229, IAP: 0\n\n\nconsci\n738.000\n0.639\n3.000\nONL: 683, A G: 544, HAR: 79, IAP: 0\n\n\nconlegis\n697.000\n0.659\n3.000\nONL: 635, HAR: 587, A G: 125, IAP: 0\n\n\nconarmy\n688.000\n0.663\n3.000\nA G: 720, ONL: 507, HAR: 129, IAP: 0\n\n\n\n\n\n\n\nCodeconfidence_panel10 |&gt;\n  filter(is.na(conarmy)) |&gt;\n  filter(is.na(conclerg)) |&gt;\n  filter(is.na(conpress))\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nThere are a lot of missing data in your data set.\nHow are missing cells related?\nDrop rows with missing data in the confidence questions.\nWhat are the dimensions of your data set?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeconfidence_data &lt;- confidence_panel10 |&gt; \n  mutate(across(starts_with(\"con\"), .fns=\\(x) !is.na(x), .names=\"not_{.col}_isna\")) |&gt;\n  rowwise() |&gt;\n  filter(all(c_across(ends_with(\"isna\")))) |&gt; \n  ungroup() |&gt; \n  select(-ends_with(\"isna\"), -firstid, -wave, -econsci) |&gt;\n  mutate(across(starts_with(\"con\"), fct_drop), sex=as_factor(sex))\n\nconfidence_data |&gt; \n  slice_sample(n=10) |&gt; \n  gt::gt()\n\n\n\n\n\nid_1: id\nsex_1: RESPONDENTS SEX\nconfinan_1: CONFID IN BANKS & FINANCIAL INSTITUTIONS\nconbus_1: CONFIDENCE IN MAJOR COMPANIES\nconclerg_1: CONFIDENCE IN ORGANIZED RELIGION\nconeduc_1: CONFIDENCE IN EDUCATION\nconfed_1: CONFID. IN EXEC BRANCH OF FED GOVT\nconlabor_1: CONFIDENCE IN ORGANIZED LABOR\nconpress_1: CONFIDENCE IN PRESS\nconmedic_1: CONFIDENCE IN MEDICINE\ncontv_1: CONFIDENCE IN TELEVISION\nconjudge_1: CONFID. IN UNITED STATES SUPREME COURT\nconsci_1: CONFIDENCE IN SCIENTIFIC COMMUNITY\nconlegis_1: CONFIDENCE IN CONGRESS\nconarmy_1: CONFIDENCE IN MILITARY\n\n\n\n1894\nFEMALE\nONLY SOME\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nONLY SOME\n\n\n1639\nFEMALE\nONLY SOME\nONLY SOME\nONLY SOME\nA GREAT DEAL\nONLY SOME\nONLY SOME\nONLY SOME\nA GREAT DEAL\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nA GREAT DEAL\n\n\n718\nMALE\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nHARDLY ANY\n\n\n1101\nFEMALE\nONLY SOME\nONLY SOME\nHARDLY ANY\nONLY SOME\nA GREAT DEAL\nHARDLY ANY\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nONLY SOME\nA GREAT DEAL\n\n\n1120\nFEMALE\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nHARDLY ANY\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\n\n\n1417\nFEMALE\nONLY SOME\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nHARDLY ANY\nHARDLY ANY\nONLY SOME\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\n\n\n1454\nMALE\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\nHARDLY ANY\nA GREAT DEAL\nA GREAT DEAL\nHARDLY ANY\nA GREAT DEAL\n\n\n1598\nMALE\nHARDLY ANY\nONLY SOME\nONLY SOME\nA GREAT DEAL\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\n\n\n62\nFEMALE\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\n\n\n1157\nMALE\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn your explore possible associations between the answers to the different confidence questions?\nHow would you test possible independence between answer to confidence in science and confidence in the Army.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nTODO"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#individuals",
    "href": "core/labs-solutions/lab-mca.html#individuals",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Individuals",
    "text": "Individuals\n\n\n\n\n\n\nQuestion\n\n\n\nPerform Individual profiles analysis.\nWhat are the classical plots? How can you build them from the output of FactoMiner::MCA?\nBuild the table of row contributions (the so-called \\(\\cos^2\\))\n\n\n\n\n\n\n\n\nSolution\n\n\n\nAttribute ind of objects of class MCA (exported from FactoMineR) is the starting point of any row profiles analysis.\nWe design an augment method. For objects of class MCA. This is like what is done for objects of class prcomp and different from what we did with objects of class CA. Indeed, MCA on the indicator matrix handles a sample just like PCA and not a contingency table like CA.\n\nCodeaugment_mca &lt;- function(res_mca, data=NULL){\n  \n  result &lt;- res_mca$ind$coord |&gt; \n    as_tibble()  \n  \n  if (!is.null(data)) {\n    result &lt;- result |&gt;\n      bind_cols(confidence_data)\n      # inner_join(confidence_data, by=c(\"id\")) \n  }\n  \n  return(result)\n}\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nAttribute ind is a named list made of \\(4\\) components.\n\ncoord\n\na matrix with named rows and columns. The number of rows of coord matches the number of rows of the table fed to MCA (here, the number of respondents in the panel). The number of columns matches the rank of the truncated SVD that underlies Multiple Correspondance Analysis. Here it is the default truncation \\(5\\).\n\n\n\nThe principal coordinates are the principal coordinates of each individual in terms of the principal component.\n\nThe columns of coord are pairwise orthogonal in the usual inner product space.\nFunction augment_mca Bind the columns of coord with the columns of the original dataset.\n\nCoderes.mca |&gt; \n#| label: augment_mca_glimpse\n  augment_mca(confidence_data) |&gt; \n  glimpse()\n\nRows: 1,183\nColumns: 20\n$ `Dim 1`  &lt;dbl&gt; 1.35465733, -0.10861408, -0.33683136, 0.72450001, 0.72525336,…\n$ `Dim 2`  &lt;dbl&gt; 0.41190167, -0.90245225, -0.45183706, 0.25425970, 0.10225521,…\n$ `Dim 3`  &lt;dbl&gt; 0.161371575, 0.010268886, 0.107542556, -0.009273224, -0.38139…\n$ `Dim 4`  &lt;dbl&gt; 0.141129674, 0.310980059, -0.097880877, -0.085996832, 0.36106…\n$ `Dim 5`  &lt;dbl&gt; 0.227285260, 0.086562193, 0.016680275, -0.115998402, -0.06681…\n$ id       &lt;dbl+lbl&gt;  4,  6,  7,  9, 11, 15, 19, 20, 21, 24, 26, 28, 31, 41, 4…\n$ sex      &lt;dbl+lbl&gt; 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, …\n$ confinan &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, HARDLY ANY, ONLY SOME, ON…\n$ conbus   &lt;fct&gt; ONLY SOME, ONLY SOME, ONLY SOME, ONLY SOME, ONLY SOME, ONLY S…\n$ conclerg &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEA…\n$ coneduc  &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, ONLY SOME, A GREAT DEAL, …\n$ confed   &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEA…\n$ conlabor &lt;fct&gt; A GREAT DEAL, ONLY SOME, HARDLY ANY, ONLY SOME, HARDLY ANY, O…\n$ conpress &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, HARDLY ANY, ONLY SOME, HA…\n$ conmedic &lt;fct&gt; A GREAT DEAL, ONLY SOME, HARDLY ANY, A GREAT DEAL, ONLY SOME,…\n$ contv    &lt;fct&gt; ONLY SOME, ONLY SOME, HARDLY ANY, ONLY SOME, ONLY SOME, ONLY …\n$ conjudge &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEA…\n$ consci   &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, ONLY SOME, …\n$ conlegis &lt;fct&gt; ONLY SOME, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEAL, …\n$ conarmy  &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEA…\n\n\nNote that, we can recover ind$coord from the left singular vectors and the singular values.\nThe output of augment_mca is the basis of the individual plots. Individual plots are defined by picking a pair of principal axes. Him use the 3rd and the 4th axis, and we map the sex column from the original dataset to the fill aesthetics.\n\nCoderes.mca |&gt; \n  augment_mca(confidence_data) |&gt; \n  ggplot() +\n  aes(x=`Dim 3`, y=`Dim 4`, fill=sex) +\n  geom_point(alpha=.2, size=.2) +\n  coord_fixed() +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncos2\n\nCoefficients of matrix cos2 are the share of row inertia from the corresponding cell in coord\n\n\n\nCoderes.mca |&gt; \n  augment_mca(confidence_data) |&gt; \n  ggplot() +\n  aes(x=`Dim 1`, y=`Dim 2`, fill=sex) +\n  geom_point(alpha=.2, size=.2) +\n  coord_fixed() +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncontrib\n\n\n\n\nNot too surprisingly, coord, contrib, and cos2 share the same row names and column names."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#variablescategories",
    "href": "core/labs-solutions/lab-mca.html#variablescategories",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Variables/Categories",
    "text": "Variables/Categories\n\n\n\n\n\n\nQuestion\n\n\n\nPerform column profiles (categories) analysis. You may use factoextra::fviz_mca_var()\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThe var component. Of an MCA object is related with the right singular vectors of the residual matrix.\n\nCodep &lt;- res.mca |&gt; \n  factoextra::fviz_mca_var() +\n  coord_fixed() \n  \np +\n  facet_zoom(\n    xy=`Dim 1`&gt;=0 & `Dim 2`&gt;=0,\n    xlim=c(0,2),ylim=c(0,2))\n\n\n\n\n\n\n\nFollowing what has been done for objects of class prcomp, we design a tidy_mca_var function that turns the var component into a dataframe. Row names which correspond to categories, are turned into a column we call levels.\n\nCodetidy_mca_var &lt;- function(res_mca){\n  res_mca$var$coord |&gt; \n  as_tibble(rownames=\"levels\")\n}\n\n\nA value in column levels indicate a variable., that is a topic (science, religion, army, …) and a confidence level (A value on a Likert scale). We enrich the data frame by extracting the variable (topic) and the value (sentiment).\nWe are then ready to plot an analog of the correlation circle.\n\nCodetidied_mca_var &lt;- tidy_mca_var(res.mca) |&gt;   \n  mutate(sentiment=str_split(as.character(levels),'_')) |&gt;\n  mutate(topic=map_chr(sentiment, \\(x)x[1]), sentiment=map_chr(sentiment, \\(x)x[2])) \n\np &lt;- tidied_mca_var |&gt; \n  ggplot() +\n    aes(x=`Dim 1`, y=`Dim 2`, label=levels, fill=topic, shape=sentiment) +\n    geom_point(alpha=.5, show.legend = F) +\n    coord_fixed() +\n    scale_color_viridis_d() +\n    theme(legend.position=\"none\") +\n    labs(\n      title = \"MCA: Variables categories\",\n      subtitle = \"Color: topic, Shape: confidence level\",\n      caption = \"GSS data, panel 2010, wave 1\"\n    )\n\np |&gt;\n  plotly::ggplotly() \n\n\n\n\n\nResorting to plotly allows us to mitigate the overplotting effect.\nThe plot on the first two axes seems easy to interpret. As we move along the first axis from the left, we see growing confidence.\nOn the positive part of the second axis, we have extreme confidence levels, either high or low. This suggests that individuals have trends: either we trust all institutions; or none of them; or finally a little bit all of them.\nHow could we comfort such a narrative?\n\nCode(\n p + \n    aes(x=`Dim 3`, y=`Dim 4`, label=levels, color=sentiment) \n) |&gt;   plotly::ggplotly() \n\n\n\n\n\n\nCode(\n  p +\n    aes(x=`Dim 4`, y=`Dim 5`, label=levels) \n) |&gt;  plotly::ggplotly() \n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the v.test component of the var component of an MCA object?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nHere is a chatbot answer:\n\nIn Multiple Correspondence Analysis (MCA), the v-test (also known as the test-value or t-test value) is a statistical measure used to assess the significance of the association between a category and a dimension (factor). It helps determine which categories contribute the most to the definition of a given dimension.\n\nDefinition and Calculation\nThe v-test is computed as:\n\\[v = \\frac{p - P}{\\sqrt{P(1 - P)/n}}\\]\nwhere: - \\(p\\) = proportion of individuals in a given category for the specific dimension, - \\(P\\) = overall proportion of individuals in that category across all dimensions, - \\(n\\) = total number of individuals.\nAlternatively, the v-test can also be expressed as:\n\\[v = \\frac{\\text{category coordinate}}{\\text{standard deviation of the coordinate}}\\]\nThis essentially provides a z-score, which follows a standard normal distribution under the null hypothesis.\nInterpretation\n\nA high absolute v-test value (typically above 2 or below -2) indicates that the category is strongly associated with the given dimension.\n\nPositive v-test: The category is overrepresented in that dimension.\n\nNegative v-test: The category is underrepresented in that dimension.\nIf the absolute value of the v-test is below 2, the association is not statistically significant (at a 5% significance level).\nWhy is it useful?\n\nIt helps interpret MCA results quantitatively rather than relying only on visual inspection.\nIt aids in identifying key categories that structure the data on a given axis.\nIt provides statistical validation for category contributions to dimensions.\n\nWould you like a practical example or interpretation of a specific MCA result?"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#adding-passivae-variables",
    "href": "core/labs-solutions/lab-mca.html#adding-passivae-variables",
    "title": "LAB: Multiple Correspondance Analysis (MCA)",
    "section": "Adding passivae variables",
    "text": "Adding passivae variables\n\n\n\n\n\n\nQuestion\n\n\n\nColor individuals according to sex\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCode# TODO"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#references",
    "href": "core/labs-solutions/lab-mca.html#references",
    "title": "LAB: Multiple Correspondance Analysis (MCA)",
    "section": "References",
    "text": "References\nCosta, Patrício & Santos, Nadine & Guimaraes, Pedro & Cotter, Jorge & Sousa, Nuno. (2013). The Use of Multiple Correspondence Analysis to Explore Associations between Categories of Qualitative Variables in Healthy Ageing. Journal of aging research. 2013. 302163. 10.1155/2013/302163.\nhttps://link.springer.com/article/10.1007/s11135-016-0458-3\nhttps://doi.org/10.1007/s11135-016-0458-3\nBarth, Alice and Schmitz, Andreas. 2018. Response quality and ideological dispositions: an integrative approach using geometric and classifying techniques. Quality & Quantity\n\nWhen analyzing survey data, response quality has consequential implications for substantial conclusions. Differences in response quality are usually explained by personality, or socio-demographic or cognitive characteristics. Little, however, is known about how respondents’ political attitudes, values, and opinions impact on quality aspects. This is a striking analytical omission, as potential associations between political values and various forms of response biases and artefacts call into question surveys’ ability to represent ‘public opinion’. In this contribution, response quality is traced back to respondents’ political and ideological dispositions. For this purpose, a relational understanding of response quality is applied that takes into account different aspects of response behaviors, as well as the interrelations between these indicators. Using data from the US General Social Survey (2010–2014), an empirical typology of response quality is created via finite mixture analysis. The resulting classes are then related to positions in the US field of ideological dispositions constructed via multiple correspondence analysis. The analyses reveal that there are (1) different combinations of response patterns and thus different empirical response types, and (2) that these types of response quality systematically vary with regard to the respondents’ political and ideological (dis)positions. Implications of the findings for public opinion surveys are discussed.\n\nhttps://doi.org/10.1007/s11135-016-0458-3"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#adding-passive-variables-to-individual-plots",
    "href": "core/labs-solutions/lab-mca.html#adding-passive-variables-to-individual-plots",
    "title": "LAB: Multiple Correspondance Analysis (MCA)",
    "section": "Adding passive variables to individual plots",
    "text": "Adding passive variables to individual plots\n\n\n\n\n\n\nQuestion\n\n\n\nColor individuals according to sex\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCode# TODO\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nTODO"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#further-references",
    "href": "core/labs-solutions/lab-mca.html#further-references",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Further references",
    "text": "Further references\nBarth, Alice and Schmitz, Andreas. 2018. Response quality and ideological dispositions: an integrative approach using geometric and classifying techniques. Quality & Quantity\n\nWhen analyzing survey data, response quality has consequential implications for substantial conclusions. Differences in response quality are usually explained by personality, or socio-demographic or cognitive characteristics. Little, however, is known about how respondents’ political attitudes, values, and opinions impact on quality aspects. This is a striking analytical omission, as potential associations between political values and various forms of response biases and artefacts call into question surveys’ ability to represent ‘public opinion’. In this contribution, response quality is traced back to respondents’ political and ideological dispositions. For this purpose, a relational understanding of response quality is applied that takes into account different aspects of response behaviors, as well as the interrelations between these indicators. Using data from the US General Social Survey (2010–2014), an empirical typology of response quality is created via finite mixture analysis. The resulting classes are then related to positions in the US field of ideological dispositions constructed via multiple correspondence analysis. The analyses reveal that there are (1) different combinations of response patterns and thus different empirical response types, and (2) that these types of response quality systematically vary with regard to the respondents’ political and ideological (dis)positions. Implications of the findings for public opinion surveys are discussed."
  },
  {
    "objectID": "core/labs/lab-mca.html",
    "href": "core/labs/lab-mca.html",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "",
    "text": "Besides the usual packages (tidyverse, …), we shall require FactoMineR and related packages.\nCodestopifnot(\n  require(FactoMineR),\n  require(factoextra),\n  require(FactoInvestigate)\n)"
  },
  {
    "objectID": "core/labs/lab-mca.html#the-gss-dataset",
    "href": "core/labs/lab-mca.html#the-gss-dataset",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The GSS dataset",
    "text": "The GSS dataset\nWe will use data coming from the General Social Survey. The General Social Survey data can be retrieved using the gssr package. If needed, install the gssr package and its companion package gssrdoc.\n\nCodestopifnot(\n  require(\"gssr\"),\n  require(\"gssrdoc\")\n)\n\nLoading required package: gssr\n\n\nPackage loaded. To attach the GSS data, type data(gss_all) at the console.\nFor the codebook, type data(gss_dict).\nFor the panel data and documentation, type e.g. data(gss_panel08_long) and data(gss_panel_doc).\nFor help on a specific GSS variable, type ?varname at the console.\n\n\nLoading required package: gssrdoc\n\n\nThe data we will use are panel data made available by the GSS. In order to explore them, it’s very useful toload gss_dict and gss_panel_doc.\n\nCodedata(gss_dict)\ndata(gss_panel_doc)\n\n\nCheck the online help.\n\nCode?gss_panel_doc\n\n\ngss_dict and gss_panel_doc are dataframes that can be queried:\n\nCodegss_panel_doc |&gt; \n  dplyr::sample_n(5) |&gt; \n  select(id, text)  |&gt; \n  gt::gt()\n\n\n\n\n\nid\ntext\n\n\n\npadeg\n20. If finished 9th-12th grade: Did he ever get a high school diploma or a GED certificate?\n\n\nrelhh13\n1632. What is (PERSON)'s relationship to head of household?\n\n\nrellife\nPlease tell me whether you strongly agree, agree, disagree, or strongly disagree with the following statements: 679. I try hard to carry my religious beliefs over into all my other dealings in life.\n\n\nraclive\n129. Are there any (negroes/blacks/African-Americans) living in this neighborhood now?\n\n\nold3\n1621. Please tell me the names of the people who usually live in this household. Let's start with the head of the household. c. How old was [PERSON] on his/her last birthday?\n\n\n\n\n\n\nIn the panel questionnaire, some questions have race in the field id. Check again the online help.\n\nCode?race\n\n\nThe answer is something like that:\nQuestion 24. What race do you consider yourself?\nAnd it contains a brief summary of the answers given through the years.\n\n\nYear\nBlack\nOther\nWhite\niap (NA)\nTotal\n\n\n\n2010\n311\n183\n1550\n-\n2044\n\n\n2012\n301\n196\n1477\n-\n1974\n\n\n2014\n386\n262\n1890\n-\n2538\n\n\n\n(this is just an excerpt)\n\nCodegss_panel_doc |&gt; \n  filter(str_detect(id, \"race\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  select(id, description, text) |&gt; \n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\n\n\n\nrace\nRACE\n24. What race do you consider yourself?\n\n\nracecen1\nRACECEN1\n1602. What is your race? Indicate one or more races that you consider yourself to be.\n\n\nnatrace\nNATRACE\n68. We are faced with many problems in this country, none of some of these problems, and for each one I'd like you to tell me whether you think we're spending too much money on it, too little money, or about the right amount. h. Improving the conditions of Blacks.\n\n\nnatracey\nNATRACEY\n69. We are faced with many problems in this country, none of some of these problems, and for each one I'd like you to tell me whether you think we're spending too much money on it, too little money, or about the right amount. h. Assistance to blacks.\n\n\nintrace3\nINTRACE3\nWhat is your race? Indicate one or more races that you consider yourself to be.\n\n\n\n\n\n\nThe data set we will use comes from year 2010 panel data.\n\nCodedata(\"gss_panel10_long\")\n\ngss_panel10_long \n\n# A tibble: 6,132 × 1,200\n   firstid  wave oversamp sampcode  sample      form         formwt vpsu  vstrat\n   &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+&gt;\n 1 1           1        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 2 1           2        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 3 1           3        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 4 2           1        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 5 2           2        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 6 2           3        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 7 3           1        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n 8 3           2        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n 9 3           3        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n10 4           1        1 501       9 [2000 FP] 2 [ALTERNAT…      1 2     2240  \n# ℹ 6,122 more rows\n# ℹ 1,191 more variables: samptype &lt;dbl+lbl&gt;, wtpan12 &lt;dbl+lbl&gt;,\n#   wtpan123 &lt;dbl+lbl&gt;, wtpannr12 &lt;dbl+lbl&gt;, wtpannr123 &lt;dbl+lbl&gt;,\n#   id &lt;dbl+lbl&gt;, mar1 &lt;dbl+lbl&gt;, mar2 &lt;dbl+lbl&gt;, mar3 &lt;dbl+lbl&gt;,\n#   mar4 &lt;dbl+lbl&gt;, mar5 &lt;dbl+lbl&gt;, mar6 &lt;dbl+lbl&gt;, mar7 &lt;dbl+lbl&gt;,\n#   mar8 &lt;dbl+lbl&gt;, mar9 &lt;dbl+lbl&gt;, mar10 &lt;dbl&gt;, mar11 &lt;dbl+lbl&gt;, mar12 &lt;dbl&gt;,\n#   mar13 &lt;dbl&gt;, mar14 &lt;dbl&gt;, abany &lt;dbl+lbl&gt;, abdefect &lt;dbl+lbl&gt;, …\n\n\nAt the beginning, the panel was made of roughly 2000 respondents. And the respondents were interviewed in 2010, 2012 and 2014 (the three waves).\n\nCodegss_panel10_long  |&gt;  \n  filter(wave==3, !is.na(id)) |&gt;\n  select(firstid, wave, id, sex)\n\n# A tibble: 1,304 × 4\n   firstid  wave id        sex       \n   &lt;fct&gt;   &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; \n 1 1           3 10001     1 [MALE]  \n 2 2           3 10002     2 [FEMALE]\n 3 3           3 10003     2 [FEMALE]\n 4 6           3 10004     1 [MALE]  \n 5 7           3 10005     2 [FEMALE]\n 6 9           3 10006     2 [FEMALE]\n 7 10          3 10007     2 [FEMALE]\n 8 11          3 10008     2 [FEMALE]\n 9 12          3 10009     1 [MALE]  \n10 13          3 10010     1 [MALE]  \n# ℹ 1,294 more rows\n\n\nSome respondents left the survey. Attrition can be monitored within the next query.\n\nCodegss_panel10_long |&gt; \n  select(wave, id) |&gt;\n  group_by(wave) |&gt;\n  summarize(observed = n_distinct(id),\n            missing = sum(is.na(id)))\n\n# A tibble: 3 × 3\n   wave observed missing\n  &lt;dbl&gt;    &lt;int&gt;   &lt;int&gt;\n1     1     2044       0\n2     2     1552     493\n3     3     1305     740"
  },
  {
    "objectID": "core/labs/lab-mca.html#the-confidence-topic",
    "href": "core/labs/lab-mca.html#the-confidence-topic",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The confidence topic",
    "text": "The confidence topic\nDo GSS surveys are made of a huge number of questions. Not all questions were asked to the respondents. Indeed, each question was asked to two thirds of the respondents. Questions were related to demographic features (Age, Sex, Level of education, Employment and so on). Answers to these questions can be used to determine whether the panel sample is representative. Data can be compared with last census data (there is a census every ten years in the US).\nA number of questions are related to the confidence topic. Respondents were asked about the level of confidence they put in a wide array of institutions.\n\nCodegss_panel_doc |&gt; \n  filter(str_detect(text, \"confidence\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  select(id, description, text) |&gt; \n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\n\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\n\n\nconbus\nCONBUS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? b. Major companies.\n\n\nconclerg\nCONCLERG\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? c. Organized religion.\n\n\nconeduc\nCONEDUC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? d. Education.\n\n\nconfed\nCONFED\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? e. Executive branch of the federal government.\n\n\nconlabor\nCONLABOR\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? f. Organized labor.\n\n\nconpress\nCONPRESS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? g. Press.\n\n\nconmedic\nCONMEDIC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? h. Medicine.\n\n\ncontv\nCONTV\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? i. Television.\n\n\nconjudge\nCONJUDGE\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? j. U.S. Supreme Court.\n\n\nconsci\nCONSCI\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? k. Scientific community.\n\n\nconlegis\nCONLEGIS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? l. Congress\n\n\nconarmy\nCONARMY\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? m. Military\n\n\n\n\n\n\nFor institutions like Science or Medicine, or Executive branch of federal government, Respondents were asked whether they have: Great deal of confidence, Only some confidence, Hardly any confidence in the institution. The same Likert scale with 3 levels was used for all institutions.\n\n\n\n\n\n\nQuestion\n\n\n\nFrom the gss_panel10_long dataset, extract columns corresponding to questions from the confidence topic\n\n\n\nCodepanel_doc &lt;- gssrdoc::gss_panel_doc"
  },
  {
    "objectID": "core/labs/lab-mca.html#table-wrangling",
    "href": "core/labs/lab-mca.html#table-wrangling",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Table wrangling",
    "text": "Table wrangling\nBefore proceeding to Multiple Correspondence Analysis (CA), let us select the set of active variables.\n\n\n\n\n\n\nQuestion\n\n\n\nProject gss_panel10_long on firstid, wave, id, sex, and columns with names in\n  confinan conbus conclerg coneduc confed conlabor conpress conmedic contv conjudge consci conlegis conarmy\nFilter so as to keep only wave 1.\nTransform all relevant columns into factors."
  },
  {
    "objectID": "core/labs/lab-mca.html#elementary-statistics",
    "href": "core/labs/lab-mca.html#elementary-statistics",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Elementary statistics",
    "text": "Elementary statistics\n\n\n\n\n\n\nQuestion\n\n\n\nUse skimmr to summarize your dataset.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nThere are a lot of missing data in your data set.\nHow are missing cells related?\nDrop rows with missing data in the confidence questions.\nWhat are the dimensions of your data set?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn your explore possible associations between the answers to the different confidence questions?\nHow would you test possible independence between answer to confidence in science and confidence in the Army."
  },
  {
    "objectID": "core/labs/lab-mca.html#the-case-for-using-mca",
    "href": "core/labs/lab-mca.html#the-case-for-using-mca",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The case for using MCA",
    "text": "The case for using MCA\n\nIn order to construct a field of ideological and political attitudes, which will subsequently be used as a reference for locating the empirical typologies of response styles and survey compliance, we apply multiple correspondence analysis (MCA). MCA belongs to the family of techniques used in geometric data analysis (Le Roux and Rouanet 2004). It allows for the extraction of the most important dimensions in a set of categorical variables, and the graphical representation of variable categories and individuals relative to each other in a coordinate system. Distances between categories as well as individuals can be interpreted as a measure of (dis)similarity: If categories often co-appear in individual’s responses, they are located close together in the space produced by MCA. Rare co-appearances, accordingly, result in a larger distance between the respective categories. Furthermore, illustrative variables can passively be projected into the field, a technique that has been termed visual regression analysis (Lebart et al. 1984). Whereas the space is determined by the distances between the categories of active variables, passive variables do not alter the structure of the constructed field, but appear in their average and hence most likely position.\n\nFrom https://doi.org/10.1007/s11135-016-0458-3\nIn this lab, we won’t look at the field of ideological and political attitudes, but rather at the field of confidence level in a variety of institutions."
  },
  {
    "objectID": "core/labs/lab-mca.html#multiple-correspondance-analysis",
    "href": "core/labs/lab-mca.html#multiple-correspondance-analysis",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Multiple Correspondance Analysis",
    "text": "Multiple Correspondance Analysis\n\n\n\n\n\n\nMCA executive summary\n\n\n\nThe input of multiple correspondence analysis is a data frame \\(X\\) with \\(n\\) rows and \\(p\\) categorical columns. Multiple Correspondence Analysis starts by building the indicator matrix. The indicator matrix is built by one-hot encoding of each categorical variable.\n\nA categorical variable \\(V_j\\) (factor) with \\(q\\) levels is mapped to \\(q\\) \\(\\{0,1\\}\\) -valued variables \\(V_{j,r}\\) for \\(r \\leq q\\)\nIf levels are indexed by \\(\\{1, \\ldots, q\\}\\), if the value of the categorical variable \\(V_j\\) from row \\(i\\) is \\(k \\in \\{1, \\ldots, q\\}\\), the bina$ \\[k \\mapsto \\underbrace{0,\\ldots, 0}_{k-1}, 1, \\underbrace{0, \\ldots, 0}_{q-k}\\]\nThe indicator matrix has as many rows as the data matrix\nThe number of columns of the indicator matrix is the sum of the number of levels of the categorical variables/columns of the data matrix\nThe indicator matrix is a numerical matrix. It is suitable for factorial methodss\n\nRecall \\(X\\) is the data matrix with \\(n\\) rows (individuals) and \\(p\\) categorical columns (variables)\nFor \\(j \\in \\{1, \\ldots, p\\}\\), let \\(J_j\\) denote the number of levels(categories) of variable \\(j\\)\nLet \\(q = \\sum_{j\\leq p} J_j\\) be the sum of the number of levels throughout the variables\nLet \\(Z\\) be the incidence matrix with \\(n\\) rows and \\(q\\) columns\nFor \\(j\\leq p\\) and \\(k \\leq J_j\\), let \\(\\langle j, k\\rangle = \\sum_{j'&lt;j} J_{j'}+k\\)\nLet \\(N = n \\times p = \\sum_{i\\leq n} \\sum_{j \\leq p} X_{i,j}\\) and \\[P = \\frac{1}{N} Z\\]\n(the correspondence matrix for MCA)\n The row wise sums of correspondence matrix \\(P\\) are all equal to \\(1/n=p/N\\)\nThe column wise sum of the correspondence matrix \\(P\\) for the \\(k\\)th level of the \\(j\\)th variable of \\(X\\) ( \\(j \\leq p\\) ) is \\[N_{\\langle j,k\\rangle}/N = f_{\\langle j,k\\rangle}/p\\]\nwhere \\(f_{\\langle j,k\\rangle}\\) stands for the relative frequency of level \\(k\\) of the \\(j\\)th variable\n\\[D_r = \\frac{1}{n}\\text{Id}_n\\qquad D_c =\\text{diag}\\left(\\frac{f_{\\langle j,k\\rangle}}{p}\\right)_{j \\leq p, k\\leq J_j}\\]\nIn MCA, we compute the SVD \\(U \\times D \\times V^\\top\\) of the standardized residuals matrix:\n\\[S = D_r^{-1/2}\\times \\left(P - r\\times c^\\top\\right) \\times D_c^{-1/2} = \\sqrt{n}\\left(P - r\\times c^\\top\\right) \\times D_c^{-1/2}\\]\nCoefficient \\(i, \\langle j, k\\rangle\\) of \\(S\\) is \\[\\frac{\\mathbb{I}_{i, \\langle j, k\\rangle}- f_{\\langle j,k\\rangle}}{\\sqrt{n f_{\\langle j,k\\rangle}/p}}\\]\nMCA consists in computing the SVD of the standardized residuals matrix \\(S =  U  \\times D \\times V^\\top\\)\nFrom the SVD, we get\n\n\n\\(D_r^{-1/2} \\times U\\) standardized coordinates of rows\n\n\n\\(D_c^{-1/2} \\times V\\) standardized coordinates of columns\n\n\n\\(D_r^{-1/2} \\times U \\times D\\) principal coordinates of rows\n\n\n\\(D_c^{-1/2} \\times V \\times D\\) principal coordinates of columns\n\nSquared singular values: the principal inertia\n\n\nWhen calling svd(.), the argument should be \\[D_r^{1/2}\\times \\left(D_r^{-1} \\times P \\times D_c^{-1}- \\mathbf{I}\\times \\mathbf{I}^\\top  \\right)\\times D_c^{1/2}= D_r^{-1/2}\\times \\left( P - r \\times c^\\top  \\right)\\times D_c^{-1/2}\\]\n\n\n\n\n\n\n\n\nMCA and extended SVD\n\n\n\nAs\n\\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^\\top = (D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^\\top\\]\n\\[(D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^\\top\\]\nis the extended SVD of\n\\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^\\top\\]\nwith respect to \\(D_r\\) and \\(D_c\\)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPerform MCA on the indicator matrix.\n\n\n\n\n\n\n\n\nYou may use FactoMineR::MCA(). It is interesting to compute the correspondence analysis in your own way, by preparing the matrix that is handled to svd() and returning a named list containing all relevant information.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIf you did use FactoMineR::MCA(), explain the organization of the result."
  },
  {
    "objectID": "core/labs/lab-mca.html#screeplots",
    "href": "core/labs/lab-mca.html#screeplots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Screeplots",
    "text": "Screeplots\n\n\n\n\n\n\nQuestion\n\n\n\nDraw screeplots. Why are they useful? Comment briefly."
  },
  {
    "objectID": "core/labs/lab-mca.html#individuals",
    "href": "core/labs/lab-mca.html#individuals",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Individuals",
    "text": "Individuals\n\n\n\n\n\n\nQuestion\n\n\n\nPerform Individual profiles analysis.\nWhat are the classical plots? How can you build them from the output of FactoMiner::MCA?\nBuild the table of row contributions (the so-called \\(\\cos^2\\))"
  },
  {
    "objectID": "core/labs/lab-mca.html#variablescategories",
    "href": "core/labs/lab-mca.html#variablescategories",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Variables/Categories",
    "text": "Variables/Categories\n\n\n\n\n\n\nQuestion\n\n\n\nPerform column profiles (categories) analysis. You may use factoextra::fviz_mca_var()\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the v.test component of the var component of an MCA object?"
  },
  {
    "objectID": "core/labs/lab-mca.html#symmetric-plots",
    "href": "core/labs/lab-mca.html#symmetric-plots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Symmetric plots",
    "text": "Symmetric plots\n\n\n\n\n\n\nQuestion\n\n\n\nBuild the symmetric plots (biplots) for multiple correspondence analysis."
  },
  {
    "objectID": "core/labs/lab-mca.html#mosaicplots",
    "href": "core/labs/lab-mca.html#mosaicplots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Mosaicplots",
    "text": "Mosaicplots\nMCA can be complemented by Mosaicplots, Double Decker plots, Chi-square tests, and Correspondence analyses between pair of variables.\n\n\n\n\n\n\nQuestion\n\n\n\nDraw a mosaic plot to visualize association between confidence levesl in Science and confidence level in Medicine."
  },
  {
    "objectID": "core/labs/lab-mca.html#further-references",
    "href": "core/labs/lab-mca.html#further-references",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Further references",
    "text": "Further references\nBarth, Alice and Schmitz, Andreas. 2018. Response quality and ideological dispositions: an integrative approach using geometric and classifying techniques. Quality & Quantity\n\nWhen analyzing survey data, response quality has consequential implications for substantial conclusions. Differences in response quality are usually explained by personality, or socio-demographic or cognitive characteristics. Little, however, is known about how respondents’ political attitudes, values, and opinions impact on quality aspects. This is a striking analytical omission, as potential associations between political values and various forms of response biases and artefacts call into question surveys’ ability to represent ‘public opinion’. In this contribution, response quality is traced back to respondents’ political and ideological dispositions. For this purpose, a relational understanding of response quality is applied that takes into account different aspects of response behaviors, as well as the interrelations between these indicators. Using data from the US General Social Survey (2010–2014), an empirical typology of response quality is created via finite mixture analysis. The resulting classes are then related to positions in the US field of ideological dispositions constructed via multiple correspondence analysis. The analyses reveal that there are (1) different combinations of response patterns and thus different empirical response types, and (2) that these types of response quality systematically vary with regard to the respondents’ political and ideological (dis)positions. Implications of the findings for public opinion surveys are discussed."
  },
  {
    "objectID": "core/labs/lab-cca.html#canonical-correlation-analysis",
    "href": "core/labs/lab-cca.html#canonical-correlation-analysis",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "Canonical Correlation Analysis",
    "text": "Canonical Correlation Analysis\n\\[C(X,Y) = \\mathbb{E}\\left[X Y^\\top\\right]\\]\n\\[\\begin{bmatrix} C_{xx} & C_{xy} \\\\ C_{xy}^{\\top} & C_{yy}\\end{bmatrix}\\]\n\nCode\\begin{bmatrix} C_{xx} & C_{xy} \\\\ C_{xy}^{\\top} & C_{yy}\\end{bmatrix}\n\n\nThe first canonical components are the solution of the next problem\n\n\n\n\n\n\nOptimization problem\n\n\n\n\\[\\begin{array}{lll}\\text{Maximize} & & u^\\top C_{xy}  v \\\\\\text{subject to} & & u^\\top C_{xx} uv=1 =v^\\top C_{yy} v     \\end{array}\\]\n\n\n\n\n\n\n\n\nProposition\n\n\n\nLet\n\\[U \\times D \\times V^\\top\\]\nbe a SVD of\n\\[C_{xx}^{-1/2} \\times C_{xy} \\times C_{yy}^{-1/2}\\]\nThe solution to the optimization problem above is\n\\[a = C_{xx}^{-1/2} u_1 \\qquad \\text{and} \\qquad b= S_{yy}^{-1/2} v_1\\]\nwhere \\(u_1\\) and \\(v_1\\) are the leading left and right singular vectors of \\(C_{xx}^{-1/2} \\times C_{xy} \\times C_{yy}^{-1/2}\\), that is the first column vectors of \\(U\\) and \\(V\\).\n\n\n\nProof: \n\n\n\n\n\n\nProposition\n\n\n\nA sequence of canonical components of \\(C_{xy}\\) can be obtained from the sequence of (extended) left and right singular vectors of \\(C_{xy}\\) with respect to \\(C_{xx}\\) and \\(C_{yy}\\)\n\n\nProof:"
  },
  {
    "objectID": "core/labs/lab-cca.html#applications",
    "href": "core/labs/lab-cca.html#applications",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "Applications",
    "text": "Applications"
  },
  {
    "objectID": "core/labs/lab-cca.html#references",
    "href": "core/labs/lab-cca.html#references",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "core/labs/lab-cca.html#canonical-correlation-analysis-cca-in-r",
    "href": "core/labs/lab-cca.html#canonical-correlation-analysis-cca-in-r",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "Canonical Correlation Analysis (CCA) in R\n",
    "text": "Canonical Correlation Analysis (CCA) in R\n\n\ncancor() from base package R\n\n/usr/lib/R/library/stats/help/cancor\nCode?LifeCycleSavings\n\nCodeLifeCycleSavings |&gt;\n  slice_sample(n=5)\n\n\n\nCodepairs(LifeCycleSavings, \n      panel = panel.smooth,\n      main = \"LifeCycleSavings data\")\n\n\n\nCodefm1 &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = LifeCycleSavings)\n \nsummary(fm1)\n\n\n\nCodepop &lt;- LifeCycleSavings |&gt; \n  select(starts_with('pop'))\noec &lt;- LifeCycleSavings |&gt; \n  select(-starts_with('pop'))\n  \nres.cca &lt;- cancor(pop, oec)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCheck that the different components of the output of cancor() satisfy all properties they should satisfy.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDesign a suite of tests (using testthat) that any contender of the implementation provided by package stats should pass.\n\n\nPackage CCA"
  },
  {
    "objectID": "core/labs-solutions/lab-cca.html#canonical-correlation-analysis",
    "href": "core/labs-solutions/lab-cca.html#canonical-correlation-analysis",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "Canonical Correlation Analysis",
    "text": "Canonical Correlation Analysis\n\\[C(X,Y) = \\mathbb{E}\\left[X Y^\\top\\right]\\]\n\\[\\begin{bmatrix} C_{xx} & C_{xy} \\\\ C_{xy}^{\\top} & C_{yy}\\end{bmatrix}\\]\nThe first canonical components are the solution of the next problem\n\n\n\n\n\n\nOptimization problem\n\n\n\n\\[\\begin{array}{lll}\\text{Maximize} & & u^\\top C_{xy}  v \\\\\\text{subject to} & & u^\\top C_{xx} uv=1 =v^\\top C_{yy} v     \\end{array}\\]\n\n\n\n\n\n\n\n\nProposition\n\n\n\nLet\n\\[U \\times D \\times V^\\top\\]\nbe a SVD of\n\\[C_{xx}^{-1/2} \\times C_{xy} \\times C_{yy}^{-1/2}\\]\nThe solution to the optimization problem above is\n\\[a = C_{xx}^{-1/2} u_1 \\qquad \\text{and} \\qquad b= S_{yy}^{-1/2} v_1\\]\nwhere \\(u_1\\) and \\(v_1\\) are the leading left and right singular vectors of \\(C_{xx}^{-1/2} \\times C_{xy} \\times C_{yy}^{-1/2}\\), that is the first column vectors of \\(U\\) and \\(V\\).\n\n\n\nProof: \n\n\n\n\n\n\nProposition\n\n\n\nA sequence of canonical components of \\(C_{xy}\\) can be obtained from the sequence of (extended) left and right singular vectors of \\(C_{xy}\\) with respect to \\(C_{xx}\\) and \\(C_{yy}\\)\n\n\nProof: \n\n\n\n\n\n\nProposition\n\n\n\nLet \\(H_X\\) (resp. \\(H_Y\\)) be orthorgonal projection matrix on the linear space spanned by the columns of \\(X\\) (resp. \\(Y\\)).\nCanonical correlations \\(ρ_1 \\geq \\ldots \\geq \\rho_s, \\ldots\\) are the positive square roots of the eigenvalues \\(\\lambda_1, \\ldots \\geq \\lambda_s, \\ldots\\) of \\(H_X \\times H_Y\\) (which are the same as \\(H_Y \\times H_X\\)): \\(ρ_s = λ_s\\)  Vectors \\(U^1, \\ldots, U^{p_1}\\) are the standardized eigenvectors corresponding to the decreasing eigenvalues \\(λ_1 \\geq \\ldots \\geq \\lambda_{p_1}\\) of \\(H_X  \\times H_Y\\)\nVectors \\(V^1, \\ldots, V^{p_2}\\) are the standardized eigenvectors corresponding to the decreasing eigenvalues \\(λ_1 \\geq \\ldots \\geq \\lambda_{p_2}\\) of \\(H_X  \\times H_Y\\)"
  },
  {
    "objectID": "core/labs-solutions/lab-cca.html#canonical-correlation-analysis-cca-in-r",
    "href": "core/labs-solutions/lab-cca.html#canonical-correlation-analysis-cca-in-r",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "Canonical Correlation Analysis (CCA) in R\n",
    "text": "Canonical Correlation Analysis (CCA) in R\n\n\ncancor() from base package R\n\nFunction cancor(x, y, xcenter=T, ycenter=T) computes the canonical correlations between two data matrices x and y. Henceforth we assume that the columns of x and y are centered. Matrices x and y have the same number \\(n\\) of rows. x (resp. y) has p1 (resp. p2) columns.\n\nThe canonical correlation analysis seeks linear combinations of the y variables which are well explained by linear combinations of the x variables. The relationship is symmetric as well explained is measured by correlations.\n\nThe result is a list of five components\n\n\ncor correlations.\n\nxcoef estimated coefficients for the x variables.\n\nycoef estimated coefficients for the y variables.\n\nOur assumption above allows us to assume xcenter and ycenter are zeros.\nThe next example is taken from the documentation. Use ?LiveCycleSavings to get more information on the dataset.\n\nCodeLifeCycleSavings |&gt; \n  as_tibble() |&gt;\n  slice_sample(n=5)\n\n# A tibble: 5 × 5\n     sr pop15 pop75   dpi  ddpi\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  12.1  23.3  4.41 1508.  3.93\n2  12.9  42.2  0.83  728.  4.56\n3  18.6  45.2  0.56  138.  5.14\n4  12.8  46.3  1.12  152.  2   \n5  13.2  23.8  4.43 2108.  3.82\n\n\n\nCodefm1 &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = LifeCycleSavings)\n \nsummary(fm1)\n\n\nCall:\nlm(formula = sr ~ pop15 + pop75 + dpi + ddpi, data = LifeCycleSavings)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.2422 -2.6857 -0.2488  2.4280  9.7509 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 28.5660865  7.3545161   3.884 0.000334 ***\npop15       -0.4611931  0.1446422  -3.189 0.002603 ** \npop75       -1.6914977  1.0835989  -1.561 0.125530    \ndpi         -0.0003369  0.0009311  -0.362 0.719173    \nddpi         0.4096949  0.1961971   2.088 0.042471 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.803 on 45 degrees of freedom\nMultiple R-squared:  0.3385,    Adjusted R-squared:  0.2797 \nF-statistic: 5.756 on 4 and 45 DF,  p-value: 0.0007904\n\n\n\nCodepop &lt;- LifeCycleSavings |&gt; \n  dplyr::select(starts_with('pop'))\noec &lt;- LifeCycleSavings |&gt; \n  dplyr::select(-starts_with('pop'))\n  \nres.cca &lt;- cancor(pop, oec)\n\nres.cca$cor\n\n[1] 0.8247966 0.3652762\n\n\nThis tells us that highest possible linear correlation beween a linear combination of pop15, pop75 and a linear combination of sr, dpi, ddpi is res.cca$cor[1]. The coefficients of the corresponding linear combinations can be found on the rows of components xcoef and ycoef\n\n\n\n\n\n\nQuestion\n\n\n\nCheck that the different components of the output of cancor() satisfy all properties they should satisfy.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodecc &lt;- cor(\n  as.matrix(pop) %*% as.matrix(res.cca$xcoef), \n  as.matrix(oec) %*% as.matrix(res.cca$ycoef)\n  )\n\ndiag(cc) - res.cca$cor\n\n[1]  2.220446e-16 -5.551115e-17\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDesign a suite of tests (using testthat) that any contender of the implementation provided by package stats should pass.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nTODO\n\n\nPackage CCA\n\nAbstract of CCA: An R Package to Extend Canonical Correlation Analysis\n\nCanonical correlations analysis (CCA) is an exploratory statistical method to highlight correlations between two data sets acquired on the same experimental units. The cancor() function in R (R Development Core Team 2007) performs the core of computations but further work was required to provide the user with additional tools to facilitate the interpretation of the results.\n\nAs in PCA, CA, MCA, several kinds of graphical representations can be displayed from the results of CCA:\n\na barplot of the squared canonical correlations (which tells us about the low rank approximations of \\(H_X \\times H_Y\\))\nscatter plots for the initial variables \\(X^j\\) and \\(Y^k\\) (ako correlation circles)\nscatter plots for the individuals (rows)\nbiplots"
  },
  {
    "objectID": "core/labs-solutions/lab-cca.html#applications",
    "href": "core/labs-solutions/lab-cca.html#applications",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "Applications",
    "text": "Applications\n\n\n\n\n\n\nQuestion\n\n\n\n\nLoad nutrimouse dataset from CCA.\nInsert the 4 elements of list nutrimouse in the global environment (see list2env())\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodestopifnot(\n  require(CCA)\n)\n\ndata(\"nutrimouse\")\n\ne &lt;- list2env(nutrimouse, .GlobalEnv)\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nCompute the cross correlation matrix between gene and lipid\n\nVisualize the cross correlation matrix\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeY &lt;- as.matrix(gene)   # 40 x 120\nX &lt;- as.matrix(lipid)  # 40 x 20\n\n\n\nCodec_XY = corrr::correlate(cbind(X, Y))\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n\nCodec_XY_long &lt;- c_XY |&gt;\n  tidyr::pivot_longer(cols=-c(term), names_to=\"term2\", values_to=\"cor\")  \n\n\n\nCodep  &lt;- c_XY_long |&gt;\n  dplyr::filter(\n    term %in% names(gene),\n    term2 %in% names(lipid)\n  ) |&gt;\n  ggplot() +\n  aes(x=term2, y=term, fill=abs(cor)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  xlab(\"diet\") +\n  ylab(\"gene\")\n\np |&gt; plotly::ggplotly()\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nCompute the canonical correlations between gene and lipid, save the result in res.cca\n\nCheck the canonical correlations.\nComment\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCoderes.cca &lt;- cancor(as.matrix(gene), as.matrix(lipid))\n\n\n\nCodesum(res.cca$cor)\n\n[1] 21\n\n\n\\(H_X\\times H_Y\\) has 21 eigenvalues equal to \\(1\\). As the subspaces defined by the columns in gene and lipid have dimensions at most 21 and 40, \\(H_X\\times H_Y\\) equals the projection of \\(\\mathbb{R}^{40}\\) over the smallest subspace.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nSample 10 columns from gene and lipid and repeat the operation\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeset.seed(42)\nn &lt;- 10\n\nss_gene &lt;- gene |&gt;\n  dplyr::select(sample(names(gene),  n)) |&gt;\n  scale()\n\nss_lipid&lt;- lipid |&gt;\n  dplyr::select(sample(names(lipid),  n)) |&gt;\n  scale()\n\nres.cca &lt;- cancor(as.matrix(ss_gene), as.matrix(ss_lipid)) \n\nres.cc &lt;- cc(X=as.matrix(ss_gene), Y=as.matrix(ss_lipid))\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nScreeplot\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCoderes.cca$cor |&gt; \n  as_tibble() |&gt;\n  gt::gt() |&gt;\n  gt::fmt_scientific() |&gt;\n  gt::tab_caption(\"Canonical correlations between `gene` columns of nutrimouse and  `lipid` columns\")\n\n\n\n\nCanonical correlations between `gene` columns of nutrimouse and `lipid` columns\n\nvalue\n\n\n\n9.62 × 10−1\n\n\n\n8.82 × 10−1\n\n\n\n7.90 × 10−1\n\n\n\n7.35 × 10−1\n\n\n\n6.96 × 10−1\n\n\n\n5.66 × 10−1\n\n\n\n5.09 × 10−1\n\n\n\n2.67 × 10−1\n\n\n\n1.58 × 10−1\n\n\n\n6.62 × 10−2\n\n\n\n\n\n\n\n\nCoderes.cca$cor |&gt; \n  as_tibble() |&gt;\n  mutate(PC=as.factor(1:n), eig=value^2, percent=eig, cumulative=cumsum(eig)) |&gt;\n  ggplot() +\n    aes(x=PC, y=eig, label=eig) +\n    geom_col(fill=\"white\", color=\"black\") +\n    theme_minimal() +\n    labs(\n      title=\"Squared Canonical Correlations\",\n      subtitle=\"sample of 10 genes and 10 lipids\", \n      caption=\"nutrimouse data\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBuild a correlation circle\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe use function plt.cc from CCA.\n\nCodeCCA::plt.cc(\n  res.cc, \n  var.label=T,\n  ind.names = str_c(genotype, diet, sep=\"-\")\n)"
  },
  {
    "objectID": "core/labs-solutions/lab-cca.html#references",
    "href": "core/labs-solutions/lab-cca.html#references",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "References",
    "text": "References\nhttps://www.jstatsoft.org/article/view/v023i12"
  },
  {
    "objectID": "core/labs-solutions/lab-cca.html",
    "href": "core/labs-solutions/lab-cca.html",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "",
    "text": "Loading required package: testthat\n\n\nLoading required package: corrr\n\n\nLoading required package: magrittr\n\n\n\nAttaching package: 'magrittr'\n\n\nThe following objects are masked from 'package:testthat':\n\n    equals, is_less_than, not\n\n\nLoading required package: lobstr\n\n\nLoading required package: sloop\n\n\nLoading required package: ggforce\n\n\nLoading required package: ggplot2\n\n\nLoading required package: gt\n\n\n\nAttaching package: 'gt'\n\n\nThe following object is masked from 'package:testthat':\n\n    matches\n\n\nLoading required package: glue\n\n\nLoading required package: skimr\n\n\n\nAttaching package: 'skimr'\n\n\nThe following object is masked from 'package:corrr':\n\n    focus\n\n\nThe following object is masked from 'package:testthat':\n\n    matches\n\n\nLoading required package: patchwork\n\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::edition_get()     masks testthat::edition_get()\n✖ magrittr::equals()       masks testthat::equals()\n✖ tidyr::extract()         masks magrittr::extract()\n✖ dplyr::filter()          masks stats::filter()\n✖ magrittr::is_less_than() masks testthat::is_less_than()\n✖ purrr::is_null()         masks testthat::is_null()\n✖ dplyr::lag()             masks stats::lag()\n✖ readr::local_edition()   masks testthat::local_edition()\n✖ dplyr::matches()         masks tidyr::matches(), skimr::matches(), gt::matches(), testthat::matches()\n✖ magrittr::not()          masks testthat::not()\n✖ purrr::set_names()       masks magrittr::set_names()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: ggfortify\n\nLoading required package: viridisLite"
  },
  {
    "objectID": "core/labs/lab-kmeans.html",
    "href": "core/labs/lab-kmeans.html",
    "title": "Lab: Clustering k-means",
    "section": "",
    "text": "Codestopifnot(\n  require(gt),\n  require(sloop),\n  require(lobstr), \n  require(skimr),\n  require(GGally),\n  require(patchwork),\n  require(ggforce),\n  require(glue),\n  require(ggfortify),\n  require(ggvoronoi),\n  require(magrittr),\n  require(broom),\n  require(tidyclust),\n  require(tidyverse)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(\n  theme_minimal(base_size=9, \n                base_family = \"Helvetica\")\n)\nCodeknitr::opts_chunk$set(\n  message = FALSE,\n  warning = FALSE,\n  comment=NA,\n  prompt=FALSE,\n  cache=FALSE,\n  echo=TRUE,\n  results='asis'\n)\nCodegc &lt;- options(ggplot2.discrete.colour=\"viridis\")\ngc &lt;- options(ggplot2.discrete.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.colour=\"viridis\")"
  },
  {
    "objectID": "core/labs/lab-kmeans.html#foreword",
    "href": "core/labs/lab-kmeans.html#foreword",
    "title": "Lab: Clustering k-means",
    "section": "Foreword",
    "text": "Foreword\nThis lab is dedicated to the k-means clustering method. In words, k-means takes as input a collection of points in \\(\\mathbb{R}^d\\) (a numerical dataset) and a positive integer \\(k\\). It returns a collection of \\(k\\) points (the centers) from \\(\\mathbb{R}^d\\). The centers define a Voronoï tesselation/partition/diagran of \\(\\mathbb{R}^d\\). The Voronoï cells define a clustering of the original dataset."
  },
  {
    "objectID": "core/labs/lab-kmeans.html#voronoi-tesselationpartitiondiagram",
    "href": "core/labs/lab-kmeans.html#voronoi-tesselationpartitiondiagram",
    "title": "Lab: Clustering k-means",
    "section": "Voronoi tesselation/partition/diagram",
    "text": "Voronoi tesselation/partition/diagram\nWikipedia on Voronoï diagrams\nIn the next chunk, we generate a Voronoï diagram on \\(\\mathbb{R}^2\\) with \\(100\\) cells defined from \\(100\\) random points drawn from the uniform distribution on a square. Function stat_voronoi() comes from ggvoronoi\n\nCodeset.seed(45056)\n\npoints &lt;- tibble(\n  x=runif(100, 0, 1),\n  y=runif(100, 0, 1),\n  distance = sqrt((x-100)^2 + (y-100)^2)\n) \n\np &lt;- ggplot(points) +\n    aes(x=x, y=y) +\n    geom_point(size=.2) +\n    coord_fixed() +\n    xlim(c(-.25, 2.25)) +\n    ylim(c(-.25, 2.25)) \n\np + (p + stat_voronoi(geom=\"path\")) +\n  patchwork::plot_annotation(\n    title=\"Voronoi tesselation\",\n    subtitle = \"Left: 100 random points\\nRight: Voronoï diagram\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo adjacent Voronoï cells are separated by a (possibly semi-infinite) line segment\nLet the so-called centers be denoted by \\(c_1, \\ldots, c_n\\). They form the codebook \\(\\mathcal{C}\\).\n\nThe Voronoï cell with center \\(c_i\\) is defined by \\[\\left\\{x : x \\in \\mathbb{R}^d, \\qquad \\| x- c_i \\|_2 = \\min_{j \\leq n} \\|x -c_j\\|_2\\right\\}\\]\n\nThe center of a Voronoï cell is usually not its barycenter"
  },
  {
    "objectID": "core/labs/lab-kmeans.html#k-means-objective-function",
    "href": "core/labs/lab-kmeans.html#k-means-objective-function",
    "title": "Lab: Clustering k-means",
    "section": "\nk-means objective function",
    "text": "k-means objective function\n\n\n\n\n\n\nDefinition\n\n\n\nThe \\(k\\)-means algorithm aims at building a codebook \\(\\mathcal{C}\\) that minimizes\n\\[\\mathcal{C} \\mapsto \\sum_{i=1}^n \\min_{c \\in \\mathcal{C}}  \\Vert X_i - c\\Vert_2^2\\]\nover all codebooks with given cardinality\nIf \\(c \\in \\mathcal{C}\\) is the closest centroid to \\(X \\in \\mathbb{R}^p\\),\n\\[\\|c - X\\|^2\\]\nis the quantization/reconstruction error suffered when using codebook \\(\\mathcal{C}\\) to approximate \\(X\\)\n\n\n\n\n\n\n\n\n If there are no restrictions on the dimension of the input space, on the number of centroids, or on sample size, computing an optimal codebook is a \\(\\mathsf{NP}\\) -hard problem"
  },
  {
    "objectID": "core/labs/lab-kmeans.html#the-kmeans-function",
    "href": "core/labs/lab-kmeans.html#the-kmeans-function",
    "title": "Lab: Clustering k-means",
    "section": "The kmeans() function",
    "text": "The kmeans() function\nkmeans() is a wrapper for a collection of Algorithms that look like the Lloyd algorithm\n\nInitialize\n\nChoose \\(k\\) centroids\n\n\nIterations: Two phases\n\nMovement\n\nAssign each sample point to the closest centroid (Assign each sample point to a class in the Voronoi partition defined by the centroids)\n\nUpdate\n\nFor each class in the current Voronoi partition, update the centroid so as to minimize the Within Cluster Sum of Squared distances.\n\n\n No guarantee to converge to a global optimum!\nProceeed by trial and error.\nRepeat the algorithm and keep the best result."
  },
  {
    "objectID": "core/labs/lab-kmeans.html#iris-data",
    "href": "core/labs/lab-kmeans.html#iris-data",
    "title": "Lab: Clustering k-means",
    "section": "Iris data",
    "text": "Iris data\n\n\n\n\n\n\nQuestion\n\n\n\nRun kmeans() on the projection of the Iris dataset (data(iris)) on the Petal plane.\nCheck ?iris and https://en.wikipedia.org/wiki/Iris_flower_data_set for more on this (historical) dataset.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nCheck the attributes of object kms\n\nUnclass the object and check the attributes again."
  },
  {
    "objectID": "core/labs/lab-kmeans.html#summarizing-a-clustering",
    "href": "core/labs/lab-kmeans.html#summarizing-a-clustering",
    "title": "Lab: Clustering k-means",
    "section": "Summarizing a clustering",
    "text": "Summarizing a clustering\n\n\n\n\n\n\nQuestion\n\n\n\nCheck the structure of objects of class kmeans and use broom::tidy() to get a summary.\nCompare with summary() from base R"
  },
  {
    "objectID": "core/labs/lab-kmeans.html#visualizing-a-clustering",
    "href": "core/labs/lab-kmeans.html#visualizing-a-clustering",
    "title": "Lab: Clustering k-means",
    "section": "Visualizing a clustering",
    "text": "Visualizing a clustering\n\n\n\n\n\n\nQuestion\n\n\n\nUse broom::augment() and broom::tidy() to prepare two dataframes that will allow you to overlay a scatterplot of the dataset and a Voronoï diagram defined by the centers output by kmeans().\nCompare the result with plot()\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nRedo the same operations but choose the Sepal.xxx dimension.\nDesign a function to avoid repetitive coding."
  },
  {
    "objectID": "core/labs/lab-kmeans.html#playing-with-k",
    "href": "core/labs/lab-kmeans.html#playing-with-k",
    "title": "Lab: Clustering k-means",
    "section": "Playing with \\(k\\)\n",
    "text": "Playing with \\(k\\)\n\nThe number of cells/clusters may not be given a priori. Conducting clustering using a method like kmeans requires picking a reasonable choice for k.\n\n\n\n\n\n\nQuestion\n\n\n\nPerform kmeans clustering with \\(k=2\\). Use glance, tidy, augment to discuss the result.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPerform k-means for \\(k=2, ... 10\\), plot within sum of squares as function of \\(k\\). Comment."
  },
  {
    "objectID": "core/labs/lab-kmeans.html#lloyds-iterations",
    "href": "core/labs/lab-kmeans.html#lloyds-iterations",
    "title": "Lab: Clustering k-means",
    "section": "Lloyd’s iterations",
    "text": "Lloyd’s iterations\nThe kmeans function does not minimize the kmeans cost. It offers a collection of iterative algorithms that aim at approximately minimizing the cost.\n\n\n\n\n\n\n\nInitialize\n\nChoose \\(k\\) centroids\n\n\nIterations: Two phases\n\nMovement\n\nAssign each sample point to the closest centroid (Assign each sample point to a class in the Voronoi partition defined by the centroids)\n\nUpdate\n\nFor each class in the current Voronoi partition, update the centroid so as to minimize the Within Cluster Sum of Squared distances."
  },
  {
    "objectID": "core/labs/lab-kmeans.html#revisiting-the-swiss-fertility-data",
    "href": "core/labs/lab-kmeans.html#revisiting-the-swiss-fertility-data",
    "title": "Lab: Clustering k-means",
    "section": "Revisiting the swiss fertility data",
    "text": "Revisiting the swiss fertility data\n\n\n\n\n\n\nQuestion\n\n\n\nRecall the dataset used in Lab PCA\nPerform kmeans clustering in original coordinates and kmeans clustering in the first principal coordinates plane\nCompare the results"
  },
  {
    "objectID": "core/labs/lab-kmeans.html#revisiting-the-mortality-dataset",
    "href": "core/labs/lab-kmeans.html#revisiting-the-mortality-dataset",
    "title": "Lab: Clustering k-means",
    "section": "Revisiting the mortality dataset",
    "text": "Revisiting the mortality dataset\n\n\n\n\n\n\nQuestion\n\n\n\nRecall the dataset used in Lab CA\nPerform kmeans clustering of categories in the row principal coordinates and the column principal coordinates"
  },
  {
    "objectID": "core/labs/lab-kmeans.html#references",
    "href": "core/labs/lab-kmeans.html#references",
    "title": "Lab: Clustering k-means",
    "section": "References",
    "text": "References\nVignette k_means from tidyclust"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html",
    "href": "core/labs-solutions/lab-kmeans.html",
    "title": "Lab: Clustering k-means",
    "section": "",
    "text": "Codestopifnot(\n  require(gt),\n  require(sloop),\n  require(lobstr), \n  require(skimr),\n  require(GGally),\n  require(patchwork),\n  require(ggforce),\n  require(glue),\n  require(ggfortify),\n  require(ggvoronoi),\n  require(magrittr),\n  require(broom),\n  require(tidyclust),\n  require(tidyverse)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(\n  theme_minimal(base_size=9, \n                base_family = \"Helvetica\")\n)\nCodeknitr::opts_chunk$set(\n  message = FALSE,\n  warning = FALSE,\n  comment=NA,\n  prompt=FALSE,\n  cache=FALSE,\n  echo=TRUE,\n  results='asis'\n)\nCodegc &lt;- options(ggplot2.discrete.colour=\"viridis\")\ngc &lt;- options(ggplot2.discrete.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.colour=\"viridis\")"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#foreword",
    "href": "core/labs-solutions/lab-kmeans.html#foreword",
    "title": "Lab: Clustering k-means",
    "section": "Foreword",
    "text": "Foreword\nThis lab is dedicated to the k-means clustering method. In words, k-means takes as input a collection of points in \\(\\mathbb{R}^d\\) (a numerical dataset) and a positive integer \\(k\\). It returns a collection of \\(k\\) points (the centers) from \\(\\mathbb{R}^d\\). The centers define a Voronoï tesselation/partition/diagran of \\(\\mathbb{R}^d\\). The Voronoï cells define a clustering of the original dataset."
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#voronoi-tesselationpartitiondiagram",
    "href": "core/labs-solutions/lab-kmeans.html#voronoi-tesselationpartitiondiagram",
    "title": "Lab: Clustering k-means",
    "section": "Voronoi tesselation/partition/diagram",
    "text": "Voronoi tesselation/partition/diagram\nWikipedia on Voronoï diagrams\nIn the next chunk, we generate a Voronoï diagram on \\(\\mathbb{R}^2\\) with \\(100\\) cells defined from \\(100\\) random points drawn from the uniform distribution on a square. Function stat_voronoi() comes from ggvoronoi\n\nCodeset.seed(45056)\n\npoints &lt;- tibble(\n  x=runif(100, 0, 1),\n  y=runif(100, 0, 1),\n  distance = sqrt((x-100)^2 + (y-100)^2)\n) \n\np &lt;- ggplot(points) +\n    aes(x=x, y=y) +\n    geom_point(size=.2) +\n    coord_fixed() +\n    xlim(c(-.25, 2.25)) +\n    ylim(c(-.25, 2.25)) \n\np + (p + stat_voronoi(geom=\"path\")) +\n  patchwork::plot_annotation(\n    title=\"Voronoi tesselation\",\n    subtitle = \"Left: 100 random points\\nRight: Voronoï diagram\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo adjacent Voronoï cells are separated by a (possibly semi-infinite) line segment\nLet the so-called centers be denoted by \\(c_1, \\ldots, c_n\\). They form the codebook \\(\\mathcal{C}\\).\n\nThe Voronoï cell with center \\(c_i\\) is defined by \\[\\left\\{x : x \\in \\mathbb{R}^d, \\qquad \\| x- c_i \\|_2 = \\min_{j \\leq n} \\|x -c_j\\|_2\\right\\}\\]\n\nThe center of a Voronoï cell is usually not its barycenter"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#k-means-objective-function",
    "href": "core/labs-solutions/lab-kmeans.html#k-means-objective-function",
    "title": "Lab: Clustering k-means",
    "section": "\nk-means objective function",
    "text": "k-means objective function\n\n\n\n\n\n\nDefinition\n\n\n\nThe \\(k\\)-means algorithm aims at building a codebook \\(\\mathcal{C}\\) that minimizes\n\\[\\mathcal{C} \\mapsto \\sum_{i=1}^n \\min_{c \\in \\mathcal{C}}  \\Vert X_i - c\\Vert_2^2\\]\nover all codebooks with given cardinality\nIf \\(c \\in \\mathcal{C}\\) is the closest centroid to \\(X \\in \\mathbb{R}^p\\),\n\\[\\|c - X\\|^2\\]\nis the quantization/reconstruction error suffered when using codebook \\(\\mathcal{C}\\) to approximate \\(X\\)\n\n\n\n\n\n\n\n\n If there are no restrictions on the dimension of the input space, on the number of centroids, or on sample size, computing an optimal codebook is a \\(\\mathsf{NP}\\) -hard problem"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#the-kmeans-function",
    "href": "core/labs-solutions/lab-kmeans.html#the-kmeans-function",
    "title": "Lab: Clustering k-means",
    "section": "The kmeans() function",
    "text": "The kmeans() function\nkmeans() is a wrapper for a collection of Algorithms that look like the Lloyd algorithm\n\nInitialize\n\nChoose \\(k\\) centroids\n\n\nIterations: Two phases\n\nMovement\n\nAssign each sample point to the closest centroid (Assign each sample point to a class in the Voronoi partition defined by the centroids)\n\nUpdate\n\nFor each class in the current Voronoi partition, update the centroid so as to minimize the Within Cluster Sum of Squared distances.\n\n\n No guarantee to converge to a global optimum!\nProceeed by trial and error.\nRepeat the algorithm and keep the best result."
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#iris-data",
    "href": "core/labs-solutions/lab-kmeans.html#iris-data",
    "title": "Lab: Clustering k-means",
    "section": "Iris data",
    "text": "Iris data\n\n\n\n\n\n\nQuestion\n\n\n\nRun kmeans() on the projection of the Iris dataset (data(iris)) on the Petal plane.\nCheck ?iris and https://en.wikipedia.org/wiki/Iris_flower_data_set for more on this (historical) dataset.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe look for a partition into three cells.\n\nCodedata(iris)\n\nkms &lt;- iris |&gt; \n  select(starts_with(\"Petal\")) |&gt;\n  kmeans(3)\n\nclass(kms)\n\n[1] \"kmeans\"\n\nCodesloop::otype(kms)\n\n[1] \"S3\"\n\n\nThe result is an object of class kmeans. The class is equiped with broom methods.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nCheck the attributes of object kms\n\nUnclass the object and check the attributes again.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeattributes(kms)\n\n$names\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n$class\n[1] \"kmeans\"\n\nCodeukms &lt;- unclass(kms)\nattributes(ukms)\n\n$names\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nCodeclass(kms)\n\n[1] \"kmeans\"\n\nCodekms_bornagain &lt;- structure(\n  ukms, \n  class=\"kmeans\")\n\nsloop::otype(kms_bornagain)\n\n[1] \"S3\"\n\nCodeclass(kms_bornagain)\n\n[1] \"kmeans\"\n\n\nObjects of class kmeans have two attributes names and class. Because of the class attribute, objects of class kmeans are not just lists with named elements."
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#summarizing-a-clustering",
    "href": "core/labs-solutions/lab-kmeans.html#summarizing-a-clustering",
    "title": "Lab: Clustering k-means",
    "section": "Summarizing a clustering",
    "text": "Summarizing a clustering\n\n\n\n\n\n\nQuestion\n\n\n\nCheck the structure of objects of class kmeans and use broom::tidy() to get a summary.\nCompare with summary() from base R\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodedf_centers &lt;- select(iris, starts_with(\"Petal\")) |&gt;\n  kmeans(centers = 3) |&gt; \n  broom::tidy() \n\ndf_centers |&gt;  \n  gt::gt() |&gt;\n  gt::fmt_number(decimals = 2) |&gt;\n  gt::tab_caption(\"Iris clustering in the Petal plane, kmeans with 3 clusters\")\n\n\n\n\nIris clustering in the Petal plane, kmeans with 3 clusters\n\nPetal.Length\nPetal.Width\nsize\nwithinss\ncluster\n\n\n\n4.27\n1.34\n52.00\n13.06\n1\n\n\n1.46\n0.25\n50.00\n2.02\n2\n\n\n5.60\n2.04\n48.00\n16.29\n3\n\n\n\n\n\n\n\nHow are the rows related to clusters?\nWhat are the coordinates good for?\nWhat does the size column mean?\n\nwithinss stands for Within Sum of Squares. How is it computed? Why is it useful?"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#visualizing-a-clustering",
    "href": "core/labs-solutions/lab-kmeans.html#visualizing-a-clustering",
    "title": "Lab: Clustering k-means",
    "section": "Visualizing a clustering",
    "text": "Visualizing a clustering\n\n\n\n\n\n\nQuestion\n\n\n\nUse broom::augment() and broom::tidy() to prepare two dataframes that will allow you to overlay a scatterplot of the dataset and a Voronoï diagram defined by the centers output by kmeans().\nCompare the result with plot()\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeq &lt;- kms |&gt; \n  augment(iris) |&gt;\n  ggplot() +\n  aes(x=Petal.Length, \n      y=Petal.Width\n      ) +\n  geom_point(aes(shape=Species), size=1, show.legend = F) +\n  coord_fixed()\n\nqq &lt;-  (q + geom_point(aes(shape=Species, \n                           colour=.cluster), \n                       size=1))+\n  stat_voronoi(data = df_centers,   #&lt;&lt;\n               geom=\"path\",\n               outline=data.frame(x=c(0, 7, 7, 0), \n                                  y=c(0, 0, 3, 3))\n               ) +\n  geom_point(data = df_centers,   #&lt;&lt;\n             colour = \"black\",\n             shape=\"+\",\n             size=5)  \n\nq / qq +\n  plot_annotation(title = \"Kmeans over Iris dataset, k=3\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodegeom_sugar &lt;- list(\n    stat_voronoi(data = df_centers,\n                 geom=\"path\",\n                 alpha=.5,\n                 outline = tribble(~x, ~y,\n                                   0., 0.,\n                                   7., 0.,\n                                   7., 3,\n                                   0., 3) \n                 ),\n    geom_point(data = df_centers,   \n               colour = \"black\",\n               shape=\"+\",\n               size=5),\n    coord_fixed(),\n    labs(col=\"Voronoï cells\")\n)\n\n\n\nCodebroom::augment(kms, iris) |&gt;\n  ggplot(aes(x=Petal.Length, y=Petal.Width)) +\n  geom_point(aes(shape=Species, color=.cluster)) +\n  geom_sugar \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodekms |&gt; \n  autoplot(data=iris)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nRedo the same operations but choose the Sepal.xxx dimension.\nDesign a function to avoid repetitive coding.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeplot_km_centroids &lt;- function(augmented_km, centroids, col1, col2){\n\n  outline &lt;- augmented_km |&gt;\n    dplyr::select({{col1}}, {{col2}}) |&gt;\n    dplyr::rename(x={{col1}}, y={{col2}}) |&gt; \n    summarise(across(everything(), .fns=list(\"min\"= min, \"max\"=max), .names=\"{.col}_{.fn}\"))\n\n  tb_outline &lt;- tibble(\n    x = with(outline, c(x_min-1.0, x_max+1.0, x_max+1.0, x_min-1.0)),\n    y = with(outline, rep(c(y_min-1, y_max+1), each=2)),\n    group=rep(1, 4)\n  )\n\n  p &lt;- augmented_km |&gt; \n  ggplot() +\n  aes(\n    x={{col1}}, \n    y={{col2}}) +\n  geom_point(aes(colour=.cluster))  +\n  stat_voronoi(data = centroids,\n               geom=\"path\",\n               outline= tb_outline\n  ) +\n  geom_point(data = centroids,\n             colour = \"black\",\n             shape=\"+\",\n             size=5) +\n  coord_fixed() +\n  theme_minimal()\n\n  if (has_rownames(augmented_km)) {\n    p &lt;- p +\n      ggrepel::geom_label_repel(\n        aes(colour=.cluster, \n            label=`.rownames`))\n  }\n\n  return(p)\n}\n\n\n\n\n\nCodekms &lt;- kmeans(select(iris, Sepal.Length, Sepal.Width), 3)\n\n\nplot_km_centroids(\n  augment(kms, iris), \n  tidy(kms), \n  Sepal.Length, \n  Sepal.Width\n)"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#playing-with-k",
    "href": "core/labs-solutions/lab-kmeans.html#playing-with-k",
    "title": "Lab: Clustering k-means",
    "section": "Playing with \\(k\\)\n",
    "text": "Playing with \\(k\\)\n\nThe number of cells/clusters may not be given a priori. Conducting clustering using a method like kmeans requires picking a reasonable choice for k.\n\n\n\n\n\n\nQuestion\n\n\n\nPerform kmeans clustering with \\(k=2\\). Use glance, tidy, augment to discuss the result.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodedf &lt;- select(iris, \n             starts_with(\"Sepal\"))\n\nkms &lt;- kmeans(df, 2)\n\n\n\nCodeplot_km_centroids(\n  augment(kms, iris),\n  tidy(kms),\n  Sepal.Length,\n  Sepal.Width\n)\n\n\n\n\n\n\n\nWe can compare the spread between inner and outer sum of squares for clusterings with \\(k \\in 2, 3\\).\n\nCodebind_rows(glance(kms),\n          glance(kmeans(df, centers=3,\n              nstart = 32L))) |&gt; \n  mutate(k=c(2, 3))  |&gt;\n  gt::gt()\n\n\n\n\n\ntotss\ntot.withinss\nbetweenss\niter\nk\n\n\n\n130.4753\n58.2150\n72.26027\n1\n2\n\n\n130.4753\n37.0507\n93.42456\n3\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPerform k-means for \\(k=2, ... 10\\), plot within sum of squares as function of \\(k\\). Comment.\n\n\n\nCodetmp &lt;-map_dfr(2:10, ~ glance(kmeans(df, \n                                   centers=.,\n                                   nstart = 32L))) |&gt; \n  rowid_to_column(var=\"k\") |&gt; \n  mutate(k=k+1, across(where(is.numeric), ~ signif(.x, 3))) \n\n\n\nCodetmp |&gt;\n  ggplot(aes(x=forcats::as_factor(k), y=tot.withinss/totss)) +\n  geom_col(width=.25) +\n  ggtitle(\"Iris data\", \"WithinSS/Total Sum of Squares as a function of k\") +\n  xlab(\"k\") +\n  ylab(\"Within Clusters Sum of Squares (relative)\") +\n  scale_x_discrete(breaks=as.character(2:10), labels=as.character(2:10))\n\n\n\n\n\n\n\n\nCodekms &lt;- kmeans(df, 4)\niris4 &lt;- broom::augment(kms, iris)\n\n\nplot_km_centroids(\n  augment(kms, iris),\n  tidy(kms),\n  Sepal.Length,\n  Sepal.Width\n)  +\n ggtitle(label=\"Kmeans Iris data\",\n         subtitle=\"k=4\") +\n labs(col=\"Clusters\")\n\n\n\n\n\n\n\n\nCodebroom::tidy(kmeans(df, 4)) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(decimals = 2)\n\n\n\n\n\nSepal.Length\nSepal.Width\nsize\nwithinss\ncluster\n\n\n\n5.52\n2.61\n33.00\n5.97\n1\n\n\n5.02\n3.45\n49.00\n11.57\n2\n\n\n6.27\n2.91\n41.00\n4.85\n3\n\n\n7.10\n3.11\n27.00\n6.02\n4"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#lloyds-iterations",
    "href": "core/labs-solutions/lab-kmeans.html#lloyds-iterations",
    "title": "Lab: Clustering k-means",
    "section": "Lloyd’s iterations",
    "text": "Lloyd’s iterations\nThe kmeans function does not minimize the kmeans cost. It offers a collection of iterative algorithms that aim at approximately minimizing the cost.\n\n\n\n\n\n\n\nInitialize\n\nChoose \\(k\\) centroids\n\n\nIterations: Two phases\n\nMovement\n\nAssign each sample point to the closest centroid (Assign each sample point to a class in the Voronoi partition defined by the centroids)\n\nUpdate\n\nFor each class in the current Voronoi partition, update the centroid so as to minimize the Within Cluster Sum of Squared distances.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodekm &lt;- list(centers=df[1:3, ]) # stupid initialization\n\nsequence &lt;- list()\n\nfor (i in 1:20) {\n  km &lt;- kmeans(df,\n               km$centers,\n               algorithm = \"Lloyd\",\n               iter.max = 1)\n  sequence[[length(sequence)+1]] &lt;- force(km)\n}\n\n\n\nCodeadd_voronoi &lt;- function(p, kmscenters, marker){\n  p +\n    geom_point(data=data.frame(kmscenters),         #&lt;&lt;\n               mapping=aes(x=Sepal.Length, y=Sepal.Width),\n               shape=marker,\n               col=\"black\",\n               size=5) +\n    stat_voronoi(data = as.data.frame(kmscenters),  #&lt;&lt;\n                 aes(x=Sepal.Length,y=Sepal.Width),\n                 geom=\"path\",\n                 outline=data.frame(x=c(4, 8, 8, 4),\n                                    y=c(2, 2, 4.5, 4.5)))\n}\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodei &lt;- 2\n\np &lt;- broom::augment(sequence[[i]], iris) |&gt;\n  ggplot() +\n  coord_fixed(ratio=1) +\n  geom_point(aes(x=Sepal.Length, y=Sepal.Width, shape=Species, col=.cluster)) +\n  ggtitle(\"Kmeans Lloyd's algorithm\", \"Iris data\")\n\np |&gt;\n  add_voronoi(sequence[[i]]$centers, marker=\"o\") +   #&lt;&lt;\n  labs(colour=paste(\"Cluster, step \", i- 1))\n\n\n\n\n\n\n\n\nCodei &lt;- 2\n\n(p %+%\n  broom::augment(sequence[[i]], iris)) |&gt;\n  add_voronoi(sequence[[i]]$centers, marker='+') +   #&lt;&lt;\n  geom_point(data=data.frame(sequence[[2]]$centers),   #&lt;&lt;\n             mapping=aes(x=Sepal.Length, y=Sepal.Width),\n             shape=\"o\", col=\"black\", size=5) +\n  labs(colour=paste(\"Cluster, step \", i- 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodei &lt;- 3\n\n(p %+%\n  broom::augment(sequence[[i]], iris)) |&gt;\n  add_voronoi(sequence[[i]]$centers, marker='+') +   #&lt;&lt;\n  geom_point(data=data.frame(sequence[[2]]$centers),   #&lt;&lt;\n             mapping=aes(x=Sepal.Length, y=Sepal.Width),\n             shape=\"o\", col=\"black\", size=5) +\n  labs(colour=paste(\"Cluster, step \", i- 1))\n\n\n\n\n\n\n\n\nCodei &lt;- 5\n\n(p %+%\n  broom::augment(sequence[[i]], iris)) |&gt;\n  add_voronoi(sequence[[i]]$centers, marker='*') +   #&lt;&lt;\n  geom_point(data=data.frame(sequence[[2]]$centers),   #&lt;&lt;\n             mapping=aes(x=Sepal.Length, y=Sepal.Width),\n             shape=\"o\", col=\"black\",size=5) +\n  labs(colour=paste(\"Cluster, step \", i- 1))"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#revisiting-the-swiss-fertility-data",
    "href": "core/labs-solutions/lab-kmeans.html#revisiting-the-swiss-fertility-data",
    "title": "Lab: Clustering k-means",
    "section": "Revisiting the swiss fertility data",
    "text": "Revisiting the swiss fertility data\n\n\n\n\n\n\nQuestion\n\n\n\nRecall the dataset used in Lab PCA\nPerform kmeans clustering in original coordinates and kmeans clustering in the first principal coordinates plane\nCompare the results\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodedata(swiss)\n\nswiss_scaled &lt;- swiss |&gt;  \n  select(-Fertility) |&gt; \n  scale() \n\nkm.2.swiss &lt;- swiss_scaled |&gt; \n  kmeans(centers = 2,  nstart = 10L)\n\ndf_centers.2 &lt;- broom::tidy(km.2.swiss)\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodekm.2.swiss.pca &lt;-  swiss_scaled |&gt;\n  prcomp() |&gt; \n  augment(data=swiss)|&gt; \n  dplyr::select(starts_with(\".fittedPC\")) |&gt; \n  kmeans(centers=2, nstart = 10L)\n\ndf_centers.2.pca &lt;- tidy(km.2.swiss.pca)\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nCodedf_centers.2 |&gt;\n  gt::gt() |&gt; \n  gt::fmt_number(decimals = 2)\n\n\n\n\n\nAgriculture\nExamination\nEducation\nCatholic\nInfant.Mortality\nsize\nwithinss\ncluster\n\n\n\n−0.46\n0.57\n0.36\n−0.67\n−0.17\n28.00\n101.66\n1\n\n\n0.68\n−0.83\n−0.53\n0.99\n0.25\n19.00\n49.26\n2\n\n\n\n\n\n\n\nCodekm.2.swiss |&gt; \n  broom::glance() |&gt;\n  gt::gt() |&gt; \n  gt::fmt_number(decimals = 2)\n\n\n\n\n\ntotss\ntot.withinss\nbetweenss\niter\n\n\n230.00\n150.91\n79.09\n1.00\n\n\n\n\n\n\n\nCodedf_centers.2.pca |&gt;\n  gt::gt() |&gt; \n  gt::fmt_number(decimals = 2)\n\n\n\n\n\n.fittedPC1\n.fittedPC2\n.fittedPC3\n.fittedPC4\n.fittedPC5\nsize\nwithinss\ncluster\n\n\n\n1.50\n0.32\n−0.36\n−0.02\n−0.07\n19.00\n49.26\n1\n\n\n−1.02\n−0.21\n0.25\n0.02\n0.05\n28.00\n101.66\n2\n\n\n\n\n\n\n\nCodekm.2.swiss.pca |&gt; \n  broom::glance() |&gt;\n  gt::gt() |&gt; \n  gt::fmt_number(decimals = 2)\n\n\n\n\n\ntotss\ntot.withinss\nbetweenss\niter\n\n\n230.00\n150.91\n79.09\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodeplot_km_centroids(\n  broom::augment(km.2.swiss, scale(swiss)),\n  broom::tidy(km.2.swiss), \n  Education, \n  Infant.Mortality\n) +\n  labs(\n    title= \"Kmeans over Swiss dataset, k=2\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeplot_km_centroids(\n  augment(km.2.swiss.pca,\n         broom::augment(prcomp(swiss_scaled), data=swiss)),\n  tidy(km.2.swiss.pca),\n  .fittedPC1,\n  .fittedPC2\n) + labs(\n      title=\"Kmeans over Swiss dataset, k=2\",\n      subtitle=\"Clustering over the Principal components\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodekm.4.swiss.pca &lt;-  swiss_scaled |&gt;\n  prcomp() |&gt; \n  broom::augment(data=swiss)|&gt; \n  dplyr::select(starts_with(\".fittedPC\")) |&gt; \n  kmeans(centers=4, nstart = 10L)\n\ndf_centers.4.pca &lt;- broom::tidy(km.4.swiss.pca)\n\n\n\nCodeplot_km_centroids(\n  augment(km.4.swiss.pca,\n         broom::augment(prcomp(swiss_scaled), data=swiss)),\n  tidy(km.4.swiss.pca),\n  .fittedPC1,\n  .fittedPC2\n) + labs(\n      title=\"Kmeans over Swiss dataset, k=4\",\n      subtitle=\"Clustering over the Principal components\"\n  )"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#revisiting-the-mortality-dataset",
    "href": "core/labs-solutions/lab-kmeans.html#revisiting-the-mortality-dataset",
    "title": "Lab: Clustering k-means",
    "section": "Revisiting the mortality dataset",
    "text": "Revisiting the mortality dataset\n\n\n\n\n\n\nQuestion\n\n\n\nRecall the dataset used in Lab CA\nPerform kmeans clustering of categories in the row principal coordinates and the column principal coordinates"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#references",
    "href": "core/labs-solutions/lab-kmeans.html#references",
    "title": "Lab: Clustering k-means",
    "section": "References",
    "text": "References\nVignette k_means from tidyclust"
  },
  {
    "objectID": "core/labs/lab-hclust.html",
    "href": "core/labs/lab-hclust.html",
    "title": "Lab: Hierarchical Clustering",
    "section": "",
    "text": "Codestopifnot(\n  require(gt),\n  require(skimr),\n  require(GGally),\n  require(patchwork),\n  require(ggforce),\n  require(glue),\n  require(ggfortify),\n  require(ggvoronoi),\n  require(magrittr),\n  require(broom),\n  require(ggdendro),\n  require(dendextend),\n  require(plotly),\n  require(tidyverse)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(\n  theme_minimal(base_size=9, \n                base_family = \"Helvetica\")\n)\nCodeknitr::opts_chunk$set(\n  message = FALSE,\n  warning = FALSE,\n  comment=NA,\n  prompt=FALSE,\n  cache=FALSE,\n  echo=TRUE,\n  results='asis'\n)"
  },
  {
    "objectID": "core/labs/lab-hclust.html#preamble",
    "href": "core/labs/lab-hclust.html#preamble",
    "title": "Lab: Hierarchical Clustering",
    "section": "Preamble",
    "text": "Preamble\nHierarchical clustering builds dendrograms\nExplore the data structure: dendrograms (objects of class dendrogram) are represented by lists of lists with attributes (not by tibbles).\nThe dendrograms created from objects of class hclust represent planar binary trees.\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow do you define abstractly planar binary trees?\nIn dendrograms created from objects of class hclust, what do the leaf nodes represent?\nIn dendrograms created from objects of class hclust, what do the internal nodes represent ?\n\n\n\n\n\n\n\n\n\nKeep an eye on Introduction to dendextend by the package author Tal Galili."
  },
  {
    "objectID": "core/labs/lab-hclust.html#playing-with-a-toy-dendrogram",
    "href": "core/labs/lab-hclust.html#playing-with-a-toy-dendrogram",
    "title": "Lab: Hierarchical Clustering",
    "section": "Playing with a toy dendrogram",
    "text": "Playing with a toy dendrogram\nCodedend &lt;- 1:5 %&gt;% \n  dist %&gt;% \n  hclust(method=\"ward.D2\") %&gt;% \n  as.dendrogram\nNodes are identified by their prefix order index (note that this depend on the chosen rotation).\nCodedend %&gt;%  \n  rotate(c(1,2,4,5,3)) %&gt;% \n  get_nodes_attr(\"members\", \n                 id = c(1, 2, 5, 7)) \n[1] 5 2 3 1\nCodecophenetic(rotate(dend, c(1,2,4,5,3)))\n     1        2        3        4\n2 1.000000\n3 3.872983 3.872983\n4 3.872983 3.872983 1.000000\n5 3.872983 3.872983 1.732051 1.732051\nCodecophenetic(dend)\n     1        2        5        3\n2 1.000000\n5 3.872983 3.872983\n3 3.872983 3.872983 1.732051\n4 3.872983 3.872983 1.732051 1.000000\nCodedend %&gt;% \n  rotate(c(1,2,4,5,3)) %&gt;% \n  get_nodes_attr(\"height\") \n[1] 3.872983 1.000000 0.000000 0.000000 1.732051 1.000000 0.000000 0.000000 [9] 0.000000\nCodeas.ggdend(rev(dend))\n\nCode# kmeans(tibble(x=1:5), centers = 2)\nCode# Get various attributes\ndend %&gt;% \n  get_nodes_attr(\"height\") # node's height\n[1] 3.872983 1.000000 0.000000 0.000000 1.732051 0.000000 1.000000 0.000000 [9] 0.000000\nHow is attributed height computed? What is its purpose?\nWhat kind of tree traversal is used by get_nodes_... helpers?\nCodedend %&gt;% \n  get_nodes_attr(\"members\")\n[1] 5 2 1 1 3 1 2 1 1"
  },
  {
    "objectID": "core/labs/lab-hclust.html#tweaking-a-dendrogram",
    "href": "core/labs/lab-hclust.html#tweaking-a-dendrogram",
    "title": "Lab: Hierarchical Clustering",
    "section": "Tweaking a dendrogram",
    "text": "Tweaking a dendrogram\nWhy should we do that?\nHow should we do that?"
  },
  {
    "objectID": "core/labs/lab-hclust.html#usarrests",
    "href": "core/labs/lab-hclust.html#usarrests",
    "title": "Lab: Hierarchical Clustering",
    "section": "USArrests",
    "text": "USArrests\nWe work on USArrests dataset. We want to classify the 50 (united) states on the basis of the arrests profile and the urbanization rate. We rely on hierarchical, bottom-up classification.\nCodedata(\"USArrests\")\n\nUSArrests &lt;- USArrests %&gt;% \n    tibble::rownames_to_column(var=\"region\")\n\nUSArrests &lt;- USArrests %&gt;%\n    mutate(region = tolower(region))\n\nrownames(USArrests) &lt;- USArrests$region\n\nglimpse(USArrests)\nRows: 50 Columns: 5 $ region  “alabama”, “alaska”, “arizona”, “arkansas”, “california”, “co… $ Murder  13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.… $ Assault  236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24… $ UrbanPop  58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6… $ Rape  21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\nCodesource(\"./UTILS/make_biotiful.R\")\nCodemake_biotifoul(USArrests, .f=is.numeric)\n\nThe function dist is used to calculate pairwise distances between individuals.\n\n\n\n\n\n\nQuestion\n\n\n\nCompute pairwise distances between rows of USArrests (with and without scaling)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPerform hierarchical clustering on unscaled and scaled dataset.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\nCodemutate(USArrests, \n       .cluster = factor(cutree(hcl.1, 5))) %&gt;% \n  inner_join(map_data(\"state\"), by = \"region\") %&gt;%\n  ggplot() +\n  aes(x=long, y=lat, group=region, fill=.cluster) +\n  geom_polygon() +\n  scale_fill_viridis_d() +\n  ggtitle(\"Components of arrest data\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "core/labs/lab-hclust.html#the-dendrogram-class",
    "href": "core/labs/lab-hclust.html#the-dendrogram-class",
    "title": "Lab: Hierarchical Clustering",
    "section": "The dendrogram class",
    "text": "The dendrogram class\n\n\n\n\n\n\nQuestion\n\n\n\nExploration of results of hierarchical clustering (objects of class hclust) is facilitated by converting to class dendrogram.\n\n\n\n\n\n\n\n\nQuestion"
  },
  {
    "objectID": "core/labs/lab-hclust.html#ward-method",
    "href": "core/labs/lab-hclust.html#ward-method",
    "title": "Lab: Hierarchical Clustering",
    "section": "Ward method",
    "text": "Ward method\nThe meth=ward.D2 option allows you to aggregate individuals according to the method of Ward, that is, according to the variance.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the distance used? Describe the method of classification by variance?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow many groups are there at step 0? at the last step?\nHow many iterations are there?\nRecall the definition of inter-class variance.\nWhat is the inter-class variance at step 0? at the last step? How is it going according to the number of groups (or according to the number of iterations)?\nBy comparing the total inertia and the `clas$height’ output, find the coefficient of proportionality between the loss of inter-class variance and height of jumps."
  },
  {
    "objectID": "core/labs/lab-hclust.html#choice-of-the-number-of-classes",
    "href": "core/labs/lab-hclust.html#choice-of-the-number-of-classes",
    "title": "Lab: Hierarchical Clustering",
    "section": "Choice of the number of classes",
    "text": "Choice of the number of classes\n\n\n\n\n\n\nQuestion\n\n\n\n\nPlot the curve corresponding to the loss of variance inter in as a function of the number of iterations :\nSelect the “optimal” number of classes.\nVerify that, for the number of classes chosen, the number by class is sufficient (we can use the cutree function).\nThese classes can be represented using a dendrogram\nYou can also colour the leaves of the tree corresponding to a class. To do this, install and load the package `dendextend’."
  },
  {
    "objectID": "core/labs/lab-hclust.html#link-with-pca",
    "href": "core/labs/lab-hclust.html#link-with-pca",
    "title": "Lab: Hierarchical Clustering",
    "section": "Link with PCA",
    "text": "Link with PCA\nWe will represent the classes obtained in the factorial design(s) obtained by the PCA. This will make it possible to represent the classes and describe them according to the variables initials.\n\n\n\n\n\n\nQuestion\n\n\n\nRepresent the coordinates of the individuals in each group in the first factorial plane (with one color for each class). The vector generated by `cutree’ can be used to form a color vector. Interpretation.\n\n\nCodemutate(USArrests, \n       .cluster = factor(cutree(hcl.1, 10))) %&gt;% \n  inner_join(map_data(\"state\"), by = \"region\") %&gt;%\n  ggplot() +\n  aes(x=long, y=lat, group=region, fill=.cluster) +\n  geom_polygon() +\n  scale_fill_viridis_d() +\n  ggtitle(\"Components of arrest data\") +\n  theme(legend.position = \"none\")\n\nCode#data(france)\nCode(map_data(\"state\") %&gt;%\n  ggplot() +\n  aes(x=long, \n      y=lat, \n      label=factor(region), \n      fill = factor(region)) +\n  geom_polygon() +\n  scale_fill_viridis_d() +\n  ggtitle(\"USA\") +\n  theme(legend.position = \"none\")) |&gt; \n    ggplotly()"
  },
  {
    "objectID": "core/labs/lab-hclust.html#cophenetic-distance",
    "href": "core/labs/lab-hclust.html#cophenetic-distance",
    "title": "Lab: Hierarchical Clustering",
    "section": "Cophenetic distance",
    "text": "Cophenetic distance\n\n\n\n\n\n\nQuestion"
  },
  {
    "objectID": "core/labs/lab-hclust.html#cophenetic-distance-between-dendrograms",
    "href": "core/labs/lab-hclust.html#cophenetic-distance-between-dendrograms",
    "title": "Lab: Hierarchical Clustering",
    "section": "Cophenetic distance between dendrograms",
    "text": "Cophenetic distance between dendrograms"
  },
  {
    "objectID": "core/labs/lab-hclust.html#references",
    "href": "core/labs/lab-hclust.html#references",
    "title": "Lab: Hierarchical Clustering",
    "section": "References",
    "text": "References\nggdendro\ndendroextra\nhier_clust, tidyclust"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html",
    "href": "core/labs-solutions/lab-hclust.html",
    "title": "Lab: Hierarchical Clustering",
    "section": "",
    "text": "Codestopifnot(\n  require(gt),\n  require(skimr),\n  require(GGally),\n  require(patchwork),\n  require(ggforce),\n  require(glue),\n  require(ggfortify),\n  require(ggvoronoi),\n  require(magrittr),\n  require(broom),\n  require(ggdendro),\n  require(dendextend),\n  require(plotly),\n  require(tidyverse)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(\n  theme_minimal(base_size=9, \n                base_family = \"Helvetica\")\n)\nCodeknitr::opts_chunk$set(\n  message = FALSE,\n  warning = FALSE,\n  comment=NA,\n  prompt=FALSE,\n  cache=FALSE,\n  echo=TRUE,\n  results='asis'\n)"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#preamble",
    "href": "core/labs-solutions/lab-hclust.html#preamble",
    "title": "Lab: Hierarchical Clustering",
    "section": "Preamble",
    "text": "Preamble\nHierarchical clustering builds dendrograms\nExplore the data structure: dendrograms (objects of class dendrogram) are represented by lists of lists with attributes (not by tibbles).\nThe dendrograms created from objects of class hclust represent planar binary trees.\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow do you define abstractly planar binary trees?\nIn dendrograms created from objects of class hclust, what do the leaf nodes represent?\nIn dendrograms created from objects of class hclust, what do the internal nodes represent ?\n\n\n\n\n\n\n\n\n\nKeep an eye on Introduction to dendextend by the package author Tal Galili."
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#playing-with-a-toy-dendrogram",
    "href": "core/labs-solutions/lab-hclust.html#playing-with-a-toy-dendrogram",
    "title": "Lab: Hierarchical Clustering",
    "section": "Playing with a toy dendrogram",
    "text": "Playing with a toy dendrogram\nCodedend &lt;- 1:5 %&gt;% \n  dist %&gt;% \n  hclust(method=\"ward.D2\") %&gt;% \n  as.dendrogram\nNodes are identified by their prefix order index (note that this depend on the chosen rotation).\nCodedend %&gt;%  \n  rotate(c(1,2,4,5,3)) %&gt;% \n  get_nodes_attr(\"members\", \n                 id = c(1, 2, 5, 7)) \n[1] 5 2 3 1\nCodecophenetic(rotate(dend, c(1,2,4,5,3)))\n     1        2        3        4\n2 1.000000\n3 3.872983 3.872983\n4 3.872983 3.872983 1.000000\n5 3.872983 3.872983 1.732051 1.732051\nCodecophenetic(dend)\n     1        2        5        3\n2 1.000000\n5 3.872983 3.872983\n3 3.872983 3.872983 1.732051\n4 3.872983 3.872983 1.732051 1.000000\nCodedend %&gt;% \n  rotate(c(1,2,4,5,3)) %&gt;% \n  get_nodes_attr(\"height\") \n[1] 3.872983 1.000000 0.000000 0.000000 1.732051 1.000000 0.000000 0.000000 [9] 0.000000\nCodeas.ggdend(rev(dend))\n\nCode# kmeans(tibble(x=1:5), centers = 2)\nCode# Get various attributes\ndend %&gt;% \n  get_nodes_attr(\"height\") # node's height\n[1] 3.872983 1.000000 0.000000 0.000000 1.732051 0.000000 1.000000 0.000000 [9] 0.000000\nHow is attributed height computed? What is its purpose?\nWhat kind of tree traversal is used by get_nodes_... helpers?\nCodedend %&gt;% \n  get_nodes_attr(\"members\")\n[1] 5 2 1 1 3 1 2 1 1"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#tweaking-a-dendrogram",
    "href": "core/labs-solutions/lab-hclust.html#tweaking-a-dendrogram",
    "title": "Lab: Hierarchical Clustering",
    "section": "Tweaking a dendrogram",
    "text": "Tweaking a dendrogram\nWhy should we do that?\nHow should we do that?"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#usarrests",
    "href": "core/labs-solutions/lab-hclust.html#usarrests",
    "title": "Lab: Hierarchical Clustering",
    "section": "USArrests",
    "text": "USArrests\nWe work on USArrests dataset. We want to classify the 50 (united) states on the basis of the arrests profile and the urbanization rate. We rely on hierarchical, bottom-up classification.\nCodedata(\"USArrests\")\n\nUSArrests &lt;- USArrests %&gt;% \n    tibble::rownames_to_column(var=\"region\")\n\nUSArrests &lt;- USArrests %&gt;%\n    mutate(region = tolower(region))\n\nrownames(USArrests) &lt;- USArrests$region\n\nglimpse(USArrests)\nRows: 50 Columns: 5 $ region  “alabama”, “alaska”, “arizona”, “arkansas”, “california”, “co… $ Murder  13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.… $ Assault  236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24… $ UrbanPop  58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6… $ Rape  21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\nCodesource(\"./UTILS/make_biotiful.R\")\nCodemake_biotifoul(USArrests, .f=is.numeric)\n\nThe function dist is used to calculate pairwise distances between individuals.\n\n\n\n\n\n\nQuestion\n\n\n\nCompute pairwise distances between rows of USArrests (with and without scaling)\n\n\nCodedist.1 &lt;-  USArrests %&gt;% \n    select(where(is.numeric)) %&gt;% \n    dist()\nCodedist.2 &lt;- USArrests %&gt;% \n    select(where(is.numeric)) %&gt;% \n    scale %&gt;% \n    dist()\n\n\n\n\n\n\nQuestion\n\n\n\nPerform hierarchical clustering on unscaled and scaled dataset.\n\n\nCodehcl.1 &lt;- hclust(dist.1, method = \"ward.D2\")\nhcl.2 &lt;- hclust(dist.2, method = \"ward.D2\") # scaled\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\nCodemutate(USArrests, \n       .cluster = factor(cutree(hcl.1, 5))) %&gt;% \n  inner_join(map_data(\"state\"), by = \"region\") %&gt;%\n  ggplot() +\n  aes(x=long, y=lat, group=region, fill=.cluster) +\n  geom_polygon() +\n  scale_fill_viridis_d() +\n  ggtitle(\"Components of arrest data\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#the-dendrogram-class",
    "href": "core/labs-solutions/lab-hclust.html#the-dendrogram-class",
    "title": "Lab: Hierarchical Clustering",
    "section": "The dendrogram class",
    "text": "The dendrogram class\n\n\n\n\n\n\nQuestion\n\n\n\nExploration of results of hierarchical clustering (objects of class hclust) is facilitated by converting to class dendrogram.\n\n\nCodedend.1 &lt;- hcl.1 %&gt;% \n  as.dendrogram()\n\nlength(dend.1[[1]][[2]])\n[1] 2\nCodeclass(dend.1)\n[1] “dendrogram”\nCodeclass(unclass(dend.1))\n[1] “list”\nCodemethods(class=class(dend.1)) %&gt;% head()\n[1] “[[.dendrogram” “all.equal.dendrogram”\n[3] “as.dendrogram.dendrogram” “as.ggdend.dendrogram”\n[5] “as.hclust.dendrogram” “click_rotate.dendrogram”\nCodedend.1 %&gt;% head()\n–[dendrogram w/ 2 branches and 50 members at h = 701] |–[dendrogram w/ 2 branches and 16 members at h = 141] | |–[dendrogram w/ 2 branches and 10 members at h = 69.3] | | |–[dendrogram w/ 2 branches and 3 members at h = 30.1] .. | | --[dendrogram w/ 2 branches and 7 members at h = 43.4] ..   |–[dendrogram w/ 2 branches and 6 members at h = 82.3] | |–[dendrogram w/ 2 branches and 4 members at h = 33.4] .. | --[dendrogram w/ 2 branches and 2 members at h = 38.5] ..–[dendrogram w/ 2 branches and 34 members at h = 353] |–[dendrogram w/ 2 branches and 14 members at h = 106] | |–[dendrogram w/ 2 branches and 6 members at h = 42.5] .. | --[dendrogram w/ 2 branches and 8 members at h = 44.8] ..–[dendrogram w/ 2 branches and 20 members at h = 163] |–[dendrogram w/ 2 branches and 10 members at h = 38.5] .. `–[dendrogram w/ 2 branches and 10 members at h = 66] .. etc…\nCodedend.1 %&gt;% \n  ggdendrogram(rotate = TRUE,labels = T) +\n  ggtitle(\"Dendrogram for USArrests\") +\n  ggdendro::theme_dendro() +\n  scale_y_reverse(expand = c(0.2, 0))\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\nCode# label(dend.1)\n\ndend.2 &lt;-  as.dendrogram(hcl.1)\n# order it the closest we can to the order of the observations:\ndend.2 &lt;- rotate(dend.2, 1:50)\n# Color the branches based on the clusters:\ndend.2 &lt;- color_branches(dend.2, k=3) #, groupLabels=iris_species)\n# Manually match the labels, as much as possible, to the real classification of the flowers:\n# labels_colors(dend.2) &lt;-\n#    rainbow_hcl(3)[sort_levels_values(\n#       as.numeric(iris[,5])[order.dendrogram(dend.2)]\n#    )]"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#ward-method",
    "href": "core/labs-solutions/lab-hclust.html#ward-method",
    "title": "Lab: Hierarchical Clustering",
    "section": "Ward method",
    "text": "Ward method\nThe meth=ward.D2 option allows you to aggregate individuals according to the method of Ward, that is, according to the variance.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the distance used? Describe the method of classification by variance?\n\n\nThe output clas$height gives the jump height of the dendrogram to each new iteration. In the case of Ward’s method, she is proportional to the loss of inter-class variance.\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow many groups are there at step 0? at the last step?\nHow many iterations are there?\nRecall the definition of inter-class variance.\nWhat is the inter-class variance at step 0? at the last step? How is it going according to the number of groups (or according to the number of iterations)?\nBy comparing the total inertia and the `clas$height’ output, find the coefficient of proportionality between the loss of inter-class variance and height of jumps."
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#choice-of-the-number-of-classes",
    "href": "core/labs-solutions/lab-hclust.html#choice-of-the-number-of-classes",
    "title": "Lab: Hierarchical Clustering",
    "section": "Choice of the number of classes",
    "text": "Choice of the number of classes\n\n\n\n\n\n\nQuestion\n\n\n\n\nPlot the curve corresponding to the loss of variance inter in as a function of the number of iterations :\nSelect the “optimal” number of classes.\nVerify that, for the number of classes chosen, the number by class is sufficient (we can use the cutree function).\nThese classes can be represented using a dendrogram\nYou can also colour the leaves of the tree corresponding to a class. To do this, install and load the package `dendextend’."
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#link-with-pca",
    "href": "core/labs-solutions/lab-hclust.html#link-with-pca",
    "title": "Lab: Hierarchical Clustering",
    "section": "Link with PCA",
    "text": "Link with PCA\nWe will represent the classes obtained in the factorial design(s) obtained by the PCA. This will make it possible to represent the classes and describe them according to the variables initials.\n\n\n\n\n\n\nQuestion\n\n\n\nRepresent the coordinates of the individuals in each group in the first factorial plane (with one color for each class). The vector generated by `cutree’ can be used to form a color vector. Interpretation.\n\n\nCodemutate(USArrests, \n       .cluster = factor(cutree(hcl.1, 10))) %&gt;% \n  inner_join(map_data(\"state\"), by = \"region\") %&gt;%\n  ggplot() +\n  aes(x=long, y=lat, group=region, fill=.cluster) +\n  geom_polygon() +\n  scale_fill_viridis_d() +\n  ggtitle(\"Components of arrest data\") +\n  theme(legend.position = \"none\")\n\nCode#data(france)\nCode(map_data(\"state\") %&gt;%\n  ggplot() +\n  aes(x=long, \n      y=lat, \n      label=factor(region), \n      fill = factor(region)) +\n  geom_polygon() +\n  scale_fill_viridis_d() +\n  ggtitle(\"USA\") +\n  theme(legend.position = \"none\")) |&gt; \n    ggplotly()"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#cophenetic-distance",
    "href": "core/labs-solutions/lab-hclust.html#cophenetic-distance",
    "title": "Lab: Hierarchical Clustering",
    "section": "Cophenetic distance",
    "text": "Cophenetic distance\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\nCodedist.coph.1 &lt;- cophenetic(dend.1)"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#cophenetic-distance-between-dendrograms",
    "href": "core/labs-solutions/lab-hclust.html#cophenetic-distance-between-dendrograms",
    "title": "Lab: Hierarchical Clustering",
    "section": "Cophenetic distance between dendrograms",
    "text": "Cophenetic distance between dendrograms\nCodedata(\"iris\")\n\nhcl.iris &lt;- iris %&gt;% \n  select(where(is.numeric)) %&gt;% \n  scale() %&gt;% \n  dist() %&gt;% \n  hclust(meth=\"ward.D2\")\n\ndend.iris &lt;-  dendro_data(hcl.iris)\n\ndend.iris %$% (\n  ggplot() +\n  geom_segment(data = segments,\n               aes(x = x, y = y,\n                   xend = xend, yend = yend)\n  ) +\n  geom_text(data = labels,\n            aes(x = x, y = y,\n                label = label, hjust = 0),\n            size = 3\n  ) +\n  coord_flip() +\n  scale_y_reverse(expand = c(0.2, 0))\n)"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#references",
    "href": "core/labs-solutions/lab-hclust.html#references",
    "title": "Lab: Hierarchical Clustering",
    "section": "References",
    "text": "References\nggdendro\ndendroextra\nhier_clust, tidyclust"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#in-words",
    "href": "core/slides/slides-kmeans.html#in-words",
    "title": "Clustering and k-means",
    "section": "In words",
    "text": "In words\nClustering consists in partitioning points collections from some metric space\nin such a way that\n\npoints within the same group are close enough\n\nwhile\n\npoints from different groups are distant\n\nIn the background: some notion of distance/similarity"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#clustering-in-ml-applications",
    "href": "core/slides/slides-kmeans.html#clustering-in-ml-applications",
    "title": "Clustering and k-means",
    "section": "Clustering in ML applications",
    "text": "Clustering in ML applications\nClustering shows up in many Machine Learning applications, for example:\n\n Marketing: finding groups of customers with similar behavior given a large database of customer data containing their properties and past buying records\n Biology: classification of plants and animals given their features\n Bookshops: book ordering (recommendation)\n Insurance: identifying groups of motor insurance policy holders with a high average claim cost; identifying frauds\n City-planning: identifying groups of houses according to their type, value and geographical location\n Internet: document classification; clustering weblog data to discover groups of similar access patterns; topic modeling\n\nMany distinct goals: clustering is often just one step in a data analysis pipeline\nFor recommendation systems, marketing, objects that fit into the same group call for the same action\nSome clustering should be hierarchical (toxonomy in life sciences) others can just be flat"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#section",
    "href": "core/slides/slides-kmeans.html#section",
    "title": "Clustering and k-means",
    "section": "",
    "text": "A clustering application relies on the elaboration of a metric/dissimilarity over some input space\nThis tasks is entangled with feature engineering\nFocus on one specific context: market segmentation \n\n Data: Base of customer data containing their properties and past buying records\n Goal: Use the customers similarities to find groups\n\nPossible directions:\n\nDimension reduction (PCA, CA, MCA, …)\nClustering \\(\\approx\\) non-supervised classification\n\n\n\nAre the directions complementary? or not?\nClustering may be done before dimension reduction or the other way"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#dimension-reduction",
    "href": "core/slides/slides-kmeans.html#dimension-reduction",
    "title": "Clustering and k-means",
    "section": "\n Dimension reduction",
    "text": "Dimension reduction\nDimension reduction technologies start form:\n\nTraining data \\(\\mathcal{D}=\\{\\vec{X}_1,\\ldots,\\vec{X}_n\\} \\in \\mathcal{X}^n\\) (i.i.d. \\(\\sim \\Pr\\))\nSpace \\(\\mathcal{X}\\) of possibly high dimension.\n\nand elaborate a Dimension Reduction Map\nDimension reduction technologies construct a map \\(\\Phi\\) from the space \\(\\mathcal{X}\\) into a space \\(\\mathcal{X}'\\) of smaller dimension"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#clustering-techniques",
    "href": "core/slides/slides-kmeans.html#clustering-techniques",
    "title": "Clustering and k-means",
    "section": "\n Clustering techniques",
    "text": "Clustering techniques\nClustering techniques start from training data:\n\\[\\mathcal{D}=\\{\\vec{X}_1,\\ldots,\\vec{X}_n\\} \\in \\mathcal{X}^n\\]\nassuming \\(\\vec{X}_i \\sim_{\\text{i.i.d.}} \\Pr\\), and partition the data into (latent?) groups,\nClustering techniques construct a map \\(f\\) from \\(\\mathcal{D}\\) to \\(\\{1,\\ldots,K\\}\\) where \\(K\\) is a number of classes to be fixed: \\(f: \\quad \\vec{X}_i \\mapsto k_i\\)"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#dimension-reduction-and-clustering-may-be-combined",
    "href": "core/slides/slides-kmeans.html#dimension-reduction-and-clustering-may-be-combined",
    "title": "Clustering and k-means",
    "section": "Dimension reduction and clustering may be combined",
    "text": "Dimension reduction and clustering may be combined\nFor example, it is commonplace to first perform PCA, project the data on the leading principal components and then perform \\(k\\)-means clustering on the projected data\nClustering tasks may be motivated along different directions:\n\nThe search for an interpretation of groups\nUse of groups in further processing (prediction, …)\n\nThis is especially true as many clustering approaches suffer from the curse of dimensionality"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#good-clustering",
    "href": "core/slides/slides-kmeans.html#good-clustering",
    "title": "Clustering and k-means",
    "section": "Good clustering",
    "text": "Good clustering\nWe need to define the quality of a cluster \nUnfortunately, no obvious quality measure exists! \nClustering quality may be assessed by scrutinizing\n\nInner homogeneity: samples in the same group should be similar\nOuter inhomogeneity: samples in two different groups should be different."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#shades-of-similarity",
    "href": "core/slides/slides-kmeans.html#shades-of-similarity",
    "title": "Clustering and k-means",
    "section": "Shades of similarity",
    "text": "Shades of similarity\nThere are many possible definitions of similar and different\nOften, they are based on the distance between the samples\nExamples based on the (squared) Euclidean distance:\n\nInner homogeneity \\(\\approx\\) intra class variance/inertia,\nOuter inhomogeneity \\(\\approx\\) inter class variance/inertia.\n\nRemember that, in flat clustering, the choice of the number \\(K\\) of clusters is often delicate"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#section-1",
    "href": "core/slides/slides-kmeans.html#section-1",
    "title": "Clustering and k-means",
    "section": "",
    "text": "Clustering is not a single method\nClustering methods address a large range of problems.\n\nTurning this informal statement into a formal definition proves challenging."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#definition-clustering-function",
    "href": "core/slides/slides-kmeans.html#definition-clustering-function",
    "title": "Clustering and k-means",
    "section": "Definition Clustering function",
    "text": "Definition Clustering function\nDefine a clustering function \\(F\\) as a function that\n\ntakes as input any finite domain \\(\\mathcal{X}\\) with a dissimilarity function \\(d\\) over its pairs\n\nand\n\nreturns a partition of \\(\\mathcal{X}\\)"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#desirable-properties",
    "href": "core/slides/slides-kmeans.html#desirable-properties",
    "title": "Clustering and k-means",
    "section": "Desirable properties",
    "text": "Desirable properties\nA clustering function should ideally satisfy the next three properties\n\nScale Invariance. For any domain set \\(\\mathcal{X}\\), dissimilarity function \\(d\\), and any \\(\\alpha&gt;0\\), the following should hold: \\(F(\\mathcal{X},d) = F(\\mathcal{X},\\alpha d)\\).\nRichness For any finite \\(\\mathcal{X}\\) and every partition \\(C = (C_1,\\ldots,C_k)\\) of \\(\\mathcal{X}\\) (into nonempty subsets) there exists some dissimilarity function \\(d\\) over \\(\\mathcal{X}\\) such that \\(F(\\mathcal{X},d)=C\\).\n\nConsistency If \\(d\\) and \\(d'\\) are dissimilarity functions over \\(\\mathcal{X}\\), such that for all \\(x, y \\in \\mathcal{X}\\),\n\nif \\(x,y\\) belong to the same cluster in \\(F(\\mathcal{X},d)\\) then \\(d'(x,y) \\leq d(x,y)\\),\nif \\(x,y\\) belong to different clusters in \\(F(\\mathcal{X},d)\\) then \\(d'(x,y) \\geq d(x,y)\\),\n\n\n\nthen \\(F(\\mathcal{X},d) = F(\\mathcal{X},d')\\).\n Designing clustering functions meeting simultaneously any two of the three properties is doable\nbut\n The three reasonable goals are conflicting"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#kleinbergs-impossibility-theorem",
    "href": "core/slides/slides-kmeans.html#kleinbergs-impossibility-theorem",
    "title": "Clustering and k-means",
    "section": "Kleinberg’s impossibility theorem",
    "text": "Kleinberg’s impossibility theorem\n\n\n\n\n\n\n\nTheorem\n\n\nNo clustering function \\(F\\) satisfies simultaneously all three properties:\n\nScale Invariance,\nRichness, and\nConsistency"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#flathierarchical-and",
    "href": "core/slides/slides-kmeans.html#flathierarchical-and",
    "title": "Clustering and k-means",
    "section": "Flat/Hierarchical and …",
    "text": "Flat/Hierarchical and …\nA wide variety of clustering methods have been used in Statistics and Machine Learning.\n\n\nFlat clustering (for example \\(k\\)-means) partitions sample into a fixed number of classes (usually denoted by \\(k\\)). The partition is determined by some algorithm\n\n.f6[The ultimate objective is to optimize some cost function. Whether the objective is achieved or even approximately achieved using a reasonable amount of computational resources is not settled]\n\n\nModel based clustering is based on a generative model: data are assumed to be sampled from a specific model (usually finite mixtures of Gaussians, the model may or may not be parametric)\n\n.f6[Clustering consists in fitting such a mixture model and then assigning sample points to mixture components]\n\n\nHierarchical clustering is the topic of next lesson"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#carte-du-tendre",
    "href": "core/slides/slides-kmeans.html#carte-du-tendre",
    "title": "Clustering and k-means",
    "section": "Carte du tendre",
    "text": "Carte du tendre\n\n\nIn Machine Learning, \\(k\\)-means and hierarchical clustering belong to a range of tasks called non-supervised learning\nThis contrasts with regression which belongs to the realm of supervised learning"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#section-2",
    "href": "core/slides/slides-kmeans.html#section-2",
    "title": "Clustering and k-means",
    "section": "",
    "text": "The \\(k\\)-means algorithm is an iterative method that constructs a sequence of Voronoï partitions\nA Voronoï diagram draws the nearest neighbor regions around a set of points."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#definition-voronoï-partitions",
    "href": "core/slides/slides-kmeans.html#definition-voronoï-partitions",
    "title": "Clustering and k-means",
    "section": "Definition: Voronoï partitions",
    "text": "Definition: Voronoï partitions\nAssume:\n\nsample \\(X_1, \\ldots, X_n\\) from \\(\\mathbb{R}^p\\)\n\n\n\\(\\mathbb{R}^p\\) is endowed with a metric \\(d\\), usually \\(\\ell_2\\), sometimes a weighted \\(\\ell_2\\) distance or \\(\\ell_1\\)\n\n\nEach cluster is defined by a centroid\nThe collection of centroids is (sometimes) called the codebook \\(\\mathcal{C}=c_1, \\ldots, c_k\\)\nEach centroid \\(c_j\\) defines a class:\n\\[C_j = \\bigg\\{ X_i : d(X_i, c_j) = \\min_{j' \\leq k} d(X_i, c_{j'})\\bigg\\}\\]\nand more generally a Voronoï cell in \\(\\mathbb{R}^p\\)\n\\[C_j = \\bigg\\{ x :  x \\in \\mathbb{R}^p, d(x, c_j) = \\min_{j' \\leq k} d(x, c_{j'})\\bigg\\}\\]"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#a-voronoï-tesselation",
    "href": "core/slides/slides-kmeans.html#a-voronoï-tesselation",
    "title": "Clustering and k-means",
    "section": "A Voronoï tesselation",
    "text": "A Voronoï tesselation\n\n\n\n\n\n\n\n\n\n\n\n\nEuclidean distance, dimension 2\nA voronoi tesselation generated by \\(100\\) points picked at random on the gred \\(\\{1,\\ldots, 200\\}^2\\)\nNote that cell boundaries are line segments\nNote that centroids may lie close to boundaries\nThe position of the centroid of a Voronoi cell depends on the positions of the centroids of the neighboring cells\n\nSee ggforce"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#a-voronoi-partition-for-projected-iris-dataset",
    "href": "core/slides/slides-kmeans.html#a-voronoi-partition-for-projected-iris-dataset",
    "title": "Clustering and k-means",
    "section": "A Voronoi partition for projected Iris dataset",
    "text": "A Voronoi partition for projected Iris dataset\n\n\nThe black points marked with a cross define three centroids.\nThe straight lines delimit the Voronoï cells defined by the three centroids.\nThe colored points come from the Iris dataset: each point is colored according to the the cell it belongs to."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#k-means-objective-function",
    "href": "core/slides/slides-kmeans.html#k-means-objective-function",
    "title": "Clustering and k-means",
    "section": "\nk-means objective function",
    "text": "k-means objective function\nThe \\(k\\)-means algorithm aims at building a codebook \\(\\mathcal{C}\\) that minimizes\n\\(\\mathcal{C} \\mapsto \\sum_{i=1}^n \\min_{c \\in \\mathcal{C}}  \\norm X_i - c \\norm_2^2\\)\nover all codebooks with given cardinality\nIf \\(c \\in \\mathcal{C}\\) is the closest centroid to \\(X \\in \\mathbb{R}^p\\),\n\\[\\|c - X\\|^2\\]\nis the quantization/reconstruction error suffered when using codebook \\(\\mathcal{C}\\) to approximate \\(X\\)\n If there are no restrictions on the dimension of the input space, on the number of centroids, or on sample size, computing an optimal codebook is a \\(\\text{NP}\\) -hard problem"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#k-means-at-work",
    "href": "core/slides/slides-kmeans.html#k-means-at-work",
    "title": "Clustering and k-means",
    "section": "\n\\(k\\)-means at work",
    "text": "\\(k\\)-means at work\n\n\nWe may figure out what an optimized Voronoï partition looks like on the Iris dataset\nkmeans with \\(k=3\\) on the Iris dataset\nFunction kmeans is run with default arguments\nWe chose the Sepal plane for clustering and visualization\nThis is arbitrary. We could have chosen a Petal plane, a Width plane, or a plane defined by principal axes."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#a-k-means-clustering-is-completely-characterized-by-the-k-centroids",
    "href": "core/slides/slides-kmeans.html#a-k-means-clustering-is-completely-characterized-by-the-k-centroids",
    "title": "Clustering and k-means",
    "section": "A \\(k\\)-means clustering is completely characterized by the \\(k\\) centroids",
    "text": "A \\(k\\)-means clustering is completely characterized by the \\(k\\) centroids\nOnce centroids are known, clusters can be recovered by searching the closest centroid for each sample point (that is by delimiting the Voronoï cells).\n\nHow can we assess the quality of a \\(k\\)-means clustering?\nCan we compare the clusterings achieved by picking different values of \\(k\\)?\n\nThere is no obvious assessment criterion!"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#caveat",
    "href": "core/slides/slides-kmeans.html#caveat",
    "title": "Clustering and k-means",
    "section": "Caveat",
    "text": "Caveat\nWhen visualizing \\(k\\)-means clustering on Iris data, we are cheating. \nWe have a gold standard classification delivered by botanists\nThe botanists classification can be challenged\nWe can compare classification originating from phenotypes (appearance) and classification based on phylogeny (comparing DNAs)"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#summarising-a-k-means-clustering",
    "href": "core/slides/slides-kmeans.html#summarising-a-k-means-clustering",
    "title": "Clustering and k-means",
    "section": "Summarising a \\(k\\)-means clustering",
    "text": "Summarising a \\(k\\)-means clustering\n\n\nA clustering can be summarized and illustrated.\nIn  A meaningful summary is provided by the generic function summary(), or a tidy summary is providede by broom::tidy(...)\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nsize\nwithinss\ncluster\n\n\n\n6.81\n3.07\n47.00\n12.62\n1\n\n\n5.01\n3.43\n50.00\n13.13\n2\n\n\n5.77\n2.69\n53.00\n11.30\n3\n\n\n\n\n\n\nThe concise summary tells us the number of points that are assigned to each cluster, and the Within Sum of Squares (WNSS). It says something about inner homogeneity and (apparently) nothing about outer homogeneity"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#k-means-with-k2",
    "href": "core/slides/slides-kmeans.html#k-means-with-k2",
    "title": "Clustering and k-means",
    "section": "\n\\(k\\)-means with \\(k=2\\)\n",
    "text": "\\(k\\)-means with \\(k=2\\)\n\n\n\nWe pursue the exploration of kmeans by building another clustering for Iris dataset.\nThis times with \\(k=2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nsize\nwithinss\ncluster\n\n\n\n5.22\n3.13\n83.00\n35.09\n1\n\n\n6.61\n2.97\n67.00\n23.11\n2"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#how-should-we-pick-k",
    "href": "core/slides/slides-kmeans.html#how-should-we-pick-k",
    "title": "Clustering and k-means",
    "section": "\n How should we pick \\(k\\)?",
    "text": "How should we pick \\(k\\)?\n\n\nEven if we could compute a provably optimal codebook for each \\(k\\), choosing \\(k\\) would not be obvious\nA common recipe consists of plotting within clusters sum of squares (WNSS) against \\(k\\)\nWithin clusters sum of squares (WNSS) decreases sharply between \\(k=2\\) and \\(k=3\\)\nFor larger values of \\(k\\), the decay is much smaller.\nThe elbow rule of thumb suggests to choose \\(k=3\\).\n\nWe have run kmeans over the Iris dataset, for \\(k\\) in range \\(2, \\ldots, 10\\). For each value of \\(k\\), we performed \\(32\\) randomized initializations, and chose the partition that minimizes within clusters sum of squares"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#incentive-to-choose-k4",
    "href": "core/slides/slides-kmeans.html#incentive-to-choose-k4",
    "title": "Clustering and k-means",
    "section": "Incentive to choose \\(k=4\\)?",
    "text": "Incentive to choose \\(k=4\\)?\n\n\nDepending on initialization, taking \\(k=4\\) creates a cluster at the boundary between versicolor and virginica or it may split the setosa cluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nsize\nwithinss\ncluster\n\n\n\n5.518182\n2.606061\n33\n5.967879\n1\n\n\n5.016327\n3.451020\n49\n11.569388\n2\n\n\n7.283333\n3.133333\n18\n3.905000\n3\n\n\n6.350000\n2.942000\n50\n6.786800\n4"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#initialization-matters",
    "href": "core/slides/slides-kmeans.html#initialization-matters",
    "title": "Clustering and k-means",
    "section": "Initialization matters!",
    "text": "Initialization matters!\n\n\n\nInitialize by samples.\nk-Mean++ try to take them as separated as possible.\nNo guarantee to converge to a global optimum!\nTrial and error.\nRepeat and keep the best result.\n\n\n\nkmeans(x,       # data\n       centers, # initial centroids or number of clusters\n       iter.max = 10,\n       nstart = 1,  # number of trials\n       algorithm = c(\"Hartigan-Wong\", # default\n                     \"Lloyd\",         #&lt;&lt; old one\n                     \"Forgy\",\n                     \"MacQueen\"),\n       trace=FALSE)"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#lloyds-algorithm-detailed-for-fixed-k-naive-k-means",
    "href": "core/slides/slides-kmeans.html#lloyds-algorithm-detailed-for-fixed-k-naive-k-means",
    "title": "Clustering and k-means",
    "section": "Lloyd’s Algorithm (detailed) for fixed k (naive k-means)",
    "text": "Lloyd’s Algorithm (detailed) for fixed k (naive k-means)\n\nInitialize Choose \\(k\\) centroids\nIterations: Two phases\n(Movement) Assign each sample point to the closest centroid Assign each sample point to a class in the Voronoi partition defined by the centroids\n(Update) For each class in the current Voronoi partition, update the centroid so as to minimize the Within Cluster Sum of Squared distances."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#lloyds-iterations",
    "href": "core/slides/slides-kmeans.html#lloyds-iterations",
    "title": "Clustering and k-means",
    "section": "Lloyd’s iterations",
    "text": "Lloyd’s iterations"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#lloyds-iterations-continued",
    "href": "core/slides/slides-kmeans.html#lloyds-iterations-continued",
    "title": "Clustering and k-means",
    "section": "Lloyd’s iterations (continued)",
    "text": "Lloyd’s iterations (continued)"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#lloyds-iterations-continued-1",
    "href": "core/slides/slides-kmeans.html#lloyds-iterations-continued-1",
    "title": "Clustering and k-means",
    "section": "Lloyd’s iterations (continued)",
    "text": "Lloyd’s iterations (continued)"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#analysis",
    "href": "core/slides/slides-kmeans.html#analysis",
    "title": "Clustering and k-means",
    "section": "Analysis",
    "text": "Analysis\nGiven\n\ncodebook \\(\\mathcal{C} =\\big\\{c_1, \\ldots, c_k\\big\\}\\) and\nclusters \\(C_1, \\ldots C_k\\),\n\nthe within-clusters sum of squares is defined as\n\\(\\sum_{j=1}^k  \\sum_{i: X_i \\in C_j} \\bigg\\Vert c_j - X_i \\bigg\\Vert^2\\)\n This is also the kmeans cost"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#section-4",
    "href": "core/slides/slides-kmeans.html#section-4",
    "title": "Clustering and k-means",
    "section": "",
    "text": "Lemma\n\n\nAt each stage, the within clusters sums of squares does not increase"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#proof",
    "href": "core/slides/slides-kmeans.html#proof",
    "title": "Clustering and k-means",
    "section": "Proof",
    "text": "Proof\nLet \\(\\mathcal{C}^{(t)} =\\big\\{ c^{(t)}_1, \\ldots, c_k^{(t)}\\big\\}\\) be the codebook after \\(t\\) steps\nLet \\(\\big({C}^{(t)}_j\\big)_{j \\leq k}\\) be the clusters after \\(t\\) steps\n\nCentroids at step \\(t+1\\) are the barycenters of clusters \\(\\big({C}^{(t)}_j\\big)_{j \\leq k}\\)\n\n\n\\(c^{(t+1)}_j = \\frac{1}{|C_j^{(t)}|} \\sum_{X_i \\in C^{(t)}_j} X_i\\)\n\nClusters \\(C^{(t+1)}_j\\) are defined by\n\n\\(C^{(t+1)}_j = \\bigg\\{ X_i : \\Vert X_i -  c^{(t+1)}_j\\Vert = \\min_{c \\in \\mathcal{C}^{(t+1)}} \\Vert X_i -  c\\Vert \\bigg\\}\\)\nEach sample point is assigned to the closest centroid"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#proof-continued",
    "href": "core/slides/slides-kmeans.html#proof-continued",
    "title": "Clustering and k-means",
    "section": "Proof (continued)",
    "text": "Proof (continued)\n\\[\\sum_{j=1}^k \\sum_{X_i \\in C^{(t)}_j} \\bigg\\Vert c^{(t)}_j  - X_i\\bigg\\Vert^2  \\geq \\sum_{j=1}^k \\sum_{X_i \\in C^{(t)}_j} \\bigg\\Vert c^{(t+1)}_j  - X_i\\bigg\\Vert^2\\]\nsince for each \\(j\\), the mean \\(c^{(t+1)}_j\\) minimizes the average square distance to points in \\(C^{(t)}_j\\)\n\\[\\sum_{j=1}^k \\sum_{X_i \\in C^{(t)}_j} \\bigg\\Vert c^{(t+1)}_j  - X_i\\bigg\\Vert^2 \\geq \\sum_{j=1}^k \\sum_{X_i \\in C^{(t)}_j} \\min_{c \\in \\mathcal{C}^{(t+1)}}\\bigg\\Vert c  - X_i\\bigg\\Vert^2\\]\n\\[\\sum_{j=1}^k \\sum_{X_i \\in C^{(t)}_j} \\min_{c \\in \\mathcal{C}^{(t+1)}}\\bigg\\Vert c  - X_i\\bigg\\Vert^2 = \\sum_{j=1}^k \\sum_{X_i \\in C^{(t+1)}_j} \\bigg\\Vert c^{(t+1)}_j  - X_i\\bigg\\Vert^2\\]"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#variants-of-k-means",
    "href": "core/slides/slides-kmeans.html#variants-of-k-means",
    "title": "Clustering and k-means",
    "section": "\n Variants of k-means",
    "text": "Variants of k-means\nImplementations of \\(k\\)-means vary with respect to\n\nInitialization\n\nk-means++\nForgy : pick initial centroids at random from the dataset\nRandom partition : pick a random partition of the dataset and initialize centroids by computing means in each class\n…\n\n\nMovement/assignment\n\nNaive \\(k\\) means uses brute-force search for closest centroid. Each step requires \\(\\Omega(n \\times k)\\) operations\nElkan (used by  scikit-learn)\nHartigan-Wong  default\n…\n\n\n\n\nLloyd’s algorithm is the standard approach for this problem. However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naïve implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd’s algorithm. .fr.f6[Wikipedia]\n\nIn base , kmeans is a wrapper for different but related algorithms.\nLloyd’s algorithm is the first and simplest versions of a series of heuristic methods designed to minimize the k-means cost\n\nMacQueen modify the mean each time a sample is assigned to a new cluster\nHartigan-Wong is the default method. It modifies the mean by removing the considered sample point, assign it to the nearby center and recompute the new mean after assignment.\nForgy"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#section-5",
    "href": "core/slides/slides-kmeans.html#section-5",
    "title": "Clustering and k-means",
    "section": "",
    "text": "The result of a clustering procedure like kmeans can be visualized by projecting the dataset on a pair of native variables and using some aesthetics to emphasize the clusters\nThis is not always the best way.\nFirst choosing a pair of native variables may not be straightforward. The projected pairwise distances may not faithfully reflect the pairwise distances that serve for clustering.\nIt makes sense to project the dataset of the \\(2\\)-dimensional subspace that maximizes the projected inertia, that is on the space generated by the first two principal components"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#pca-projection-k-means",
    "href": "core/slides/slides-kmeans.html#pca-projection-k-means",
    "title": "Clustering and k-means",
    "section": "PCA, projection, \\(k\\)-means",
    "text": "PCA, projection, \\(k\\)-means\n\n\nThe kmeans clustering of the Iris dataset is projected on the first two principal components: prcomp is used to perform PCA with neither centering nor scaling\nkmeans is applied to the rotated data\n\n\n\n\n\n\n\n\n\n The straight lines are the not the projections of the boundaries of the (4-dimensional) Voronoï cells defined by the clusters centroids, but the boundaries of the 2-dimensional Voronoï celles defined by the projections of the cluster centroids"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#questions-around-k-means",
    "href": "core/slides/slides-kmeans.html#questions-around-k-means",
    "title": "Clustering and k-means",
    "section": "\n Questions around k-means",
    "text": "Questions around k-means\n\nChoosing \\(k\\)\nAssessing clustering quality\nScaling or not scaling ?\nChoosing a distance\nInitialization methods\nMovement/assignment update\nStopping rules"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#conclusion",
    "href": "core/slides/slides-kmeans.html#conclusion",
    "title": "Clustering and k-means",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nEuclidean distance is used as a metric and inertia is used as a measure of cluster scatter\nThe number of clusters \\(k\\) is an input parameter\nConvergence to a local minimum may produce counterintuitive (“wrong”) results"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#adding-centroids",
    "href": "core/slides/slides-kmeans.html#adding-centroids",
    "title": "Clustering and k-means",
    "section": "Adding centroids",
    "text": "Adding centroids"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#k--means-at-work",
    "href": "core/slides/slides-kmeans.html#k--means-at-work",
    "title": "Clustering and k-means",
    "section": "\n\\(k\\) -means at work",
    "text": "\\(k\\) -means at work\n\n\nWe may figure out what an optimized Voronoï partition looks like on the Iris dataset\nkmeans with \\(k=3\\) on the Iris dataset\nFunction kmeans is run with default arguments\n\n\n\n\n\n\n\n\n\nWe chose the Sepal plane for clustering and visualization\nThis is arbitrary. We could have chosen a Petal plane, a Width plane, or a plane defined by principal axes."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#after-1-step",
    "href": "core/slides/slides-kmeans.html#after-1-step",
    "title": "Clustering and k-means",
    "section": "After 1 step",
    "text": "After 1 step"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#lloyds-iterations-after-2-steps-continued",
    "href": "core/slides/slides-kmeans.html#lloyds-iterations-after-2-steps-continued",
    "title": "Clustering and k-means",
    "section": "Lloyd’s iterations after 2 steps (continued)",
    "text": "Lloyd’s iterations after 2 steps (continued)"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#section-3",
    "href": "core/slides/slides-kmeans.html#section-3",
    "title": "Clustering and k-means",
    "section": "",
    "text": "The quality of a clustering can be appreciated according to a wide variety of performance indicators\n\nDistortion: this is the \\(k\\)-means cost\nShape of clusters\nRelevance of clusters\nStability: does clustering depend on few points?"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#lloyds-iterations-after-2-steps",
    "href": "core/slides/slides-kmeans.html#lloyds-iterations-after-2-steps",
    "title": "Clustering and k-means",
    "section": "Lloyd’s iterations after 2 steps",
    "text": "Lloyd’s iterations after 2 steps"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#lloyds-iterations-after-4-steps",
    "href": "core/slides/slides-kmeans.html#lloyds-iterations-after-4-steps",
    "title": "Clustering and k-means",
    "section": "Lloyd’s iterations after 4 steps",
    "text": "Lloyd’s iterations after 4 steps"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section",
    "href": "core/slides/slides-hclust.html#section",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Hierarchical clustering […] is a method of cluster analysis which seeks to build a hierarchy of clusters\nfrom Wikipedia\n\n\nRecall that a clustering is a partition of some dataset\nA partition \\(D\\) of \\(\\mathcal{X}\\) is a refinement of another partition \\(D'\\) if every class in \\(D\\) is a subset of a class in \\(D'\\). Partitions \\(D\\) and \\(D'\\) are said to be nested\n\n\nA hierarchical clustering of \\(\\mathcal{X}\\) is a sequence of \\(|\\mathcal{X}|\\) nested partitions of \\(\\mathcal{X}\\), starting from the trivial partition into \\(|\\mathcal{X}|\\) singletons and ending into the trivial partition in \\(1\\) subset ( \\(\\mathcal{X}\\) itself)\nA hierarchical clustering consists of \\(|\\mathcal{X}|\\) nested flat clusterings\nWe will explore agglomerative or bottom-up methods to build hierarchical clusterings"
  },
  {
    "objectID": "core/slides/slides-hclust.html#hierchical-clustering-and-dendrograms",
    "href": "core/slides/slides-hclust.html#hierchical-clustering-and-dendrograms",
    "title": "Hierarchical Clustering",
    "section": "Hierchical clustering and dendrograms",
    "text": "Hierchical clustering and dendrograms\n\n\nThe result of hierarchical clustering is a tree where leafs are labelled by sample points and internal nodes correspond to merging operations\nThe tree conveys more information: if the tree is properly decorated, it is possible to reconstruct the different merging steps and to know which rule was applied when some merging operation was performed\nThe tree is called a dendrogram"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-1",
    "href": "core/slides/slides-hclust.html#section-1",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Dendrogram and trees show up in several areas.\nClassification and Regression trees play an important role in Machine Learning.\nggdendro and dendextend may also be used to manipulate regression trees"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram: Iris illustration",
    "text": "Cutting a dendrogram: Iris illustration\n\nThe famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.\n\n\nThe Iris flower data set is fun for learning supervised classification algorithms, and is known as a difficult case for unsupervised learning.\n\n\nThe Setosa species are distinctly different from Versicolor and Virginica (they have lower petal length and width). But Versicolor and Virginica cannot easily be separated based on measurements of their sepal and petal width/length."
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-2",
    "href": "core/slides/slides-hclust.html#section-2",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Cutting a dendrogram: getting a flat clustering\nBuilding a dendrogram: inside hclust\nDisplaying, reporting dendrograms"
  },
  {
    "objectID": "core/slides/slides-hclust.html#hclust-pipeline",
    "href": "core/slides/slides-hclust.html#hclust-pipeline",
    "title": "Hierarchical Clustering",
    "section": "\nhclust pipeline",
    "text": "hclust pipeline\n\ndendro_iris &lt;- iris |&gt; \n  select(where(is.numeric)) |&gt;  \n  dist() |&gt; \n  hclust() |&gt; \n  dendro_data() \n\n\nmydendro &lt;- . %$% {  #&lt;&lt;\n  ggplot() + \n    geom_segment(\n      data = segments,   #&lt;&lt;\n      aes(\n        x = x, y = y, \n        xend = xend, \n        yend = yend)\n  ) +\n  geom_text(\n    data = labels, #&lt;&lt;\n    aes(\n      x = x, y = y, \n      label = label, hjust = 0),\n    size = 2\n  ) +\n  coord_flip() +\n  scale_y_reverse(expand = c(0.2, 0)) +\n  theme_dendro()  \n}"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-3",
    "href": "core/slides/slides-hclust.html#section-3",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "&lt;ScaleContinuousPosition&gt;\n Range:  \n Limits:    0 --    1"
  },
  {
    "objectID": "core/slides/slides-hclust.html#inside-ggdendro",
    "href": "core/slides/slides-hclust.html#inside-ggdendro",
    "title": "Hierarchical Clustering",
    "section": "Inside ggdendro",
    "text": "Inside ggdendro"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-5",
    "href": "core/slides/slides-hclust.html#section-5",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "mydendro(dendro_iris)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram:  Iris illustration (continued)",
    "text": "Cutting a dendrogram:  Iris illustration (continued)\n\np &lt;- iris %&gt;%\n  ggplot() +\n  aes(x=Petal.Length, y=Petal.Width)\n\np +\n  geom_point(\n    aes(\n      shape=Species, \n      colour=Species)\n  ) +\n# labs(shape= \"Species\") +\n  ggtitle(\n    label= \"Iris data\",\n    subtitle = \"Species in Petal plane\"\n  )"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-1",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-1",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram: Iris illustration (continued)",
    "text": "Cutting a dendrogram: Iris illustration (continued)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-2",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-2",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram: Iris illustration (continued)",
    "text": "Cutting a dendrogram: Iris illustration (continued)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-3",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-3",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram: Iris illustration (continued)",
    "text": "Cutting a dendrogram: Iris illustration (continued)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-4",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-4",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram: Iris illustration (continued)",
    "text": "Cutting a dendrogram: Iris illustration (continued)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-better-iris-illustration-continued",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-better-iris-illustration-continued",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram: better Iris illustration (continued)",
    "text": "Cutting a dendrogram: better Iris illustration (continued)\nThe dendextend package offers a set of functions for extending dendrogram objects in , letting you\n\nvisualize and\ncompare trees of hierarchical clusterings,\n\nFeatures:\n\nAdjust a tree’s graphical parameters - the color, size, type, etc, of its branches, nodes and labels\nVisually and statistically compare different dendrograms to one another"
  },
  {
    "objectID": "core/slides/slides-hclust.html#about-class-hclust",
    "href": "core/slides/slides-hclust.html#about-class-hclust",
    "title": "Hierarchical Clustering",
    "section": "About class hclust\n",
    "text": "About class hclust\n\nResults from function hclust() are objects of class hclust :\niris_hclust is an object of class hclust\nFunction cutree() returns a flat clustering of the dataset\nWhat does height stand for?\nWhat does merge stand for?\nWhat does order stand for?\nHow different are the different method?"
  },
  {
    "objectID": "core/slides/slides-hclust.html#hierarchical-clustering-of-usarrests",
    "href": "core/slides/slides-hclust.html#hierarchical-clustering-of-usarrests",
    "title": "Hierarchical Clustering",
    "section": "Hierarchical clustering of USArrests\n",
    "text": "Hierarchical clustering of USArrests\n\n\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…"
  },
  {
    "objectID": "core/slides/slides-hclust.html#about-dendrograms-output-dendro_data",
    "href": "core/slides/slides-hclust.html#about-dendrograms-output-dendro_data",
    "title": "Hierarchical Clustering",
    "section": "About dendrograms (output dendro_data())",
    "text": "About dendrograms (output dendro_data())\nAn object of class dendro is a list of 4 objects:\n\nsegments\nlabels\nleaf_labels\nclass"
  },
  {
    "objectID": "core/slides/slides-hclust.html#questions",
    "href": "core/slides/slides-hclust.html#questions",
    "title": "Hierarchical Clustering",
    "section": "Questions",
    "text": "Questions\n\nHow to build the dendrogram?\nHow to choose the cut?"
  },
  {
    "objectID": "core/slides/slides-hclust.html#bird-eye-view-at-hierarchical-agglomerative-clustering-methods",
    "href": "core/slides/slides-hclust.html#bird-eye-view-at-hierarchical-agglomerative-clustering-methods",
    "title": "Hierarchical Clustering",
    "section": "Bird-eye view at hierarchical agglomerative clustering methods",
    "text": "Bird-eye view at hierarchical agglomerative clustering methods\nAll hierarchical agglomerative clustering methods (HACMs) can be described by the following general algorithm.\n\nAt each stage distances between clusters are recomputed by the Lance-Williams dissimilarity update formula according to the particular clustering method being used.\nIdentify the 2 closest points and combine them into a cluster (treating existing clusters as points too)\nIf more than one cluster remains, return to step 1."
  },
  {
    "objectID": "core/slides/slides-hclust.html#greed-is-good",
    "href": "core/slides/slides-hclust.html#greed-is-good",
    "title": "Hierarchical Clustering",
    "section": "Greed is good!",
    "text": "Greed is good!\n\nHierarchical agglomerative clustering methods are examples of greedy algorithms\n\nGreedy algorithms sometimes compute optimal solutions\n\nHuffmann coding (Information Theory)\nMinimum spanning tree (Graph algorithms)\n\n\n\nGreedy algorithms sometimes compute sub-optimal solutions\n\nSet cover (NP-hard problem)\n…\n\n\n\nEfficient greedy algorithms rely on ad hoc data structures\n\nPriority queues\nUnion-Find"
  },
  {
    "objectID": "core/slides/slides-hclust.html#algorithm-detailed",
    "href": "core/slides/slides-hclust.html#algorithm-detailed",
    "title": "Hierarchical Clustering",
    "section": "\n Algorithm (detailed)",
    "text": "Algorithm (detailed)\n\nStart with \\((\\mathcal{C}_{i}^{(0)})= (\\{ \\vec{X}_i \\})\\) the collection of all singletons.\n\nAt step \\(s\\), we have \\(n-s\\) clusters \\((\\mathcal{C}_{i}^{(s)})\\):\n\nFind the two most similar clusters according to a criterion \\(\\Delta\\): \\[(i,i') = \\operatorname{argmin}_{(j,j')} \\Delta(\\mathcal{C}_{j}^{(s)},\\mathcal{C}_{j'}^{(s)})\\]\nMerge \\(\\mathcal{C}_{i}^{(s)}\\) and \\(\\mathcal{C}_{i'}^{(s)}\\) into \\(\\mathcal{C}_{i}^{(s+1)}\\)\nKeep the \\(n-s-2\\) other clusters \\(\\mathcal{C}_{i''}^{(s+1)} = \\mathcal{C}_{i''}^{(s)}\\)\n\n\nRepeat until there is only one cluster left"
  },
  {
    "objectID": "core/slides/slides-hclust.html#analysis",
    "href": "core/slides/slides-hclust.html#analysis",
    "title": "Hierarchical Clustering",
    "section": "Analysis",
    "text": "Analysis\n\nComplexity: \\(O(n^3)\\) in general.\n\nCan be reduced to \\(O(n^2)\\) (sometimes to \\(O(n \\log n)\\))\n\nif the number of possible mergers for a given cluster is bounded.\nfor the most classical distances by maintaining a nearest neighbors list."
  },
  {
    "objectID": "core/slides/slides-hclust.html#merging-criterion-based-on-the-distance-between-points",
    "href": "core/slides/slides-hclust.html#merging-criterion-based-on-the-distance-between-points",
    "title": "Hierarchical Clustering",
    "section": "Merging criterion based on the distance between points",
    "text": "Merging criterion based on the distance between points\nMinimum linkage:\n\\[\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) =\\min_{\\vec{X}_i \\in \\mathcal{C}_i} \\min_{\\vec{X}_j \\in    \\mathcal{C}_j} d(\\vec{X}_i, \\vec{X}_j)\\]\nMaximum linkage:\n\\[\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) = \\max_{\\vec{X}_i \\in \\mathcal{C}_i} \\max_{\\vec{X}_j \\in    \\mathcal{C}_j} d(\\vec{X}_i, \\vec{X}_j)\\]\nAverage linkage:\n\\[\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) =\\frac{1}{|\\mathcal{C}_i||\\mathcal{C}_j|} \\sum_{\\vec{X}_i \\in    \\mathcal{C}_i}\\sum_{\\vec{X}_j \\in \\mathcal{C}_j} d(\\vec{X}_i, \\vec{X}_j)\\]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#wards-criterion-minimum-varianceinertia-criterion",
    "href": "core/slides/slides-hclust.html#wards-criterion-minimum-varianceinertia-criterion",
    "title": "Hierarchical Clustering",
    "section": "Ward’s criterion : minimum variance/inertia criterion",
    "text": "Ward’s criterion : minimum variance/inertia criterion\n\\(\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) = \\sum_{\\vec{X}_i \\in \\mathcal{C}_i} \\left( d^2(\\vec{X}_i, \\mu_{\\mathcal{C}_i \\cup \\mathcal{C}_j} ) - d^2(\\vec{X}_i, \\mu_{\\mathcal{C}_i}) \\right) +\\)\n\\(\\qquad\\qquad \\qquad \\sum_{\\vec{X}_j \\in \\mathcal{C}_j} \\left( d^2(\\vec{X}_j, \\mu_{\\mathcal{C}_i \\cup \\mathcal{C}_j} ) - d^2(\\vec{X}_j, \\mu_{\\mathcal{C}_j}) \\right)\\)\nIf \\(d\\) is the euclidean distance\n\\[\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) = \\frac{ |\\mathcal{C}_i||\\mathcal{C}_j|}{|\\mathcal{C}_i|+ |\\mathcal{C}_j|} d^2(\\mu_{\\mathcal{C}_i}, \\mu_{\\mathcal{C}_j})\\]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#lance-williams-update-formulae",
    "href": "core/slides/slides-hclust.html#lance-williams-update-formulae",
    "title": "Hierarchical Clustering",
    "section": "Lance-Williams update formulae",
    "text": "Lance-Williams update formulae\nSuppose that clusters \\(C_{i}\\) and \\(C_{j}\\) were next to be merged\nAt this point, all of the current pairwise cluster distances are known\nThe recursive update formula gives the updated cluster distances following the pending merge of clusters \\(C_{i}\\) and \\(C_{j}\\)\nLet\n\n\\(d_{ij}, d_{ik}\\), and \\(d_{jk}\\) be shortands for the pairwise distances between clusters \\(C_{i}, C_{j}\\) and \\(C_{k}\\)\n\\(d_{{(ij)k}}\\) be shortand for the distance between the new cluster \\(C_{i}\\cup C_{j}\\) and \\(C_{k}\\) ( \\(k\\not\\in \\{i,j\\}\\) )"
  },
  {
    "objectID": "core/slides/slides-hclust.html#lance-williams-update-formulae-continued",
    "href": "core/slides/slides-hclust.html#lance-williams-update-formulae-continued",
    "title": "Hierarchical Clustering",
    "section": "Lance-Williams update formulae (continued)",
    "text": "Lance-Williams update formulae (continued)\nAn algorithm belongs to the Lance-Williams family if the updated cluster distance \\(d_{{(ij)k}}\\) can be computed recursively by\n\\[d_{(ij)k} = \\alpha _{i}d_{ik}+ \\alpha _{j}d_{jk}+ \\beta d_{ij}+ \\gamma |d_{ik}-d_{jk}|\\]\nwhere \\(\\alpha_{i},\\alpha _{j},\\beta\\) , and \\(\\gamma\\) are parameters, which may depend on cluster sizes, that together with the cluster distance function \\(d_{ij}\\) determine the clustering algorithm.\nClustering algorithms such as\n\nsingle linkage,\ncomplete linkage, and\ngroup average\n\nmethod have a recursive formula of the above type"
  },
  {
    "objectID": "core/slides/slides-hclust.html#lance-williams-update-formula-for-wards-criterion",
    "href": "core/slides/slides-hclust.html#lance-williams-update-formula-for-wards-criterion",
    "title": "Hierarchical Clustering",
    "section": "Lance-Williams update formula for Ward’s criterion",
    "text": "Lance-Williams update formula for Ward’s criterion\n\\[\\begin{array}{rl}d\\left(C_i \\cup C_j, C_k\\right) & = \\frac{n_i+n_k}{n_i+n_j+n_k}d\\left(C_i, C_k\\right)  +\\frac{n_j+n_k}{n_i+n_j+n_k}d\\left(C_j, C_k\\right) \\\\ & \\phantom{==}- \\frac{n_k}{n_i+n_j+n_k} d\\left(C_i, C_j\\right)\\end{array}\\]\n\\[\\alpha_i = \\frac{n_i+n_k}{n_i+n_j+n_k} \\qquad \\alpha_j = \\frac{n_j+n_k}{n_i+n_j+n_k}\\qquad \\beta = \\frac{- n_k}{n_i+n_j+n_k}\\]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#an-unfair-quotation",
    "href": "core/slides/slides-hclust.html#an-unfair-quotation",
    "title": "Hierarchical Clustering",
    "section": "An unfair quotation",
    "text": "An unfair quotation\n\nWard’s minimum variance criterion minimizes the total within-cluster variance .fr[Wikipedia]\n\n\nIs that correct?\nIf corrected, what does it mean?\n\nIf we understand the statement as:\n\nfor any \\(k\\), the flat clustering obtained by cutting the dendrogram so as to obtain a \\(k\\)-clusters partition minimizes the total within-cluster variance/inertia amongst all \\(k\\)-clusterings\n\nthen, the statement is not proved. If it were proved, it would imply \\(\\mathsf{P}=\\mathsf{NP}\\)\nThe total within-cluster variance/inertia is the objective function in the \\(k\\)-means problem.\nThe statement is misleading"
  },
  {
    "objectID": "core/slides/slides-hclust.html#what-happens-in-wards-method",
    "href": "core/slides/slides-hclust.html#what-happens-in-wards-method",
    "title": "Hierarchical Clustering",
    "section": "What happens in Ward’s method?",
    "text": "What happens in Ward’s method?\n\nAt each step find the pair of clusters that leads to minimum increase in total within-cluster variance after merging .fr[Wikipedia]\n\n\nThis increase is a weighted squared distance between cluster centers .fr[Wikipedia]\n\n\nAt the initial step, all clusters are singletons (clusters containing a single point). To apply a recursive algorithm under this objective function, the initial distance between individual objects must be (proportional to) squared Euclidean distance."
  },
  {
    "objectID": "core/slides/slides-hclust.html#views-on-inertia",
    "href": "core/slides/slides-hclust.html#views-on-inertia",
    "title": "Hierarchical Clustering",
    "section": "Views on Inertia:",
    "text": "Views on Inertia:\n\\[I   = \\frac{1}{n} \\sum_{i=1}^n \\|\\vec{X}_i - \\vec{m} \\|^2\\]\nwhere \\(\\vec{m} = \\sum_{i=1}^n \\frac{1}{n}\\vec{X}_i\\)\n\\[I = \\frac{1}{2n^2} \\sum_{i,j} \\|\\vec{X}_i - \\vec{X}_j\\|^2\\]\nTwice the mean squared distance to the mean equals the mean squared distance between sample points\nRecall that for a real random variable \\(Z\\) with mean \\(\\mu\\)\n\\[\\operatorname{var}(Z) = \\mathbb{E}(Z -m)^2 = \\inf_a \\mathbb{E}(Z -a)^2\\]\nand\n\\[\\operatorname{var}(Z) = \\frac{1}{2} \\mathbb{E}(Z -Z')^2\\]\nwhere \\(Z'\\) is an independent copy of \\(Z\\)\nThe different formulae for inertia is just mirroring the different views at variance\nThe inertia is the trace of an empirical covariance matrix."
  },
  {
    "objectID": "core/slides/slides-hclust.html#decompositions-of-inertia-huyghens-formula",
    "href": "core/slides/slides-hclust.html#decompositions-of-inertia-huyghens-formula",
    "title": "Hierarchical Clustering",
    "section": "Decompositions of inertia (Huyghens formula)",
    "text": "Decompositions of inertia (Huyghens formula)\n\nSample \\(x_1,\\ldots, x_{n+m}\\) with mean \\(\\bar{X}_{n+m}\\) and variance \\(V\\)\nPartition \\(\\{1,\\ldots,n+m\\} = A \\cup B\\) with \\(|A|=n, |B|=m\\), \\(A \\cap B =\\emptyset\\)\nLet \\(\\bar{X}_n = \\frac{1}{n}\\sum_{i \\in A} X_i\\) and \\(\\bar{X}_m=\\frac{1}{m}\\sum_{i \\in B}X_i\\) \\[\\bar{X}_{n+m} =  \\frac{n}{n+m} \\bar{X}_{n}  +\\frac{m}{n+m} \\bar{X}_{m}\\]\nLet \\(V_A\\) be the variance of \\((x_i)_{i\\in A}\\), \\(V_B\\) be the variance of \\((x_i)_{i\\in B}\\)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#decompositions-of-inertia-huyghens-formula-1",
    "href": "core/slides/slides-hclust.html#decompositions-of-inertia-huyghens-formula-1",
    "title": "Hierarchical Clustering",
    "section": "Decompositions of inertia (Huyghens formula)",
    "text": "Decompositions of inertia (Huyghens formula)\n\nLet \\(V_{\\text{between}}\\) be the variance of a ghost sample with \\(n\\) copies of \\(\\bar{X}_n\\) and \\(m\\) copies of \\(\\bar{X}_m\\) \\[V_{\\text{between}} =  \\frac{n}{n+m} (\\bar{X}_n -\\bar{X}_{n+m})^2 + \\frac{m}{n+m} (\\bar{X}_m -\\bar{X}_{n+m})^2\\]\nLet \\(V_{\\text{within}}\\) be the weighted mean of variances within classes \\(A\\) and \\(B\\) \\[V_{\\text{within}}  = \\frac{n}{n+m}  V_A + \\frac{m}{n+m} V_B\\]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#decompositions-of-inertia",
    "href": "core/slides/slides-hclust.html#decompositions-of-inertia",
    "title": "Hierarchical Clustering",
    "section": "Decompositions of inertia",
    "text": "Decompositions of inertia\n\n\n\n\n\n\nProposition: Huyghens formula I\n\n\n\\[V = V_{\\text{within}} +  V_{\\text{between}}\\]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#huyghens-formula-can-be-extended-to-any-number-of-classes",
    "href": "core/slides/slides-hclust.html#huyghens-formula-can-be-extended-to-any-number-of-classes",
    "title": "Hierarchical Clustering",
    "section": "Huyghens formula can be extended to any number of classes",
    "text": "Huyghens formula can be extended to any number of classes\n\n\n\n\n\n\nProposition: Huyghens (II)\n\n\n\nSample \\(\\vec{x}_1, \\ldots,\\vec{x}_n\\) from \\(\\mathbb{R}^p\\) with mean \\(\\bar{X}_n\\), inertia \\(I\\).\nLet \\(A_1, A_2\\ldots, A_k\\) be a partition of \\(\\{1,\\ldots,n\\}\\).\nLet \\(I_\\ell\\) (resp. \\(\\bar{X}^\\ell\\)) be the inertia (resp. the mean) of sub-sample \\(\\vec{x}_i, i\\in A_\\ell\\)\nLet \\(I_{\\text{between}}\\) be the inertia of the ghost sample, formed by \\(|A_1|\\) copies of \\(\\bar{X}^1\\), \\(|A_2|\\) copies of \\(\\bar{X}^2\\), … \\(|A_k|\\) copies of \\(\\bar{X}^k\\)\nLet \\(I_{\\text{within}} =  \\sum_{\\ell=1}^k \\frac{|A_\\ell|}{n}  I_\\ell\\)\n\n\\[I =   I_{\\text{within}} +  I_{\\text{between}}\\]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cophenetic-disimilarity",
    "href": "core/slides/slides-hclust.html#cophenetic-disimilarity",
    "title": "Hierarchical Clustering",
    "section": "Cophenetic disimilarity",
    "text": "Cophenetic disimilarity\nGiven a dendrogram, the cophenetic disimilarity between two sample points \\(x, x'\\) is the intergroup disimilarity at which observations \\(x\\) and \\(x'\\) are first joined.\n\n\n\n\n\n\nProposition\n\n\nA cophenetic disimilarity has the ultrametric property\n\n\n\n\n\nAll triangles are isoceles and the unequal length should be no longer than the length of the two equal sides"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cophenetic-correlation-coefficient",
    "href": "core/slides/slides-hclust.html#cophenetic-correlation-coefficient",
    "title": "Hierarchical Clustering",
    "section": "Cophenetic correlation coefficient",
    "text": "Cophenetic correlation coefficient\nThe cophenetic correlation coefficient measures how faithfully a dendrogram preserves the pairwise distances between the original unmodeled data points\n\n\nwikipedia]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#computing-cophenetic-correlation-coefficient",
    "href": "core/slides/slides-hclust.html#computing-cophenetic-correlation-coefficient",
    "title": "Hierarchical Clustering",
    "section": "Computing cophenetic correlation coefficient",
    "text": "Computing cophenetic correlation coefficient\nIn  use the dendextend package\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsingle\ncomplete\naverage\nmcquitty\nward.D\ncentroid\nmedian\nward.D2\n\n\n0.86\n0.73\n0.88\n0.87\n0.86\n0.87\n0.86\n0.87"
  },
  {
    "objectID": "core/slides/slides-hclust.html#how-to-apply-general-algorithm",
    "href": "core/slides/slides-hclust.html#how-to-apply-general-algorithm",
    "title": "Hierarchical Clustering",
    "section": "How to apply general algorithm?",
    "text": "How to apply general algorithm?\n\nLance-Williams dissimilarity update formula calculates dissimilarities between a new cluster and existing points, based on the dissimilarities prior to forming the new cluster\nThis formula has 3 parameters\nEach HACM is characterized by its own set of Lance-Williams parameters"
  },
  {
    "objectID": "core/slides/slides-hclust.html#implementations-of-the-general-algorithm",
    "href": "core/slides/slides-hclust.html#implementations-of-the-general-algorithm",
    "title": "Hierarchical Clustering",
    "section": "Implementations of the general algorithm",
    "text": "Implementations of the general algorithm\nStored matrix approach\nUse matrix, and then apply Lance-Williams to recalculate dissimilarities between cluster centers. Storage \\(O(N^2)\\) and time at least \\(O(N^2)\\), but is \\(\\Theta(N^3)\\) if matrix is scanned linearly\nStored data approach\n\\(O(N)\\) space for data but recompute pairwise dissimilarities, needs \\(\\Theta(N^3)\\) time\nSorted matrix approach\n\\(O(N^2)\\) to calculate dissimilarity matrix, \\(O(N^2 \\log N)\\) to sort it, \\(O(N^2)\\) to construct hierarchy, but one need not store the data set, and the matrix can be processed linearly, which reduces disk accesses"
  },
  {
    "objectID": "core/slides/slides-hclust.html#agglomerative-clustering-heuristic",
    "href": "core/slides/slides-hclust.html#agglomerative-clustering-heuristic",
    "title": "Hierarchical Clustering",
    "section": "Agglomerative Clustering Heuristic",
    "text": "Agglomerative Clustering Heuristic\n\nStart with very small clusters (a sample point by cluster?)\nMerge iteratively the most similar clusters according to some greedy criterion \\(\\Delta\\).\nGenerates a hierarchy of clusterings instead of a single one.\nNeed to select the number of cluster afterwards.\nSeveral choice for the merging criterion\n\nExamples:\n\nMinimum Linkage: merge the closest cluster in term of the usual distance\nWard’s criterion: merge the two clusters yielding the less inner inertia loss (minimum variance criterion)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#packages",
    "href": "core/slides/slides-hclust.html#packages",
    "title": "Hierarchical Clustering",
    "section": "Packages",
    "text": "Packages\n\n\n\nggdendro\ndendextend\ndendroextras\n\n\n\n\nscipy\nscikit-learn"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-4",
    "href": "core/slides/slides-hclust.html#section-4",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "as.matrix(dist(iris[,1:4])) returns the matrix of pairwise distances\nDefault distance is Euclidean distance\nWhat about using broom::augment?\nThere is no augment.hclust method: No augment method for objects of class hclust\n\n\n:::"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-6",
    "href": "core/slides/slides-hclust.html#section-6",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "mydendro(dendro_iris)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-8",
    "href": "core/slides/slides-hclust.html#section-8",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "What is an object of class dendro made of?\nIt a list of four elements:\n\nsegments\nlabels\nleaf_labels\nclass\n\nElement segments is a data frame with four columns. Each row represent a segment that is part of a graphical representation of the hierarchy. There are horizontal and vertical segments\nElement labels is used to label the tree leafs."
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-10",
    "href": "core/slides/slides-hclust.html#section-10",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Does the flat clustering obtained by cutting the dendrogram at some height reflect the partition into species"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-12",
    "href": "core/slides/slides-hclust.html#section-12",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "dendextend\nA dendro gallery\nclass of dend"
  },
  {
    "objectID": "core/slides/slides-hclust.html#hierchical-clustering-and-dendrogram",
    "href": "core/slides/slides-hclust.html#hierchical-clustering-and-dendrogram",
    "title": "Hierarchical Clustering",
    "section": "Hierchical clustering and dendrogram",
    "text": "Hierchical clustering and dendrogram\n\n\nThe result of hierarchical clustering is a tree where leafs are labelled by sample points and internal nodes correspond to merging operations\nThe tree conveys more information: if the tree is properly decorated, it is possible to reconstruct the different merging steps and to know which rule was applied when some merging operation was performed\nThe tree is called a dendrogram\n\n\n\n\n\n\n\n\n\nViolent Crime Rates by US State\n\nDescription\n\nThis data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas."
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-7",
    "href": "core/slides/slides-hclust.html#section-7",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "What is an object of class dendro made of?\nIt a list of four elements:\n\nsegments\nlabels\nleaf_labels\nclass\n\nElement segments is a data frame with four columns. Each row represent a segment that is part of a graphical representation of the hierarchy. There are horizontal and vertical segments\nElement labels is used to label the tree leafs."
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-9",
    "href": "core/slides/slides-hclust.html#section-9",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Does the flat clustering obtained by cutting the dendrogram at some height reflect the partition into species?"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-11",
    "href": "core/slides/slides-hclust.html#section-11",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "dendextend\nA dendro gallery\nclass of dend"
  },
  {
    "objectID": "core/projects/hmw-dev-2025.html",
    "href": "core/projects/hmw-dev-2025.html",
    "title": "Hmw II: Regression, Package Development",
    "section": "",
    "text": "Important\n\n\n\n\nDue : April 27 2025\nWork in pairs\n Deliver your work through a github repository\n Present your work (10 minutes) on 2025-04-29 between 11:00 and 13:00 (Olympe de Gouges 358)\n\n\n\n\n Objectives\nThis homework is about\n\ndevelopping methods for objects produced by functions like gpava() and activeSet() from package isotone (R)\napplying monotone regression techniques to a dataset.\n\nI. Supplementing the broom and the ggfortify packages\nThe broom package offers S3 generic functions for building dataframes from the output of a variety of statistical techniques (for example lm, prcomp, or kmeans): augment(), tidy(), and glance().\nThe first goal of this homework is to design and code methods for generic functions augment, tidy, glance, and autoplot for classes gpava and activeSet defined in the isotone package (monotone regression).\nPackage ggplot2 offers a generic function autoplot(). See Tidyverse documentation on autoplot. Package ‘ggfortify’ offers a large collection of methods for autoplot.\nMore generally have a look at automatic plotting Also have a look at autolayer() generic in ggplot2.\n\nII. Applying monotone regression methods to real data\nThe Abalone dataset can be obtained from UIC Machine learning repository site The usual problem consists in predicting age (rings) as a function of the other variables (this is a regression problem).\nYou are expected to conduct exploratory analysis on this dataset. Besides linear regression of the response variable with respect to the others, you shall fit a monotone regression function to the dataset. Use the functions developped in the first part of the homework to discuss your results.\nUse the quarto package for reproducible research to write your report.\nThe report should be rendered at least in HTML format, and possibly also in PDF format.\n\n\nPackage development\nThe function and methods coded in this homework should be delivered as a package.\nFollow the package development guidelines in R Packages (2e).\n\nDefine a dedicated rstudio project for this homework\nDon’t forget documentation and testing\n\n\n\nReferences\n\nAdvanced R Programming\n\nFunctional programming\nS3\nMeta programming\n\nPackages\nProgramming with/for ggplot2\nProgramming with dplyr\ntidyeval helpers\nCheatsheets\nPackage isotone on CRAN\n\n\n\n Grading criteria\n\n\n\n\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nDocumentation/Report\n40%\nEnglish/French \n\n\nTesting\n25%\n\n\n\nCoding\n35%"
  },
  {
    "objectID": "core/projects/hmw-svd-2025.html",
    "href": "core/projects/hmw-svd-2025.html",
    "title": "Hmw III: SVD methods",
    "section": "",
    "text": "Important\n\n\n\n\nDue : May 29 2025\nWork in pairs\n Deliver your work through a github repository\n Present your work (15 minutes) on 2025-05-30"
  },
  {
    "objectID": "core/projects/hmw-svd-2025.html#references",
    "href": "core/projects/hmw-svd-2025.html#references",
    "title": "Hmw III: SVD methods",
    "section": "References",
    "text": "References\n\nAdvanced R Programming\n\nFunctional programming\nS3\nMeta programming\n\nPackages\nProgramming with/for ggplot2\nProgramming with dplyr\ntidyeval helpers\nCheatsheets\n\n\n\n Grading criteria\n\n\n\n\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nDocumentation/Report\n45%\nEnglish/French \n\n\nPresentation\n40%\n\n\n\nData gathering/cleaning pipelines\n15%"
  },
  {
    "objectID": "core/projects/hmw-votes-2025.html",
    "href": "core/projects/hmw-votes-2025.html",
    "title": "Hmw III: SVD methods and Elections Data",
    "section": "",
    "text": "Important\n\n\n\n\nDue : May 29 2025\nWork in pairs\n Deliver your work through a github repository\n Present your work (15 minutes) on 2025-06-03\nThis homework is about\nI. Voting Data\nVoting data per polling station can be obtained from a variety of websites.\nData concerning polling stations can also be gathered from various sources.\nYou first task will be to design an extraction pipeline to obtain the voting data you will analyse. You will gather data corresponding to different types of elections (Municipales, Régionales, Législatives, Européennes, Présidentielles) that took place since Year 2000.\ntitle\nrounds\nyear\n\n\n\n\nLocal\n2\n2008\n\n\nParliament\n2\n2012\n\n\nParliament\n2\n2017\n\n\nLocal\n2\n2020\n\n\nEuropean P\n1\n2024\nII. Data cleaning\nSome data cleaning may be necessary, for example\nIII. Applying Matrix Factorization Methods (SVD)\nFor one election round, the outcome is summarized by a tibble where rows (individuals) are polling stations and variables/columns are the number of votes obtained by the different condidates/parties.\nPerform PCA on different elections. Visualize and describe the result (attention, this is data analysis, not political science).\nYou may also perform CCA to compare different elections.\nFeel free to combine different methods.\nIV. Clustering\nPerform clustering on the outcomes of the Principal Component Analyses."
  },
  {
    "objectID": "core/projects/hmw-votes-2025.html#references",
    "href": "core/projects/hmw-votes-2025.html#references",
    "title": "Hmw III: SVD methods and Elections Data",
    "section": "References",
    "text": "References\n\nAdvanced R Programming\n\nFunctional programming\nS3\nMeta programming\n\nPackages\nProgramming with/for ggplot2\nProgramming with dplyr\ntidyeval helpers\nCheatsheets\n\n\n\n Grading criteria\n\n\n\n\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nDocumentation/Report\n45%\nEnglish/French \n\n\nPresentation\n40%\n\n\n\nData gathering/cleaning pipelines\n15%"
  },
  {
    "objectID": "core/projects/hmw-II.html",
    "href": "core/projects/hmw-II.html",
    "title": "Hmw II : Tables and visualization",
    "section": "",
    "text": "Important\n\n\n\n\nDue : May 29, 2025\nWork in pairs\nDeliver your work as a qmd file through a github  repository\nUse the quarto package for reproducible research\nUse pyspark or sparlyr\nUse spark-nlp for text annotation\nThe report should be rendered at least in HTML format, and possibly also in PDF format"
  },
  {
    "objectID": "core/projects/hmw-II.html#report-organization",
    "href": "core/projects/hmw-II.html#report-organization",
    "title": "Hmw II : Tables and visualization",
    "section": " Report organization",
    "text": "Report organization\nThe first part (introduction) of the report shall be dedicated to the description of the data to be extracted and to the extraction pipeline (not different from Homework I).\nThe second part of the report shall be dedicated to the description of load/transform pipeline.\nThe third part of the report shall be dedicated to the description of the annotation pipeline\nThe fourth part of the report shall be dedicated to the stylometric analysis: which questions did you pick up (and why?), plots, summary tables and comments. Refrain from overplaying your hand: yours plots are not likely to provide a new literary interpretation of Balzac opera. Comment the data, all the data, and nothing but the data.\nThe fifth part is the appendix. The first four parts should be mostly text and plots. The fifth part should be code only.\nThe appendix shall be dedicated to the details of the pipelines. You shall give the code.\nYou shall also give the code of the graphical pipelines in the appendix.\nYou shall avoid copy-paste coding. Don’t Repeat Yourself. knitr provide the tools to organize the Quarto file so that you can write your code once and use it many times, once for data wrangling and plotting (without echoing), then for listing and explanation.\n\n\n\n\n\n\nOrganizing a report using the jupyter engine"
  },
  {
    "objectID": "core/projects/hmw-II.html#references",
    "href": "core/projects/hmw-II.html#references",
    "title": "Hmw II : Tables and visualization",
    "section": " References",
    "text": "References\n\nData Humanities with R\nSpacy\nSpark NLP\nWeb scrapping\nscrapy\nstylo\n\npy2r\nComputational stylistics"
  },
  {
    "objectID": "core/projects/hmw-II.html#data-sources",
    "href": "core/projects/hmw-II.html#data-sources",
    "title": "Hmw II : Tables and visualization",
    "section": "Data sources",
    "text": "Data sources\n\nProject Gutenberg: you can find the La Comédie Humaine using a simple search. All volumes can be downloaded as text files from there."
  },
  {
    "objectID": "core/projects/hmw-II.html#grading-criteria",
    "href": "core/projects/hmw-II.html#grading-criteria",
    "title": "Hmw II : Tables and visualization",
    "section": " Grading criteria",
    "text": "Grading criteria\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nNarrative, spelling and syntax\n20%\nEnglish/French \n\n\nPlots correction\n15%\nchoice of aesthetics, geom, scale … \n\n\nPlot style\n10%\nTitles, legends, labels, breaks … \n\n\nETL\n20%\nETL, SQL like manipulations \n\n\nAnnotation\n10%\nAnnotations \n\n\nComputing Statistics\n5%\n… \n\n\nDRY compliance\n20%\nDRY principle at  Wikipedia"
  }
]