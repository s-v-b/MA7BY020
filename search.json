[
  {
    "objectID": "core/slides/slides-dplyr.html#tables-examples",
    "href": "core/slides/slides-dplyr.html#tables-examples",
    "title": "Tables manipulation with dplyr",
    "section": "Tables (examples)",
    "text": "Tables (examples)\n\nSpeadsheets (Excel)\n Relational tables\n\nDataframes in datascience frameworks\n\n\n: data.frame, tibble, …\n\n: pandas.dataframe\n\n\nspark: dataframe\n\n\nDask: dataframe\n\nand many others"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tables-why",
    "href": "core/slides/slides-dplyr.html#tables-why",
    "title": "Tables manipulation with dplyr",
    "section": "Tables (Why ?)",
    "text": "Tables (Why ?)\nIn Data Science, each framework comes with its own flavor(s) of table(s)\n Tables from relational databases serve as inspiration\nIn  legacy dataframes shape the life of statisticians and data scientists\nThe purpose of this session is\n\ndescribe dataframes from an end-user viewpoint (we leave aside implementations)\n\npresenting tools for\n\naccessing information within dataframes (querying)\nsummarizing information (aggregation queries)\ncleaning/cleaning dataframes (tidying)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#loading-tables-and-packages",
    "href": "core/slides/slides-dplyr.html#loading-tables-and-packages",
    "title": "Tables manipulation with dplyr",
    "section": "Loading tables and packages",
    "text": "Loading tables and packages\n\nrequire(\"tidyverse\")      # All we need is there\nrequire(\"nycflights13\")    # for flight data\nrequire(\"gt\")\nrequire(\"kableExtra\")\n# \ndata(flights)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#about-loaded-packages",
    "href": "core/slides/slides-dplyr.html#about-loaded-packages",
    "title": "Tables manipulation with dplyr",
    "section": "About loaded packages",
    "text": "About loaded packages\n\nMetapackage tidyverse provides tools to create, query, tidy dataframes as well as tools to load data from various sources and save them in persistent storage\nnycflights13 provides the dataframes we play with\ngt for tayloring table displays"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#the-flights-table",
    "href": "core/slides/slides-dplyr.html#the-flights-table",
    "title": "Tables manipulation with dplyr",
    "section": "The flights table",
    "text": "The flights table\n\n\n\nhead(flights) |&gt;\n  glimpse(width = 30) \n\nRows: 6\nColumns: 19\n$ year           &lt;int&gt; 2013,…\n$ month          &lt;int&gt; 1, 1,…\n$ day            &lt;int&gt; 1, 1,…\n$ dep_time       &lt;int&gt; 517, …\n$ sched_dep_time &lt;int&gt; 515, …\n$ dep_delay      &lt;dbl&gt; 2, 4,…\n$ arr_time       &lt;int&gt; 830, …\n$ sched_arr_time &lt;int&gt; 819, …\n$ arr_delay      &lt;dbl&gt; 11, 2…\n$ carrier        &lt;chr&gt; \"UA\",…\n$ flight         &lt;int&gt; 1545,…\n$ tailnum        &lt;chr&gt; \"N142…\n$ origin         &lt;chr&gt; \"EWR\"…\n$ dest           &lt;chr&gt; \"IAH\"…\n$ air_time       &lt;dbl&gt; 227, …\n$ distance       &lt;dbl&gt; 1400,…\n$ hour           &lt;dbl&gt; 5, 5,…\n$ minute         &lt;dbl&gt; 15, 2…\n$ time_hour      &lt;dttm&gt; 2013…\n\n\n\n\nA dataframe is a two-ways (two-dimensional) table\nhead(df) displays the first 6 rows of its first argument\nThe vectors making a dataframe may have different types/classes (a dataframe is not a matrix)\nCompare str(), glimpse(), head()"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#table-schema",
    "href": "core/slides/slides-dplyr.html#table-schema",
    "title": "Tables manipulation with dplyr",
    "section": "Table schema",
    "text": "Table schema\nA table is a list of columns\nEach column has\n\n\nname and\n\ntype (class in \n\n\n\n\nglimpse(flights,   #&lt;&lt;\n        width=50)\n\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12,…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 46…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, …\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 7…\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0,…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 201…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section",
    "href": "core/slides/slides-dplyr.html#section",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "flights has 19 columns\nEach column is a sequence (vector) of items with the same type/class\nAll columns have the same length\n\nflights has 336776 rows\nIn  parlance, a row is (often) called a tuple\n\nIn  parlance, a column is (often) called a variable"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#column-types",
    "href": "core/slides/slides-dplyr.html#column-types",
    "title": "Tables manipulation with dplyr",
    "section": "Column types",
    "text": "Column types\n\n\n\n\n\n\n\n\nclass\ncolumns\n\n\n\ninteger\n‘year’ ‘month’ ‘day’ ‘dep_time’ ‘sched_dep_time’ ‘arr_time’ ‘sched_arr_time’ ‘flight’\n\n\nnumeric\n‘dep_delay’ ‘arr_delay’ ‘air_time’ ‘distance’ ‘hour’ ‘minute’\n\n\ncharacter\n‘carrier’ ‘tailnum’ ‘origin’ ‘dest’\n\n\nPOSIXct\n‘time_hour’\n\n\nPOSIXt\n‘time_hour’\n\n\n\n\nA column, as a vector, may be belong to different classes\nOther classes: factor for categorical variables\nColumns dest, origin carrier could be coerced as factors\nShould columns dest and origin be coerced to the same factor?"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#nycflights13",
    "href": "core/slides/slides-dplyr.html#nycflights13",
    "title": "Tables manipulation with dplyr",
    "section": "nycflights13",
    "text": "nycflights13"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#columns-specification",
    "href": "core/slides/slides-dplyr.html#columns-specification",
    "title": "Tables manipulation with dplyr",
    "section": "Columns specification",
    "text": "Columns specification\n\n\n\nas.col_spec(flights)\n\n\n\ncols(\n  year = col_integer(),\n  month = col_integer(),\n  day = col_integer(),\n  dep_time = col_integer(),\n  sched_dep_time = col_integer(),\n  dep_delay = col_double(),\n  arr_time = col_integer(),\n  sched_arr_time = col_integer(),\n  arr_delay = col_double(),\n  carrier = col_character(),\n  flight = col_integer(),\n  tailnum = col_character(),\n  origin = col_character(),\n  dest = col_character(),\n  air_time = col_double(),\n  distance = col_double(),\n  hour = col_double(),\n  minute = col_double(),\n  time_hour = col_datetime(format = \"\")\n)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-1",
    "href": "core/slides/slides-dplyr.html#section-1",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "\\(\\approx\\) table schema in relational databases\nColumn specifications are useful when loading dataframes from structured text files like .csv files\n.csv files do not contain typing information\nFile loaders from package readr can be tipped about column classes using column specifications"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-2",
    "href": "core/slides/slides-dplyr.html#section-2",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "SQL stands for structured/simple Query Language\nA query language elaborated during the 1970’s at IBM by E. Codd\nGeared towards exploitation of collections of relational tables\nLess powerful but simpler to use than a programming language\ndplyr is a principled -friendly implementation of SQL ideas (and other things)\n\nAt the core of SQL lies the idea of a table calculus called relational algebra"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#relational-algebra-basics",
    "href": "core/slides/slides-dplyr.html#relational-algebra-basics",
    "title": "Tables manipulation with dplyr",
    "section": "Relational algebra (basics)",
    "text": "Relational algebra (basics)\nConvention: \\(R\\) is a table with columns \\(A_1, \\ldots, A_k\\)\n\n\n\n\n\n\nProjection (picking columns)\n\n\n\\(\\pi(R, A_1, A_3)\\)\n\n\n\n\n\n\n\n\n\nSelection/Filtering (picking rows)\n\n\n\\(\\sigma(R, {\\text{condition}})\\)\n\n\n\n\n\n\n\n\n\nJoin (mulitple tables operation)\n\n\n\\(\\bowtie(R,S, {\\text{condition}})\\)\n\n\n\n Any operation produces a table\n The schema of the derived table depends on the operation (but does not depend on the content/value of the operands)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-3",
    "href": "core/slides/slides-dplyr.html#section-3",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Table calculus relies on a small set of basic operations \\(\\pi, \\sigma, \\bowtie\\)\nEach operation has one or two table operands and produce a table\n There is more to SQL than relational algebra"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#projection-pi",
    "href": "core/slides/slides-dplyr.html#projection-pi",
    "title": "Tables manipulation with dplyr",
    "section": "Projection \\(\\pi\\)\n",
    "text": "Projection \\(\\pi\\)\n\n\\(\\pi(R, {A_1, A_3})\\)\nA projection \\(\\pi(\\cdot, {A_1, A_3})\\) is defined by a set of column names, say \\(A_1, A_3\\)\nIf \\(R\\) has columns with given names, the result is a table with names \\(A_1, A_3\\) and one row per row of \\(R\\)\nA projection is parametrized by a list of column names"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#package-dplyr",
    "href": "core/slides/slides-dplyr.html#package-dplyr",
    "title": "Tables manipulation with dplyr",
    "section": "\n Package dplyr\n",
    "text": "Package dplyr\n\n\n\n\nTranformation chapter in R4DS\nCheat sheet I\nCheat sheet II\n\n\n\n\n\n\nhttps://dplyr.tidyverse.org"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-4",
    "href": "core/slides/slides-dplyr.html#section-4",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Base  provides tools to perform relational algebra operations\nBut:\n\nBase  does not provide a consistent API\nThe lack of a consistent API makes operation chaining tricky"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#dplyr-verbs",
    "href": "core/slides/slides-dplyr.html#dplyr-verbs",
    "title": "Tables manipulation with dplyr",
    "section": "\ndplyr verbs",
    "text": "dplyr verbs\nFive basic verbs:\n\nPick observations/rows by their values (filter()) σ(…)\nPick variables by their names (select()) π(…)\nReorder the rows (arrange())\nCreate new variables with functions of existing variables (mutate())\nCollapse many values down to a single summary (summarise())\n\n\nAnd\n\n\ngroup_by() changes the scope of each function from operating on the entire dataset to operating on it group-by-group"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tidyverse",
    "href": "core/slides/slides-dplyr.html#tidyverse",
    "title": "Tables manipulation with dplyr",
    "section": "\n tidyverse",
    "text": "tidyverse\n\n\n\nAll verbs work similarly:\n\n\nThe first argument is a data frame (table).\n\n\nThe subsequent arguments describe what to do with the data frame, using the variable/column names (without quotes)\n\n\nThe result is a new data frame (table)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#dplyrselect-as-a-projection-operator-π",
    "href": "core/slides/slides-dplyr.html#dplyrselect-as-a-projection-operator-π",
    "title": "Tables manipulation with dplyr",
    "section": "\ndplyr::select() as a projection operator (π)",
    "text": "dplyr::select() as a projection operator (π)\n\\(\\pi(R, \\underbrace{A_1, \\ldots, A_3}_{\\text{column names}})\\)\nselect(R, A1, A3) #&lt;&lt;\nor, equivalently\nR |&gt; select(A1, A3) #&lt;&lt;\n |&gt; is the pipe operator\n x |&gt; f(y, z) is translated to f(x, y, z) and then evaluated"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#dplyrselect",
    "href": "core/slides/slides-dplyr.html#dplyrselect",
    "title": "Tables manipulation with dplyr",
    "section": "dplyr::select()",
    "text": "dplyr::select()\n\nFunction select has a variable number of arguments\nFunction select has a variable number of arguments\nFunction select allows to pick column by names (and much more)\nNote that in the current environment, there are no objects called A1, A3\nThe consistent API allows to use the pipe operator\n\n\n\n\n\n\n\nCaution\n\n\nThere is also a select() function in base R"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-tables",
    "href": "core/slides/slides-dplyr.html#toy-tables",
    "title": "Tables manipulation with dplyr",
    "section": "Toy tables",
    "text": "Toy tables\n\n\n\nspam &lt;- set.seed(42)\n\nR &lt;-  tibble(A1=seq(2, 10, 2),\n             A2=sample(letters, 5),\n             A3=seq(from=date(\"2021-10-21\"),\n                    to=date(\"2021-11-20\"),\n                    by=7),\n             D=sample(letters, 5))\n\nS &lt;- tibble(E=c(3,4,6,9, 10),\n            F=sample(letters, 5),\n            G=seq(from=date(\"2021-10-21\"),\n                   to=date(\"2021-10-21\")+4, by=1),\n            D=sample(letters,5)\n          )\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#projecting-toy-tables",
    "href": "core/slides/slides-dplyr.html#projecting-toy-tables",
    "title": "Tables manipulation with dplyr",
    "section": "Projecting toy tables",
    "text": "Projecting toy tables\n\n\n\nR |&gt; \n  dplyr::select(A2,D) |&gt; \n  knitr::kable(caption=\"Projecting R\")\n\n\nProjecting R\n\nA2\nD\n\n\n\nq\nr\n\n\ne\nq\n\n\na\no\n\n\nj\ng\n\n\nd\nd\n\n\n\n\n\n\n\nR |&gt; \n  dplyr::select(- where(is.character)) |&gt; \n  knitr::kable(caption=\"Projecting R, all but character columns\")\n\n\nProjecting R, all but character columns\n\nA1\nA3\n\n\n\n2\n2021-10-21\n\n\n4\n2021-10-28\n\n\n6\n2021-11-04\n\n\n8\n2021-11-11\n\n\n10\n2021-11-18"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#projecting-flights-on-origin-and-dest",
    "href": "core/slides/slides-dplyr.html#projecting-flights-on-origin-and-dest",
    "title": "Tables manipulation with dplyr",
    "section": "Projecting flights on origin and dest\n",
    "text": "Projecting flights on origin and dest\n\n\n\nflights |&gt;\n  select(origin, dest) |&gt;  #&lt;&lt;\n  head()\n\n\n# A tibble: 6 × 2\n  origin dest \n  &lt;chr&gt;  &lt;chr&gt;\n1 EWR    IAH  \n2 LGA    IAH  \n3 JFK    MIA  \n4 JFK    BQN  \n5 LGA    ATL  \n6 EWR    ORD  \n\n\nA more readable equivalent of\nhead(select(flights, origin, dest), 10)\nor\nSELECT \n  origin, dest\nFROM \n  flights\nLIMIT 6;"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#sigmar-textcondition",
    "href": "core/slides/slides-dplyr.html#sigmar-textcondition",
    "title": "Tables manipulation with dplyr",
    "section": "\\(\\sigma(R, \\text{condition})\\)",
    "text": "\\(\\sigma(R, \\text{condition})\\)\n\nA selection/filtering operation is defined by a condition that can be checked on the rows of tables with convenient schema\n\\(\\sigma(R, \\text{condition})\\) returns a table with the same schema as \\(R\\)\nThe resulting table contains the rows/tuples of \\(R\\) that satisfy \\(\\text{condition}\\)\n\\(\\sigma(R, \\text{FALSE})\\) returns an empty table with the same schema as \\(R\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#chaining-filtering-and-projecting",
    "href": "core/slides/slides-dplyr.html#chaining-filtering-and-projecting",
    "title": "Tables manipulation with dplyr",
    "section": "Chaining filtering and projecting",
    "text": "Chaining filtering and projecting\n\n\nstart &lt;- date(\"2021-10-27\")\nend &lt;- start + 21\n\nR |&gt;\n#  filter(A2 &gt; \"n\") |&gt;  #&lt;&lt;\n  filter(between(A3, start, end)) |&gt;\n  select(A1, A3) #&lt;&lt;\n\n\n# A tibble: 3 × 2\n     A1 A3        \n  &lt;dbl&gt; &lt;date&gt;    \n1     4 2021-10-28\n2     6 2021-11-04\n3     8 2021-11-11\n\n\n\n\nFiltering dropped one row\nProjecting dropped two columns"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#selecting-flights-based-on-origin-and-dest",
    "href": "core/slides/slides-dplyr.html#selecting-flights-based-on-origin-and-dest",
    "title": "Tables manipulation with dplyr",
    "section": "Selecting flights based on origin and dest\n",
    "text": "Selecting flights based on origin and dest\n\nand then projecting on dest, time_hour, carrier\n\n\nflights |&gt;\n  filter(dest %in% c('ATL', 'LAX'), #&lt;&lt;\n         origin == 'JFK') |&gt;\n  select(dest, time_hour, carrier) |&gt; #&lt;&lt;\n  head()\n\n\n# A tibble: 6 × 3\n  dest  time_hour           carrier\n  &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt;  \n1 LAX   2013-01-01 06:00:00 UA     \n2 ATL   2013-01-01 06:00:00 DL     \n3 LAX   2013-01-01 07:00:00 VX     \n4 LAX   2013-01-01 07:00:00 B6     \n5 LAX   2013-01-01 07:00:00 AA     \n6 ATL   2013-01-01 08:00:00 DL     \n\n\nIn SQL ( parlance:\nSELECT \n  dest, time_hour, carrier\nFROM \n  flights\nWHERE \n  dest IN ('ATL', 'LAX') AND\n  origin = 'JFK'\nLIMIT 6"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#logical-operations",
    "href": "core/slides/slides-dplyr.html#logical-operations",
    "title": "Tables manipulation with dplyr",
    "section": "Logical operations",
    "text": "Logical operations\n\nfilter(R, condition_1, condition_2) is meant to return the rows of R that satisfy condition_1 and condition_2\nfilter(R, condition_1 & condition_2) is an equivalent formulation\nfilter(R, condition_1 | condition_2) is meant to return the rows of R that satisfy condition_1 or condition_2 (possibly both)\nfilter(R, xor(condition_1,condition_2)) is meant to return the rows of R that satisfy either condition_1 or condition_2 (just one of them)\nfilter(R, ! condition_1) is meant to return the rows of R that do not satisfy condition_1"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#missing-values",
    "href": "core/slides/slides-dplyr.html#missing-values",
    "title": "Tables manipulation with dplyr",
    "section": "\n Missing values!",
    "text": "Missing values!\nNumerical column dep_time contains many NA's (missing values)\n\n# flights |&gt; pull(dep_time) |&gt; summary()\nsummary(flights$dep_time)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      1     907    1401    1349    1744    2400    8255 \n\n\n Missing values (NA and variants) should be handled with care\nNA & TRUE\n[1] NA\nNA | TRUE\n[1] TRUE"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#truth-tables-for-three-valued-logic",
    "href": "core/slides/slides-dplyr.html#truth-tables-for-three-valued-logic",
    "title": "Tables manipulation with dplyr",
    "section": "Truth tables for three-valued logic",
    "text": "Truth tables for three-valued logic\n\n\n  uses three-valued logic\n Generate complete truth tables for and, or, xor\n\nv &lt;- c(TRUE, FALSE, NA) # truth values\n\nlist_tt &lt;- map(c(`&`, `|`, xor),  #&lt;&lt;\n               ~ outer(v, v, .x)) #&lt;&lt;\n\nfor (i in seq_along(list_tt)){\n  colnames(list_tt[[i]]) &lt;- v\n  rownames(list_tt[[i]]) &lt;- v\n}\n\nnames(list_tt) &lt;- c('& AND',\n                    'OR',\n                    'XOR')\n\n\n\n\n\n\n& AND\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\n\nTRUE\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\nFALSE\n\n\nFALSE\n\n\nFALSE\n\n\nFALSE\n\n\n\n\nNA\n\n\nNA\n\n\nFALSE\n\n\nNA\n\n\n\n\n\n\n\n\nOR\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\n\nTRUE\n\n\nTRUE\n\n\nTRUE\n\n\nTRUE\n\n\n\n\nFALSE\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\nNA\n\n\nTRUE\n\n\nNA\n\n\nNA\n\n\n\n\n\n\n\n\nXOR\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nTRUE\n\n\nNA\n\n\n\n\nFALSE\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#slice-choosing-rows-based-on-location",
    "href": "core/slides/slides-dplyr.html#slice-choosing-rows-based-on-location",
    "title": "Tables manipulation with dplyr",
    "section": "\nslice(): choosing rows based on location",
    "text": "slice(): choosing rows based on location\n\n\nIn base  dataframe cells can be addressed by indices\nflights[5000:5010,seq(1, 19, by=5)] returns rows 5000:5010 and columns 1, 6, 11 from dataframe flights\nThis can be done in a (verbose) dplyr way using slice() and select()\n\n\nflights |&gt;\n  slice(5001:5005) |&gt;  #&lt;&lt;\n  select(seq(1, 19, by=5))\n\n# A tibble: 5 × 4\n   year dep_delay flight distance\n  &lt;int&gt;     &lt;dbl&gt;  &lt;int&gt;    &lt;dbl&gt;\n1  2013         3   4437      602\n2  2013        43   1016      187\n3  2013        -2   2190     1089\n4  2013        -1     91     2576\n5  2013         5   2131      502\n\n\n\n combined with aggregation (group_by()) variants of slice_ may be used to perform windowing operations\n Useful variant slice_sample()"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-5",
    "href": "core/slides/slides-dplyr.html#section-5",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Note\n\n\n\\(\\bowtie(R,S, {\\text{condition}})\\)\nstands for\n\njoin rows/tuples of \\(R\\) and rows/tuples of \\(S\\) that satisfy \\(\\text{condition}\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#nycflights-tables",
    "href": "core/slides/slides-dplyr.html#nycflights-tables",
    "title": "Tables manipulation with dplyr",
    "section": "\nnycflights tables",
    "text": "nycflights tables\n\n\nThe nycflights13 package offers five related tables:\n\n\nFact tables:\n\nflights\n\nweather (hourly weather conditions at different locations)\n\n\n\nDimension tables:\n\n\nairports (airports full names, location, …)\n\nplanes (model, manufacturer, year, …)\n\nairlines (full names)\n\n\n\nThis is an instance of a Star Schema"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#about-star-schemas",
    "href": "core/slides/slides-dplyr.html#about-star-schemas",
    "title": "Tables manipulation with dplyr",
    "section": "About Star schemas\n",
    "text": "About Star schemas\n\n\nFact tables record measurements for a specific event\nFact tables generally consist of numeric values, and foreign keys to dimensional data where descriptive information is kept\nDimension tables record informations about entities involved in events recorded in Fact tables\n\n\n\nFrom Wikipedia]"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#weather-conditions",
    "href": "core/slides/slides-dplyr.html#weather-conditions",
    "title": "Tables manipulation with dplyr",
    "section": "\n weather conditions",
    "text": "weather conditions\n\n\nweather |&gt;\n  glimpse(width = 50)\n\n\nRows: 26,115\nColumns: 15\n$ origin     &lt;chr&gt; \"EWR\", \"EWR\", \"EWR\", \"EWR\", \"…\n$ year       &lt;int&gt; 2013, 2013, 2013, 2013, 2013,…\n$ month      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ day        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ hour       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10…\n$ temp       &lt;dbl&gt; 39.02, 39.02, 39.02, 39.92, 3…\n$ dewp       &lt;dbl&gt; 26.06, 26.96, 28.04, 28.04, 2…\n$ humid      &lt;dbl&gt; 59.37, 61.63, 64.43, 62.21, 6…\n$ wind_dir   &lt;dbl&gt; 270, 250, 240, 250, 260, 240,…\n$ wind_speed &lt;dbl&gt; 10.35702, 8.05546, 11.50780, …\n$ wind_gust  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, N…\n$ precip     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pressure   &lt;dbl&gt; 1012.0, 1012.3, 1012.5, 1012.…\n$ visib      &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 1…\n$ time_hour  &lt;dttm&gt; 2013-01-01 01:00:00, 2013-01…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#connecting-flights-and-weather",
    "href": "core/slides/slides-dplyr.html#connecting-flights-and-weather",
    "title": "Tables manipulation with dplyr",
    "section": "Connecting flights and weather\n",
    "text": "Connecting flights and weather\n\nWe want to complement information in flights using data weather\nMotivation: we would like to relate delays (arr_delay) and weather conditions\n\ncan we explain (justify) delays using weather data?\ncan we predict delays using weather data?"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-6",
    "href": "core/slides/slides-dplyr.html#section-6",
    "title": "Tables manipulation with dplyr",
    "section": "\n ⋈ \n",
    "text": "⋈ \n\nFor each flight (row in flights)\n\nyear, month, day, hour (computed from time_hour) indicate the approaximate time of departure\norigin indicates the airport where the plane takes off\n\nEach row of weather contains corresponding information\n for each row of flights we look for rows of weather with matching values in year, month, day, hour and origin\n NATURAL INNER JOIN between the tables"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#inner_join-natural-join",
    "href": "core/slides/slides-dplyr.html#inner_join-natural-join",
    "title": "Tables manipulation with dplyr",
    "section": "\ninner_join: natural join",
    "text": "inner_join: natural join\n\n\nf_w &lt;- flights |&gt;\n  inner_join(weather) #&lt;&lt;\n\nf_w |&gt; \n  select(seq(1, \n             ncol(f_w),\n             by=2)) |&gt; \n  glimpse(width=50)\n\n\nRows: 335,220\nColumns: 14\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12,…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 46…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 201…\n$ dewp           &lt;dbl&gt; 28.04, 24.98, 26.96, 26.9…\n$ wind_dir       &lt;dbl&gt; 260, 250, 260, 260, 260, …\n$ wind_gust      &lt;dbl&gt; NA, 21.86482, NA, NA, 23.…\n$ pressure       &lt;dbl&gt; 1011.9, 1011.4, 1012.1, 1…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#join-schema",
    "href": "core/slides/slides-dplyr.html#join-schema",
    "title": "Tables manipulation with dplyr",
    "section": "Join schema",
    "text": "Join schema\n\n\nRows: 335,220\nColumns: 28\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12,…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 46…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, …\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 7…\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0,…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 201…\n$ temp           &lt;dbl&gt; 39.02, 39.92, 39.02, 39.0…\n$ dewp           &lt;dbl&gt; 28.04, 24.98, 26.96, 26.9…\n$ humid          &lt;dbl&gt; 64.43, 54.81, 61.63, 61.6…\n$ wind_dir       &lt;dbl&gt; 260, 250, 260, 260, 260, …\n$ wind_speed     &lt;dbl&gt; 12.65858, 14.96014, 14.96…\n$ wind_gust      &lt;dbl&gt; NA, 21.86482, NA, NA, 23.…\n$ precip         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pressure       &lt;dbl&gt; 1011.9, 1011.4, 1012.1, 1…\n$ visib          &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 1…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-7",
    "href": "core/slides/slides-dplyr.html#section-7",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "The schema of the result is the union of the schemas of the operands\nA tuple from flights matches a tuple from weather if the tuple have the same values in the common columns:\n\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\"      \"temp\"          \n[21] \"dewp\"           \"humid\"          \"wind_dir\"       \"wind_speed\"    \n[25] \"wind_gust\"      \"precip\"         \"pressure\"       \"visib\""
  },
  {
    "objectID": "core/slides/slides-dplyr.html#which-columns-are-used-when-joining-tables-r-and-s",
    "href": "core/slides/slides-dplyr.html#which-columns-are-used-when-joining-tables-r-and-s",
    "title": "Tables manipulation with dplyr",
    "section": "Which columns are used when joining tables \\(R\\) and \\(S\\)?",
    "text": "Which columns are used when joining tables \\(R\\) and \\(S\\)?\n\ndefault behavior of inner_join: all columns shared by \\(R\\) and \\(S\\). Common columns have the same name in both schema. They are expected to have the same class\nmanual definition: in many settings, we want to overrule the default behavior. We specify manually which column from \\(R\\) should match which column from \\(S\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#natural-join-of-flights-and-weather",
    "href": "core/slides/slides-dplyr.html#natural-join-of-flights-and-weather",
    "title": "Tables manipulation with dplyr",
    "section": "Natural join of flights and weather:",
    "text": "Natural join of flights and weather:\n\ncommon_names &lt;- base::intersect(names(weather),\n                                names(flights))\n\nsetequal(\n  inner_join(flights, weather),\n  inner_join(flights,\n             weather,\n             by=common_names)\n)\n\n[1] TRUE"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#are-you-surprised-by-the-next-chunk",
    "href": "core/slides/slides-dplyr.html#are-you-surprised-by-the-next-chunk",
    "title": "Tables manipulation with dplyr",
    "section": "\n Are you surprised by the next chunk?",
    "text": "Are you surprised by the next chunk?\n\ndtu  &lt;- inner_join(flights,\n           weather,\n           by=c(\"year\", \"month\", \"day\", \"origin\", \"hour\"))\n\ndtv &lt;- inner_join(flights,\n           weather,\n           by=c(\"origin\", \"time_hour\"))\n\n# setequal(dtu, dtv)\n\nRecall that columns year, month day hour can be computed from time_hour\n\nflights |&gt;\n  filter(year!=year(time_hour) |\n         month!=month(time_hour) |\n         day!=day(time_hour) |\n         hour!=hour(time_hour)) |&gt;\n  nrow()\n\n[1] 0"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#the-two-results-do-not-have-the-same-schema",
    "href": "core/slides/slides-dplyr.html#the-two-results-do-not-have-the-same-schema",
    "title": "Tables manipulation with dplyr",
    "section": "The two results do not have the same schema!",
    "text": "The two results do not have the same schema!\n\nsetdiff(colnames(dtv), colnames(dtu))\n\n[1] \"year.x\"    \"month.x\"   \"day.x\"     \"hour.x\"    \"time_hour\" \"year.y\"   \n[7] \"month.y\"   \"day.y\"     \"hour.y\"   \n\nsetdiff(colnames(dtu), colnames(dtv))\n\n[1] \"year\"        \"month\"       \"day\"         \"hour\"        \"time_hour.x\"\n[6] \"time_hour.y\""
  },
  {
    "objectID": "core/slides/slides-dplyr.html#fixing",
    "href": "core/slides/slides-dplyr.html#fixing",
    "title": "Tables manipulation with dplyr",
    "section": "Fixing",
    "text": "Fixing\n\ndtu  &lt;- inner_join(flights,\n           weather,\n           by=c(\"year\", \"month\", \"day\", \"origin\", \"hour\"),\n           suffix= c(\"\", \".y\")) |&gt;  #&lt;&lt;\n           select(-ends_with(\".y\"))  #&lt;&lt;\n\ndtv &lt;- inner_join(flights,\n           weather,\n           by=c(\"origin\", \"time_hour\"),\n           suffix= c(\"\", \".y\")) |&gt;  #&lt;&lt;\n           select(-ends_with(\".y\"))  #&lt;&lt;\n\nsetequal(dtu, dtv)\n\n[1] TRUE"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#about-inner_join",
    "href": "core/slides/slides-dplyr.html#about-inner_join",
    "title": "Tables manipulation with dplyr",
    "section": "About inner_join\n",
    "text": "About inner_join\n\n\n\ninner_join(\n  x, y,\n  by = NULL,      #&lt;&lt;\n  copy = FALSE,\n  suffix = c(\".x\", \".y\"), #&lt;&lt;\n  ...,\n  keep = FALSE,  #&lt;&lt;\n  na_matches = \"na\")  #&lt;&lt;\n\n\n\nby:\n\n\nby=c(\"A1\", \"A3\", \"A7\") row r from R and s from S match if r.A1 == s.A1, r.A3 == s.A3, r.A7 == s.A7\n\n\nby=c(\"A1\"=\"B\", \"A3\"=\"C\", \"A7\"=\"D\") row r from R and s from S match if r.A1 == s.B, r.A3 == s.C, r.A7 == s.D\n\n\n\nsuffix: If there are non-joined duplicate variables in x and y, these suffixes will be added to the output to disambiguate them.\nkeep: Should the join keys from both x and y be preserved in the output?\nna_matches: Should NA and NaN values match one another?\n\n\n\n\nFrom online documentation"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#join-flavors",
    "href": "core/slides/slides-dplyr.html#join-flavors",
    "title": "Tables manipulation with dplyr",
    "section": "Join flavors",
    "text": "Join flavors\nDifferent flavors of join can be used to join one table to columns from another, matching values with the rows that they correspond to\nEach join retains a different combination of values from the tables\n\n\nleft_join(x, y, by = NULL, suffix = c(\".x\", \".y\"), ...) Join matching values from y to x. Retain all rows of x padding missing values from y by NA\nsemi_join …\nanti_join …"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-examples-inner_join",
    "href": "core/slides/slides-dplyr.html#toy-examples-inner_join",
    "title": "Tables manipulation with dplyr",
    "section": "Toy examples : inner_join\n",
    "text": "Toy examples : inner_join\n\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\ninner_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD.x\nA2\nA3\nD.y\n\n\n\n4\ne\n2021-10-22\nc\ne\n2021-10-28\nq\n\n\n6\nn\n2021-10-23\ni\na\n2021-11-04\no\n\n\n10\nr\n2021-10-25\ne\nd\n2021-11-18\nd"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-examples-left_join",
    "href": "core/slides/slides-dplyr.html#toy-examples-left_join",
    "title": "Tables manipulation with dplyr",
    "section": "Toy examples : left_join\n",
    "text": "Toy examples : left_join\n\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\nleft_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD.x\nA2\nA3\nD.y\n\n\n\n3\ny\n2021-10-21\no\nNA\nNA\nNA\n\n\n4\ne\n2021-10-22\nc\ne\n2021-10-28\nq\n\n\n6\nn\n2021-10-23\ni\na\n2021-11-04\no\n\n\n9\nt\n2021-10-24\nd\nNA\nNA\nNA\n\n\n10\nr\n2021-10-25\ne\nd\n2021-11-18\nd"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-examples-semi_join-anti_join",
    "href": "core/slides/slides-dplyr.html#toy-examples-semi_join-anti_join",
    "title": "Tables manipulation with dplyr",
    "section": "Toy examples : semi_join anti_join\n",
    "text": "Toy examples : semi_join anti_join\n\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\nsemi_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD\n\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\nanti_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n9\nt\n2021-10-24\nd"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#conditional-theta--join",
    "href": "core/slides/slides-dplyr.html#conditional-theta--join",
    "title": "Tables manipulation with dplyr",
    "section": "Conditional/ \\(\\theta\\) -join",
    "text": "Conditional/ \\(\\theta\\) -join\nIn relational databases, joins are not restricted to natural joins\n\n\\[U \\leftarrow R \\bowtie_{\\theta} S\\]\nreads as\n\\[\\begin{array}{rl} T & \\leftarrow R \\times S\\\\ U & \\leftarrow \\sigma(T, \\theta)\\end{array}\\]\nwhere\n\n\\(R \\times S\\) is the cartesian product of \\(R\\) and \\(S\\)\n\\(\\theta\\) is a boolean expression that can be evaluated on any tuple of \\(R \\times S\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins",
    "href": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins",
    "title": "Tables manipulation with dplyr",
    "section": "Do we need conditional/ \\(\\theta\\) -joins?",
    "text": "Do we need conditional/ \\(\\theta\\) -joins?\n\n\n\n\n\n\nNote\n\n\n: We can implement \\(\\theta\\)/conditional-joins by pipelining a cross product and a filtering\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n: Cross products are costly:\n\n\\(\\#\\text{rows}(R \\times S) = \\#\\text{rows}(R) \\times \\#\\text{rows}(S)\\)\n\\(\\#\\text{cols}(R \\times S) = \\#\\text{cols}(R) + \\#\\text{cols}(S)\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins-1",
    "href": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins-1",
    "title": "Tables manipulation with dplyr",
    "section": "Do we need conditional/ \\(\\theta\\) -joins?",
    "text": "Do we need conditional/ \\(\\theta\\) -joins?\n\n\n\n\n\n\nNote\n\n\n: RDBMS use query planning and optimization, indexing to circumvent the cross product bottleneck (when possible)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n: if we need to perform a \\(\\theta\\)-join\n\noutsource it to a RDBMS, or\ndesign an ad hoc pipeline\n\n\n\n\n\n\n\nAbout conditional join"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#a-conditional-join-between-flights-and-weather",
    "href": "core/slides/slides-dplyr.html#a-conditional-join-between-flights-and-weather",
    "title": "Tables manipulation with dplyr",
    "section": "A conditional join between flights and weather\n",
    "text": "A conditional join between flights and weather\n\n\nThe natural join between flights and weather we implemented can be regarded as an ad hoc conditional join between normalized versions of weather and flights \nTable flights and weather are redundant: year, month, day, hour can be computed from time_hour\nAssume flights and weather are trimmed so as to become irredundant\nThe conditional join is then based on truncations of variables time_hour\n\nSELECT \n  *\nFROM \n  flights AS f, weather AS w\nWHERE \n  date_trunc('hour', f.time_hour) = date_trunc('hour', w.time_hour)\n\nAdding redundant columns to flights and weather allows us to transform a tricky conditional join into a simple natural join \n\n\n\n\nPostgreSQL documentation"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-8",
    "href": "core/slides/slides-dplyr.html#section-8",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Creation of new columns may happen\n\non the fly\nwhen altering (enriching) the schema of a table\n\nIn databases, creation of new columns may be the result of a query or be the result of altering a table schema with ALTER TABLE ADD COLUMN ...\nIn tidyverse() we use verbs mutate or add_column to add columns to the input table"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#mutate",
    "href": "core/slides/slides-dplyr.html#mutate",
    "title": "Tables manipulation with dplyr",
    "section": "mutate",
    "text": "mutate\n\n\nmutate(   #&lt;&lt;\n  .data,\n  new_col= expression, #&lt;&lt;\n  ...,   #&lt;&lt;\n  .keep = c(\"all\", \"used\", \"unused\", \"none\"),\n  .before = NULL,\n  .after = NULL\n)\n\n.data: the input data frame\nnew_col= expression:\n\nnew_col is the name of a new column\nexpression is evaluated on each row of .data or it is a vector of length 1\nall is the default behavior, retains all columns from .data"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#creating-a-categorical-column-to-spot-large-delays",
    "href": "core/slides/slides-dplyr.html#creating-a-categorical-column-to-spot-large-delays",
    "title": "Tables manipulation with dplyr",
    "section": "Creating a categorical column to spot large delays",
    "text": "Creating a categorical column to spot large delays\n\n\nbreaks_delay &lt;- with(flights,\n  c(min(arr_delay, na.rm=TRUE),\n    0, 30,\n    max(arr_delay, na.rm=TRUE))\n)\n\nlevel_delay &lt;- c(\"None\",\n                 \"Moderate\",\n                 \"Large\")\n\nflights |&gt;\n  mutate(large_delay = cut(\n    arr_delay,  #&lt;&lt;\n    breaks=breaks_delay, #&lt;&lt;\n    labels=level_delay,  #&lt;&lt;\n    ordered_result=TRUE)) |&gt;   #&lt;&lt;\n  select(large_delay, arr_delay) |&gt;\n  sample_n(5)\n\n\n# A tibble: 5 × 2\n  large_delay arr_delay\n  &lt;ord&gt;           &lt;dbl&gt;\n1 Large             219\n2 Moderate           18\n3 None              -19\n4 None              -16\n5 None               -1"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-9",
    "href": "core/slides/slides-dplyr.html#section-9",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "flights |&gt;\n  mutate(foo = if_else(arr_time &gt; sched_arr_time,        #&lt;&lt;\n                              arr_time - sched_arr_time,\n                              0L,\n                              missing = NA_integer_)) |&gt;\n  group_by( (foo &gt;0) & abs(foo - arr_delay)  &gt; 100) |&gt;\n  summarise(N=n())\n\n\n# A tibble: 3 × 2\n  `(foo &gt; 0) & abs(foo - arr_delay) &gt; 100`      N\n  &lt;lgl&gt;                                     &lt;int&gt;\n1 FALSE                                    322281\n2 TRUE                                       5157\n3 NA                                         9338"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#changing-the-class-of-a-column",
    "href": "core/slides/slides-dplyr.html#changing-the-class-of-a-column",
    "title": "Tables manipulation with dplyr",
    "section": "Changing the class of a column",
    "text": "Changing the class of a column\n\n\nflights |&gt;\n  mutate(large_delay = cut(arr_delay,  #&lt;&lt;\n    breaks=breaks_delay,\n    labels=level_delay,\n    ordered_result=TRUE),\n    origin = as.factor(origin), #&lt;&lt;\n    dest = as.factor(dest)    #&lt;&lt;\n  ) |&gt;\n  select(\n    large_delay,\n    arr_delay,\n    origin,\n    dest) |&gt;\n  sample_n(5)\n\n\n# A tibble: 5 × 4\n  large_delay arr_delay origin dest \n  &lt;ord&gt;           &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;\n1 None              -44 LGA    CVG  \n2 None              -15 EWR    DAY  \n3 Large             136 EWR    DEN  \n4 None               -9 EWR    TPA  \n5 Moderate           14 LGA    TPA"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tidying-tables-is-part-of-data-cleaning",
    "href": "core/slides/slides-dplyr.html#tidying-tables-is-part-of-data-cleaning",
    "title": "Tables manipulation with dplyr",
    "section": "Tidying tables is part of data cleaning",
    "text": "Tidying tables is part of data cleaning\n\nA (tidy) dataset is a collection of values, usually either numbers (if quantitative) or strings (if qualitative)\n\n\nValues are organised in two ways\n\n\nEvery value belongs to a variable and an observation\n\n\nA variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units\n\n\nAn observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes\n\n\nThe principles of tidy data are tied to those of relational databases and Codd’s relational algebra\n\n\n\n The tidy data paper"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#codds-principles",
    "href": "core/slides/slides-dplyr.html#codds-principles",
    "title": "Tables manipulation with dplyr",
    "section": "Codd’s principles",
    "text": "Codd’s principles\n\nInformation is represented logically in tables\n\nData must be logically accessible by table, primary key, and column.\n\nNull values must be uniformly treated as “missing information,” not as empty strings, blanks, or zeros.\nMetadata (data about the database) must be stored in the database just as regular data is\nA single language must be able to define data, views, integrity constraints, authorization, transactions, and data manipulation\n\nViews must show the updates of their base tables and vice versa\nA single operation must be available to do each of the following operations: retrieve data, insert data, update data, or delete data\nBatch and end-user operations are logically separate from physical storage and access methods\nBatch and end-user operations can change the database schema without having to recreate it or the applications built upon it\n\nIntegrity constraints must be available and stored in the metadata, not in an application program\nThe data manipulation language of the relational system should not care where or how the physical data is distributed and should not require alteration if the physical data is centralized or distributed\nAny row processing done in the system must obey the same integrity rules and constraints that set-processing operations do"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-10",
    "href": "core/slides/slides-dplyr.html#section-10",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "dplyr functions expect and return tidy tables\nIn a tidy table\n\nEach variable is a column\nEach observation is a row\nEvery cell is a single value\n\n\n\n The tidy data paper]"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-11",
    "href": "core/slides/slides-dplyr.html#section-11",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "In order to tell whether a table is tidy, we need to know what is the population under investigation, what are the observations/individuals, which measures are performed on each individual, …"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#untidy-data",
    "href": "core/slides/slides-dplyr.html#untidy-data",
    "title": "Tables manipulation with dplyr",
    "section": "Untidy data",
    "text": "Untidy data\n\nColumn headers are values, not variable names.\n\n\nMultiple variables are stored in one column.\n\n\nVariables are stored in both rows and columns.\n\n\nMultiple types of observational units are stored in the same table.\n\n\nA single observational unit is stored in multiple tables.\n\n\n…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#functions-from-tidyr...",
    "href": "core/slides/slides-dplyr.html#functions-from-tidyr...",
    "title": "Tables manipulation with dplyr",
    "section": "Functions from tidyr::...\n",
    "text": "Functions from tidyr::...\n\n\npivot_wider and pivot_longer\nseparate and unite\nHandling missing values with complete, fill, …\n…\n\ntidyr website"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#pivot-longer",
    "href": "core/slides/slides-dplyr.html#pivot-longer",
    "title": "Tables manipulation with dplyr",
    "section": "Pivot longer",
    "text": "Pivot longer\n\n\npivot_longer() is commonly needed to tidy wild-caught datasets as they often optimise for ease of data entry or ease of comparison rather than ease of analysis.\n\n\n\nmessy &lt;- tibble::tribble(\n  ~row, ~a, ~b, ~c,\n  \"A\", 1, 4, 7,\n  \"B\", 2, 5, 8,\n  \"C\", 3, 6, 9,\n)\nmessy |&gt; kable()\n\n\n\n\nrow\na\nb\nc\n\n\n\nA\n1\n4\n7\n\n\nB\n2\n5\n8\n\n\nC\n3\n6\n9\n\n\n\n\n\n\n\nmessy_long &lt;- messy |&gt; \n  pivot_longer(\n    cols=c(-row),  #&lt;&lt;\n    names_to = \"name\",\n    values_to = \"value\")\n  \nmessy_long  |&gt; \n  kable()\n\n\n\n\nrow\nname\nvalue\n\n\n\nA\na\n1\n\n\nA\nb\n4\n\n\nA\nc\n7\n\n\nB\na\n2\n\n\nB\nb\n5\n\n\nB\nc\n8\n\n\nC\na\n3\n\n\nC\nb\n6\n\n\nC\nc\n9\n\n\n\n\n\n\n\n\n\npivot_longer() makes datasets longer by increasing the number of rows and decreasing the number of columns. I don’t believe it makes sense to describe a dataset as being in “long form”. Length is a relative term, and you can only say (e.g.) that dataset A is longer than dataset B."
  },
  {
    "objectID": "core/slides/slides-dplyr.html#pivot-wider",
    "href": "core/slides/slides-dplyr.html#pivot-wider",
    "title": "Tables manipulation with dplyr",
    "section": "Pivot wider",
    "text": "Pivot wider\n\n\npivot_wider(  #&lt;&lt;\n  data,\n  id_cols = NULL, #&lt;&lt;\n  names_from = name, #&lt;&lt;\n  names_prefix = \"\",\n  values_from = value, #&lt;&lt;\n  ...\n)\n some optional arguments are missing\n\nWhen reporting, we often use pivot_wider (explicitely or implicitely) to make results more readable, possibly to conform to a tradition\n\nLife tables in demography and actuarial science\nLongitudinal data\nSee slide How many flights per day of week per departure airport?"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#pivot_wider-in-action",
    "href": "core/slides/slides-dplyr.html#pivot_wider-in-action",
    "title": "Tables manipulation with dplyr",
    "section": "\npivot_wider() in action",
    "text": "pivot_wider() in action\n\n\nmessy_long |&gt;\n  pivot_wider( \n  id_cols = c(\"row\"), #&lt;&lt;\n  names_from = name, #&lt;&lt;\n  names_prefix = \"\",\n  values_from = value\n)\n\n\n# A tibble: 3 × 4\n  row       a     b     c\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         1     4     7\n2 B         2     5     8\n3 C         3     6     9"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#how-many-flights-per-carrier",
    "href": "core/slides/slides-dplyr.html#how-many-flights-per-carrier",
    "title": "Tables manipulation with dplyr",
    "section": "How many flights per carrier?",
    "text": "How many flights per carrier?\n\n\nflights |&gt;\n  group_by(carrier) |&gt;  #&lt;&lt;\n  summarise(count=n()) |&gt;  #&lt;&lt;\n  arrange(desc(count))\n\n\n# A tibble: 16 × 2\n   carrier count\n   &lt;chr&gt;   &lt;int&gt;\n 1 UA      58665\n 2 B6      54635\n 3 EV      54173\n 4 DL      48110\n 5 AA      32729\n 6 MQ      26397\n 7 US      20536\n 8 9E      18460\n 9 WN      12275\n10 VX       5162\n11 FL       3260\n12 AS        714\n13 F9        685\n14 YV        601\n15 HA        342\n16 OO         32\n\n\nSELECT \n  carrier, COUNT(*) AS n\nFROM \n  flights\nGROUP BY \n  carrier\nORDER BY \n  n DESCENDING"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#how-many-flights-per-day-of-week-per-departure-airport",
    "href": "core/slides/slides-dplyr.html#how-many-flights-per-day-of-week-per-departure-airport",
    "title": "Tables manipulation with dplyr",
    "section": "How many flights per day of week per departure airport?",
    "text": "How many flights per day of week per departure airport?\n\n\nflights |&gt;\n  group_by(origin,  wday(time_hour, abbr=T, label=T)) |&gt;  #&lt;&lt;\n  summarise(count=n(), .groups=\"drop\") |&gt;       #&lt;&lt;\n  rename(day_of_week=`wday(time_hour, abbr = T, label = T)`) |&gt;\n  pivot_wider(  #&lt;&lt;\n    id_cols=\"origin\",   #&lt;&lt;\n    names_from=\"day_of_week\", #&lt;&lt;\n    values_from=\"count\") |&gt;  #&lt;&lt;\n  kable(caption=\"Departures per day\")\n\n\n\nDepartures per day\n\norigin\ndim.\nlun.\nmar.\nmer.\njeu.\nven.\nsam.\n\n\n\nEWR\n16425\n18329\n18243\n18180\n18169\n18142\n13347\n\n\nJFK\n15966\n16104\n16017\n15841\n16087\n16176\n15088\n\n\nLGA\n13966\n16257\n16162\n16039\n15963\n15990\n10285"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#window-queries-1",
    "href": "core/slides/slides-dplyr.html#window-queries-1",
    "title": "Tables manipulation with dplyr",
    "section": "Window queries",
    "text": "Window queries\nAssume we want to answer the question: for each day of week (Monday, Tuesday, …), what are the five carriers that experience the largest average delay?\n\n\nflights |&gt;\n  group_by(weekdays(time_hour), carrier) |&gt;\n  summarise(avg_dep_delay=mean(dep_delay, na.rm=T)) |&gt;\n  slice_max(n=2, order_by=avg_dep_delay)\n\n\n# A tibble: 14 × 3\n# Groups:   weekdays(time_hour) [7]\n   `weekdays(time_hour)` carrier avg_dep_delay\n   &lt;chr&gt;                 &lt;chr&gt;           &lt;dbl&gt;\n 1 dimanche              F9               23.7\n 2 dimanche              VX               17.4\n 3 jeudi                 YV               29.7\n 4 jeudi                 F9               26.5\n 5 lundi                 FL               24.8\n 6 lundi                 EV               23.4\n 7 mardi                 YV               19.1\n 8 mardi                 FL               17.7\n 9 mercredi              OO               52  \n10 mercredi              HA               24.5\n11 samedi                OO               41  \n12 samedi                F9               15.8\n13 vendredi              OO               29  \n14 vendredi              F9               25.6"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#the-sql-way",
    "href": "core/slides/slides-dplyr.html#the-sql-way",
    "title": "Tables manipulation with dplyr",
    "section": "The SQL way",
    "text": "The SQL way\nWITH R AS (\n  SELECT \n    EXTRACT(dow FROM time_hour) AS day_of_week,\n    carrier,\n    AVG(dep_delay) AS avg_dep_delay\n  FROM \n    flights\n  GROUP BY \n    EXTRACT(dow FROM time_hour), carrier\n), S AS (\n  SELECT \n    day_of_week,\n    carrier,\n    rank() OVER (PARTITION by day_of_week ORDER BY avg_dep_delay DESC) AS rnk\n  FROM \n    R\n)\n\nSELECT \n  day_of_week, \n  carrier, \n  rnk\nFROM \n  S\nWHERE \n  rnk &lt;= 10 ;"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#sliding-windows-and-package-slider",
    "href": "core/slides/slides-dplyr.html#sliding-windows-and-package-slider",
    "title": "Tables manipulation with dplyr",
    "section": "Sliding windows and package slider\n",
    "text": "Sliding windows and package slider\n\nTODO"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#and-other-pipes",
    "href": "core/slides/slides-dplyr.html#and-other-pipes",
    "title": "Tables manipulation with dplyr",
    "section": "\n|>, %>% and other pipes",
    "text": "|&gt;, %&gt;% and other pipes\n\nAll dplyr functions take a table as the first argument\nRather than forcing the user to either save intermediate objects or nest functions, dplyr provides the |&gt; operator from magrittr\nx |&gt; f(y) turns into f(x, y)\nThe result from one step is piped into the next step\nUse |&gt; to rewrite multiple operations that you can read left-to-right/top-to-bottom\n\n\ng(f(x, y), z)\n\nx |&gt;\n  f(y) |&gt;\n  g(z)\n\n\n\nFrom dplyr vignette"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#magrittr",
    "href": "core/slides/slides-dplyr.html#magrittr",
    "title": "Tables manipulation with dplyr",
    "section": "Magrittr %>%\n",
    "text": "Magrittr %&gt;%\n\n\n\n\n\n%&gt;% is not tied to dplyr\n\n\n%&gt;% can be used with packages from tidyverse\n\n\n%&gt;% can be used outside tidyverse that is with functions which take a table (or something else) as a second, third or keyword argument\n\n Use pronoun . to denote the LHS of the pipe expression\n\nSecond argument of g has the same type as the result of f\n\ng(z, f(x, y))\n\nx %&gt;%\n  f(y) %&gt;%\n  g(z, .)   #&lt;&lt;\n\nx %&gt;% f(y) is a shorthand for x %&gt;% f(., y)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#standard-pipe-version-4.",
    "href": "core/slides/slides-dplyr.html#standard-pipe-version-4.",
    "title": "Tables manipulation with dplyr",
    "section": "Standard pipe |> (version > 4.)",
    "text": "Standard pipe |&gt; (version &gt; 4.)\nAs of version 4.1 (2021), base  offers a pipe operator denoted by |&gt;\n\n\nx |&gt; f(y) turns into f(x, y)\n\ng(f(x, y), z)\n\nx |&gt;\n  f(y) |&gt;\n  g(z)\n\n\n the standard pipe |&gt; has no pronoun/placeholder to denote the LHS of the pipe expression\nThe roundabout consists in using another new construct \\(x)\ng(z, w)\n\nx |&gt;\n  (\\(x) g(z, w=x))()\n\n\"une\" |&gt;\n  (\\(x) str_c(\"ceci n'est pas\", x, sep=\" \"))() |&gt;\n  str_c(\"pipe\", sep=\" \") |&gt;\n  cat()\n\nceci n'est pas une pipe\n\n\n\n\n\nSee Blog on the new standard pipe]"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#other-pipes",
    "href": "core/slides/slides-dplyr.html#other-pipes",
    "title": "Tables manipulation with dplyr",
    "section": "Other pipes",
    "text": "Other pipes\nMagrittr offers several variants of |&gt;\n\nTee operator %T&gt;%\n\nAssignement pipe %&lt;&gt;%\n\nExposition operator %$%\n\n…\n\nSee pipes for beginners\n Base  has a pipe() function to manipulate connections (Files, URLs, …)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#references",
    "href": "core/slides/slides-dplyr.html#references",
    "title": "Tables manipulation with dplyr",
    "section": "\n References",
    "text": "References\n\n\nR for Data Science\n\nData transformation\n\n\nRstudio cheat sheets\n\ndplyr\ntidyr\ndatatable\nreadr"
  },
  {
    "objectID": "labs-listings.html",
    "href": "labs-listings.html",
    "title": "Labs",
    "section": "",
    "text": "Note\n\n\n\nSessions are organized around labs. Feel free to look at the lab before sessions. Do not rush to solutions proposed here\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 15, 2025\n\n\nIntroduction and Visualization\n\n\nVisualization, Public Statistics\n\n\n\n\nJan 15, 2025\n\n\nBrush up your R\n\n\nR language, Tidyverse, IDE\n\n\n\n\nJan 22, 2025\n\n\nTable wranglig\n\n\nR language, dplyr, tabula data\n\n\n\n\nJan 29, 2025\n\n\nUnivariate categorical data\n\n\nUnivariate data, GSS\n\n\n\n\nJan 30, 2025\n\n\nUnivariate numeric data\n\n\nUnivariate data, GSS\n\n\n\n\nFeb 5, 2025\n\n\nBivariate data\n\n\nbivariate data, mosaicplots, scatterplots, simple linear regression\n\n\n\n\nFeb 19, 2025\n\n\nLinear Regression I\n\n\nLinear regression, OLS, lm\n\n\n\n\nMar 5, 2025\n\n\nLinear regression II\n\n\nLinear regression, OLS, lm, diagnostics, variable selection\n\n\n\n\nMar 12, 2025\n\n\nSVD and PCA\n\n\nSVD, PCA\n\n\n\n\nMar 19, 2025\n\n\nCorrespondence Analysis\n\n\nSVD, CA, Correspondence Analysis\n\n\n\n\nMar 26, 2025\n\n\nSVD, MCA, CCA\n\n\nSVD, PCA, CA, MCA, CCA\n\n\n\n\nApr 2, 2025\n\n\nClustering: kmeans\n\n\nClustering, k-means\n\n\n\n\nApr 7, 2025\n\n\nClustering: Hierarchical\n\n\nClustering, Hierarchical\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nTip\n\n\n\nBefore working out a lab, make sure the relevant packages are installed in your environment.",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "labs-solutions-listings.html",
    "href": "labs-solutions-listings.html",
    "title": "Labs Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 15, 2025\n\n\nIntroduction and Visualization\n\n\nVisualization, Public Statistics\n\n\n\n\nJan 15, 2025\n\n\nBrush up your R\n\n\nR language, Tidyverse, IDE\n\n\n\n\nJan 22, 2025\n\n\nTable wranglig\n\n\nR language, dplyr, tabula data\n\n\n\n\nJan 29, 2025\n\n\nUnivariate categorical data\n\n\nUnivariate data, GSS\n\n\n\n\nJan 30, 2025\n\n\nUnivariate numeric data\n\n\nUnivariate data, GSS\n\n\n\n\nFeb 5, 2025\n\n\nBivariate data\n\n\nbivariate data, mosaicplots, scatterplots, simple linear regression\n\n\n\n\nFeb 19, 2025\n\n\nLinear regression I\n\n\nLinear regression, OLS, lm\n\n\n\n\nMar 5, 2025\n\n\nLinear regression II\n\n\nLinear regression, OLS, lm, diagnostics, variable selection\n\n\n\n\nMar 12, 2025\n\n\nSVD and PCA\n\n\nSVD, PCA\n\n\n\n\nMar 19, 2025\n\n\nCorrespondence Analysis\n\n\nSVD, CA, Correspondence Analysis\n\n\n\n\nMar 26, 2025\n\n\nSVD, MCA, CCA\n\n\nSVD, PCA, CA, MCA, CCA\n\n\n\n\nApr 2, 2025\n\n\nClustering: kmeans\n\n\nClustering, k-means\n\n\n\n\nApr 7, 2025\n\n\nClustering: Hierarchical\n\n\nClustering, Hierarchical\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Solutions"
    ]
  },
  {
    "objectID": "projects-listings.html",
    "href": "projects-listings.html",
    "title": "Projects",
    "section": "",
    "text": "Note\n\n\n\nCourse evaluation is based on Projects \n\n Find a friend : all work done by pairs of students\n Create a single private GitHub repository for each project and each pair of students.\n Grant me access to these repositories\n All work is transmitted through your private repository and nowhere else\n No emails for project submission\n All projects deliverables consist of Quarto notebooks\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Description\n        \n         \n          Tags\n        \n         \n          Due date - Oldest\n        \n         \n          Due date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDue date\n\n\nTitle\n\n\nDescription\n\n\nTags\n\n\n\n\n\n\nFeb 7, 2025\n\n\nData wrangling and Visualization\n\n\nData extraction, wrangling, visualization, reproducible data science\n\n\nOECD, Visualization, Quarto, tidyverse, plotly\n\n\n\n\nApr 27, 2025\n\n\nRegression, Package Development\n\n\nStatistical summaries, Regression, Package, Generics, isotone\n\n\nRegression, Diagnostics, Selection, Quarto, tidyverse, isotone, tidymodels\n\n\n\n\nMay 16, 2025\n\n\nTBA\n\n\nSVD, Clustering, Regression, Package management\n\n\nSpatial data, Package, Tidy evaluation, Quarto\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nEvaluation guidelines\n\n\n\nGrading criteria are given per project. They involve the following aspects:\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nNarrative, spelling and syntax\n\nEnglish/French \n\n\nPlots correction\n\nchoice of aesthetics, geom, scale … \n\n\nPlots style\n\nTitles, legends, labels, breaks … \n\n\nTable wrangling\n\nETL, SQL like manipulations \n\n\nComputing Statistics\n\nAggregations, LR, PCA, CA, … \n\n\nDRY compliance\n\nDRY principle at  Wikipedia\n\n\nReport organization\n\n\n\n\nCode organization",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "slides-listings.html",
    "href": "slides-listings.html",
    "title": "Slides",
    "section": "",
    "text": "Slides summarize the lectures. Feel free to watch them before and after the lectures.\n point to material to be developped on blackboard.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Tags\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 22, 2025\n\n\nHandling tables with dplyr\n\n\nSQL Tables, Relational Algebra, tidyverse\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nMode d’emploi\n\n\n\nSlides use libraries revealjs or remark from . They are displayed in your browser.\nTo get help, press",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "weeks-listings.html",
    "href": "weeks-listings.html",
    "title": "Journal",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 15, 2025\n\n\nWeek 1\n\n\nVisualization, Public Statistics, R language, Tidyverse\n\n\n\n\nJan 22, 2025\n\n\nWeek 2\n\n\nR language, Tidyverse, Table manipulations\n\n\n\n\nJan 29, 2025\n\n\nWeek 3\n\n\nUnivariate data\n\n\n\n\nFeb 5, 2025\n\n\nWeek 4\n\n\nBivariate data\n\n\n\n\nFeb 19, 2025\n\n\nWeek 6\n\n\nLinear regression, OLS, lm, Model diagnostics\n\n\n\n\nMar 5, 2025\n\n\nWeek 7\n\n\nLinear regression, OLS, lm, Model diagnostics\n\n\n\n\nMar 12, 2025\n\n\nWeek 8\n\n\nPCA, SVD\n\n\n\n\nMar 19, 2025\n\n\nWeek 9\n\n\nCA, SVD\n\n\n\n\nMar 26, 2025\n\n\nWeek 10\n\n\nCCA, MCA, SVD\n\n\n\n\nApr 2, 2025\n\n\nWeek 11\n\n\nClustering, k-means\n\n\n\n\nApr 9, 2025\n\n\nWeek 12\n\n\nClustering, Hierarchical\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Journal"
    ]
  },
  {
    "objectID": "core/labs/lab-pack.html",
    "href": "core/labs/lab-pack.html",
    "title": "R programming: packages",
    "section": "",
    "text": "This lab is just an incentive to dig into R Packages (2e) by Wickham and Bryan"
  },
  {
    "objectID": "core/labs/lab-pack.html#introduction",
    "href": "core/labs/lab-pack.html#introduction",
    "title": "R programming: packages",
    "section": "Introduction",
    "text": "Introduction\nSharing/Storing/Organizing\nPackages come with conventions\nPackaging can be helpful to perform data analysis."
  },
  {
    "objectID": "core/labs/lab-pack.html#setup",
    "href": "core/labs/lab-pack.html#setup",
    "title": "R programming: packages",
    "section": "Setup",
    "text": "Setup\nAutomating package development is facilitated by a collection of packages.\n\nCodestopifnot(\n  require(\"devtools\"),\n  require(\"usethis\"),\n  require(\"testthat\"),\n  require(\"styler\"),\n  require(\"roxygen2\")\n)\n\n\ndevtools is the cornerstone of the collections\n\nwith rstudio (hands in hands)\nwith vs code\n\nwith positron"
  },
  {
    "objectID": "core/labs/lab-pack.html#package-organization",
    "href": "core/labs/lab-pack.html#package-organization",
    "title": "R programming: packages",
    "section": "Package organization",
    "text": "Package organization\nMake sure you understand the different states of a package : source, bundled, binary, installed, in-memory.\n\nAt first, we deal with a source package. Where is it hosted?\nHow is the source organized?\nWhat are metadata for?\nWhat is the purpose of file .Rbuildignore?\nWhy do we bundle packages?\n\n\nStructure and states of a package\nTo initialize a package development directory.\n\nCodepackpath &lt;- \"hmw2.ma7by020\"\n\nif (!fs::dir_exists(packpath)){\n\n  fs::dir_create(packpath)\n}\n\nusethis::create_package(packpath)\n\n\nWithin directory hmw2.ma7by020, you should have the next organization\nhmw2.ma7by020/\n├── DESCRIPTION\n├── .gitignore\n├── hmw2.ma7by020.Rproj\n├── NAMESPACE\n├── R\n└── .Rbuildignore\n\n2 directories, 5 files\n\n\n\n\n\n\nWhat is the difference between loading and attaching a package?\n\n\n\nYou will have to\n\npopulate the R subdirectory\nupdate DESCRIPTION"
  },
  {
    "objectID": "core/labs/lab-pack.html#reusing-scripts",
    "href": "core/labs/lab-pack.html#reusing-scripts",
    "title": "R programming: packages",
    "section": "Reusing scripts",
    "text": "Reusing scripts"
  },
  {
    "objectID": "core/labs/lab-pack.html#styling",
    "href": "core/labs/lab-pack.html#styling",
    "title": "R programming: packages",
    "section": "Styling",
    "text": "Styling"
  },
  {
    "objectID": "core/labs/lab-pack.html#testing",
    "href": "core/labs/lab-pack.html#testing",
    "title": "R programming: packages",
    "section": "Testing",
    "text": "Testing\nThe workhorse of the development process is\n\nCodedevtools::load_all()\n\n\n\nCode and/or fix bugs\nLoad the code\nTest the code"
  },
  {
    "objectID": "core/labs/lab-pack.html#checking-package-state",
    "href": "core/labs/lab-pack.html#checking-package-state",
    "title": "R programming: packages",
    "section": "Checking Package state",
    "text": "Checking Package state\n\nCodedevtools::check()"
  },
  {
    "objectID": "core/labs/lab-pack.html#namespace-and-dependencies",
    "href": "core/labs/lab-pack.html#namespace-and-dependencies",
    "title": "R programming: packages",
    "section": "\nNAMESPACE and dependencies",
    "text": "NAMESPACE and dependencies"
  },
  {
    "objectID": "core/labs/lab-pack.html#versioning-git",
    "href": "core/labs/lab-pack.html#versioning-git",
    "title": "R programming: packages",
    "section": "Versioning (git)",
    "text": "Versioning (git)"
  },
  {
    "objectID": "core/labs/lab-pack.html#references",
    "href": "core/labs/lab-pack.html#references",
    "title": "R programming: packages",
    "section": "References",
    "text": "References\nWriting R extensions"
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (103A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-9.html#blackboard",
    "href": "weeks/week-9.html#blackboard",
    "title": "Week 9",
    "section": " Blackboard",
    "text": "Blackboard\n\nExtended SVD"
  },
  {
    "objectID": "weeks/week-9.html#labs",
    "href": "weeks/week-9.html#labs",
    "title": "Week 9",
    "section": " Labs",
    "text": "Labs\n\nLab 9 Correspondence Analysis as an application of extended SVD\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\nLab 6 - Linear Regression I\nLab 6 - Linear Regression II\nLab 7 - Linear Regression II\nLab 8 Principal Component Analysis as an application of SVD"
  },
  {
    "objectID": "weeks/week-9.html#further-work",
    "href": "weeks/week-9.html#further-work",
    "title": "Week 9",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-9.html#further-reading",
    "href": "weeks/week-9.html#further-reading",
    "title": "Week 9",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes"
  },
  {
    "objectID": "weeks/week-9.html#logistics",
    "href": "weeks/week-9.html#logistics",
    "title": "Week 9",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (255) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-7.html#blackboard",
    "href": "weeks/week-7.html#blackboard",
    "title": "Week 7",
    "section": " Blackboard",
    "text": "Blackboard\n\nOrdinary Least Squares\nQR factorizatio\nRidge regression (Regularized Least Squares)\nPseudo-Inversion\nLinear regrssion with R"
  },
  {
    "objectID": "weeks/week-7.html#labs",
    "href": "weeks/week-7.html#labs",
    "title": "Week 7",
    "section": " Labs",
    "text": "Labs\n\nLab 6 - Linear Regression I\nLab 7 - Linear Regression II\n –&gt;\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables"
  },
  {
    "objectID": "weeks/week-7.html#further-work",
    "href": "weeks/week-7.html#further-work",
    "title": "Week 7",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-7.html#further-reading",
    "href": "weeks/week-7.html#further-reading",
    "title": "Week 7",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes\n\nCours de Statistique Fondamentale\n\nModèles linéaires\nSherman–Morrison–Woodbury formula\n\nSourav Chatterjee: A new correlation coefficient"
  },
  {
    "objectID": "weeks/week-7.html#logistics",
    "href": "weeks/week-7.html#logistics",
    "title": "Week 7",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (RH04A) Wednesday 13h30-15h\nSession Ibis Sophie Germain (014) Thursday 10h45-12h45\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-3.html#blackboard",
    "href": "weeks/week-3.html#blackboard",
    "title": "Week 3",
    "section": " Blackboard",
    "text": "Blackboard\nWe reviewed the toolkit of univariate analysis\n\nLexicon\nCategorical samples\n\nCounts, Contingency tables, barplots, colplots\n\nNumeric samples\n\nNumerical summaries for location and scale\nCumulative distribution functions and Quantiles functions\nQuantile-quantile plots\nHistograms"
  },
  {
    "objectID": "weeks/week-3.html#labs",
    "href": "weeks/week-3.html#labs",
    "title": "Week 3",
    "section": " Labs",
    "text": "Labs\nWe went (briefly) through the two labs\n\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\n\nWe relied on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr"
  },
  {
    "objectID": "weeks/week-3.html#further-work",
    "href": "weeks/week-3.html#further-work",
    "title": "Week 3",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-3.html#further-reading",
    "href": "weeks/week-3.html#further-reading",
    "title": "Week 3",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\nCours de Statistique Fondamentale\n\nChap Estimation de densité\nChap Test de Kolmogorov-Smirnov"
  },
  {
    "objectID": "weeks/week-3.html#logistics",
    "href": "weeks/week-3.html#logistics",
    "title": "Week 3",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (130) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-11.html#labs",
    "href": "weeks/week-11.html#labs",
    "title": "Week 11",
    "section": " Labs",
    "text": "Labs\n\nLab 11 Clustering with \\(k\\)-means\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\nLab 6 - Linear Regression I\nLab 6 - Linear Regression II\nLab 7 - Linear Regression II\nLab 8 Principal Component Analysis as an application of SVD\nLab 9 Correspondence Analysis as an application of extended SVD\nLab 10 Multiple Correspondence Analysis and Canonical Correspondence Analysis as applications of extended SVD)"
  },
  {
    "objectID": "weeks/week-11.html#further-work",
    "href": "weeks/week-11.html#further-work",
    "title": "Week 11",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-11.html#further-reading",
    "href": "weeks/week-11.html#further-reading",
    "title": "Week 11",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes"
  },
  {
    "objectID": "weeks/week-11.html#logistics",
    "href": "weeks/week-11.html#logistics",
    "title": "Week 11",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (163) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-1.html#labs",
    "href": "weeks/week-1.html#labs",
    "title": "Week 1",
    "section": " Labs",
    "text": "Labs\nWe worked on the next two labs (solutions )\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\n\nWe used Rstudio, created and initialized a dedicated R project (without git and renv). We installed packages tidyverse, pak and gapminder.\nWe reconstructed the gapminder demo using ggplot2 and plotly.\nWe went through several aspects of R following Intro to R. This is an incentive to install packages lobstr and rlang. They can be very hepful when visualizing the data structures that underpin vectors, matrices, lists, and data frames."
  },
  {
    "objectID": "weeks/week-1.html#further-work",
    "href": "weeks/week-1.html#further-work",
    "title": "Week 1",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-1.html#further-reading",
    "href": "weeks/week-1.html#further-reading",
    "title": "Week 1",
    "section": " Further reading",
    "text": "Further reading\n\nThe ggplot book\nR for data science\n\nData visualization\nTransform\n\nLogical vectors\nNumbers\nFactors\nMissing values\nFunctions\nIteration\nA field guide to base R\n\n\nAdvanced R\n\nNames and values\nVectors\nSubsetting"
  },
  {
    "objectID": "weeks/week-1.html#logistics",
    "href": "weeks/week-1.html#logistics",
    "title": "Week 1",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (103A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\nSession II bis Olympe de Gouges (130) Friday 15h45-17h15\n Calendar"
  },
  {
    "objectID": "weeks/week-10.html#labs",
    "href": "weeks/week-10.html#labs",
    "title": "Week 10",
    "section": " Labs",
    "text": "Labs\n\nLab 10 Multiple Correspondence Analysis as application of extended SVD\nLab 10 bis Canonical Correspondence Analysis\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\nLab 6 - Linear Regression I\nLab 6 - Linear Regression II\nLab 7 - Linear Regression II\nLab 8 Principal Component Analysis as an application of SVD\nLab 9 Correspondence Analysis as an application of extended SVD"
  },
  {
    "objectID": "weeks/week-10.html#further-work",
    "href": "weeks/week-10.html#further-work",
    "title": "Week 10",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-10.html#further-reading",
    "href": "weeks/week-10.html#further-reading",
    "title": "Week 10",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes"
  },
  {
    "objectID": "weeks/week-10.html#logistics",
    "href": "weeks/week-10.html#logistics",
    "title": "Week 10",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 10",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Sophie Germain (2032) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-12.html#labs",
    "href": "weeks/week-12.html#labs",
    "title": "Week 10",
    "section": " Labs",
    "text": "Labs\n\nLab 12 Hierarchical clustering\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\nLab 6 - Linear Regression I\nLab 6 - Linear Regression II\nLab 7 - Linear Regression II\nLab 8 Principal Component Analysis as an application of SVD\nLab 9 Correspondence Analysis as an application of extended SVD\nLab 10 Multiple Correspondence Analysis and Canonical Correspondence Analysis as applications of extended SVD\nLab 11 Clustering with \\(k\\)-means"
  },
  {
    "objectID": "weeks/week-12.html#further-work",
    "href": "weeks/week-12.html#further-work",
    "title": "Week 10",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-12.html#further-reading",
    "href": "weeks/week-12.html#further-reading",
    "title": "Week 10",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes"
  },
  {
    "objectID": "weeks/week-12.html#logistics",
    "href": "weeks/week-12.html#logistics",
    "title": "Week 10",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (163) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-2.html#labs",
    "href": "weeks/week-2.html#labs",
    "title": "Week 2",
    "section": " Labs",
    "text": "Labs\nWe revisit the first two labs (solutions )\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\n\nWe use Rstudio, and our dedicated R project (without git and renv). We install packages gt, nycflights and slider.\nWe review our first slide deck:\n\nIntroduction to dplyr\n\nand dig into\n\nLab 3 - Working with dplyr"
  },
  {
    "objectID": "weeks/week-2.html#further-work",
    "href": "weeks/week-2.html#further-work",
    "title": "Week 2",
    "section": " Further work",
    "text": "Further work\n Review the content of the three labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-2.html#further-reading",
    "href": "weeks/week-2.html#further-reading",
    "title": "Week 2",
    "section": " Further reading",
    "text": "Further reading\n\nR for data science\nAdvanced R"
  },
  {
    "objectID": "weeks/week-2.html#logistics",
    "href": "weeks/week-2.html#logistics",
    "title": "Week 2",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (RH10A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-4.html#blackboard",
    "href": "weeks/week-4.html#blackboard",
    "title": "Week 4",
    "section": " Blackboard",
    "text": "Blackboard\n\nCategorical samples\n\n2-ways Contingency tables\nChi-square statistics\n\nNumeric samples\n\nCovariance\nCorrelation(s)"
  },
  {
    "objectID": "weeks/week-4.html#labs",
    "href": "weeks/week-4.html#labs",
    "title": "Week 4",
    "section": " Labs",
    "text": "Labs\n\nLab 6 - Introduction to bivariate analysis\n\nWe relied on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables"
  },
  {
    "objectID": "weeks/week-4.html#further-work",
    "href": "weeks/week-4.html#further-work",
    "title": "Week 4",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-4.html#further-reading",
    "href": "weeks/week-4.html#further-reading",
    "title": "Week 4",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\nCours de Statistique Fondamentale\n\nChap Tests du chi-deux\n\nSourav Chatterjee: A new correlation coefficient"
  },
  {
    "objectID": "weeks/week-4.html#logistics",
    "href": "weeks/week-4.html#logistics",
    "title": "Week 4",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (RH10A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-6.html#blackboard",
    "href": "weeks/week-6.html#blackboard",
    "title": "Week 6",
    "section": " Blackboard",
    "text": "Blackboard\n\nOrdinary Least Squares\nQR factorizatio\nRidge regression (Regularized Least Squares)\nPseudo-Inversion"
  },
  {
    "objectID": "weeks/week-6.html#labs",
    "href": "weeks/week-6.html#labs",
    "title": "Week 6",
    "section": " Labs",
    "text": "Labs\n\nLab 6 - Linear Regression I\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables"
  },
  {
    "objectID": "weeks/week-6.html#further-work",
    "href": "weeks/week-6.html#further-work",
    "title": "Week 6",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-6.html#further-reading",
    "href": "weeks/week-6.html#further-reading",
    "title": "Week 6",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\nCours de Statistique Fondamentale\n\nChap Tests du chi-deux\nModèles linéaires\n\nSourav Chatterjee: A new correlation coefficient"
  },
  {
    "objectID": "weeks/week-6.html#logistics",
    "href": "weeks/week-6.html#logistics",
    "title": "Week 6",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (103A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-8.html#blackboard",
    "href": "weeks/week-8.html#blackboard",
    "title": "Week 8",
    "section": " Blackboard",
    "text": "Blackboard\n\nSVD\n\nDefinition\nExistence\n\nMatrix norms\nSVD and low rank approximations\n\nEckart-Young-Mirsky theorem for operator norm\nEckart-Young-Mirsky theorem for Frobenius-Hilbert-Schmidt norm"
  },
  {
    "objectID": "weeks/week-8.html#labs",
    "href": "weeks/week-8.html#labs",
    "title": "Week 8",
    "section": " Labs",
    "text": "Labs\n\nLab 8 Principal Component Analysis as an application of SVD\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\nLab 6 - Linear Regression I\nLab 6 - Linear Regression II\nLab 7 - Linear Regression II"
  },
  {
    "objectID": "weeks/week-8.html#further-work",
    "href": "weeks/week-8.html#further-work",
    "title": "Week 8",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-8.html#further-reading",
    "href": "weeks/week-8.html#further-reading",
    "title": "Week 8",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes"
  },
  {
    "objectID": "weeks/week-8.html#logistics",
    "href": "weeks/week-8.html#logistics",
    "title": "Week 8",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html",
    "href": "core/labs-solutions/lab-mca.html",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "",
    "text": "Besides the usual packages (tidyverse, …), we shall require FactoMineR and related packages.\nCodestopifnot(\n  require(FactoMineR),\n  require(factoextra),\n  require(FactoInvestigate)\n)"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#the-gss-dataset",
    "href": "core/labs-solutions/lab-mca.html#the-gss-dataset",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The GSS dataset",
    "text": "The GSS dataset\nWe will use data coming from the General Social Survey. The General Social Survey data can be retrieved using the gssr package. If needed, install the gssr package and its companion package gssrdoc.\n\nCodestopifnot(\n  require(\"gssr\"),\n  require(\"gssrdoc\")\n)\n\nLoading required package: gssr\n\n\nPackage loaded. To attach the GSS data, type data(gss_all) at the console.\nFor the codebook, type data(gss_dict).\nFor the panel data and documentation, type e.g. data(gss_panel08_long) and data(gss_panel_doc).\nFor help on a specific GSS variable, type ?varname at the console.\n\n\nLoading required package: gssrdoc\n\n\nThe data we will use are panel data made available by the GSS. In order to explore them, it’s very useful toload gss_dict and gss_panel_doc.\n\nCodedata(gss_dict)\ndata(gss_panel_doc)\n\n\nCheck the online help.\n\nCode?gss_panel_doc\n\n\ngss_dict and gss_panel_doc are dataframes that can be queried:\n\nCodegss_panel_doc |&gt; \n  dplyr::sample_n(5) |&gt; \n  select(id, text)  |&gt; \n  gt::gt()\n\n\n\n\n\nid\ntext\n\n\n\nrelhhd8\n1622. What is (PERSON)'s relationship to (HEAD OF HOUSEHOLD)?\n\n\ndenom16\n115a. If Protestant: What specific denomination is that, if any?\n\n\nconjudge\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? j. U.S. Supreme Court.\n\n\nwordh\n1612. We would like to know something about how people go about guessing words they do not know. On this card are listed some words--you may know some of them, and you may not know quite a few of them. On each line the first word is in capital letters like BEAST. Then there are five other words. Tell me the number of the word that comes closest to the meaning of the word in capital letters. For example, if the word in capital letters is BEAST, you would say \"4\" since \"animal\" come closer to BEAST than any of the other words. If you wish, I will read the words to you. These words are difficult for almost everyone just give me your best guess if you are not sure of the answer.\n\n\ncompwage\n1087. Do you believe your fixed annual wages in calendar year 2005 were higher or lower than those of employees with similar experience and job descriptions in other companies in your region? Please answer on a 1 to 5 scale.\n\n\n\n\n\n\nIn the panel questionnaire, some questions have race in the field id. Check again the online help.\n\nCode?race\n\n\nThe answer is something like that:\nQuestion 24. What race do you consider yourself?\nAnd it contains a brief summary of the answers given through the years.\n\n\nYear\nBlack\nOther\nWhite\niap (NA)\nTotal\n\n\n\n2010\n311\n183\n1550\n-\n2044\n\n\n2012\n301\n196\n1477\n-\n1974\n\n\n2014\n386\n262\n1890\n-\n2538\n\n\n\n(this is just an excerpt)\n\nCodegss_panel_doc |&gt; \n  filter(str_detect(id, \"race\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  select(id, description, text) |&gt; \n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\n\n\n\nrace\nRACE\n24. What race do you consider yourself?\n\n\nracecen3\nRACECEN3\n1602. What is your race? Indicate one or more races that you consider yourself to be.\n\n\nnatrace\nNATRACE\n68. We are faced with many problems in this country, none of some of these problems, and for each one I'd like you to tell me whether you think we're spending too much money on it, too little money, or about the right amount. h. Improving the conditions of Blacks.\n\n\nnatracey\nNATRACEY\n69. We are faced with many problems in this country, none of some of these problems, and for each one I'd like you to tell me whether you think we're spending too much money on it, too little money, or about the right amount. h. Assistance to blacks.\n\n\nintrace3\nINTRACE3\nWhat is your race? Indicate one or more races that you consider yourself to be.\n\n\n\n\n\n\nThe data set we will use comes from year 2010 panel data.\n\nCodedata(\"gss_panel10_long\")\n\ngss_panel10_long \n\n# A tibble: 6,132 × 1,200\n   firstid  wave oversamp sampcode  sample      form         formwt vpsu  vstrat\n   &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+&gt;\n 1 1           1        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 2 1           2        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 3 1           3        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 4 2           1        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 5 2           2        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 6 2           3        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 7 3           1        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n 8 3           2        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n 9 3           3        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n10 4           1        1 501       9 [2000 FP] 2 [ALTERNAT…      1 2     2240  \n# ℹ 6,122 more rows\n# ℹ 1,191 more variables: samptype &lt;dbl+lbl&gt;, wtpan12 &lt;dbl+lbl&gt;,\n#   wtpan123 &lt;dbl+lbl&gt;, wtpannr12 &lt;dbl+lbl&gt;, wtpannr123 &lt;dbl+lbl&gt;,\n#   id &lt;dbl+lbl&gt;, mar1 &lt;dbl+lbl&gt;, mar2 &lt;dbl+lbl&gt;, mar3 &lt;dbl+lbl&gt;,\n#   mar4 &lt;dbl+lbl&gt;, mar5 &lt;dbl+lbl&gt;, mar6 &lt;dbl+lbl&gt;, mar7 &lt;dbl+lbl&gt;,\n#   mar8 &lt;dbl+lbl&gt;, mar9 &lt;dbl+lbl&gt;, mar10 &lt;dbl&gt;, mar11 &lt;dbl+lbl&gt;, mar12 &lt;dbl&gt;,\n#   mar13 &lt;dbl&gt;, mar14 &lt;dbl&gt;, abany &lt;dbl+lbl&gt;, abdefect &lt;dbl+lbl&gt;, …\n\n\nAt the beginning, the panel was made of roughly 2000 respondents. And the respondents were interviewed in 2010, 2012 and 2014 (the three waves).\n\nCodegss_panel10_long  |&gt;  \n  filter(wave==3, !is.na(id)) |&gt;\n  select(firstid, wave, id, sex)\n\n# A tibble: 1,304 × 4\n   firstid  wave id        sex       \n   &lt;fct&gt;   &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; \n 1 1           3 10001     1 [MALE]  \n 2 2           3 10002     2 [FEMALE]\n 3 3           3 10003     2 [FEMALE]\n 4 6           3 10004     1 [MALE]  \n 5 7           3 10005     2 [FEMALE]\n 6 9           3 10006     2 [FEMALE]\n 7 10          3 10007     2 [FEMALE]\n 8 11          3 10008     2 [FEMALE]\n 9 12          3 10009     1 [MALE]  \n10 13          3 10010     1 [MALE]  \n# ℹ 1,294 more rows\n\n\nSome respondents left the survey. Attrition can be monitored within the next query.\n\nCodegss_panel10_long |&gt; \n  select(wave, id) |&gt;\n  group_by(wave) |&gt;\n  summarize(observed = n_distinct(id),\n            missing = sum(is.na(id)))\n\n# A tibble: 3 × 3\n   wave observed missing\n  &lt;dbl&gt;    &lt;int&gt;   &lt;int&gt;\n1     1     2044       0\n2     2     1552     493\n3     3     1305     740"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#the-case-for-using-mca",
    "href": "core/labs-solutions/lab-mca.html#the-case-for-using-mca",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The case for using MCA",
    "text": "The case for using MCA\n\nIn order to construct a field of ideological and political attitudes, which will subsequently be used as a reference for locating the empirical typologies of response styles and survey compliance, we apply multiple correspondence analysis (MCA). MCA belongs to the family of techniques used in geometric data analysis (Le Roux and Rouanet 2004). It allows for the extraction of the most important dimensions in a set of categorical variables, and the graphical representation of variable categories and individuals relative to each other in a coordinate system. Distances between categories as well as individuals can be interpreted as a measure of (dis)similarity: If categories often co-appear in individual’s responses, they are located close together in the space produced by MCA. Rare co-appearances, accordingly, result in a larger distance between the respective categories. Furthermore, illustrative variables can passively be projected into the field, a technique that has been termed visual regression analysis (Lebart et al. 1984). Whereas the space is determined by the distances between the categories of active variables, passive variables do not alter the structure of the constructed field, but appear in their average and hence most likely position.\n\nFrom https://doi.org/10.1007/s11135-016-0458-3\nIn this lab, we won’t look at the field of ideological and political attitudes, but rather at the field of confidence level in a variety of institutions."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#us-field-of-ideological-dispositions",
    "href": "core/labs-solutions/lab-mca.html#us-field-of-ideological-dispositions",
    "title": "LAB: Multiple Correspondance Analysis",
    "section": "US field of ideological dispositions",
    "text": "US field of ideological dispositions\n\nCodepanel_doc &lt;- gssrdoc::gss_panel_doc"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#elementary-statistics-and-table-wrangling",
    "href": "core/labs-solutions/lab-mca.html#elementary-statistics-and-table-wrangling",
    "title": "LAB: Multiple Correspondance Analysis",
    "section": "Elementary statistics and table wrangling",
    "text": "Elementary statistics and table wrangling\nBefore proceeding to Multiple Correspondence Analysis (CA), let us select the set of active variables.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBuild a bar plot to display the importance of Causes of deaths in France in years 1979 and 2006\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nCounts have already been computed above."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#multiple-correspondance-analysis",
    "href": "core/labs-solutions/lab-mca.html#multiple-correspondance-analysis",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Multiple Correspondance Analysis",
    "text": "Multiple Correspondance Analysis\n\n\n\n\n\n\nMCA executive summary\n\n\n\nThe input of multiple correspondence analysis is a data frame \\(X\\) with \\(n\\) rows and \\(p\\) categorical columns. Multiple Correspondence Analysis starts by building the indicator matrix. The indicator matrix is built by one-hot encoding of each categorical variable.\n\nA categorical variable \\(V_j\\) (factor) with \\(q\\) levels is mapped to \\(q\\) \\(\\{0,1\\}\\) -valued variables \\(V_{j,r}\\) for \\(r \\leq q\\)\nIf levels are indexed by \\(\\{1, \\ldots, q\\}\\), if the value of the categorical variable \\(V_j\\) from row \\(i\\) is \\(k \\in \\{1, \\ldots, q\\}\\), the bina$ \\[k \\mapsto \\underbrace{0,\\ldots, 0}_{k-1}, 1, \\underbrace{0, \\ldots, 0}_{q-k}\\]\nThe indicator matrix has as many rows as the data matrix\nThe number of columns of the indicator matrix is the sum of the number of levels of the categorical variables/columns of the data matrix\nThe indicator matrix is a numerical matrix. It is suitable for factorial methodss\n\nRecall \\(X\\) is the data matrix with \\(n\\) rows (individuals) and \\(p\\) categorical columns (variables)\nFor \\(j \\in \\{1, \\ldots, p\\}\\), let \\(J_j\\) denote the number of levels(categories) of variable \\(j\\)\nLet \\(q = \\sum_{j\\leq p} J_j\\) be the sum of the number of levels throughout the variables\nLet \\(Z\\) be the incidence matrix with \\(n\\) rows and \\(q\\) columns\nFor \\(j\\leq p\\) and \\(k \\leq J_j\\), let \\(\\langle j, k\\rangle = \\sum_{j'&lt;j} J_{j'}+k\\)\nLet \\(N = n \\times p = \\sum_{i\\leq n} \\sum_{j \\leq p} X_{i,j}\\) and \\[P = \\frac{1}{N} Z\\]\n(the correspondence matrix for MCA)\n The row wise sums of correspondence matrix \\(P\\) are all equal to \\(1/n=p/N\\)\nThe column wise sum of the correspondence matrix \\(P\\) for the \\(k\\)th level of the \\(j\\)th variable of \\(X\\) ( \\(j \\leq p\\) ) is \\[N_{\\langle j,k\\rangle}/N = f_{\\langle j,k\\rangle}/p\\]\nwhere \\(f_{\\langle j,k\\rangle}\\) stands for the relative frequency of level \\(k\\) of the \\(j\\)th variable\n\\[D_r = \\frac{1}{n}\\text{Id}_n\\qquad D_c =\\text{diag}\\left(\\frac{f_{\\langle j,k\\rangle}}{p}\\right)_{j \\leq p, k\\leq J_j}\\]\nIn MCA, we compute the SVD \\(U \\times D \\times V^\\top\\) of the standardized residuals matrix:\n\\[S = D_r^{-1/2}\\times \\left(P - r\\times c^\\top\\right) \\times D_c^{-1/2} = \\sqrt{n}\\left(P - r\\times c^\\top\\right) \\times D_c^{-1/2}\\]\nCoefficient \\(i, \\langle j, k\\rangle\\) of \\(S\\) is \\[\\frac{\\mathbb{I}_{i, \\langle j, k\\rangle}- f_{\\langle j,k\\rangle}}{\\sqrt{n f_{\\langle j,k\\rangle}/p}}\\]\nMCA consists in computing the SVD of the standardized residuals matrix \\(S =  U  \\times D \\times V^\\top\\)\nFrom the SVD, we get\n\n\n\\(D_r^{-1/2} \\times U\\) standardized coordinates of rows\n\n\n\\(D_c^{-1/2} \\times V\\) standardized coordinates of columns\n\n\n\\(D_r^{-1/2} \\times U \\times D\\) principal coordinates of rows\n\n\n\\(D_c^{-1/2} \\times V \\times D\\) principal coordinates of columns\n\nSquared singular values: the principal inertia\n\n\nWhen calling svd(.), the argument should be \\[D_r^{1/2}\\times \\left(D_r^{-1} \\times P \\times D_c^{-1}- \\mathbf{I}\\times \\mathbf{I}^\\top  \\right)\\times D_c^{1/2}= D_r^{-1/2}\\times \\left( P - r \\times c^\\top  \\right)\\times D_c^{-1/2}\\]\n\n\n\n\n\n\n\n\nMCA and extended SVD\n\n\n\nAs\n\\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^\\top = (D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^\\top\\]\n\\[(D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^\\top\\]\nis the extended SVD of\n\\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^\\top\\]\nwith respect to \\(D_r\\) and \\(D_c\\)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPerform MCA on the indicator matrix.\n\n\n\n\n\n\n\n\nYou may use FactoMineR::MCA(). It is interesting to compute the correspondence analysis in your own way, by preparing the matrix that is handled to svd() and returning a named list containing all relevant information.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe lazyly rely on FactoMineR::MCA.\n\nCoderes.mca &lt;- confidence_data |&gt; \n  select(starts_with(\"con\")) |&gt; \n  FactoMineR::MCA(graph=F)\n\n\nThe result is an object of S3 class MCA.\n\n\ns\n\n\n\n\n\n\nQuestion\n\n\n\nIf you did use FactoMineR::MCA(), explain the organization of the result.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThe result of FactoMineR::MCA(...) is a named and nested list with five elements:\n\neig\n\na matrix/array containing enough information to build a screeplot.\n\ncall\n\na list of 9, containing the call to CA(), an object of type language, telling (in principle) the user how CA() was called. However, this is a quoted expression. Here we need to guess the value of y in the calling environment understand what’s going on.\n\n\nElement call also contains the table margin distributions marge.col and marge.row. The truncation rank ncp (number of components) can be assigned before computing the SVD (default value is 5). Element \\(X\\) stores the contingency table that was effectively used for computing Correpondence Analysis.\n\nind\n\nInformation gathered from SVD to facilitate individulals (respndents) analysis.\n\nvar\n\na list structured in the same way as element ind. Used for variables/categories profiles analysis\n\nsvd\n\na list of 3, just as the resuld of svd() containing the singular values, the left and right singular vectors of matrix \\(...\\)\n\n\n\n\n\n\n\n\n\n\nSolution, output of print(res.mca)\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n1\n$eig\n“eigenvalues”\n\n\n2\n$var\n“results for the variables (categories)”\n\n\n3\n$var$coord\n“coord. of the categories”\n\n\n4\n$var$cos2\n“cos2 for the categories”\n\n\n5\n$var$contrib\n“contributions of the categories”\n\n\n6\n$var$v.test\n“v-test for the categories”\n\n\n7\n$ind\n“results for the individuals”\n\n\n8\n$ind$coord\n“coord. for the individuals”\n\n\n9\n$ind$cos2\n“cos2 for the individuals”\n\n\n10\n$ind$contrib\n“contributions of the individuals”\n\n\n11\n$quali.sup\n“results for the supplementary categorical variables”\n\n\n12\n$quali.sup$coord\n“coord. for the supplementary categories”\n\n\n13\n$quali.sup$cos2\n“cos2 for the supplementary categories”\n\n\n14\n$quali.sup$v.test\n“v-test for the supplementary categories”\n\n\n15\n$call\n“intermediate results”\n\n\n16\n$call$marge.col\n“weights of columns”\n\n\n17\n$call$marge.li\n“weights of rows”\n\n\n\n res.mca$svd is not part of the output!\n\neig is computed from the singular values in res.mca$svd\nvar contains material for plotting information about categories and variables on factorial planes\nind conatins material for plotting information about individuals on on factorial planes\n\n\n\n\n\n\n\n\n\nIn principle, all relevant information can be gathered from components svd, call.marge.li, and call.marge.col."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#screeplots",
    "href": "core/labs-solutions/lab-mca.html#screeplots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Screeplots",
    "text": "Screeplots\n\n\n\n\n\n\nQuestion\n\n\n\nDraw screeplots. Why are they useful? Comment briefly.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe first define a tidying function for preparing the screeplot. It is designed along the lines of method tidy for objects of class prcomp. It consists in building a data frame from component eig of the MCA object.\n\nCodetidy_mca_scree &lt;- function(res_mca){\n  res_mca$eig |&gt; \n    as_tibble(rownames=\"Dim\") |&gt;\n    rename(\n      `% Inertia` = `percentage of variance`,\n      `Cumulative % Inertia` = `cumulative percentage of variance`\n    ) |&gt; \n    mutate(\n      PC = as.integer(str_match(Dim, 'dim ([0-9]*)')[,2])\n    )\n}\n\n\nWe can now build a column plot.\n\nCoderes.mca |&gt; \n  tidy_mca_scree() |&gt;\n  ggplot() +\n  aes(x=PC, y=eigenvalue, label=glue(\"{round(`% Inertia`,2)} %\")) +\n  geom_col() +\n  geom_text(angle=45, vjust=-1, hjust=-.1) \n\n\n\n\n\n\n\nNote that the share of cumulated inertia doesn’t grow very fast with the number of components. Situation is different from what we met when performing Correspondence Analysis or Principal Component Analysis. This tells us that we will hqve to look beyond the first two axis. This is commonplace with multiple correspondence analysis. Look at the reference below where most of the interesting information can be found on the 3rd and 4th axis."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#row-profiles-analysis",
    "href": "core/labs-solutions/lab-mca.html#row-profiles-analysis",
    "title": "LAB: Multiple Correspondance Analysis",
    "section": "Row profiles analysis",
    "text": "Row profiles analysis\n\n\n\n\n\n\nQuestion\n\n\n\nPerform row profiles analysis.\nWhat are the classical plots? How can you build them from the output of FactoMiner::CA?\nBuild the table of row contributions (the so-called \\(\\cos^2\\))\n\n\n\n\n\n\n\n\nSolution\n\n\n\nAttribute row of objects of class MCA (exported from FactoMineR) is the starting point of any row profiles analysis.\nAttribute row is a named list made of \\(4\\) components.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncoord\n\na matrix with named rows and columns. The number of rows of coord matches the number of rows of the contingency table (here, the number of possible death Causes). The number of columns matches the rank of the truncated SVD that underlies Correspondance Analysis. Here it is \\(5\\) which also the rank of the standardized contingency table.\n\n\n\nThe row principal coordinates are the principal coordinates of each row profile in terms of the principal component.\n\nThe columns of coord are pairwise orthogonal in the inner product space defined by diag(call$marge.row) (which embodies the marginal probabilities of the so-called Causes of deaths)\nWe can recover row$coord from the left singular vectors and the singular values:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ninertia\n\na numerical vector with length matching the number of rows of coord, contrib and cos2.\n\n\n\nInertia is the way CA measures variation between row profiles. Total inertia is the \\(\\chi^2\\) statistic divided by sample size.\n\nRow inertia can be obtained by multiplying the row marginal probability by the squared Euclidean norm of the row in the principal coordinate matrix.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncos2\n\nCoefficients of matrix cos2 are the share of row inertia from the corresponding cell in coord\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncontrib\n\n\n\n\nNot too surprisingly, coord, contrib, and cos2 share the same row names and column names.\nThe Row Profiles are the rows of matrix R below\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe can now display a scatterplot from component coord. This is called a Row Plot.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPerform column profiles analysis\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#symmetric-plots",
    "href": "core/labs-solutions/lab-mca.html#symmetric-plots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Symmetric plots",
    "text": "Symmetric plots\n\n\n\n\n\n\nQuestion\n\n\n\nBuild the symmetric plots (biplots) for multiple correspondence analysis.\n\n\nFrom the shelf\nWe resort to factoextra. In principle, the biplot can be constructed from the coord matrices in the var and the ind components of the MCA object.\n\nCodep &lt;- res.mca |&gt; \n  factoextra::fviz_mca_biplot(\n    label= \"var\",\n    alpha.ind=.5\n  )\n\np\n\n\n\n\n\n\nCode# p |&gt;  plotly::ggplotly()\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#mosaicplots",
    "href": "core/labs-solutions/lab-mca.html#mosaicplots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Mosaicplots",
    "text": "Mosaicplots\nMCA can be complemented by Mosaicplots, Double Decker plots, Chi-square tests, and Correspondence analyses between pair of variables.\n\n\n\n\n\n\nQuestion\n\n\n\nDraw a mosaic plot to visualize association between confidence levesl in Science and confidence level in Medicine.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeconfidence_data |&gt; \n  select(consci, conmedic) |&gt;\n  table() |&gt; \n  mosaicplot(color=T)\n\n\n\n\n\n\n\n\nCodeconfidence_data |&gt; \n  select(consci, conmedic) |&gt; \n  table() |&gt;\n  chisq.test() |&gt; \n  broom::glance() |&gt; \n  gt::gt() |&gt; \n  gt::tab_caption(\"Chi2 independence test for Confidence in Science and Confidence in Medicine\")\n\n\n\n\nChi2 independence test for Confidence in Science and Confidence in Medicine\n\nstatistic\np.value\nparameter\nmethod\n\n\n137.8878\n7.994493e-29\n4\nPearson's Chi-squared test\n\n\n\n\n\n\nCodeconfidence_data |&gt; \n  select(consci, conclerg) |&gt; \n  table() |&gt;\n  chisq.test() |&gt; \n  broom::glance() |&gt; \n  gt::gt() |&gt; \n  gt::tab_caption(\"Chi2 independence test for Confidence in Science and Confidence in Clergy\")\n\n\n\n\nChi2 independence test for Confidence in Science and Confidence in Clergy\n\nstatistic\np.value\nparameter\nmethod\n\n\n39.27785\n6.103953e-08\n4\nPearson's Chi-squared test\n\n\n\n\n\n\nCodet_sex_clerg_sci &lt;- confidence_data |&gt; \n  select(sex, conclerg, consci) |&gt; \n  table()\n  \nt_sex_clerg_sci  |&gt;\n  vcd::doubledecker( )\n\n\n\n\n\n\n\n\nCodet_sex_clerg_sci |&gt; \n  mosaicplot(color=T, main=\"Confidence in Clergy and Science\")"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#the-confidence-topic",
    "href": "core/labs-solutions/lab-mca.html#the-confidence-topic",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The confidence topic",
    "text": "The confidence topic\nDo GSS surveys are made of a huge number of questions. Not all questions were asked to the respondents. Indeed, each question was asked to two thirds of the respondents. Questions were related to demographic features (Age, Sex, Level of education, Employment and so on). Answers to these questions can be used to determine whether the panel sample is representative. Data can be compared with last census data (there is a census every ten years in the US).\nA number of questions are related to the confidence topic. Respondents were asked about the level of confidence they put in a wide array of institutions.\n\nCodegss_panel_doc |&gt; \n  filter(str_detect(text, \"confidence\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  select(id, description, text) |&gt; \n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\n\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\n\n\nconbus\nCONBUS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? b. Major companies.\n\n\nconclerg\nCONCLERG\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? c. Organized religion.\n\n\nconeduc\nCONEDUC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? d. Education.\n\n\nconfed\nCONFED\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? e. Executive branch of the federal government.\n\n\nconlabor\nCONLABOR\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? f. Organized labor.\n\n\nconpress\nCONPRESS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? g. Press.\n\n\nconmedic\nCONMEDIC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? h. Medicine.\n\n\ncontv\nCONTV\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? i. Television.\n\n\nconjudge\nCONJUDGE\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? j. U.S. Supreme Court.\n\n\nconsci\nCONSCI\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? k. Scientific community.\n\n\nconlegis\nCONLEGIS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? l. Congress\n\n\nconarmy\nCONARMY\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? m. Military\n\n\n\n\n\n\nFor institutions like Science or Medicine, or Executive branch of federal government, Respondents were asked whether they have: Great deal of confidence, Only some confidence, Hardly any confidence in the institution. The same Likert scale with 3 levels was used for all institutions.\n\n\n\n\n\n\nQuestion\n\n\n\nFrom the gss_panel10_long dataset, extract columns corresponding to questions from the confidence topic\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe first gather the id of the questions pertaining to the confidence topic.\n\nCodeconfidence_ids &lt;- gss_panel_doc |&gt; \n  filter(str_detect(text, \"confidence\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  pull(id)\n\n\nThen we protect the panel data on the questions pertaining to the confidence topic. We also keep the respondent key, sex, and the relevant wave. s\n\nCodegss_panel10_long |&gt; \n  filter(wave==1) |&gt;\n  select(firstid, wave, id, sex, contains(confidence_ids)) |&gt;\n  sample_n(10) |&gt;\n  gt::gt()\n\n\n\n\n\nfirstid\nwave\nid_1: id\nsex_1: RESPONDENTS SEX\nconfinan_1: CONFID IN BANKS & FINANCIAL INSTITUTIONS\nconbus_1: CONFIDENCE IN MAJOR COMPANIES\nconclerg_1: CONFIDENCE IN ORGANIZED RELIGION\nconeduc_1: CONFIDENCE IN EDUCATION\nconfed_1: CONFID. IN EXEC BRANCH OF FED GOVT\nconlabor_1: CONFIDENCE IN ORGANIZED LABOR\nconpress_1: CONFIDENCE IN PRESS\nconmedic_1: CONFIDENCE IN MEDICINE\ncontv_1: CONFIDENCE IN TELEVISION\nconjudge_1: CONFID. IN UNITED STATES SUPREME COURT\nconsci_1: CONFIDENCE IN SCIENTIFIC COMMUNITY\neconsci\nconlegis_1: CONFIDENCE IN CONGRESS\nconarmy_1: CONFIDENCE IN MILITARY\n\n\n\n1225\n1\n1225\n2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1511\n1\n1511\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1153\n1\n1153\n2\n3\n2\n2\n1\n2\n2\n2\n2\n1\n2\n2\nNA\n2\n1\n\n\n1585\n1\n1585\n2\n3\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\n1\n3\n\n\n940\n1\n940\n2\n3\n2\n3\n2\n3\n3\n2\n1\n2\n3\n2\nNA\n3\n2\n\n\n110\n1\n110\n2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n559\n1\n559\n2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1536\n1\n1536\n1\n3\n2\n2\n2\n2\n2\n3\n2\n3\n2\n2\nNA\n3\n1\n\n\n846\n1\n846\n2\n3\n2\n2\n3\n3\n2\n3\n1\n3\n2\n1\n4\n3\n2\n\n\n220\n1\n220\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nLet us go back to the documentation.\n\nCodegss_panel_doc |&gt;\n  filter(id %in% confidence_ids) |&gt; \n  select(-ends_with(\"2\"), -ends_with(\"3\")) |&gt;\n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\nproperties_1\nmarginals_1\n\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/265\"), c(\"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\")\nc(30.1, 55.2, 14.7, NA, NA, 100), c(\"395\", \"725\", \"193\", \"671\", \"16\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\")\n\n\nconbus\nCONBUS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? b. Major companies.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/247\"), c(\"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\")\nc(17.6, 62.3, 20.1, NA, NA, NA, 100), c(\"228\", \"806\", \"260\", \"671\", \"32\", \"3\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\")\n\n\nconclerg\nCONCLERG\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? c. Organized religion.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/250\"), c(\"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\")\nc(25.1, 52.5, 22.4, NA, NA, NA, 100), c(\"324\", \"678\", \"289\", \"671\", \"35\", \"3\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\")\n\n\nconeduc\nCONEDUC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? d. Education.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/259\"), c(\"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\")\nc(27.1, 56.5, 16.4, NA, NA, 100), c(\"359\", \"747\", \"217\", \"671\", \"6\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\")\n\n\nconfed\nCONFED\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? e. Executive branch of the federal government.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/262\"), c(\"CONFED_1\", \"CONFED_1\", \"CONFED_1\")\nc(13.8, 45.4, 40.8, NA, NA, NA, 100), c(\"179\", \"590\", \"531\", \"671\", \"26\", \"3\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONFED_1\", \"CONFED_1\", \"CONFED_1\", \"CONFED_1\", \"CONFED_1\", \"CONFED_1\", \"CONFED_1\")\n\n\nconlabor\nCONLABOR\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? f. Organized labor.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/289\"), c(\"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\")\nc(12.3, 58.1, 29.6, NA, NA, 100), c(\"155\", \"731\", \"372\", \"671\", \"71\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\")\n\n\nconpress\nCONPRESS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? g. Press.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/298\"), c(\"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\")\nc(10, 48.5, 41.5, NA, NA, NA, 100), c(\"132\", \"638\", \"546\", \"671\", \"12\", \"1\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\")\n\n\nconmedic\nCONMEDIC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? h. Medicine.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/295\"), c(\"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\")\nc(40.1, 49.3, 10.6, NA, NA, 100), c(\"530\", \"651\", \"140\", \"671\", \"8\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\")\n\n\ncontv\nCONTV\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? i. Television.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/328\"), c(\"CONTV_1\", \"CONTV_1\", \"CONTV_1\")\nc(9.4, 48.2, 42.3, NA, NA, NA, 100), c(\"124\", \"634\", \"556\", \"671\", \"14\", \"1\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONTV_1\", \"CONTV_1\", \"CONTV_1\", \"CONTV_1\", \"CONTV_1\", \"CONTV_1\", \"CONTV_1\")\n\n\nconjudge\nCONJUDGE\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? j. U.S. Supreme Court.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/286\"), c(\"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\")\nc(31.8, 50.3, 17.8, NA, NA, NA, 100), c(\"410\", \"649\", \"230\", \"671\", \"38\", \"2\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\")\n\n\nconsci\nCONSCI\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? k. Scientific community.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/319\"), c(\"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\")\nc(41.6, 51, 7.3, NA, NA, NA, 100), c(\"527\", \"646\", \"93\", \"671\", \"62\", \"1\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\")\n\n\nconlegis\nCONLEGIS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? l. Congress\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/292\"), c(\"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\")\nc(10.8, 51.2, 37.9, NA, NA, NA, 100), c(\"141\", \"666\", \"493\", \"671\", \"28\", \"1\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\")\n\n\nconarmy\nCONARMY\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? m. Military\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/244\"), c(\"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\")\nc(46.8, 39.8, 13.4, NA, NA, 100), c(\"613\", \"521\", \"175\", \"671\", \"20\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\")\n\n\n\n\n\n\nLevels labels for answers to confidence questions are:\n\n\nCodefoo &lt;- gss_panel_doc |&gt;\n   filter(id %in% confidence_ids) |&gt; \n   select(-ends_with(\"2\"), -ends_with(\"3\")) |&gt; \n   unnest(\n    cols=c(properties_1), names_repair = \"universal\") |&gt; \n   unnest(\n    cols=c(marginals_1), names_repair = \"universal\") |&gt; \n   head(6) \n\nNew names:\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...6`\n\nCodefoo |&gt; gt::gt()\n\n\n\n\n\nid...1\ndescription\ntext\nproperty\nvalue...5\nid...6\npercent\nn\nvalue...9\nlabel\nid...11\n\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\n30.1\n395\n1\nA GREAT DEAL\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\n55.2\n725\n2\nONLY SOME\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\n14.7\n193\n3\nHARDLY ANY\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\nNA\n671\n0\nIAP\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\nNA\n16\n8\nDK\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\n100.0\n2,000\nNA\nTotal\nCONFINAN_1\n\n\n\n\n\n\n\n\nIAP stands for respondent was not asked the question.\n\nDK stands for Don’t Know\n\n\n\nIf respondents were not given the opportunity to provide a response to a question because it was not included in the survey they were administered, then their responses are coded as “IAP.”\n\n\n\n\nCodepanel_doc &lt;- gssrdoc::gss_panel_doc"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#table-wrangling",
    "href": "core/labs-solutions/lab-mca.html#table-wrangling",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Table wrangling",
    "text": "Table wrangling\nBefore proceeding to Multiple Correspondence Analysis (CA), let us select the set of active variables.\n\n\n\n\n\n\nQuestion\n\n\n\nProject gss_panel10_long on firstid, wave, id, sex, and columns with names in\n  confinan conbus conclerg coneduc confed conlabor conpress conmedic contv conjudge consci conlegis conarmy\nFilter so as to keep only wave 1.\nTransform all relevant columns into factors.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeconfidence_panel10 &lt;- gss_panel10_long |&gt; \n  filter(wave==1) |&gt;\n  select(firstid, wave, id, sex, contains(confidence_ids)) |&gt;\n  mutate(across(starts_with(\"con\"), as_factor)) |&gt;\n  mutate(sex=as_factor(sex))\n\n\n\nCodeconfidence_10 &lt;- gss_panel_doc |&gt;\n   filter(id %in% confidence_ids) |&gt; \n   select(-ends_with(\"2\"), -ends_with(\"3\")) |&gt; \n   unnest(\n    cols=c(properties_1), names_repair = \"universal\") |&gt; \n   unnest(\n    cols=c(marginals_1), names_repair = \"universal\") |&gt;\n   mutate(text=str_squish(text)) |&gt;\n   mutate(text= str_match(text, \"\\\\? [\\\\w]\\\\.(.*)\")) |&gt; \n   mutate(text=text[,2]) |&gt; \n   filter(property == \"Missing-data codes\") |&gt;\n   select(-starts_with(\"id\"), -property, -`value...5`)\n\nNew names:\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...6`\n\n\n\nCodelevels_table &lt;- confidence_10 |&gt; \n  filter(label!='Total' | is.na(label)) |&gt;\n  distinct(`value...9`, label)"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#elementary-statistics",
    "href": "core/labs-solutions/lab-mca.html#elementary-statistics",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Elementary statistics",
    "text": "Elementary statistics\n\n\n\n\n\n\nQuestion\n\n\n\nUse skimmr to summarize your dataset.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeconfidence_panel10 |&gt;\n  skimr::skim(where(is.factor)) |&gt;\n  select(-factor.ordered, -skim_type)|&gt;\n  filter(str_starts(skim_variable,\"con\")) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(decimals=3) |&gt;\n  gt::tab_caption(\n    caption = \"GSS Panel data 2010 First wave, Topic confidence\"\n  )\n\n\n\n\nGSS Panel data 2010 First wave, Topic confidence\n\nskim_variable\nn_missing\ncomplete_rate\nfactor.n_unique\nfactor.top_counts\n\n\n\nconfinan\n680.000\n0.667\n3.000\nONL: 656, HAR: 564, A G: 144, IAP: 0\n\n\nconbus\n705.000\n0.655\n3.000\nONL: 848, HAR: 317, A G: 174, IAP: 0\n\n\nconclerg\n725.000\n0.645\n3.000\nONL: 720, HAR: 328, A G: 271, IAP: 0\n\n\nconeduc\n681.000\n0.667\n3.000\nONL: 793, A G: 367, HAR: 203, IAP: 0\n\n\nconfed\n698.000\n0.659\n3.000\nONL: 620, HAR: 499, A G: 227, IAP: 0\n\n\nconlabor\n744.000\n0.636\n3.000\nONL: 781, HAR: 373, A G: 146, IAP: 0\n\n\nconpress\n689.000\n0.663\n3.000\nONL: 621, HAR: 594, A G: 140, IAP: 0\n\n\nconmedic\n681.000\n0.667\n3.000\nONL: 647, A G: 558, HAR: 158, IAP: 0\n\n\ncontv\n681.000\n0.667\n3.000\nONL: 677, HAR: 521, A G: 165, IAP: 0\n\n\nconjudge\n709.000\n0.653\n3.000\nONL: 706, A G: 400, HAR: 229, IAP: 0\n\n\nconsci\n738.000\n0.639\n3.000\nONL: 683, A G: 544, HAR: 79, IAP: 0\n\n\nconlegis\n697.000\n0.659\n3.000\nONL: 635, HAR: 587, A G: 125, IAP: 0\n\n\nconarmy\n688.000\n0.663\n3.000\nA G: 720, ONL: 507, HAR: 129, IAP: 0\n\n\n\n\n\n\n\nCodeconfidence_panel10 |&gt;\n  filter(is.na(conarmy)) |&gt;\n  filter(is.na(conclerg)) |&gt;\n  filter(is.na(conpress))\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nThere are a lot of missing data in your data set.\nHow are missing cells related?\nDrop rows with missing data in the confidence questions.\nWhat are the dimensions of your data set?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeconfidence_data &lt;- confidence_panel10 |&gt; \n  mutate(across(starts_with(\"con\"), .fns=\\(x) !is.na(x), .names=\"not_{.col}_isna\")) |&gt;\n  rowwise() |&gt;\n  filter(all(c_across(ends_with(\"isna\")))) |&gt; \n  ungroup() |&gt; \n  select(-ends_with(\"isna\"), -firstid, -wave, -econsci) |&gt;\n  mutate(across(starts_with(\"con\"), fct_drop), sex=as_factor(sex))\n\nconfidence_data |&gt; \n  slice_sample(n=10) |&gt; \n  gt::gt()\n\n\n\n\n\nid_1: id\nsex_1: RESPONDENTS SEX\nconfinan_1: CONFID IN BANKS & FINANCIAL INSTITUTIONS\nconbus_1: CONFIDENCE IN MAJOR COMPANIES\nconclerg_1: CONFIDENCE IN ORGANIZED RELIGION\nconeduc_1: CONFIDENCE IN EDUCATION\nconfed_1: CONFID. IN EXEC BRANCH OF FED GOVT\nconlabor_1: CONFIDENCE IN ORGANIZED LABOR\nconpress_1: CONFIDENCE IN PRESS\nconmedic_1: CONFIDENCE IN MEDICINE\ncontv_1: CONFIDENCE IN TELEVISION\nconjudge_1: CONFID. IN UNITED STATES SUPREME COURT\nconsci_1: CONFIDENCE IN SCIENTIFIC COMMUNITY\nconlegis_1: CONFIDENCE IN CONGRESS\nconarmy_1: CONFIDENCE IN MILITARY\n\n\n\n1894\nFEMALE\nONLY SOME\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nONLY SOME\n\n\n1639\nFEMALE\nONLY SOME\nONLY SOME\nONLY SOME\nA GREAT DEAL\nONLY SOME\nONLY SOME\nONLY SOME\nA GREAT DEAL\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nA GREAT DEAL\n\n\n718\nMALE\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nHARDLY ANY\n\n\n1101\nFEMALE\nONLY SOME\nONLY SOME\nHARDLY ANY\nONLY SOME\nA GREAT DEAL\nHARDLY ANY\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nONLY SOME\nA GREAT DEAL\n\n\n1120\nFEMALE\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nHARDLY ANY\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\n\n\n1417\nFEMALE\nONLY SOME\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nHARDLY ANY\nHARDLY ANY\nONLY SOME\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\n\n\n1454\nMALE\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\nHARDLY ANY\nA GREAT DEAL\nA GREAT DEAL\nHARDLY ANY\nA GREAT DEAL\n\n\n1598\nMALE\nHARDLY ANY\nONLY SOME\nONLY SOME\nA GREAT DEAL\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\n\n\n62\nFEMALE\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\n\n\n1157\nMALE\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn your explore possible associations between the answers to the different confidence questions?\nHow would you test possible independence between answer to confidence in science and confidence in the Army.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nTODO"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#individuals",
    "href": "core/labs-solutions/lab-mca.html#individuals",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Individuals",
    "text": "Individuals\n\n\n\n\n\n\nQuestion\n\n\n\nPerform Individual profiles analysis.\nWhat are the classical plots? How can you build them from the output of FactoMiner::MCA?\nBuild the table of row contributions (the so-called \\(\\cos^2\\))\n\n\n\n\n\n\n\n\nSolution\n\n\n\nAttribute ind of objects of class MCA (exported from FactoMineR) is the starting point of any row profiles analysis.\nWe design an augment method. For objects of class MCA. This is like what is done for objects of class prcomp and different from what we did with objects of class CA. Indeed, MCA on the indicator matrix handles a sample just like PCA and not a contingency table like CA.\n\nCodeaugment_mca &lt;- function(res_mca, data=NULL){\n  \n  result &lt;- res_mca$ind$coord |&gt; \n    as_tibble()  \n  \n  if (!is.null(data)) {\n    result &lt;- result |&gt;\n      bind_cols(confidence_data)\n      # inner_join(confidence_data, by=c(\"id\")) \n  }\n  \n  return(result)\n}\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nAttribute ind is a named list made of \\(4\\) components.\n\ncoord\n\na matrix with named rows and columns. The number of rows of coord matches the number of rows of the table fed to MCA (here, the number of respondents in the panel). The number of columns matches the rank of the truncated SVD that underlies Multiple Correspondance Analysis. Here it is the default truncation \\(5\\).\n\n\n\nThe principal coordinates are the principal coordinates of each individual in terms of the principal component.\n\nThe columns of coord are pairwise orthogonal in the usual inner product space.\nFunction augment_mca Bind the columns of coord with the columns of the original dataset.\n\nCoderes.mca |&gt; \n#| label: augment_mca_glimpse\n  augment_mca(confidence_data) |&gt; \n  glimpse()\n\nRows: 1,183\nColumns: 20\n$ `Dim 1`  &lt;dbl&gt; 1.35465733, -0.10861408, -0.33683136, 0.72450001, 0.72525336,…\n$ `Dim 2`  &lt;dbl&gt; 0.41190167, -0.90245225, -0.45183706, 0.25425970, 0.10225521,…\n$ `Dim 3`  &lt;dbl&gt; 0.161371575, 0.010268886, 0.107542556, -0.009273224, -0.38139…\n$ `Dim 4`  &lt;dbl&gt; 0.141129674, 0.310980059, -0.097880877, -0.085996832, 0.36106…\n$ `Dim 5`  &lt;dbl&gt; 0.227285260, 0.086562193, 0.016680275, -0.115998402, -0.06681…\n$ id       &lt;dbl+lbl&gt;  4,  6,  7,  9, 11, 15, 19, 20, 21, 24, 26, 28, 31, 41, 4…\n$ sex      &lt;dbl+lbl&gt; 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, …\n$ confinan &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, HARDLY ANY, ONLY SOME, ON…\n$ conbus   &lt;fct&gt; ONLY SOME, ONLY SOME, ONLY SOME, ONLY SOME, ONLY SOME, ONLY S…\n$ conclerg &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEA…\n$ coneduc  &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, ONLY SOME, A GREAT DEAL, …\n$ confed   &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEA…\n$ conlabor &lt;fct&gt; A GREAT DEAL, ONLY SOME, HARDLY ANY, ONLY SOME, HARDLY ANY, O…\n$ conpress &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, HARDLY ANY, ONLY SOME, HA…\n$ conmedic &lt;fct&gt; A GREAT DEAL, ONLY SOME, HARDLY ANY, A GREAT DEAL, ONLY SOME,…\n$ contv    &lt;fct&gt; ONLY SOME, ONLY SOME, HARDLY ANY, ONLY SOME, ONLY SOME, ONLY …\n$ conjudge &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEA…\n$ consci   &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, ONLY SOME, …\n$ conlegis &lt;fct&gt; ONLY SOME, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEAL, …\n$ conarmy  &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEA…\n\n\nNote that, we can recover ind$coord from the left singular vectors and the singular values.\nThe output of augment_mca is the basis of the individual plots. Individual plots are defined by picking a pair of principal axes. Him use the 3rd and the 4th axis, and we map the sex column from the original dataset to the fill aesthetics.\n\nCoderes.mca |&gt; \n  augment_mca(confidence_data) |&gt; \n  ggplot() +\n  aes(x=`Dim 3`, y=`Dim 4`, fill=sex) +\n  geom_point(alpha=.2, size=.2) +\n  coord_fixed() +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncos2\n\nCoefficients of matrix cos2 are the share of row inertia from the corresponding cell in coord\n\n\n\nCoderes.mca |&gt; \n  augment_mca(confidence_data) |&gt; \n  ggplot() +\n  aes(x=`Dim 1`, y=`Dim 2`, fill=sex) +\n  geom_point(alpha=.2, size=.2) +\n  coord_fixed() +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncontrib\n\n\n\n\nNot too surprisingly, coord, contrib, and cos2 share the same row names and column names."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#variablescategories",
    "href": "core/labs-solutions/lab-mca.html#variablescategories",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Variables/Categories",
    "text": "Variables/Categories\n\n\n\n\n\n\nQuestion\n\n\n\nPerform column profiles (categories) analysis. You may use factoextra::fviz_mca_var()\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThe var component. Of an MCA object is related with the right singular vectors of the residual matrix.\n\nCodep &lt;- res.mca |&gt; \n  factoextra::fviz_mca_var() +\n  coord_fixed() \n  \np +\n  facet_zoom(\n    xy=`Dim 1`&gt;=0 & `Dim 2`&gt;=0,\n    xlim=c(0,2),ylim=c(0,2))\n\n\n\n\n\n\n\nFollowing what has been done for objects of class prcomp, we design a tidy_mca_var function that turns the var component into a dataframe. Row names which correspond to categories, are turned into a column we call levels.\n\nCodetidy_mca_var &lt;- function(res_mca){\n  res_mca$var$coord |&gt; \n  as_tibble(rownames=\"levels\")\n}\n\n\nA value in column levels indicate a variable., that is a topic (science, religion, army, …) and a confidence level (A value on a Likert scale). We enrich the data frame by extracting the variable (topic) and the value (sentiment).\nWe are then ready to plot an analog of the correlation circle.\n\nCodetidied_mca_var &lt;- tidy_mca_var(res.mca) |&gt;   \n  mutate(sentiment=str_split(as.character(levels),'_')) |&gt;\n  mutate(topic=map_chr(sentiment, \\(x)x[1]), sentiment=map_chr(sentiment, \\(x)x[2])) \n\np &lt;- tidied_mca_var |&gt; \n  ggplot() +\n    aes(x=`Dim 1`, y=`Dim 2`, label=levels, fill=topic, shape=sentiment) +\n    geom_point(alpha=.5, show.legend = F) +\n    coord_fixed() +\n    scale_color_viridis_d() +\n    theme(legend.position=\"none\") +\n    labs(\n      title = \"MCA: Variables categories\",\n      subtitle = \"Color: topic, Shape: confidence level\",\n      caption = \"GSS data, panel 2010, wave 1\"\n    )\n\np |&gt;\n  plotly::ggplotly() \n\n\n\n\n\nResorting to plotly allows us to mitigate the overplotting effect.\nThe plot on the first two axes seems easy to interpret. As we move along the first axis from the left, we see growing confidence.\nOn the positive part of the second axis, we have extreme confidence levels, either high or low. This suggests that individuals have trends: either we trust all institutions; or none of them; or finally a little bit all of them.\nHow could we comfort such a narrative?\n\nCode(\n p + \n    aes(x=`Dim 3`, y=`Dim 4`, label=levels, color=sentiment) \n) |&gt;   plotly::ggplotly() \n\n\n\n\n\n\nCode(\n  p +\n    aes(x=`Dim 4`, y=`Dim 5`, label=levels) \n) |&gt;  plotly::ggplotly() \n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the v.test component of the var component of an MCA object?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nHere is a chatbot answer:\n\nIn Multiple Correspondence Analysis (MCA), the v-test (also known as the test-value or t-test value) is a statistical measure used to assess the significance of the association between a category and a dimension (factor). It helps determine which categories contribute the most to the definition of a given dimension.\n\nDefinition and Calculation\nThe v-test is computed as:\n\\[v = \\frac{p - P}{\\sqrt{P(1 - P)/n}}\\]\nwhere: - \\(p\\) = proportion of individuals in a given category for the specific dimension, - \\(P\\) = overall proportion of individuals in that category across all dimensions, - \\(n\\) = total number of individuals.\nAlternatively, the v-test can also be expressed as:\n\\[v = \\frac{\\text{category coordinate}}{\\text{standard deviation of the coordinate}}\\]\nThis essentially provides a z-score, which follows a standard normal distribution under the null hypothesis.\nInterpretation\n\nA high absolute v-test value (typically above 2 or below -2) indicates that the category is strongly associated with the given dimension.\n\nPositive v-test: The category is overrepresented in that dimension.\n\nNegative v-test: The category is underrepresented in that dimension.\nIf the absolute value of the v-test is below 2, the association is not statistically significant (at a 5% significance level).\nWhy is it useful?\n\nIt helps interpret MCA results quantitatively rather than relying only on visual inspection.\nIt aids in identifying key categories that structure the data on a given axis.\nIt provides statistical validation for category contributions to dimensions.\n\nWould you like a practical example or interpretation of a specific MCA result?"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#adding-passivae-variables",
    "href": "core/labs-solutions/lab-mca.html#adding-passivae-variables",
    "title": "LAB: Multiple Correspondance Analysis (MCA)",
    "section": "Adding passivae variables",
    "text": "Adding passivae variables\n\n\n\n\n\n\nQuestion\n\n\n\nColor individuals according to sex\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCode# TODO"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#references",
    "href": "core/labs-solutions/lab-mca.html#references",
    "title": "LAB: Multiple Correspondance Analysis (MCA)",
    "section": "References",
    "text": "References\nCosta, Patrício & Santos, Nadine & Guimaraes, Pedro & Cotter, Jorge & Sousa, Nuno. (2013). The Use of Multiple Correspondence Analysis to Explore Associations between Categories of Qualitative Variables in Healthy Ageing. Journal of aging research. 2013. 302163. 10.1155/2013/302163.\nhttps://link.springer.com/article/10.1007/s11135-016-0458-3\nhttps://doi.org/10.1007/s11135-016-0458-3\nBarth, Alice and Schmitz, Andreas. 2018. Response quality and ideological dispositions: an integrative approach using geometric and classifying techniques. Quality & Quantity\n\nWhen analyzing survey data, response quality has consequential implications for substantial conclusions. Differences in response quality are usually explained by personality, or socio-demographic or cognitive characteristics. Little, however, is known about how respondents’ political attitudes, values, and opinions impact on quality aspects. This is a striking analytical omission, as potential associations between political values and various forms of response biases and artefacts call into question surveys’ ability to represent ‘public opinion’. In this contribution, response quality is traced back to respondents’ political and ideological dispositions. For this purpose, a relational understanding of response quality is applied that takes into account different aspects of response behaviors, as well as the interrelations between these indicators. Using data from the US General Social Survey (2010–2014), an empirical typology of response quality is created via finite mixture analysis. The resulting classes are then related to positions in the US field of ideological dispositions constructed via multiple correspondence analysis. The analyses reveal that there are (1) different combinations of response patterns and thus different empirical response types, and (2) that these types of response quality systematically vary with regard to the respondents’ political and ideological (dis)positions. Implications of the findings for public opinion surveys are discussed.\n\nhttps://doi.org/10.1007/s11135-016-0458-3"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#adding-passive-variables-to-individual-plots",
    "href": "core/labs-solutions/lab-mca.html#adding-passive-variables-to-individual-plots",
    "title": "LAB: Multiple Correspondance Analysis (MCA)",
    "section": "Adding passive variables to individual plots",
    "text": "Adding passive variables to individual plots\n\n\n\n\n\n\nQuestion\n\n\n\nColor individuals according to sex\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCode# TODO\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nTODO"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#further-references",
    "href": "core/labs-solutions/lab-mca.html#further-references",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Further references",
    "text": "Further references\nBarth, Alice and Schmitz, Andreas. 2018. Response quality and ideological dispositions: an integrative approach using geometric and classifying techniques. Quality & Quantity\n\nWhen analyzing survey data, response quality has consequential implications for substantial conclusions. Differences in response quality are usually explained by personality, or socio-demographic or cognitive characteristics. Little, however, is known about how respondents’ political attitudes, values, and opinions impact on quality aspects. This is a striking analytical omission, as potential associations between political values and various forms of response biases and artefacts call into question surveys’ ability to represent ‘public opinion’. In this contribution, response quality is traced back to respondents’ political and ideological dispositions. For this purpose, a relational understanding of response quality is applied that takes into account different aspects of response behaviors, as well as the interrelations between these indicators. Using data from the US General Social Survey (2010–2014), an empirical typology of response quality is created via finite mixture analysis. The resulting classes are then related to positions in the US field of ideological dispositions constructed via multiple correspondence analysis. The analyses reveal that there are (1) different combinations of response patterns and thus different empirical response types, and (2) that these types of response quality systematically vary with regard to the respondents’ political and ideological (dis)positions. Implications of the findings for public opinion surveys are discussed."
  },
  {
    "objectID": "core/labs/lab-mca.html",
    "href": "core/labs/lab-mca.html",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "",
    "text": "Besides the usual packages (tidyverse, …), we shall require FactoMineR and related packages.\nCodestopifnot(\n  require(FactoMineR),\n  require(factoextra),\n  require(FactoInvestigate)\n)"
  },
  {
    "objectID": "core/labs/lab-mca.html#the-gss-dataset",
    "href": "core/labs/lab-mca.html#the-gss-dataset",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The GSS dataset",
    "text": "The GSS dataset\nWe will use data coming from the General Social Survey. The General Social Survey data can be retrieved using the gssr package. If needed, install the gssr package and its companion package gssrdoc.\n\nCodestopifnot(\n  require(\"gssr\"),\n  require(\"gssrdoc\")\n)\n\nLoading required package: gssr\n\n\nPackage loaded. To attach the GSS data, type data(gss_all) at the console.\nFor the codebook, type data(gss_dict).\nFor the panel data and documentation, type e.g. data(gss_panel08_long) and data(gss_panel_doc).\nFor help on a specific GSS variable, type ?varname at the console.\n\n\nLoading required package: gssrdoc\n\n\nThe data we will use are panel data made available by the GSS. In order to explore them, it’s very useful toload gss_dict and gss_panel_doc.\n\nCodedata(gss_dict)\ndata(gss_panel_doc)\n\n\nCheck the online help.\n\nCode?gss_panel_doc\n\n\ngss_dict and gss_panel_doc are dataframes that can be queried:\n\nCodegss_panel_doc |&gt; \n  dplyr::sample_n(5) |&gt; \n  select(id, text)  |&gt; \n  gt::gt()\n\n\n\n\n\nid\ntext\n\n\n\npadeg\n20. If finished 9th-12th grade: Did he ever get a high school diploma or a GED certificate?\n\n\nrelhh13\n1632. What is (PERSON)'s relationship to head of household?\n\n\nrellife\nPlease tell me whether you strongly agree, agree, disagree, or strongly disagree with the following statements: 679. I try hard to carry my religious beliefs over into all my other dealings in life.\n\n\nraclive\n129. Are there any (negroes/blacks/African-Americans) living in this neighborhood now?\n\n\nold3\n1621. Please tell me the names of the people who usually live in this household. Let's start with the head of the household. c. How old was [PERSON] on his/her last birthday?\n\n\n\n\n\n\nIn the panel questionnaire, some questions have race in the field id. Check again the online help.\n\nCode?race\n\n\nThe answer is something like that:\nQuestion 24. What race do you consider yourself?\nAnd it contains a brief summary of the answers given through the years.\n\n\nYear\nBlack\nOther\nWhite\niap (NA)\nTotal\n\n\n\n2010\n311\n183\n1550\n-\n2044\n\n\n2012\n301\n196\n1477\n-\n1974\n\n\n2014\n386\n262\n1890\n-\n2538\n\n\n\n(this is just an excerpt)\n\nCodegss_panel_doc |&gt; \n  filter(str_detect(id, \"race\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  select(id, description, text) |&gt; \n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\n\n\n\nrace\nRACE\n24. What race do you consider yourself?\n\n\nracecen1\nRACECEN1\n1602. What is your race? Indicate one or more races that you consider yourself to be.\n\n\nnatrace\nNATRACE\n68. We are faced with many problems in this country, none of some of these problems, and for each one I'd like you to tell me whether you think we're spending too much money on it, too little money, or about the right amount. h. Improving the conditions of Blacks.\n\n\nnatracey\nNATRACEY\n69. We are faced with many problems in this country, none of some of these problems, and for each one I'd like you to tell me whether you think we're spending too much money on it, too little money, or about the right amount. h. Assistance to blacks.\n\n\nintrace3\nINTRACE3\nWhat is your race? Indicate one or more races that you consider yourself to be.\n\n\n\n\n\n\nThe data set we will use comes from year 2010 panel data.\n\nCodedata(\"gss_panel10_long\")\n\ngss_panel10_long \n\n# A tibble: 6,132 × 1,200\n   firstid  wave oversamp sampcode  sample      form         formwt vpsu  vstrat\n   &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+&gt;\n 1 1           1        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 2 1           2        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 3 1           3        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 4 2           1        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 5 2           2        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 6 2           3        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 7 3           1        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n 8 3           2        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n 9 3           3        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n10 4           1        1 501       9 [2000 FP] 2 [ALTERNAT…      1 2     2240  \n# ℹ 6,122 more rows\n# ℹ 1,191 more variables: samptype &lt;dbl+lbl&gt;, wtpan12 &lt;dbl+lbl&gt;,\n#   wtpan123 &lt;dbl+lbl&gt;, wtpannr12 &lt;dbl+lbl&gt;, wtpannr123 &lt;dbl+lbl&gt;,\n#   id &lt;dbl+lbl&gt;, mar1 &lt;dbl+lbl&gt;, mar2 &lt;dbl+lbl&gt;, mar3 &lt;dbl+lbl&gt;,\n#   mar4 &lt;dbl+lbl&gt;, mar5 &lt;dbl+lbl&gt;, mar6 &lt;dbl+lbl&gt;, mar7 &lt;dbl+lbl&gt;,\n#   mar8 &lt;dbl+lbl&gt;, mar9 &lt;dbl+lbl&gt;, mar10 &lt;dbl&gt;, mar11 &lt;dbl+lbl&gt;, mar12 &lt;dbl&gt;,\n#   mar13 &lt;dbl&gt;, mar14 &lt;dbl&gt;, abany &lt;dbl+lbl&gt;, abdefect &lt;dbl+lbl&gt;, …\n\n\nAt the beginning, the panel was made of roughly 2000 respondents. And the respondents were interviewed in 2010, 2012 and 2014 (the three waves).\n\nCodegss_panel10_long  |&gt;  \n  filter(wave==3, !is.na(id)) |&gt;\n  select(firstid, wave, id, sex)\n\n# A tibble: 1,304 × 4\n   firstid  wave id        sex       \n   &lt;fct&gt;   &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; \n 1 1           3 10001     1 [MALE]  \n 2 2           3 10002     2 [FEMALE]\n 3 3           3 10003     2 [FEMALE]\n 4 6           3 10004     1 [MALE]  \n 5 7           3 10005     2 [FEMALE]\n 6 9           3 10006     2 [FEMALE]\n 7 10          3 10007     2 [FEMALE]\n 8 11          3 10008     2 [FEMALE]\n 9 12          3 10009     1 [MALE]  \n10 13          3 10010     1 [MALE]  \n# ℹ 1,294 more rows\n\n\nSome respondents left the survey. Attrition can be monitored within the next query.\n\nCodegss_panel10_long |&gt; \n  select(wave, id) |&gt;\n  group_by(wave) |&gt;\n  summarize(observed = n_distinct(id),\n            missing = sum(is.na(id)))\n\n# A tibble: 3 × 3\n   wave observed missing\n  &lt;dbl&gt;    &lt;int&gt;   &lt;int&gt;\n1     1     2044       0\n2     2     1552     493\n3     3     1305     740"
  },
  {
    "objectID": "core/labs/lab-mca.html#the-confidence-topic",
    "href": "core/labs/lab-mca.html#the-confidence-topic",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The confidence topic",
    "text": "The confidence topic\nDo GSS surveys are made of a huge number of questions. Not all questions were asked to the respondents. Indeed, each question was asked to two thirds of the respondents. Questions were related to demographic features (Age, Sex, Level of education, Employment and so on). Answers to these questions can be used to determine whether the panel sample is representative. Data can be compared with last census data (there is a census every ten years in the US).\nA number of questions are related to the confidence topic. Respondents were asked about the level of confidence they put in a wide array of institutions.\n\nCodegss_panel_doc |&gt; \n  filter(str_detect(text, \"confidence\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  select(id, description, text) |&gt; \n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\n\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\n\n\nconbus\nCONBUS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? b. Major companies.\n\n\nconclerg\nCONCLERG\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? c. Organized religion.\n\n\nconeduc\nCONEDUC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? d. Education.\n\n\nconfed\nCONFED\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? e. Executive branch of the federal government.\n\n\nconlabor\nCONLABOR\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? f. Organized labor.\n\n\nconpress\nCONPRESS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? g. Press.\n\n\nconmedic\nCONMEDIC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? h. Medicine.\n\n\ncontv\nCONTV\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? i. Television.\n\n\nconjudge\nCONJUDGE\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? j. U.S. Supreme Court.\n\n\nconsci\nCONSCI\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? k. Scientific community.\n\n\nconlegis\nCONLEGIS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? l. Congress\n\n\nconarmy\nCONARMY\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? m. Military\n\n\n\n\n\n\nFor institutions like Science or Medicine, or Executive branch of federal government, Respondents were asked whether they have: Great deal of confidence, Only some confidence, Hardly any confidence in the institution. The same Likert scale with 3 levels was used for all institutions.\n\n\n\n\n\n\nQuestion\n\n\n\nFrom the gss_panel10_long dataset, extract columns corresponding to questions from the confidence topic\n\n\n\nCodepanel_doc &lt;- gssrdoc::gss_panel_doc"
  },
  {
    "objectID": "core/labs/lab-mca.html#table-wrangling",
    "href": "core/labs/lab-mca.html#table-wrangling",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Table wrangling",
    "text": "Table wrangling\nBefore proceeding to Multiple Correspondence Analysis (CA), let us select the set of active variables.\n\n\n\n\n\n\nQuestion\n\n\n\nProject gss_panel10_long on firstid, wave, id, sex, and columns with names in\n  confinan conbus conclerg coneduc confed conlabor conpress conmedic contv conjudge consci conlegis conarmy\nFilter so as to keep only wave 1.\nTransform all relevant columns into factors."
  },
  {
    "objectID": "core/labs/lab-mca.html#elementary-statistics",
    "href": "core/labs/lab-mca.html#elementary-statistics",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Elementary statistics",
    "text": "Elementary statistics\n\n\n\n\n\n\nQuestion\n\n\n\nUse skimmr to summarize your dataset.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nThere are a lot of missing data in your data set.\nHow are missing cells related?\nDrop rows with missing data in the confidence questions.\nWhat are the dimensions of your data set?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn your explore possible associations between the answers to the different confidence questions?\nHow would you test possible independence between answer to confidence in science and confidence in the Army."
  },
  {
    "objectID": "core/labs/lab-mca.html#the-case-for-using-mca",
    "href": "core/labs/lab-mca.html#the-case-for-using-mca",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The case for using MCA",
    "text": "The case for using MCA\n\nIn order to construct a field of ideological and political attitudes, which will subsequently be used as a reference for locating the empirical typologies of response styles and survey compliance, we apply multiple correspondence analysis (MCA). MCA belongs to the family of techniques used in geometric data analysis (Le Roux and Rouanet 2004). It allows for the extraction of the most important dimensions in a set of categorical variables, and the graphical representation of variable categories and individuals relative to each other in a coordinate system. Distances between categories as well as individuals can be interpreted as a measure of (dis)similarity: If categories often co-appear in individual’s responses, they are located close together in the space produced by MCA. Rare co-appearances, accordingly, result in a larger distance between the respective categories. Furthermore, illustrative variables can passively be projected into the field, a technique that has been termed visual regression analysis (Lebart et al. 1984). Whereas the space is determined by the distances between the categories of active variables, passive variables do not alter the structure of the constructed field, but appear in their average and hence most likely position.\n\nFrom https://doi.org/10.1007/s11135-016-0458-3\nIn this lab, we won’t look at the field of ideological and political attitudes, but rather at the field of confidence level in a variety of institutions."
  },
  {
    "objectID": "core/labs/lab-mca.html#multiple-correspondance-analysis",
    "href": "core/labs/lab-mca.html#multiple-correspondance-analysis",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Multiple Correspondance Analysis",
    "text": "Multiple Correspondance Analysis\n\n\n\n\n\n\nMCA executive summary\n\n\n\nThe input of multiple correspondence analysis is a data frame \\(X\\) with \\(n\\) rows and \\(p\\) categorical columns. Multiple Correspondence Analysis starts by building the indicator matrix. The indicator matrix is built by one-hot encoding of each categorical variable.\n\nA categorical variable \\(V_j\\) (factor) with \\(q\\) levels is mapped to \\(q\\) \\(\\{0,1\\}\\) -valued variables \\(V_{j,r}\\) for \\(r \\leq q\\)\nIf levels are indexed by \\(\\{1, \\ldots, q\\}\\), if the value of the categorical variable \\(V_j\\) from row \\(i\\) is \\(k \\in \\{1, \\ldots, q\\}\\), the bina$ \\[k \\mapsto \\underbrace{0,\\ldots, 0}_{k-1}, 1, \\underbrace{0, \\ldots, 0}_{q-k}\\]\nThe indicator matrix has as many rows as the data matrix\nThe number of columns of the indicator matrix is the sum of the number of levels of the categorical variables/columns of the data matrix\nThe indicator matrix is a numerical matrix. It is suitable for factorial methodss\n\nRecall \\(X\\) is the data matrix with \\(n\\) rows (individuals) and \\(p\\) categorical columns (variables)\nFor \\(j \\in \\{1, \\ldots, p\\}\\), let \\(J_j\\) denote the number of levels(categories) of variable \\(j\\)\nLet \\(q = \\sum_{j\\leq p} J_j\\) be the sum of the number of levels throughout the variables\nLet \\(Z\\) be the incidence matrix with \\(n\\) rows and \\(q\\) columns\nFor \\(j\\leq p\\) and \\(k \\leq J_j\\), let \\(\\langle j, k\\rangle = \\sum_{j'&lt;j} J_{j'}+k\\)\nLet \\(N = n \\times p = \\sum_{i\\leq n} \\sum_{j \\leq p} X_{i,j}\\) and \\[P = \\frac{1}{N} Z\\]\n(the correspondence matrix for MCA)\n The row wise sums of correspondence matrix \\(P\\) are all equal to \\(1/n=p/N\\)\nThe column wise sum of the correspondence matrix \\(P\\) for the \\(k\\)th level of the \\(j\\)th variable of \\(X\\) ( \\(j \\leq p\\) ) is \\[N_{\\langle j,k\\rangle}/N = f_{\\langle j,k\\rangle}/p\\]\nwhere \\(f_{\\langle j,k\\rangle}\\) stands for the relative frequency of level \\(k\\) of the \\(j\\)th variable\n\\[D_r = \\frac{1}{n}\\text{Id}_n\\qquad D_c =\\text{diag}\\left(\\frac{f_{\\langle j,k\\rangle}}{p}\\right)_{j \\leq p, k\\leq J_j}\\]\nIn MCA, we compute the SVD \\(U \\times D \\times V^\\top\\) of the standardized residuals matrix:\n\\[S = D_r^{-1/2}\\times \\left(P - r\\times c^\\top\\right) \\times D_c^{-1/2} = \\sqrt{n}\\left(P - r\\times c^\\top\\right) \\times D_c^{-1/2}\\]\nCoefficient \\(i, \\langle j, k\\rangle\\) of \\(S\\) is \\[\\frac{\\mathbb{I}_{i, \\langle j, k\\rangle}- f_{\\langle j,k\\rangle}}{\\sqrt{n f_{\\langle j,k\\rangle}/p}}\\]\nMCA consists in computing the SVD of the standardized residuals matrix \\(S =  U  \\times D \\times V^\\top\\)\nFrom the SVD, we get\n\n\n\\(D_r^{-1/2} \\times U\\) standardized coordinates of rows\n\n\n\\(D_c^{-1/2} \\times V\\) standardized coordinates of columns\n\n\n\\(D_r^{-1/2} \\times U \\times D\\) principal coordinates of rows\n\n\n\\(D_c^{-1/2} \\times V \\times D\\) principal coordinates of columns\n\nSquared singular values: the principal inertia\n\n\nWhen calling svd(.), the argument should be \\[D_r^{1/2}\\times \\left(D_r^{-1} \\times P \\times D_c^{-1}- \\mathbf{I}\\times \\mathbf{I}^\\top  \\right)\\times D_c^{1/2}= D_r^{-1/2}\\times \\left( P - r \\times c^\\top  \\right)\\times D_c^{-1/2}\\]\n\n\n\n\n\n\n\n\nMCA and extended SVD\n\n\n\nAs\n\\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^\\top = (D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^\\top\\]\n\\[(D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^\\top\\]\nis the extended SVD of\n\\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^\\top\\]\nwith respect to \\(D_r\\) and \\(D_c\\)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPerform MCA on the indicator matrix.\n\n\n\n\n\n\n\n\nYou may use FactoMineR::MCA(). It is interesting to compute the correspondence analysis in your own way, by preparing the matrix that is handled to svd() and returning a named list containing all relevant information.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIf you did use FactoMineR::MCA(), explain the organization of the result."
  },
  {
    "objectID": "core/labs/lab-mca.html#screeplots",
    "href": "core/labs/lab-mca.html#screeplots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Screeplots",
    "text": "Screeplots\n\n\n\n\n\n\nQuestion\n\n\n\nDraw screeplots. Why are they useful? Comment briefly."
  },
  {
    "objectID": "core/labs/lab-mca.html#individuals",
    "href": "core/labs/lab-mca.html#individuals",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Individuals",
    "text": "Individuals\n\n\n\n\n\n\nQuestion\n\n\n\nPerform Individual profiles analysis.\nWhat are the classical plots? How can you build them from the output of FactoMiner::MCA?\nBuild the table of row contributions (the so-called \\(\\cos^2\\))"
  },
  {
    "objectID": "core/labs/lab-mca.html#variablescategories",
    "href": "core/labs/lab-mca.html#variablescategories",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Variables/Categories",
    "text": "Variables/Categories\n\n\n\n\n\n\nQuestion\n\n\n\nPerform column profiles (categories) analysis. You may use factoextra::fviz_mca_var()\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the v.test component of the var component of an MCA object?"
  },
  {
    "objectID": "core/labs/lab-mca.html#symmetric-plots",
    "href": "core/labs/lab-mca.html#symmetric-plots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Symmetric plots",
    "text": "Symmetric plots\n\n\n\n\n\n\nQuestion\n\n\n\nBuild the symmetric plots (biplots) for multiple correspondence analysis."
  },
  {
    "objectID": "core/labs/lab-mca.html#mosaicplots",
    "href": "core/labs/lab-mca.html#mosaicplots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Mosaicplots",
    "text": "Mosaicplots\nMCA can be complemented by Mosaicplots, Double Decker plots, Chi-square tests, and Correspondence analyses between pair of variables.\n\n\n\n\n\n\nQuestion\n\n\n\nDraw a mosaic plot to visualize association between confidence levesl in Science and confidence level in Medicine."
  },
  {
    "objectID": "core/labs/lab-mca.html#further-references",
    "href": "core/labs/lab-mca.html#further-references",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Further references",
    "text": "Further references\nBarth, Alice and Schmitz, Andreas. 2018. Response quality and ideological dispositions: an integrative approach using geometric and classifying techniques. Quality & Quantity\n\nWhen analyzing survey data, response quality has consequential implications for substantial conclusions. Differences in response quality are usually explained by personality, or socio-demographic or cognitive characteristics. Little, however, is known about how respondents’ political attitudes, values, and opinions impact on quality aspects. This is a striking analytical omission, as potential associations between political values and various forms of response biases and artefacts call into question surveys’ ability to represent ‘public opinion’. In this contribution, response quality is traced back to respondents’ political and ideological dispositions. For this purpose, a relational understanding of response quality is applied that takes into account different aspects of response behaviors, as well as the interrelations between these indicators. Using data from the US General Social Survey (2010–2014), an empirical typology of response quality is created via finite mixture analysis. The resulting classes are then related to positions in the US field of ideological dispositions constructed via multiple correspondence analysis. The analyses reveal that there are (1) different combinations of response patterns and thus different empirical response types, and (2) that these types of response quality systematically vary with regard to the respondents’ political and ideological (dis)positions. Implications of the findings for public opinion surveys are discussed."
  }
]