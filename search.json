[
  {
    "objectID": "core/slides/slides-kmeans.html#in-words",
    "href": "core/slides/slides-kmeans.html#in-words",
    "title": "Clustering and k-means",
    "section": "In words",
    "text": "In words\nClustering consists in partitioning points collections from some metric space\nin such a way that\n\npoints within the same group are close enough\n\nwhile\n\npoints from different groups are distant\n\nIn the background: some notion of distance/similarity"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#clustering-in-ml-applications",
    "href": "core/slides/slides-kmeans.html#clustering-in-ml-applications",
    "title": "Clustering and k-means",
    "section": "Clustering in ML applications",
    "text": "Clustering in ML applications\nClustering shows up in many Machine Learning applications, for example:\n\n Marketing: finding groups of customers with similar behavior given a large database of customer data containing their properties and past buying records\n Biology: classification of plants and animals given their features\n Bookshops: book ordering (recommendation)\n Insurance: identifying groups of motor insurance policy holders with a high average claim cost; identifying frauds\n City-planning: identifying groups of houses according to their type, value and geographical location\n Internet: document classification; clustering weblog data to discover groups of similar access patterns; topic modeling\n\nMany distinct goals: clustering is often just one step in a data analysis pipeline\nFor recommendation systems, marketing, objects that fit into the same group call for the same action\nSome clustering should be hierarchical (toxonomy in life sciences) others can just be flat"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#section",
    "href": "core/slides/slides-kmeans.html#section",
    "title": "Clustering and k-means",
    "section": "",
    "text": "A clustering application relies on the elaboration of a metric/dissimilarity over some input space\nThis tasks is entangled with feature engineering\nFocus on one specific context: market segmentation \n\n Data: Base of customer data containing their properties and past buying records\n Goal: Use the customers similarities to find groups\n\nPossible directions:\n\nDimension reduction (PCA, CA, MCA, …)\nClustering \\(\\approx\\) non-supervised classification\n\n\n\nAre the directions complementary? or not?\nClustering may be done before dimension reduction or the other way"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#dimension-reduction",
    "href": "core/slides/slides-kmeans.html#dimension-reduction",
    "title": "Clustering and k-means",
    "section": "\n Dimension reduction",
    "text": "Dimension reduction\nDimension reduction technologies start form:\n\nTraining data \\(\\mathcal{D}=\\{\\vec{X}_1,\\ldots,\\vec{X}_n\\} \\in \\mathcal{X}^n\\) (i.i.d. \\(\\sim \\Pr\\))\nSpace \\(\\mathcal{X}\\) of possibly high dimension.\n\nand elaborate a Dimension Reduction Map\nDimension reduction technologies construct a map \\(\\Phi\\) from the space \\(\\mathcal{X}\\) into a space \\(\\mathcal{X}'\\) of smaller dimension"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#clustering-techniques",
    "href": "core/slides/slides-kmeans.html#clustering-techniques",
    "title": "Clustering and k-means",
    "section": "\n Clustering techniques",
    "text": "Clustering techniques\nClustering techniques start from training data:\n\\[\\mathcal{D}=\\{\\vec{X}_1,\\ldots,\\vec{X}_n\\} \\in \\mathcal{X}^n\\]\nassuming \\(\\vec{X}_i \\sim_{\\text{i.i.d.}} \\Pr\\), and partition the data into (latent?) groups,\nClustering techniques construct a map \\(f\\) from \\(\\mathcal{D}\\) to \\(\\{1,\\ldots,K\\}\\) where \\(K\\) is a number of classes to be fixed: \\(f: \\quad \\vec{X}_i \\mapsto k_i\\)"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#dimension-reduction-and-clustering-may-be-combined",
    "href": "core/slides/slides-kmeans.html#dimension-reduction-and-clustering-may-be-combined",
    "title": "Clustering and k-means",
    "section": "Dimension reduction and clustering may be combined",
    "text": "Dimension reduction and clustering may be combined\nFor example, it is commonplace to first perform PCA, project the data on the leading principal components and then perform \\(k\\)-means clustering on the projected data\nClustering tasks may be motivated along different directions:\n\nThe search for an interpretation of groups\nUse of groups in further processing (prediction, …)\n\nThis is especially true as many clustering approaches suffer from the curse of dimensionality"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#good-clustering",
    "href": "core/slides/slides-kmeans.html#good-clustering",
    "title": "Clustering and k-means",
    "section": "Good clustering",
    "text": "Good clustering\nWe need to define the quality of a cluster \nUnfortunately, no obvious quality measure exists! \nClustering quality may be assessed by scrutinizing\n\nInner homogeneity: samples in the same group should be similar\nOuter inhomogeneity: samples in two different groups should be different."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#shades-of-similarity",
    "href": "core/slides/slides-kmeans.html#shades-of-similarity",
    "title": "Clustering and k-means",
    "section": "Shades of similarity",
    "text": "Shades of similarity\nThere are many possible definitions of similar and different\nOften, they are based on the distance between the samples\nExamples based on the (squared) Euclidean distance:\n\nInner homogeneity \\(\\approx\\) intra class variance/inertia,\nOuter inhomogeneity \\(\\approx\\) inter class variance/inertia.\n\nRemember that, in flat clustering, the choice of the number \\(K\\) of clusters is often delicate"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#section-1",
    "href": "core/slides/slides-kmeans.html#section-1",
    "title": "Clustering and k-means",
    "section": "",
    "text": "Clustering is not a single method\nClustering methods address a large range of problems.\n\nTurning this informal statement into a formal definition proves challenging."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#definition-clustering-function",
    "href": "core/slides/slides-kmeans.html#definition-clustering-function",
    "title": "Clustering and k-means",
    "section": "Definition Clustering function",
    "text": "Definition Clustering function\nDefine a clustering function \\(F\\) as a function that\n\ntakes as input any finite domain \\(\\mathcal{X}\\) with a dissimilarity function \\(d\\) over its pairs\n\nand\n\nreturns a partition of \\(\\mathcal{X}\\)"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#desirable-properties",
    "href": "core/slides/slides-kmeans.html#desirable-properties",
    "title": "Clustering and k-means",
    "section": "Desirable properties",
    "text": "Desirable properties\nA clustering function should ideally satisfy the next three properties\n\nScale Invariance. For any domain set \\(\\mathcal{X}\\), dissimilarity function \\(d\\), and any \\(\\alpha&gt;0\\), the following should hold: \\(F(\\mathcal{X},d) = F(\\mathcal{X},\\alpha d)\\).\nRichness For any finite \\(\\mathcal{X}\\) and every partition \\(C = (C_1,\\ldots,C_k)\\) of \\(\\mathcal{X}\\) (into nonempty subsets) there exists some dissimilarity function \\(d\\) over \\(\\mathcal{X}\\) such that \\(F(\\mathcal{X},d)=C\\).\n\nConsistency If \\(d\\) and \\(d'\\) are dissimilarity functions over \\(\\mathcal{X}\\), such that for all \\(x, y \\in \\mathcal{X}\\),\n\nif \\(x,y\\) belong to the same cluster in \\(F(\\mathcal{X},d)\\) then \\(d'(x,y) \\leq d(x,y)\\),\nif \\(x,y\\) belong to different clusters in \\(F(\\mathcal{X},d)\\) then \\(d'(x,y) \\geq d(x,y)\\),\n\n\n\nthen \\(F(\\mathcal{X},d) = F(\\mathcal{X},d')\\).\n Designing clustering functions meeting simultaneously any two of the three properties is doable\nbut\n The three reasonable goals are conflicting"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#kleinbergs-impossibility-theorem",
    "href": "core/slides/slides-kmeans.html#kleinbergs-impossibility-theorem",
    "title": "Clustering and k-means",
    "section": "Kleinberg’s impossibility theorem",
    "text": "Kleinberg’s impossibility theorem\n\n\n\n\n\n\n\nTheorem\n\n\nNo clustering function \\(F\\) satisfies simultaneously all three properties:\n\nScale Invariance,\nRichness, and\nConsistency"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#flathierarchical-and",
    "href": "core/slides/slides-kmeans.html#flathierarchical-and",
    "title": "Clustering and k-means",
    "section": "Flat/Hierarchical and …",
    "text": "Flat/Hierarchical and …\nA wide variety of clustering methods have been used in Statistics and Machine Learning.\n\n\nFlat clustering (for example \\(k\\)-means) partitions sample into a fixed number of classes (usually denoted by \\(k\\)). The partition is determined by some algorithm\n\n.f6[The ultimate objective is to optimize some cost function. Whether the objective is achieved or even approximately achieved using a reasonable amount of computational resources is not settled]\n\n\nModel based clustering is based on a generative model: data are assumed to be sampled from a specific model (usually finite mixtures of Gaussians, the model may or may not be parametric)\n\n.f6[Clustering consists in fitting such a mixture model and then assigning sample points to mixture components]\n\n\nHierarchical clustering is the topic of next lesson"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#carte-du-tendre",
    "href": "core/slides/slides-kmeans.html#carte-du-tendre",
    "title": "Clustering and k-means",
    "section": "Carte du tendre",
    "text": "Carte du tendre\n\n\nIn Machine Learning, \\(k\\)-means and hierarchical clustering belong to a range of tasks called non-supervised learning\nThis contrasts with regression which belongs to the realm of supervised learning"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#section-2",
    "href": "core/slides/slides-kmeans.html#section-2",
    "title": "Clustering and k-means",
    "section": "",
    "text": "The \\(k\\)-means algorithm is an iterative method that constructs a sequence of Voronoï partitions\nA Voronoï diagram draws the nearest neighbor regions around a set of points."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#definition-voronoï-partitions",
    "href": "core/slides/slides-kmeans.html#definition-voronoï-partitions",
    "title": "Clustering and k-means",
    "section": "Definition: Voronoï partitions",
    "text": "Definition: Voronoï partitions\nAssume:\n\nsample \\(X_1, \\ldots, X_n\\) from \\(\\mathbb{R}^p\\)\n\n\n\\(\\mathbb{R}^p\\) is endowed with a metric \\(d\\), usually \\(\\ell_2\\), sometimes a weighted \\(\\ell_2\\) distance or \\(\\ell_1\\)\n\n\nEach cluster is defined by a centroid\nThe collection of centroids is (sometimes) called the codebook \\(\\mathcal{C}=c_1, \\ldots, c_k\\)\nEach centroid \\(c_j\\) defines a class:\n\\[C_j = \\bigg\\{ X_i : d(X_i, c_j) = \\min_{j' \\leq k} d(X_i, c_{j'})\\bigg\\}\\]\nand more generally a Voronoï cell in \\(\\mathbb{R}^p\\)\n\\[C_j = \\bigg\\{ x :  x \\in \\mathbb{R}^p, d(x, c_j) = \\min_{j' \\leq k} d(x, c_{j'})\\bigg\\}\\]"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#a-voronoï-tesselation",
    "href": "core/slides/slides-kmeans.html#a-voronoï-tesselation",
    "title": "Clustering and k-means",
    "section": "A Voronoï tesselation",
    "text": "A Voronoï tesselation\n\n\n\n\n\n\n\n\n\n\n\n\nEuclidean distance, dimension 2\nA voronoi tesselation generated by \\(100\\) points picked at random on the gred \\(\\{1,\\ldots, 200\\}^2\\)\nNote that cell boundaries are line segments\nNote that centroids may lie close to boundaries\nThe position of the centroid of a Voronoi cell depends on the positions of the centroids of the neighboring cells\n\nSee ggforce"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#a-voronoi-partition-for-projected-iris-dataset",
    "href": "core/slides/slides-kmeans.html#a-voronoi-partition-for-projected-iris-dataset",
    "title": "Clustering and k-means",
    "section": "A Voronoi partition for projected Iris dataset",
    "text": "A Voronoi partition for projected Iris dataset\n\n\nThe black points marked with a cross define three centroids.\nThe straight lines delimit the Voronoï cells defined by the three centroids.\nThe colored points come from the Iris dataset: each point is colored according to the the cell it belongs to."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#adding-centroids",
    "href": "core/slides/slides-kmeans.html#adding-centroids",
    "title": "Clustering and k-means",
    "section": "Adding centroids",
    "text": "Adding centroids"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#k-means-objective-function",
    "href": "core/slides/slides-kmeans.html#k-means-objective-function",
    "title": "Clustering and k-means",
    "section": "\nk-means objective function",
    "text": "k-means objective function\nThe \\(k\\)-means algorithm aims at building a codebook \\(\\mathcal{C}\\) that minimizes\n\\(\\mathcal{C} \\mapsto \\sum_{i=1}^n \\min_{c \\in \\mathcal{C}}  \\norm X_i - c \\norm_2^2\\)\nover all codebooks with given cardinality\nIf \\(c \\in \\mathcal{C}\\) is the closest centroid to \\(X \\in \\mathbb{R}^p\\),\n\\[\\|c - X\\|^2\\]\nis the quantization/reconstruction error suffered when using codebook \\(\\mathcal{C}\\) to approximate \\(X\\)\n If there are no restrictions on the dimension of the input space, on the number of centroids, or on sample size, computing an optimal codebook is a \\(\\text{NP}\\) -hard problem"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#k--means-at-work",
    "href": "core/slides/slides-kmeans.html#k--means-at-work",
    "title": "Clustering and k-means",
    "section": "\n\\(k\\) -means at work",
    "text": "\\(k\\) -means at work\n\n\nWe may figure out what an optimized Voronoï partition looks like on the Iris dataset\nkmeans with \\(k=3\\) on the Iris dataset\nFunction kmeans is run with default arguments\n\n\n\n\n\n\n\n\n\nWe chose the Sepal plane for clustering and visualization\nThis is arbitrary. We could have chosen a Petal plane, a Width plane, or a plane defined by principal axes."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#a-k-means-clustering-is-completely-characterized-by-the-k-centroids",
    "href": "core/slides/slides-kmeans.html#a-k-means-clustering-is-completely-characterized-by-the-k-centroids",
    "title": "Clustering and k-means",
    "section": "A \\(k\\)-means clustering is completely characterized by the \\(k\\) centroids",
    "text": "A \\(k\\)-means clustering is completely characterized by the \\(k\\) centroids\nOnce centroids are known, clusters can be recovered by searching the closest centroid for each sample point (that is by delimiting the Voronoï cells).\n\nHow can we assess the quality of a \\(k\\)-means clustering?\nCan we compare the clusterings achieved by picking different values of \\(k\\)?\n\nThere is no obvious assessment criterion!"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#section-3",
    "href": "core/slides/slides-kmeans.html#section-3",
    "title": "Clustering and k-means",
    "section": "",
    "text": "The quality of a clustering can be appreciated according to a wide variety of performance indicators\n\nDistortion: this is the \\(k\\)-means cost\nShape of clusters\nRelevance of clusters\nStability: does clustering depend on few points?"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#caveat",
    "href": "core/slides/slides-kmeans.html#caveat",
    "title": "Clustering and k-means",
    "section": "Caveat",
    "text": "Caveat\nWhen visualizing \\(k\\)-means clustering on Iris data, we are cheating. \nWe have a gold standard classification delivered by botanists\nThe botanists classification can be challenged\nWe can compare classification originating from phenotypes (appearance) and classification based on phylogeny (comparing DNAs)"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#summarising-a-k-means-clustering",
    "href": "core/slides/slides-kmeans.html#summarising-a-k-means-clustering",
    "title": "Clustering and k-means",
    "section": "Summarising a \\(k\\)-means clustering",
    "text": "Summarising a \\(k\\)-means clustering\n\n\nA clustering can be summarized and illustrated.\nIn  A meaningful summary is provided by the generic function summary(), or a tidy summary is providede by broom::tidy(...)\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nsize\nwithinss\ncluster\n\n\n\n6.81\n3.07\n47.00\n12.62\n1\n\n\n5.01\n3.43\n50.00\n13.13\n2\n\n\n5.77\n2.69\n53.00\n11.30\n3\n\n\n\n\n\n\nThe concise summary tells us the number of points that are assigned to each cluster, and the Within Sum of Squares (WNSS). It says something about inner homogeneity and (apparently) nothing about outer homogeneity"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#k-means-with-k2",
    "href": "core/slides/slides-kmeans.html#k-means-with-k2",
    "title": "Clustering and k-means",
    "section": "\n\\(k\\)-means with \\(k=2\\)\n",
    "text": "\\(k\\)-means with \\(k=2\\)\n\n\n\nWe pursue the exploration of kmeans by building another clustering for Iris dataset.\nThis times with \\(k=2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nsize\nwithinss\ncluster\n\n\n\n5.22\n3.13\n83.00\n35.09\n1\n\n\n6.61\n2.97\n67.00\n23.11\n2"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#how-should-we-pick-k",
    "href": "core/slides/slides-kmeans.html#how-should-we-pick-k",
    "title": "Clustering and k-means",
    "section": "\n How should we pick \\(k\\)?",
    "text": "How should we pick \\(k\\)?\n\n\nEven if we could compute a provably optimal codebook for each \\(k\\), choosing \\(k\\) would not be obvious\nA common recipe consists of plotting within clusters sum of squares (WNSS) against \\(k\\)\nWithin clusters sum of squares (WNSS) decreases sharply between \\(k=2\\) and \\(k=3\\)\nFor larger values of \\(k\\), the decay is much smaller.\nThe elbow rule of thumb suggests to choose \\(k=3\\).\n\nWe have run kmeans over the Iris dataset, for \\(k\\) in range \\(2, \\ldots, 10\\). For each value of \\(k\\), we performed \\(32\\) randomized initializations, and chose the partition that minimizes within clusters sum of squares"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#incentive-to-choose-k4",
    "href": "core/slides/slides-kmeans.html#incentive-to-choose-k4",
    "title": "Clustering and k-means",
    "section": "Incentive to choose \\(k=4\\)?",
    "text": "Incentive to choose \\(k=4\\)?\n\n\nDepending on initialization, taking \\(k=4\\) creates a cluster at the boundary between versicolor and virginica or it may split the setosa cluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nsize\nwithinss\ncluster\n\n\n\n5.518182\n2.606061\n33\n5.967879\n1\n\n\n5.016327\n3.451020\n49\n11.569388\n2\n\n\n7.283333\n3.133333\n18\n3.905000\n3\n\n\n6.350000\n2.942000\n50\n6.786800\n4"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#initialization-matters",
    "href": "core/slides/slides-kmeans.html#initialization-matters",
    "title": "Clustering and k-means",
    "section": "Initialization matters!",
    "text": "Initialization matters!\n\n\n\nInitialize by samples.\nk-Mean++ try to take them as separated as possible.\nNo guarantee to converge to a global optimum!\nTrial and error.\nRepeat and keep the best result.\n\n\n\nkmeans(x,       # data\n       centers, # initial centroids or number of clusters\n       iter.max = 10,\n       nstart = 1,  # number of trials\n       algorithm = c(\"Hartigan-Wong\", # default\n                     \"Lloyd\",         #&lt;&lt; old one\n                     \"Forgy\",\n                     \"MacQueen\"),\n       trace=FALSE)"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#lloyds-algorithm-detailed-for-fixed-k-naive-k-means",
    "href": "core/slides/slides-kmeans.html#lloyds-algorithm-detailed-for-fixed-k-naive-k-means",
    "title": "Clustering and k-means",
    "section": "Lloyd’s Algorithm (detailed) for fixed k (naive k-means)",
    "text": "Lloyd’s Algorithm (detailed) for fixed k (naive k-means)\n\nInitialize Choose \\(k\\) centroids\nIterations: Two phases\n(Movement) Assign each sample point to the closest centroid Assign each sample point to a class in the Voronoi partition defined by the centroids\n(Update) For each class in the current Voronoi partition, update the centroid so as to minimize the Within Cluster Sum of Squared distances."
  },
  {
    "objectID": "core/slides/slides-kmeans.html#lloyds-iterations",
    "href": "core/slides/slides-kmeans.html#lloyds-iterations",
    "title": "Clustering and k-means",
    "section": "Lloyd’s iterations",
    "text": "Lloyd’s iterations"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#after-1-step",
    "href": "core/slides/slides-kmeans.html#after-1-step",
    "title": "Clustering and k-means",
    "section": "After 1 step",
    "text": "After 1 step"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#lloyds-iterations-after-2-steps",
    "href": "core/slides/slides-kmeans.html#lloyds-iterations-after-2-steps",
    "title": "Clustering and k-means",
    "section": "Lloyd’s iterations after 2 steps",
    "text": "Lloyd’s iterations after 2 steps"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#lloyds-iterations-after-4-steps",
    "href": "core/slides/slides-kmeans.html#lloyds-iterations-after-4-steps",
    "title": "Clustering and k-means",
    "section": "Lloyd’s iterations after 4 steps",
    "text": "Lloyd’s iterations after 4 steps"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#analysis",
    "href": "core/slides/slides-kmeans.html#analysis",
    "title": "Clustering and k-means",
    "section": "Analysis",
    "text": "Analysis\nGiven\n\ncodebook \\(\\mathcal{C} =\\big\\{c_1, \\ldots, c_k\\big\\}\\) and\nclusters \\(C_1, \\ldots C_k\\),\n\nthe within-clusters sum of squares is defined as\n\\(\\sum_{j=1}^k  \\sum_{i: X_i \\in C_j} \\bigg\\Vert c_j - X_i \\bigg\\Vert^2\\)\n This is also the kmeans cost"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#section-4",
    "href": "core/slides/slides-kmeans.html#section-4",
    "title": "Clustering and k-means",
    "section": "",
    "text": "Lemma\n\n\nAt each stage, the within clusters sums of squares does not increase"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#proof",
    "href": "core/slides/slides-kmeans.html#proof",
    "title": "Clustering and k-means",
    "section": "Proof",
    "text": "Proof\nLet \\(\\mathcal{C}^{(t)} =\\big\\{ c^{(t)}_1, \\ldots, c_k^{(t)}\\big\\}\\) be the codebook after \\(t\\) steps\nLet \\(\\big({C}^{(t)}_j\\big)_{j \\leq k}\\) be the clusters after \\(t\\) steps\n\nCentroids at step \\(t+1\\) are the barycenters of clusters \\(\\big({C}^{(t)}_j\\big)_{j \\leq k}\\)\n\n\n\\(c^{(t+1)}_j = \\frac{1}{|C_j^{(t)}|} \\sum_{X_i \\in C^{(t)}_j} X_i\\)\n\nClusters \\(C^{(t+1)}_j\\) are defined by\n\n\\(C^{(t+1)}_j = \\bigg\\{ X_i : \\Vert X_i -  c^{(t+1)}_j\\Vert = \\min_{c \\in \\mathcal{C}^{(t+1)}} \\Vert X_i -  c\\Vert \\bigg\\}\\)\nEach sample point is assigned to the closest centroid"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#proof-continued",
    "href": "core/slides/slides-kmeans.html#proof-continued",
    "title": "Clustering and k-means",
    "section": "Proof (continued)",
    "text": "Proof (continued)\n\\[\\sum_{j=1}^k \\sum_{X_i \\in C^{(t)}_j} \\bigg\\Vert c^{(t)}_j  - X_i\\bigg\\Vert^2  \\geq \\sum_{j=1}^k \\sum_{X_i \\in C^{(t)}_j} \\bigg\\Vert c^{(t+1)}_j  - X_i\\bigg\\Vert^2\\]\nsince for each \\(j\\), the mean \\(c^{(t+1)}_j\\) minimizes the average square distance to points in \\(C^{(t)}_j\\)\n\\[\\sum_{j=1}^k \\sum_{X_i \\in C^{(t)}_j} \\bigg\\Vert c^{(t+1)}_j  - X_i\\bigg\\Vert^2 \\geq \\sum_{j=1}^k \\sum_{X_i \\in C^{(t)}_j} \\min_{c \\in \\mathcal{C}^{(t+1)}}\\bigg\\Vert c  - X_i\\bigg\\Vert^2\\]\n\\[\\sum_{j=1}^k \\sum_{X_i \\in C^{(t)}_j} \\min_{c \\in \\mathcal{C}^{(t+1)}}\\bigg\\Vert c  - X_i\\bigg\\Vert^2 = \\sum_{j=1}^k \\sum_{X_i \\in C^{(t+1)}_j} \\bigg\\Vert c^{(t+1)}_j  - X_i\\bigg\\Vert^2\\]"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#variants-of-k-means",
    "href": "core/slides/slides-kmeans.html#variants-of-k-means",
    "title": "Clustering and k-means",
    "section": "\n Variants of k-means",
    "text": "Variants of k-means\nImplementations of \\(k\\)-means vary with respect to\n\nInitialization\n\nk-means++\nForgy : pick initial centroids at random from the dataset\nRandom partition : pick a random partition of the dataset and initialize centroids by computing means in each class\n…\n\n\nMovement/assignment\n\nNaive \\(k\\) means uses brute-force search for closest centroid. Each step requires \\(\\Omega(n \\times k)\\) operations\nElkan (used by  scikit-learn)\nHartigan-Wong  default\n…\n\n\n\n\nLloyd’s algorithm is the standard approach for this problem. However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naïve implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd’s algorithm. .fr.f6[Wikipedia]\n\nIn base , kmeans is a wrapper for different but related algorithms.\nLloyd’s algorithm is the first and simplest versions of a series of heuristic methods designed to minimize the k-means cost\n\nMacQueen modify the mean each time a sample is assigned to a new cluster\nHartigan-Wong is the default method. It modifies the mean by removing the considered sample point, assign it to the nearby center and recompute the new mean after assignment.\nForgy"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#section-5",
    "href": "core/slides/slides-kmeans.html#section-5",
    "title": "Clustering and k-means",
    "section": "",
    "text": "The result of a clustering procedure like kmeans can be visualized by projecting the dataset on a pair of native variables and using some aesthetics to emphasize the clusters\nThis is not always the best way.\nFirst choosing a pair of native variables may not be straightforward. The projected pairwise distances may not faithfully reflect the pairwise distances that serve for clustering.\nIt makes sense to project the dataset of the \\(2\\)-dimensional subspace that maximizes the projected inertia, that is on the space generated by the first two principal components"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#pca-projection-k-means",
    "href": "core/slides/slides-kmeans.html#pca-projection-k-means",
    "title": "Clustering and k-means",
    "section": "PCA, projection, \\(k\\)-means",
    "text": "PCA, projection, \\(k\\)-means\n\n\nThe kmeans clustering of the Iris dataset is projected on the first two principal components: prcomp is used to perform PCA with neither centering nor scaling\nkmeans is applied to the rotated data\n\n\n\n\n\n\n\n\n\n The straight lines are the not the projections of the boundaries of the (4-dimensional) Voronoï cells defined by the clusters centroids, but the boundaries of the 2-dimensional Voronoï celles defined by the projections of the cluster centroids"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#questions-around-k-means",
    "href": "core/slides/slides-kmeans.html#questions-around-k-means",
    "title": "Clustering and k-means",
    "section": "\n Questions around k-means",
    "text": "Questions around k-means\n\nChoosing \\(k\\)\nAssessing clustering quality\nScaling or not scaling ?\nChoosing a distance\nInitialization methods\nMovement/assignment update\nStopping rules"
  },
  {
    "objectID": "core/slides/slides-kmeans.html#conclusion",
    "href": "core/slides/slides-kmeans.html#conclusion",
    "title": "Clustering and k-means",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nEuclidean distance is used as a metric and inertia is used as a measure of cluster scatter\nThe number of clusters \\(k\\) is an input parameter\nConvergence to a local minimum may produce counterintuitive (“wrong”) results"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tables-examples",
    "href": "core/slides/slides-dplyr.html#tables-examples",
    "title": "Tables manipulation with dplyr",
    "section": "Tables (examples)",
    "text": "Tables (examples)\n\nSpeadsheets (Excel)\n Relational tables\n\nDataframes in datascience frameworks\n\n\n: data.frame, tibble, …\n\n: pandas.dataframe\n\n\nspark: dataframe\n\n\nDask: dataframe\n\nand many others"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tables-why",
    "href": "core/slides/slides-dplyr.html#tables-why",
    "title": "Tables manipulation with dplyr",
    "section": "Tables (Why ?)",
    "text": "Tables (Why ?)\nIn Data Science, each framework comes with its own flavor(s) of table(s)\n Tables from relational databases serve as inspiration\nIn  legacy dataframes shape the life of statisticians and data scientists\nThe purpose of this session is\n\ndescribe dataframes from an end-user viewpoint (we leave aside implementations)\n\npresenting tools for\n\naccessing information within dataframes (querying)\nsummarizing information (aggregation queries)\ncleaning/cleaning dataframes (tidying)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#loading-tables-and-packages",
    "href": "core/slides/slides-dplyr.html#loading-tables-and-packages",
    "title": "Tables manipulation with dplyr",
    "section": "Loading tables and packages",
    "text": "Loading tables and packages\n\nrequire(\"tidyverse\")      # All we need is there\nrequire(\"nycflights13\")    # for flight data\nrequire(\"gt\")\nrequire(\"kableExtra\")\n# \ndata(flights)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#about-loaded-packages",
    "href": "core/slides/slides-dplyr.html#about-loaded-packages",
    "title": "Tables manipulation with dplyr",
    "section": "About loaded packages",
    "text": "About loaded packages\n\nMetapackage tidyverse provides tools to create, query, tidy dataframes as well as tools to load data from various sources and save them in persistent storage\nnycflights13 provides the dataframes we play with\ngt for tayloring table displays"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#the-flights-table",
    "href": "core/slides/slides-dplyr.html#the-flights-table",
    "title": "Tables manipulation with dplyr",
    "section": "The flights table",
    "text": "The flights table\n\n\n\nhead(flights) |&gt;\n  glimpse(width = 30) \n\nRows: 6\nColumns: 19\n$ year           &lt;int&gt; 2013,…\n$ month          &lt;int&gt; 1, 1,…\n$ day            &lt;int&gt; 1, 1,…\n$ dep_time       &lt;int&gt; 517, …\n$ sched_dep_time &lt;int&gt; 515, …\n$ dep_delay      &lt;dbl&gt; 2, 4,…\n$ arr_time       &lt;int&gt; 830, …\n$ sched_arr_time &lt;int&gt; 819, …\n$ arr_delay      &lt;dbl&gt; 11, 2…\n$ carrier        &lt;chr&gt; \"UA\",…\n$ flight         &lt;int&gt; 1545,…\n$ tailnum        &lt;chr&gt; \"N142…\n$ origin         &lt;chr&gt; \"EWR\"…\n$ dest           &lt;chr&gt; \"IAH\"…\n$ air_time       &lt;dbl&gt; 227, …\n$ distance       &lt;dbl&gt; 1400,…\n$ hour           &lt;dbl&gt; 5, 5,…\n$ minute         &lt;dbl&gt; 15, 2…\n$ time_hour      &lt;dttm&gt; 2013…\n\n\n\n\nA dataframe is a two-ways (two-dimensional) table\nhead(df) displays the first 6 rows of its first argument\nThe vectors making a dataframe may have different types/classes (a dataframe is not a matrix)\nCompare str(), glimpse(), head()"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#table-schema",
    "href": "core/slides/slides-dplyr.html#table-schema",
    "title": "Tables manipulation with dplyr",
    "section": "Table schema",
    "text": "Table schema\nA table is a list of columns\nEach column has\n\n\nname and\n\ntype (class in \n\n\n\n\nglimpse(flights,   #&lt;&lt;\n        width=50)\n\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12,…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 46…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, …\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 7…\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0,…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 201…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section",
    "href": "core/slides/slides-dplyr.html#section",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "flights has 19 columns\nEach column is a sequence (vector) of items with the same type/class\nAll columns have the same length\n\nflights has 336776 rows\nIn  parlance, a row is (often) called a tuple\n\nIn  parlance, a column is (often) called a variable"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#column-types",
    "href": "core/slides/slides-dplyr.html#column-types",
    "title": "Tables manipulation with dplyr",
    "section": "Column types",
    "text": "Column types\n\n\n\n\n\n\n\n\nclass\ncolumns\n\n\n\ninteger\n‘year’ ‘month’ ‘day’ ‘dep_time’ ‘sched_dep_time’ ‘arr_time’ ‘sched_arr_time’ ‘flight’\n\n\nnumeric\n‘dep_delay’ ‘arr_delay’ ‘air_time’ ‘distance’ ‘hour’ ‘minute’\n\n\ncharacter\n‘carrier’ ‘tailnum’ ‘origin’ ‘dest’\n\n\nPOSIXct\n‘time_hour’\n\n\nPOSIXt\n‘time_hour’\n\n\n\n\nA column, as a vector, may be belong to different classes\nOther classes: factor for categorical variables\nColumns dest, origin carrier could be coerced as factors\nShould columns dest and origin be coerced to the same factor?"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#nycflights13",
    "href": "core/slides/slides-dplyr.html#nycflights13",
    "title": "Tables manipulation with dplyr",
    "section": "nycflights13",
    "text": "nycflights13"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#columns-specification",
    "href": "core/slides/slides-dplyr.html#columns-specification",
    "title": "Tables manipulation with dplyr",
    "section": "Columns specification",
    "text": "Columns specification\n\n\n\nas.col_spec(flights)\n\n\n\ncols(\n  year = col_integer(),\n  month = col_integer(),\n  day = col_integer(),\n  dep_time = col_integer(),\n  sched_dep_time = col_integer(),\n  dep_delay = col_double(),\n  arr_time = col_integer(),\n  sched_arr_time = col_integer(),\n  arr_delay = col_double(),\n  carrier = col_character(),\n  flight = col_integer(),\n  tailnum = col_character(),\n  origin = col_character(),\n  dest = col_character(),\n  air_time = col_double(),\n  distance = col_double(),\n  hour = col_double(),\n  minute = col_double(),\n  time_hour = col_datetime(format = \"\")\n)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-1",
    "href": "core/slides/slides-dplyr.html#section-1",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "\\(\\approx\\) table schema in relational databases\nColumn specifications are useful when loading dataframes from structured text files like .csv files\n.csv files do not contain typing information\nFile loaders from package readr can be tipped about column classes using column specifications"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-2",
    "href": "core/slides/slides-dplyr.html#section-2",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "SQL stands for structured/simple Query Language\nA query language elaborated during the 1970’s at IBM by E. Codd\nGeared towards exploitation of collections of relational tables\nLess powerful but simpler to use than a programming language\ndplyr is a principled -friendly implementation of SQL ideas (and other things)\n\nAt the core of SQL lies the idea of a table calculus called relational algebra"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#relational-algebra-basics",
    "href": "core/slides/slides-dplyr.html#relational-algebra-basics",
    "title": "Tables manipulation with dplyr",
    "section": "Relational algebra (basics)",
    "text": "Relational algebra (basics)\nConvention: \\(R\\) is a table with columns \\(A_1, \\ldots, A_k\\)\n\n\n\n\n\n\nProjection (picking columns)\n\n\n\\(\\pi(R, A_1, A_3)\\)\n\n\n\n\n\n\n\n\n\nSelection/Filtering (picking rows)\n\n\n\\(\\sigma(R, {\\text{condition}})\\)\n\n\n\n\n\n\n\n\n\nJoin (mulitple tables operation)\n\n\n\\(\\bowtie(R,S, {\\text{condition}})\\)\n\n\n\n Any operation produces a table\n The schema of the derived table depends on the operation (but does not depend on the content/value of the operands)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-3",
    "href": "core/slides/slides-dplyr.html#section-3",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Table calculus relies on a small set of basic operations \\(\\pi, \\sigma, \\bowtie\\)\nEach operation has one or two table operands and produce a table\n There is more to SQL than relational algebra"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#projection-pi",
    "href": "core/slides/slides-dplyr.html#projection-pi",
    "title": "Tables manipulation with dplyr",
    "section": "Projection \\(\\pi\\)\n",
    "text": "Projection \\(\\pi\\)\n\n\\(\\pi(R, {A_1, A_3})\\)\nA projection \\(\\pi(\\cdot, {A_1, A_3})\\) is defined by a set of column names, say \\(A_1, A_3\\)\nIf \\(R\\) has columns with given names, the result is a table with names \\(A_1, A_3\\) and one row per row of \\(R\\)\nA projection is parametrized by a list of column names"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#package-dplyr",
    "href": "core/slides/slides-dplyr.html#package-dplyr",
    "title": "Tables manipulation with dplyr",
    "section": "\n Package dplyr\n",
    "text": "Package dplyr\n\n\n\n\nTranformation chapter in R4DS\nCheat sheet I\nCheat sheet II\n\n\n\n\n\n\nhttps://dplyr.tidyverse.org"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-4",
    "href": "core/slides/slides-dplyr.html#section-4",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Base  provides tools to perform relational algebra operations\nBut:\n\nBase  does not provide a consistent API\nThe lack of a consistent API makes operation chaining tricky"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#dplyr-verbs",
    "href": "core/slides/slides-dplyr.html#dplyr-verbs",
    "title": "Tables manipulation with dplyr",
    "section": "\ndplyr verbs",
    "text": "dplyr verbs\nFive basic verbs:\n\nPick observations/rows by their values (filter()) σ(…)\nPick variables by their names (select()) π(…)\nReorder the rows (arrange())\nCreate new variables with functions of existing variables (mutate())\nCollapse many values down to a single summary (summarise())\n\n\nAnd\n\n\ngroup_by() changes the scope of each function from operating on the entire dataset to operating on it group-by-group"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tidyverse",
    "href": "core/slides/slides-dplyr.html#tidyverse",
    "title": "Tables manipulation with dplyr",
    "section": "\n tidyverse",
    "text": "tidyverse\n\n\n\nAll verbs work similarly:\n\n\nThe first argument is a data frame (table).\n\n\nThe subsequent arguments describe what to do with the data frame, using the variable/column names (without quotes)\n\n\nThe result is a new data frame (table)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#dplyrselect-as-a-projection-operator-π",
    "href": "core/slides/slides-dplyr.html#dplyrselect-as-a-projection-operator-π",
    "title": "Tables manipulation with dplyr",
    "section": "\ndplyr::select() as a projection operator (π)",
    "text": "dplyr::select() as a projection operator (π)\n\\(\\pi(R, \\underbrace{A_1, \\ldots, A_3}_{\\text{column names}})\\)\nselect(R, A1, A3) #&lt;&lt;\nor, equivalently\nR |&gt; select(A1, A3) #&lt;&lt;\n |&gt; is the pipe operator\n x |&gt; f(y, z) is translated to f(x, y, z) and then evaluated"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#dplyrselect",
    "href": "core/slides/slides-dplyr.html#dplyrselect",
    "title": "Tables manipulation with dplyr",
    "section": "dplyr::select()",
    "text": "dplyr::select()\n\nFunction select has a variable number of arguments\nFunction select has a variable number of arguments\nFunction select allows to pick column by names (and much more)\nNote that in the current environment, there are no objects called A1, A3\nThe consistent API allows to use the pipe operator\n\n\n\n\n\n\n\nCaution\n\n\nThere is also a select() function in base R"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-tables",
    "href": "core/slides/slides-dplyr.html#toy-tables",
    "title": "Tables manipulation with dplyr",
    "section": "Toy tables",
    "text": "Toy tables\n\n\n\nspam &lt;- set.seed(42)\n\nR &lt;-  tibble(A1=seq(2, 10, 2),\n             A2=sample(letters, 5),\n             A3=seq(from=date(\"2021-10-21\"),\n                    to=date(\"2021-11-20\"),\n                    by=7),\n             D=sample(letters, 5))\n\nS &lt;- tibble(E=c(3,4,6,9, 10),\n            F=sample(letters, 5),\n            G=seq(from=date(\"2021-10-21\"),\n                   to=date(\"2021-10-21\")+4, by=1),\n            D=sample(letters,5)\n          )\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#projecting-toy-tables",
    "href": "core/slides/slides-dplyr.html#projecting-toy-tables",
    "title": "Tables manipulation with dplyr",
    "section": "Projecting toy tables",
    "text": "Projecting toy tables\n\n\n\nR |&gt; \n  dplyr::select(A2,D) |&gt; \n  knitr::kable(caption=\"Projecting R\")\n\n\nProjecting R\n\nA2\nD\n\n\n\nq\nr\n\n\ne\nq\n\n\na\no\n\n\nj\ng\n\n\nd\nd\n\n\n\n\n\n\n\nR |&gt; \n  dplyr::select(- where(is.character)) |&gt; \n  knitr::kable(caption=\"Projecting R, all but character columns\")\n\n\nProjecting R, all but character columns\n\nA1\nA3\n\n\n\n2\n2021-10-21\n\n\n4\n2021-10-28\n\n\n6\n2021-11-04\n\n\n8\n2021-11-11\n\n\n10\n2021-11-18"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#projecting-flights-on-origin-and-dest",
    "href": "core/slides/slides-dplyr.html#projecting-flights-on-origin-and-dest",
    "title": "Tables manipulation with dplyr",
    "section": "Projecting flights on origin and dest\n",
    "text": "Projecting flights on origin and dest\n\n\n\nflights |&gt;\n  select(origin, dest) |&gt;  #&lt;&lt;\n  head()\n\n\n# A tibble: 6 × 2\n  origin dest \n  &lt;chr&gt;  &lt;chr&gt;\n1 EWR    IAH  \n2 LGA    IAH  \n3 JFK    MIA  \n4 JFK    BQN  \n5 LGA    ATL  \n6 EWR    ORD  \n\n\nA more readable equivalent of\nhead(select(flights, origin, dest), 10)\nor\nSELECT \n  origin, dest\nFROM \n  flights\nLIMIT 6;"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#sigmar-textcondition",
    "href": "core/slides/slides-dplyr.html#sigmar-textcondition",
    "title": "Tables manipulation with dplyr",
    "section": "\\(\\sigma(R, \\text{condition})\\)",
    "text": "\\(\\sigma(R, \\text{condition})\\)\n\nA selection/filtering operation is defined by a condition that can be checked on the rows of tables with convenient schema\n\\(\\sigma(R, \\text{condition})\\) returns a table with the same schema as \\(R\\)\nThe resulting table contains the rows/tuples of \\(R\\) that satisfy \\(\\text{condition}\\)\n\\(\\sigma(R, \\text{FALSE})\\) returns an empty table with the same schema as \\(R\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#chaining-filtering-and-projecting",
    "href": "core/slides/slides-dplyr.html#chaining-filtering-and-projecting",
    "title": "Tables manipulation with dplyr",
    "section": "Chaining filtering and projecting",
    "text": "Chaining filtering and projecting\n\n\nstart &lt;- date(\"2021-10-27\")\nend &lt;- start + 21\n\nR |&gt;\n#  filter(A2 &gt; \"n\") |&gt;  #&lt;&lt;\n  filter(between(A3, start, end)) |&gt;\n  select(A1, A3) #&lt;&lt;\n\n\n# A tibble: 3 × 2\n     A1 A3        \n  &lt;dbl&gt; &lt;date&gt;    \n1     4 2021-10-28\n2     6 2021-11-04\n3     8 2021-11-11\n\n\n\n\nFiltering dropped one row\nProjecting dropped two columns"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#selecting-flights-based-on-origin-and-dest",
    "href": "core/slides/slides-dplyr.html#selecting-flights-based-on-origin-and-dest",
    "title": "Tables manipulation with dplyr",
    "section": "Selecting flights based on origin and dest\n",
    "text": "Selecting flights based on origin and dest\n\nand then projecting on dest, time_hour, carrier\n\n\nflights |&gt;\n  filter(dest %in% c('ATL', 'LAX'), #&lt;&lt;\n         origin == 'JFK') |&gt;\n  select(dest, time_hour, carrier) |&gt; #&lt;&lt;\n  head()\n\n\n# A tibble: 6 × 3\n  dest  time_hour           carrier\n  &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt;  \n1 LAX   2013-01-01 06:00:00 UA     \n2 ATL   2013-01-01 06:00:00 DL     \n3 LAX   2013-01-01 07:00:00 VX     \n4 LAX   2013-01-01 07:00:00 B6     \n5 LAX   2013-01-01 07:00:00 AA     \n6 ATL   2013-01-01 08:00:00 DL     \n\n\nIn SQL ( parlance:\nSELECT \n  dest, time_hour, carrier\nFROM \n  flights\nWHERE \n  dest IN ('ATL', 'LAX') AND\n  origin = 'JFK'\nLIMIT 6"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#logical-operations",
    "href": "core/slides/slides-dplyr.html#logical-operations",
    "title": "Tables manipulation with dplyr",
    "section": "Logical operations",
    "text": "Logical operations\n\nfilter(R, condition_1, condition_2) is meant to return the rows of R that satisfy condition_1 and condition_2\nfilter(R, condition_1 & condition_2) is an equivalent formulation\nfilter(R, condition_1 | condition_2) is meant to return the rows of R that satisfy condition_1 or condition_2 (possibly both)\nfilter(R, xor(condition_1,condition_2)) is meant to return the rows of R that satisfy either condition_1 or condition_2 (just one of them)\nfilter(R, ! condition_1) is meant to return the rows of R that do not satisfy condition_1"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#missing-values",
    "href": "core/slides/slides-dplyr.html#missing-values",
    "title": "Tables manipulation with dplyr",
    "section": "\n Missing values!",
    "text": "Missing values!\nNumerical column dep_time contains many NA's (missing values)\n\n# flights |&gt; pull(dep_time) |&gt; summary()\nsummary(flights$dep_time)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      1     907    1401    1349    1744    2400    8255 \n\n\n Missing values (NA and variants) should be handled with care\nNA & TRUE\n[1] NA\nNA | TRUE\n[1] TRUE"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#truth-tables-for-three-valued-logic",
    "href": "core/slides/slides-dplyr.html#truth-tables-for-three-valued-logic",
    "title": "Tables manipulation with dplyr",
    "section": "Truth tables for three-valued logic",
    "text": "Truth tables for three-valued logic\n\n\n  uses three-valued logic\n Generate complete truth tables for and, or, xor\n\nv &lt;- c(TRUE, FALSE, NA) # truth values\n\nlist_tt &lt;- map(c(`&`, `|`, xor),  #&lt;&lt;\n               ~ outer(v, v, .x)) #&lt;&lt;\n\nfor (i in seq_along(list_tt)){\n  colnames(list_tt[[i]]) &lt;- v\n  rownames(list_tt[[i]]) &lt;- v\n}\n\nnames(list_tt) &lt;- c('& AND',\n                    'OR',\n                    'XOR')\n\n\n\n\n\n\n& AND\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\n\nTRUE\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\nFALSE\n\n\nFALSE\n\n\nFALSE\n\n\nFALSE\n\n\n\n\nNA\n\n\nNA\n\n\nFALSE\n\n\nNA\n\n\n\n\n\n\n\n\nOR\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\n\nTRUE\n\n\nTRUE\n\n\nTRUE\n\n\nTRUE\n\n\n\n\nFALSE\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\nNA\n\n\nTRUE\n\n\nNA\n\n\nNA\n\n\n\n\n\n\n\n\nXOR\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nTRUE\n\n\nNA\n\n\n\n\nFALSE\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#slice-choosing-rows-based-on-location",
    "href": "core/slides/slides-dplyr.html#slice-choosing-rows-based-on-location",
    "title": "Tables manipulation with dplyr",
    "section": "\nslice(): choosing rows based on location",
    "text": "slice(): choosing rows based on location\n\n\nIn base  dataframe cells can be addressed by indices\nflights[5000:5010,seq(1, 19, by=5)] returns rows 5000:5010 and columns 1, 6, 11 from dataframe flights\nThis can be done in a (verbose) dplyr way using slice() and select()\n\n\nflights |&gt;\n  slice(5001:5005) |&gt;  #&lt;&lt;\n  select(seq(1, 19, by=5))\n\n# A tibble: 5 × 4\n   year dep_delay flight distance\n  &lt;int&gt;     &lt;dbl&gt;  &lt;int&gt;    &lt;dbl&gt;\n1  2013         3   4437      602\n2  2013        43   1016      187\n3  2013        -2   2190     1089\n4  2013        -1     91     2576\n5  2013         5   2131      502\n\n\n\n combined with aggregation (group_by()) variants of slice_ may be used to perform windowing operations\n Useful variant slice_sample()"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-5",
    "href": "core/slides/slides-dplyr.html#section-5",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Note\n\n\n\\(\\bowtie(R,S, {\\text{condition}})\\)\nstands for\n\njoin rows/tuples of \\(R\\) and rows/tuples of \\(S\\) that satisfy \\(\\text{condition}\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#nycflights-tables",
    "href": "core/slides/slides-dplyr.html#nycflights-tables",
    "title": "Tables manipulation with dplyr",
    "section": "\nnycflights tables",
    "text": "nycflights tables\n\n\nThe nycflights13 package offers five related tables:\n\n\nFact tables:\n\nflights\n\nweather (hourly weather conditions at different locations)\n\n\n\nDimension tables:\n\n\nairports (airports full names, location, …)\n\nplanes (model, manufacturer, year, …)\n\nairlines (full names)\n\n\n\nThis is an instance of a Star Schema"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#about-star-schemas",
    "href": "core/slides/slides-dplyr.html#about-star-schemas",
    "title": "Tables manipulation with dplyr",
    "section": "About Star schemas\n",
    "text": "About Star schemas\n\n\nFact tables record measurements for a specific event\nFact tables generally consist of numeric values, and foreign keys to dimensional data where descriptive information is kept\nDimension tables record informations about entities involved in events recorded in Fact tables\n\n\n\nFrom Wikipedia]"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#weather-conditions",
    "href": "core/slides/slides-dplyr.html#weather-conditions",
    "title": "Tables manipulation with dplyr",
    "section": "\n weather conditions",
    "text": "weather conditions\n\n\nweather |&gt;\n  glimpse(width = 50)\n\n\nRows: 26,115\nColumns: 15\n$ origin     &lt;chr&gt; \"EWR\", \"EWR\", \"EWR\", \"EWR\", \"…\n$ year       &lt;int&gt; 2013, 2013, 2013, 2013, 2013,…\n$ month      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ day        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ hour       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10…\n$ temp       &lt;dbl&gt; 39.02, 39.02, 39.02, 39.92, 3…\n$ dewp       &lt;dbl&gt; 26.06, 26.96, 28.04, 28.04, 2…\n$ humid      &lt;dbl&gt; 59.37, 61.63, 64.43, 62.21, 6…\n$ wind_dir   &lt;dbl&gt; 270, 250, 240, 250, 260, 240,…\n$ wind_speed &lt;dbl&gt; 10.35702, 8.05546, 11.50780, …\n$ wind_gust  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, N…\n$ precip     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pressure   &lt;dbl&gt; 1012.0, 1012.3, 1012.5, 1012.…\n$ visib      &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 1…\n$ time_hour  &lt;dttm&gt; 2013-01-01 01:00:00, 2013-01…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#connecting-flights-and-weather",
    "href": "core/slides/slides-dplyr.html#connecting-flights-and-weather",
    "title": "Tables manipulation with dplyr",
    "section": "Connecting flights and weather\n",
    "text": "Connecting flights and weather\n\nWe want to complement information in flights using data weather\nMotivation: we would like to relate delays (arr_delay) and weather conditions\n\ncan we explain (justify) delays using weather data?\ncan we predict delays using weather data?"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-6",
    "href": "core/slides/slides-dplyr.html#section-6",
    "title": "Tables manipulation with dplyr",
    "section": "\n ⋈ \n",
    "text": "⋈ \n\nFor each flight (row in flights)\n\nyear, month, day, hour (computed from time_hour) indicate the approaximate time of departure\norigin indicates the airport where the plane takes off\n\nEach row of weather contains corresponding information\n for each row of flights we look for rows of weather with matching values in year, month, day, hour and origin\n NATURAL INNER JOIN between the tables"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#inner_join-natural-join",
    "href": "core/slides/slides-dplyr.html#inner_join-natural-join",
    "title": "Tables manipulation with dplyr",
    "section": "\ninner_join: natural join",
    "text": "inner_join: natural join\n\n\nf_w &lt;- flights |&gt;\n  inner_join(weather) #&lt;&lt;\n\nf_w |&gt; \n  select(seq(1, \n             ncol(f_w),\n             by=2)) |&gt; \n  glimpse(width=50)\n\n\nRows: 335,220\nColumns: 14\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12,…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 46…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 201…\n$ dewp           &lt;dbl&gt; 28.04, 24.98, 26.96, 26.9…\n$ wind_dir       &lt;dbl&gt; 260, 250, 260, 260, 260, …\n$ wind_gust      &lt;dbl&gt; NA, 21.86482, NA, NA, 23.…\n$ pressure       &lt;dbl&gt; 1011.9, 1011.4, 1012.1, 1…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#join-schema",
    "href": "core/slides/slides-dplyr.html#join-schema",
    "title": "Tables manipulation with dplyr",
    "section": "Join schema",
    "text": "Join schema\n\n\nRows: 335,220\nColumns: 28\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12,…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 46…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, …\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 7…\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0,…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 201…\n$ temp           &lt;dbl&gt; 39.02, 39.92, 39.02, 39.0…\n$ dewp           &lt;dbl&gt; 28.04, 24.98, 26.96, 26.9…\n$ humid          &lt;dbl&gt; 64.43, 54.81, 61.63, 61.6…\n$ wind_dir       &lt;dbl&gt; 260, 250, 260, 260, 260, …\n$ wind_speed     &lt;dbl&gt; 12.65858, 14.96014, 14.96…\n$ wind_gust      &lt;dbl&gt; NA, 21.86482, NA, NA, 23.…\n$ precip         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pressure       &lt;dbl&gt; 1011.9, 1011.4, 1012.1, 1…\n$ visib          &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 1…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-7",
    "href": "core/slides/slides-dplyr.html#section-7",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "The schema of the result is the union of the schemas of the operands\nA tuple from flights matches a tuple from weather if the tuple have the same values in the common columns:\n\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\"      \"temp\"          \n[21] \"dewp\"           \"humid\"          \"wind_dir\"       \"wind_speed\"    \n[25] \"wind_gust\"      \"precip\"         \"pressure\"       \"visib\""
  },
  {
    "objectID": "core/slides/slides-dplyr.html#which-columns-are-used-when-joining-tables-r-and-s",
    "href": "core/slides/slides-dplyr.html#which-columns-are-used-when-joining-tables-r-and-s",
    "title": "Tables manipulation with dplyr",
    "section": "Which columns are used when joining tables \\(R\\) and \\(S\\)?",
    "text": "Which columns are used when joining tables \\(R\\) and \\(S\\)?\n\ndefault behavior of inner_join: all columns shared by \\(R\\) and \\(S\\). Common columns have the same name in both schema. They are expected to have the same class\nmanual definition: in many settings, we want to overrule the default behavior. We specify manually which column from \\(R\\) should match which column from \\(S\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#natural-join-of-flights-and-weather",
    "href": "core/slides/slides-dplyr.html#natural-join-of-flights-and-weather",
    "title": "Tables manipulation with dplyr",
    "section": "Natural join of flights and weather:",
    "text": "Natural join of flights and weather:\n\ncommon_names &lt;- base::intersect(names(weather),\n                                names(flights))\n\nsetequal(\n  inner_join(flights, weather),\n  inner_join(flights,\n             weather,\n             by=common_names)\n)\n\n[1] TRUE"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#are-you-surprised-by-the-next-chunk",
    "href": "core/slides/slides-dplyr.html#are-you-surprised-by-the-next-chunk",
    "title": "Tables manipulation with dplyr",
    "section": "\n Are you surprised by the next chunk?",
    "text": "Are you surprised by the next chunk?\n\ndtu  &lt;- inner_join(flights,\n           weather,\n           by=c(\"year\", \"month\", \"day\", \"origin\", \"hour\"))\n\ndtv &lt;- inner_join(flights,\n           weather,\n           by=c(\"origin\", \"time_hour\"))\n\n# setequal(dtu, dtv)\n\nRecall that columns year, month day hour can be computed from time_hour\n\nflights |&gt;\n  filter(year!=year(time_hour) |\n         month!=month(time_hour) |\n         day!=day(time_hour) |\n         hour!=hour(time_hour)) |&gt;\n  nrow()\n\n[1] 0"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#the-two-results-do-not-have-the-same-schema",
    "href": "core/slides/slides-dplyr.html#the-two-results-do-not-have-the-same-schema",
    "title": "Tables manipulation with dplyr",
    "section": "The two results do not have the same schema!",
    "text": "The two results do not have the same schema!\n\nsetdiff(colnames(dtv), colnames(dtu))\n\n[1] \"year.x\"    \"month.x\"   \"day.x\"     \"hour.x\"    \"time_hour\" \"year.y\"   \n[7] \"month.y\"   \"day.y\"     \"hour.y\"   \n\nsetdiff(colnames(dtu), colnames(dtv))\n\n[1] \"year\"        \"month\"       \"day\"         \"hour\"        \"time_hour.x\"\n[6] \"time_hour.y\""
  },
  {
    "objectID": "core/slides/slides-dplyr.html#fixing",
    "href": "core/slides/slides-dplyr.html#fixing",
    "title": "Tables manipulation with dplyr",
    "section": "Fixing",
    "text": "Fixing\n\ndtu  &lt;- inner_join(flights,\n           weather,\n           by=c(\"year\", \"month\", \"day\", \"origin\", \"hour\"),\n           suffix= c(\"\", \".y\")) |&gt;  #&lt;&lt;\n           select(-ends_with(\".y\"))  #&lt;&lt;\n\ndtv &lt;- inner_join(flights,\n           weather,\n           by=c(\"origin\", \"time_hour\"),\n           suffix= c(\"\", \".y\")) |&gt;  #&lt;&lt;\n           select(-ends_with(\".y\"))  #&lt;&lt;\n\nsetequal(dtu, dtv)\n\n[1] TRUE"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#about-inner_join",
    "href": "core/slides/slides-dplyr.html#about-inner_join",
    "title": "Tables manipulation with dplyr",
    "section": "About inner_join\n",
    "text": "About inner_join\n\n\n\ninner_join(\n  x, y,\n  by = NULL,      #&lt;&lt;\n  copy = FALSE,\n  suffix = c(\".x\", \".y\"), #&lt;&lt;\n  ...,\n  keep = FALSE,  #&lt;&lt;\n  na_matches = \"na\")  #&lt;&lt;\n\n\n\nby:\n\n\nby=c(\"A1\", \"A3\", \"A7\") row r from R and s from S match if r.A1 == s.A1, r.A3 == s.A3, r.A7 == s.A7\n\n\nby=c(\"A1\"=\"B\", \"A3\"=\"C\", \"A7\"=\"D\") row r from R and s from S match if r.A1 == s.B, r.A3 == s.C, r.A7 == s.D\n\n\n\nsuffix: If there are non-joined duplicate variables in x and y, these suffixes will be added to the output to disambiguate them.\nkeep: Should the join keys from both x and y be preserved in the output?\nna_matches: Should NA and NaN values match one another?\n\n\n\n\nFrom online documentation"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#join-flavors",
    "href": "core/slides/slides-dplyr.html#join-flavors",
    "title": "Tables manipulation with dplyr",
    "section": "Join flavors",
    "text": "Join flavors\nDifferent flavors of join can be used to join one table to columns from another, matching values with the rows that they correspond to\nEach join retains a different combination of values from the tables\n\n\nleft_join(x, y, by = NULL, suffix = c(\".x\", \".y\"), ...) Join matching values from y to x. Retain all rows of x padding missing values from y by NA\nsemi_join …\nanti_join …"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-examples-inner_join",
    "href": "core/slides/slides-dplyr.html#toy-examples-inner_join",
    "title": "Tables manipulation with dplyr",
    "section": "Toy examples : inner_join\n",
    "text": "Toy examples : inner_join\n\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\ninner_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD.x\nA2\nA3\nD.y\n\n\n\n4\ne\n2021-10-22\nc\ne\n2021-10-28\nq\n\n\n6\nn\n2021-10-23\ni\na\n2021-11-04\no\n\n\n10\nr\n2021-10-25\ne\nd\n2021-11-18\nd"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-examples-left_join",
    "href": "core/slides/slides-dplyr.html#toy-examples-left_join",
    "title": "Tables manipulation with dplyr",
    "section": "Toy examples : left_join\n",
    "text": "Toy examples : left_join\n\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\nleft_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD.x\nA2\nA3\nD.y\n\n\n\n3\ny\n2021-10-21\no\nNA\nNA\nNA\n\n\n4\ne\n2021-10-22\nc\ne\n2021-10-28\nq\n\n\n6\nn\n2021-10-23\ni\na\n2021-11-04\no\n\n\n9\nt\n2021-10-24\nd\nNA\nNA\nNA\n\n\n10\nr\n2021-10-25\ne\nd\n2021-11-18\nd"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-examples-semi_join-anti_join",
    "href": "core/slides/slides-dplyr.html#toy-examples-semi_join-anti_join",
    "title": "Tables manipulation with dplyr",
    "section": "Toy examples : semi_join anti_join\n",
    "text": "Toy examples : semi_join anti_join\n\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\nsemi_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD\n\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\nanti_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n9\nt\n2021-10-24\nd"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#conditional-theta--join",
    "href": "core/slides/slides-dplyr.html#conditional-theta--join",
    "title": "Tables manipulation with dplyr",
    "section": "Conditional/ \\(\\theta\\) -join",
    "text": "Conditional/ \\(\\theta\\) -join\nIn relational databases, joins are not restricted to natural joins\n\n\\[U \\leftarrow R \\bowtie_{\\theta} S\\]\nreads as\n\\[\\begin{array}{rl} T & \\leftarrow R \\times S\\\\ U & \\leftarrow \\sigma(T, \\theta)\\end{array}\\]\nwhere\n\n\\(R \\times S\\) is the cartesian product of \\(R\\) and \\(S\\)\n\\(\\theta\\) is a boolean expression that can be evaluated on any tuple of \\(R \\times S\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins",
    "href": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins",
    "title": "Tables manipulation with dplyr",
    "section": "Do we need conditional/ \\(\\theta\\) -joins?",
    "text": "Do we need conditional/ \\(\\theta\\) -joins?\n\n\n\n\n\n\nNote\n\n\n: We can implement \\(\\theta\\)/conditional-joins by pipelining a cross product and a filtering\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n: Cross products are costly:\n\n\\(\\#\\text{rows}(R \\times S) = \\#\\text{rows}(R) \\times \\#\\text{rows}(S)\\)\n\\(\\#\\text{cols}(R \\times S) = \\#\\text{cols}(R) + \\#\\text{cols}(S)\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins-1",
    "href": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins-1",
    "title": "Tables manipulation with dplyr",
    "section": "Do we need conditional/ \\(\\theta\\) -joins?",
    "text": "Do we need conditional/ \\(\\theta\\) -joins?\n\n\n\n\n\n\nNote\n\n\n: RDBMS use query planning and optimization, indexing to circumvent the cross product bottleneck (when possible)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n: if we need to perform a \\(\\theta\\)-join\n\noutsource it to a RDBMS, or\ndesign an ad hoc pipeline\n\n\n\n\n\n\n\nAbout conditional join"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#a-conditional-join-between-flights-and-weather",
    "href": "core/slides/slides-dplyr.html#a-conditional-join-between-flights-and-weather",
    "title": "Tables manipulation with dplyr",
    "section": "A conditional join between flights and weather\n",
    "text": "A conditional join between flights and weather\n\n\nThe natural join between flights and weather we implemented can be regarded as an ad hoc conditional join between normalized versions of weather and flights \nTable flights and weather are redundant: year, month, day, hour can be computed from time_hour\nAssume flights and weather are trimmed so as to become irredundant\nThe conditional join is then based on truncations of variables time_hour\n\nSELECT \n  *\nFROM \n  flights AS f, weather AS w\nWHERE \n  date_trunc('hour', f.time_hour) = date_trunc('hour', w.time_hour)\n\nAdding redundant columns to flights and weather allows us to transform a tricky conditional join into a simple natural join \n\n\n\n\nPostgreSQL documentation"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-8",
    "href": "core/slides/slides-dplyr.html#section-8",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Creation of new columns may happen\n\non the fly\nwhen altering (enriching) the schema of a table\n\nIn databases, creation of new columns may be the result of a query or be the result of altering a table schema with ALTER TABLE ADD COLUMN ...\nIn tidyverse() we use verbs mutate or add_column to add columns to the input table"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#mutate",
    "href": "core/slides/slides-dplyr.html#mutate",
    "title": "Tables manipulation with dplyr",
    "section": "mutate",
    "text": "mutate\n\n\nmutate(   #&lt;&lt;\n  .data,\n  new_col= expression, #&lt;&lt;\n  ...,   #&lt;&lt;\n  .keep = c(\"all\", \"used\", \"unused\", \"none\"),\n  .before = NULL,\n  .after = NULL\n)\n\n.data: the input data frame\nnew_col= expression:\n\nnew_col is the name of a new column\nexpression is evaluated on each row of .data or it is a vector of length 1\nall is the default behavior, retains all columns from .data"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#creating-a-categorical-column-to-spot-large-delays",
    "href": "core/slides/slides-dplyr.html#creating-a-categorical-column-to-spot-large-delays",
    "title": "Tables manipulation with dplyr",
    "section": "Creating a categorical column to spot large delays",
    "text": "Creating a categorical column to spot large delays\n\n\nbreaks_delay &lt;- with(flights,\n  c(min(arr_delay, na.rm=TRUE),\n    0, 30,\n    max(arr_delay, na.rm=TRUE))\n)\n\nlevel_delay &lt;- c(\"None\",\n                 \"Moderate\",\n                 \"Large\")\n\nflights |&gt;\n  mutate(large_delay = cut(\n    arr_delay,  #&lt;&lt;\n    breaks=breaks_delay, #&lt;&lt;\n    labels=level_delay,  #&lt;&lt;\n    ordered_result=TRUE)) |&gt;   #&lt;&lt;\n  select(large_delay, arr_delay) |&gt;\n  sample_n(5)\n\n\n# A tibble: 5 × 2\n  large_delay arr_delay\n  &lt;ord&gt;           &lt;dbl&gt;\n1 Large             219\n2 Moderate           18\n3 None              -19\n4 None              -16\n5 None               -1"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-9",
    "href": "core/slides/slides-dplyr.html#section-9",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "flights |&gt;\n  mutate(foo = if_else(arr_time &gt; sched_arr_time,        #&lt;&lt;\n                              arr_time - sched_arr_time,\n                              0L,\n                              missing = NA_integer_)) |&gt;\n  group_by( (foo &gt;0) & abs(foo - arr_delay)  &gt; 100) |&gt;\n  summarise(N=n())\n\n\n# A tibble: 3 × 2\n  `(foo &gt; 0) & abs(foo - arr_delay) &gt; 100`      N\n  &lt;lgl&gt;                                     &lt;int&gt;\n1 FALSE                                    322281\n2 TRUE                                       5157\n3 NA                                         9338"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#changing-the-class-of-a-column",
    "href": "core/slides/slides-dplyr.html#changing-the-class-of-a-column",
    "title": "Tables manipulation with dplyr",
    "section": "Changing the class of a column",
    "text": "Changing the class of a column\n\n\nflights |&gt;\n  mutate(large_delay = cut(arr_delay,  #&lt;&lt;\n    breaks=breaks_delay,\n    labels=level_delay,\n    ordered_result=TRUE),\n    origin = as.factor(origin), #&lt;&lt;\n    dest = as.factor(dest)    #&lt;&lt;\n  ) |&gt;\n  select(\n    large_delay,\n    arr_delay,\n    origin,\n    dest) |&gt;\n  sample_n(5)\n\n\n# A tibble: 5 × 4\n  large_delay arr_delay origin dest \n  &lt;ord&gt;           &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;\n1 None              -44 LGA    CVG  \n2 None              -15 EWR    DAY  \n3 Large             136 EWR    DEN  \n4 None               -9 EWR    TPA  \n5 Moderate           14 LGA    TPA"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tidying-tables-is-part-of-data-cleaning",
    "href": "core/slides/slides-dplyr.html#tidying-tables-is-part-of-data-cleaning",
    "title": "Tables manipulation with dplyr",
    "section": "Tidying tables is part of data cleaning",
    "text": "Tidying tables is part of data cleaning\n\nA (tidy) dataset is a collection of values, usually either numbers (if quantitative) or strings (if qualitative)\n\n\nValues are organised in two ways\n\n\nEvery value belongs to a variable and an observation\n\n\nA variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units\n\n\nAn observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes\n\n\nThe principles of tidy data are tied to those of relational databases and Codd’s relational algebra\n\n\n\n The tidy data paper"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#codds-principles",
    "href": "core/slides/slides-dplyr.html#codds-principles",
    "title": "Tables manipulation with dplyr",
    "section": "Codd’s principles",
    "text": "Codd’s principles\n\nInformation is represented logically in tables\n\nData must be logically accessible by table, primary key, and column.\n\nNull values must be uniformly treated as “missing information,” not as empty strings, blanks, or zeros.\nMetadata (data about the database) must be stored in the database just as regular data is\nA single language must be able to define data, views, integrity constraints, authorization, transactions, and data manipulation\n\nViews must show the updates of their base tables and vice versa\nA single operation must be available to do each of the following operations: retrieve data, insert data, update data, or delete data\nBatch and end-user operations are logically separate from physical storage and access methods\nBatch and end-user operations can change the database schema without having to recreate it or the applications built upon it\n\nIntegrity constraints must be available and stored in the metadata, not in an application program\nThe data manipulation language of the relational system should not care where or how the physical data is distributed and should not require alteration if the physical data is centralized or distributed\nAny row processing done in the system must obey the same integrity rules and constraints that set-processing operations do"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-10",
    "href": "core/slides/slides-dplyr.html#section-10",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "dplyr functions expect and return tidy tables\nIn a tidy table\n\nEach variable is a column\nEach observation is a row\nEvery cell is a single value\n\n\n\n The tidy data paper]"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-11",
    "href": "core/slides/slides-dplyr.html#section-11",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "In order to tell whether a table is tidy, we need to know what is the population under investigation, what are the observations/individuals, which measures are performed on each individual, …"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#untidy-data",
    "href": "core/slides/slides-dplyr.html#untidy-data",
    "title": "Tables manipulation with dplyr",
    "section": "Untidy data",
    "text": "Untidy data\n\nColumn headers are values, not variable names.\n\n\nMultiple variables are stored in one column.\n\n\nVariables are stored in both rows and columns.\n\n\nMultiple types of observational units are stored in the same table.\n\n\nA single observational unit is stored in multiple tables.\n\n\n…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#functions-from-tidyr...",
    "href": "core/slides/slides-dplyr.html#functions-from-tidyr...",
    "title": "Tables manipulation with dplyr",
    "section": "Functions from tidyr::...\n",
    "text": "Functions from tidyr::...\n\n\npivot_wider and pivot_longer\nseparate and unite\nHandling missing values with complete, fill, …\n…\n\ntidyr website"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#pivot-longer",
    "href": "core/slides/slides-dplyr.html#pivot-longer",
    "title": "Tables manipulation with dplyr",
    "section": "Pivot longer",
    "text": "Pivot longer\n\n\npivot_longer() is commonly needed to tidy wild-caught datasets as they often optimise for ease of data entry or ease of comparison rather than ease of analysis.\n\n\n\nmessy &lt;- tibble::tribble(\n  ~row, ~a, ~b, ~c,\n  \"A\", 1, 4, 7,\n  \"B\", 2, 5, 8,\n  \"C\", 3, 6, 9,\n)\nmessy |&gt; kable()\n\n\n\n\nrow\na\nb\nc\n\n\n\nA\n1\n4\n7\n\n\nB\n2\n5\n8\n\n\nC\n3\n6\n9\n\n\n\n\n\n\n\nmessy_long &lt;- messy |&gt; \n  pivot_longer(\n    cols=c(-row),  #&lt;&lt;\n    names_to = \"name\",\n    values_to = \"value\")\n  \nmessy_long  |&gt; \n  kable()\n\n\n\n\nrow\nname\nvalue\n\n\n\nA\na\n1\n\n\nA\nb\n4\n\n\nA\nc\n7\n\n\nB\na\n2\n\n\nB\nb\n5\n\n\nB\nc\n8\n\n\nC\na\n3\n\n\nC\nb\n6\n\n\nC\nc\n9\n\n\n\n\n\n\n\n\n\npivot_longer() makes datasets longer by increasing the number of rows and decreasing the number of columns. I don’t believe it makes sense to describe a dataset as being in “long form”. Length is a relative term, and you can only say (e.g.) that dataset A is longer than dataset B."
  },
  {
    "objectID": "core/slides/slides-dplyr.html#pivot-wider",
    "href": "core/slides/slides-dplyr.html#pivot-wider",
    "title": "Tables manipulation with dplyr",
    "section": "Pivot wider",
    "text": "Pivot wider\n\n\npivot_wider(  #&lt;&lt;\n  data,\n  id_cols = NULL, #&lt;&lt;\n  names_from = name, #&lt;&lt;\n  names_prefix = \"\",\n  values_from = value, #&lt;&lt;\n  ...\n)\n some optional arguments are missing\n\nWhen reporting, we often use pivot_wider (explicitely or implicitely) to make results more readable, possibly to conform to a tradition\n\nLife tables in demography and actuarial science\nLongitudinal data\nSee slide How many flights per day of week per departure airport?"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#pivot_wider-in-action",
    "href": "core/slides/slides-dplyr.html#pivot_wider-in-action",
    "title": "Tables manipulation with dplyr",
    "section": "\npivot_wider() in action",
    "text": "pivot_wider() in action\n\n\nmessy_long |&gt;\n  pivot_wider( \n  id_cols = c(\"row\"), #&lt;&lt;\n  names_from = name, #&lt;&lt;\n  names_prefix = \"\",\n  values_from = value\n)\n\n\n# A tibble: 3 × 4\n  row       a     b     c\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         1     4     7\n2 B         2     5     8\n3 C         3     6     9"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#how-many-flights-per-carrier",
    "href": "core/slides/slides-dplyr.html#how-many-flights-per-carrier",
    "title": "Tables manipulation with dplyr",
    "section": "How many flights per carrier?",
    "text": "How many flights per carrier?\n\n\nflights |&gt;\n  group_by(carrier) |&gt;  #&lt;&lt;\n  summarise(count=n()) |&gt;  #&lt;&lt;\n  arrange(desc(count))\n\n\n# A tibble: 16 × 2\n   carrier count\n   &lt;chr&gt;   &lt;int&gt;\n 1 UA      58665\n 2 B6      54635\n 3 EV      54173\n 4 DL      48110\n 5 AA      32729\n 6 MQ      26397\n 7 US      20536\n 8 9E      18460\n 9 WN      12275\n10 VX       5162\n11 FL       3260\n12 AS        714\n13 F9        685\n14 YV        601\n15 HA        342\n16 OO         32\n\n\nSELECT \n  carrier, COUNT(*) AS n\nFROM \n  flights\nGROUP BY \n  carrier\nORDER BY \n  n DESCENDING"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#how-many-flights-per-day-of-week-per-departure-airport",
    "href": "core/slides/slides-dplyr.html#how-many-flights-per-day-of-week-per-departure-airport",
    "title": "Tables manipulation with dplyr",
    "section": "How many flights per day of week per departure airport?",
    "text": "How many flights per day of week per departure airport?\n\n\nflights |&gt;\n  group_by(origin,  wday(time_hour, abbr=T, label=T)) |&gt;  #&lt;&lt;\n  summarise(count=n(), .groups=\"drop\") |&gt;       #&lt;&lt;\n  rename(day_of_week=`wday(time_hour, abbr = T, label = T)`) |&gt;\n  pivot_wider(  #&lt;&lt;\n    id_cols=\"origin\",   #&lt;&lt;\n    names_from=\"day_of_week\", #&lt;&lt;\n    values_from=\"count\") |&gt;  #&lt;&lt;\n  kable(caption=\"Departures per day\")\n\n\n\nDepartures per day\n\norigin\ndim.\nlun.\nmar.\nmer.\njeu.\nven.\nsam.\n\n\n\nEWR\n16425\n18329\n18243\n18180\n18169\n18142\n13347\n\n\nJFK\n15966\n16104\n16017\n15841\n16087\n16176\n15088\n\n\nLGA\n13966\n16257\n16162\n16039\n15963\n15990\n10285"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#window-queries-1",
    "href": "core/slides/slides-dplyr.html#window-queries-1",
    "title": "Tables manipulation with dplyr",
    "section": "Window queries",
    "text": "Window queries\nAssume we want to answer the question: for each day of week (Monday, Tuesday, …), what are the five carriers that experience the largest average delay?\n\n\nflights |&gt;\n  group_by(weekdays(time_hour), carrier) |&gt;\n  summarise(avg_dep_delay=mean(dep_delay, na.rm=T)) |&gt;\n  slice_max(n=2, order_by=avg_dep_delay)\n\n\n# A tibble: 14 × 3\n# Groups:   weekdays(time_hour) [7]\n   `weekdays(time_hour)` carrier avg_dep_delay\n   &lt;chr&gt;                 &lt;chr&gt;           &lt;dbl&gt;\n 1 dimanche              F9               23.7\n 2 dimanche              VX               17.4\n 3 jeudi                 YV               29.7\n 4 jeudi                 F9               26.5\n 5 lundi                 FL               24.8\n 6 lundi                 EV               23.4\n 7 mardi                 YV               19.1\n 8 mardi                 FL               17.7\n 9 mercredi              OO               52  \n10 mercredi              HA               24.5\n11 samedi                OO               41  \n12 samedi                F9               15.8\n13 vendredi              OO               29  \n14 vendredi              F9               25.6"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#the-sql-way",
    "href": "core/slides/slides-dplyr.html#the-sql-way",
    "title": "Tables manipulation with dplyr",
    "section": "The SQL way",
    "text": "The SQL way\nWITH R AS (\n  SELECT \n    EXTRACT(dow FROM time_hour) AS day_of_week,\n    carrier,\n    AVG(dep_delay) AS avg_dep_delay\n  FROM \n    flights\n  GROUP BY \n    EXTRACT(dow FROM time_hour), carrier\n), S AS (\n  SELECT \n    day_of_week,\n    carrier,\n    rank() OVER (PARTITION by day_of_week ORDER BY avg_dep_delay DESC) AS rnk\n  FROM \n    R\n)\n\nSELECT \n  day_of_week, \n  carrier, \n  rnk\nFROM \n  S\nWHERE \n  rnk &lt;= 10 ;"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#sliding-windows-and-package-slider",
    "href": "core/slides/slides-dplyr.html#sliding-windows-and-package-slider",
    "title": "Tables manipulation with dplyr",
    "section": "Sliding windows and package slider\n",
    "text": "Sliding windows and package slider\n\nTODO"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#and-other-pipes",
    "href": "core/slides/slides-dplyr.html#and-other-pipes",
    "title": "Tables manipulation with dplyr",
    "section": "\n|>, %>% and other pipes",
    "text": "|&gt;, %&gt;% and other pipes\n\nAll dplyr functions take a table as the first argument\nRather than forcing the user to either save intermediate objects or nest functions, dplyr provides the |&gt; operator from magrittr\nx |&gt; f(y) turns into f(x, y)\nThe result from one step is piped into the next step\nUse |&gt; to rewrite multiple operations that you can read left-to-right/top-to-bottom\n\n\ng(f(x, y), z)\n\nx |&gt;\n  f(y) |&gt;\n  g(z)\n\n\n\nFrom dplyr vignette"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#magrittr",
    "href": "core/slides/slides-dplyr.html#magrittr",
    "title": "Tables manipulation with dplyr",
    "section": "Magrittr %>%\n",
    "text": "Magrittr %&gt;%\n\n\n\n\n\n%&gt;% is not tied to dplyr\n\n\n%&gt;% can be used with packages from tidyverse\n\n\n%&gt;% can be used outside tidyverse that is with functions which take a table (or something else) as a second, third or keyword argument\n\n Use pronoun . to denote the LHS of the pipe expression\n\nSecond argument of g has the same type as the result of f\n\ng(z, f(x, y))\n\nx %&gt;%\n  f(y) %&gt;%\n  g(z, .)   #&lt;&lt;\n\nx %&gt;% f(y) is a shorthand for x %&gt;% f(., y)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#standard-pipe-version-4.",
    "href": "core/slides/slides-dplyr.html#standard-pipe-version-4.",
    "title": "Tables manipulation with dplyr",
    "section": "Standard pipe |> (version > 4.)",
    "text": "Standard pipe |&gt; (version &gt; 4.)\nAs of version 4.1 (2021), base  offers a pipe operator denoted by |&gt;\n\n\nx |&gt; f(y) turns into f(x, y)\n\ng(f(x, y), z)\n\nx |&gt;\n  f(y) |&gt;\n  g(z)\n\n\n the standard pipe |&gt; has no pronoun/placeholder to denote the LHS of the pipe expression\nThe roundabout consists in using another new construct \\(x)\ng(z, w)\n\nx |&gt;\n  (\\(x) g(z, w=x))()\n\n\"une\" |&gt;\n  (\\(x) str_c(\"ceci n'est pas\", x, sep=\" \"))() |&gt;\n  str_c(\"pipe\", sep=\" \") |&gt;\n  cat()\n\nceci n'est pas une pipe\n\n\n\n\n\nSee Blog on the new standard pipe]"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#other-pipes",
    "href": "core/slides/slides-dplyr.html#other-pipes",
    "title": "Tables manipulation with dplyr",
    "section": "Other pipes",
    "text": "Other pipes\nMagrittr offers several variants of |&gt;\n\nTee operator %T&gt;%\n\nAssignement pipe %&lt;&gt;%\n\nExposition operator %$%\n\n…\n\nSee pipes for beginners\n Base  has a pipe() function to manipulate connections (Files, URLs, …)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#references",
    "href": "core/slides/slides-dplyr.html#references",
    "title": "Tables manipulation with dplyr",
    "section": "\n References",
    "text": "References\n\n\nR for Data Science\n\nData transformation\n\n\nRstudio cheat sheets\n\ndplyr\ntidyr\ndatatable\nreadr"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-12",
    "href": "core/slides/slides-dplyr.html#section-12",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "The document Comparison with R in Pandas documentation is somewhat outdated, yet, it remains useful.\nIn a single package, through classes DataFrame and Series, Pandas covers things that are handled by a collection of packages in R :\n\nQuerying (dplyr, dtplyr, dbplyr)\nReshaping (tidyr)\nSummary statistics (skimr)\nBridging with graphical backends (ggplot2)\nTable display (gt)\nReaders and writers (readr, readxl, …)\n…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tidy-selection-and-tidy-evaluation",
    "href": "core/slides/slides-dplyr.html#tidy-selection-and-tidy-evaluation",
    "title": "Tables manipulation with dplyr",
    "section": "Tidy selection and tidy evaluation",
    "text": "Tidy selection and tidy evaluation\nThanks to tidy evaluation, R programmers can avoid quoting column names. Some programmers like it that way."
  },
  {
    "objectID": "core/slides/slides-dplyr.html#near-future",
    "href": "core/slides/slides-dplyr.html#near-future",
    "title": "Tables manipulation with dplyr",
    "section": "Near future",
    "text": "Near future\nPandas (up to 2.xx) has not been praised for its treatment of strings and missing data. This is changing with the Arrow revolution and Pandas 3.0"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section",
    "href": "core/slides/slides-hclust.html#section",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Hierarchical clustering […] is a method of cluster analysis which seeks to build a hierarchy of clusters\nfrom Wikipedia\n\n\nRecall that a clustering is a partition of some dataset\nA partition \\(D\\) of \\(\\mathcal{X}\\) is a refinement of another partition \\(D'\\) if every class in \\(D\\) is a subset of a class in \\(D'\\). Partitions \\(D\\) and \\(D'\\) are said to be nested\n\n\nA hierarchical clustering of \\(\\mathcal{X}\\) is a sequence of \\(|\\mathcal{X}|\\) nested partitions of \\(\\mathcal{X}\\), starting from the trivial partition into \\(|\\mathcal{X}|\\) singletons and ending into the trivial partition in \\(1\\) subset ( \\(\\mathcal{X}\\) itself)\nA hierarchical clustering consists of \\(|\\mathcal{X}|\\) nested flat clusterings\nWe will explore agglomerative or bottom-up methods to build hierarchical clusterings"
  },
  {
    "objectID": "core/slides/slides-hclust.html#hierchical-clustering-and-dendrogram",
    "href": "core/slides/slides-hclust.html#hierchical-clustering-and-dendrogram",
    "title": "Hierarchical Clustering",
    "section": "Hierchical clustering and dendrogram",
    "text": "Hierchical clustering and dendrogram\n\n\nThe result of hierarchical clustering is a tree where leafs are labelled by sample points and internal nodes correspond to merging operations\nThe tree conveys more information: if the tree is properly decorated, it is possible to reconstruct the different merging steps and to know which rule was applied when some merging operation was performed\nThe tree is called a dendrogram\n\n\n\n\n\n\n\n\n\nViolent Crime Rates by US State\n\nDescription\n\nThis data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas."
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-1",
    "href": "core/slides/slides-hclust.html#section-1",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Dendrogram and trees show up in several areas.\nClassification and Regression trees play an important role in Machine Learning.\nggdendro and dendextend may also be used to manipulate regression trees"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-2",
    "href": "core/slides/slides-hclust.html#section-2",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Cutting a dendrogram: getting a flat clustering\nBuilding a dendrogram: inside hclust\nDisplaying, reporting dendrograms"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram: Iris illustration",
    "text": "Cutting a dendrogram: Iris illustration\n\nThe famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.\n\n\nThe Iris flower data set is fun for learning supervised classification algorithms, and is known as a difficult case for unsupervised learning.\n\n\nThe Setosa species are distinctly different from Versicolor and Virginica (they have lower petal length and width). But Versicolor and Virginica cannot easily be separated based on measurements of their sepal and petal width/length."
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-3",
    "href": "core/slides/slides-hclust.html#section-3",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "&lt;ScaleContinuousPosition&gt;\n Range:  \n Limits:    0 --    1"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-4",
    "href": "core/slides/slides-hclust.html#section-4",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "as.matrix(dist(iris[,1:4])) returns the matrix of pairwise distances\nDefault distance is Euclidean distance\nWhat about using broom::augment?\nThere is no augment.hclust method: No augment method for objects of class hclust"
  },
  {
    "objectID": "core/slides/slides-hclust.html#hclust-pipeline",
    "href": "core/slides/slides-hclust.html#hclust-pipeline",
    "title": "Hierarchical Clustering",
    "section": "\nhclust pipeline",
    "text": "hclust pipeline\n\ndendro_iris &lt;- iris |&gt; \n  select(where(is.numeric)) |&gt;  \n  dist() |&gt; \n  hclust() |&gt; \n  dendro_data() \n\n\nmydendro &lt;- . %$% {  #&lt;&lt;\n  ggplot() + \n    geom_segment(\n      data = segments,   #&lt;&lt;\n      aes(\n        x = x, y = y, \n        xend = xend, \n        yend = yend)\n  ) +\n  geom_text(\n    data = labels, #&lt;&lt;\n    aes(\n      x = x, y = y, \n      label = label, hjust = 0),\n    size = 2\n  ) +\n  coord_flip() +\n  scale_y_reverse(expand = c(0.2, 0)) +\n  theme_dendro()  \n}"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-5",
    "href": "core/slides/slides-hclust.html#section-5",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "mydendro(dendro_iris)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#inside-ggdendro",
    "href": "core/slides/slides-hclust.html#inside-ggdendro",
    "title": "Hierarchical Clustering",
    "section": "Inside ggdendro",
    "text": "Inside ggdendro"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-8",
    "href": "core/slides/slides-hclust.html#section-8",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "What is an object of class dendro made of?\nIt a list of four elements:\n\nsegments\nlabels\nleaf_labels\nclass\n\nElement segments is a data frame with four columns. Each row represent a segment that is part of a graphical representation of the hierarchy. There are horizontal and vertical segments\nElement labels is used to label the tree leafs."
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram:  Iris illustration (continued)",
    "text": "Cutting a dendrogram:  Iris illustration (continued)\n\np &lt;- iris %&gt;%\n  ggplot() +\n  aes(x=Petal.Length, y=Petal.Width)\n\np +\n  geom_point(\n    aes(\n      shape=Species, \n      colour=Species)\n  ) +\n# labs(shape= \"Species\") +\n  ggtitle(\n    label= \"Iris data\",\n    subtitle = \"Species in Petal plane\"\n  )"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-10",
    "href": "core/slides/slides-hclust.html#section-10",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Does the flat clustering obtained by cutting the dendrogram at some height reflect the partition into species?"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-1",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-1",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram: Iris illustration (continued)",
    "text": "Cutting a dendrogram: Iris illustration (continued)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-2",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-2",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram: Iris illustration (continued)",
    "text": "Cutting a dendrogram: Iris illustration (continued)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-3",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-3",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram: Iris illustration (continued)",
    "text": "Cutting a dendrogram: Iris illustration (continued)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-4",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-iris-illustration-continued-4",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram: Iris illustration (continued)",
    "text": "Cutting a dendrogram: Iris illustration (continued)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cutting-a-dendrogram-better-iris-illustration-continued",
    "href": "core/slides/slides-hclust.html#cutting-a-dendrogram-better-iris-illustration-continued",
    "title": "Hierarchical Clustering",
    "section": "Cutting a dendrogram: better Iris illustration (continued)",
    "text": "Cutting a dendrogram: better Iris illustration (continued)\nThe dendextend package offers a set of functions for extending dendrogram objects in , letting you\n\nvisualize and\ncompare trees of hierarchical clusterings,\n\nFeatures:\n\nAdjust a tree’s graphical parameters - the color, size, type, etc, of its branches, nodes and labels\nVisually and statistically compare different dendrograms to one another"
  },
  {
    "objectID": "core/slides/slides-hclust.html#section-12",
    "href": "core/slides/slides-hclust.html#section-12",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "dendextend\nA dendro gallery\nclass of dend"
  },
  {
    "objectID": "core/slides/slides-hclust.html#about-class-hclust",
    "href": "core/slides/slides-hclust.html#about-class-hclust",
    "title": "Hierarchical Clustering",
    "section": "About class hclust\n",
    "text": "About class hclust\n\nResults from function hclust() are objects of class hclust :\niris_hclust is an object of class hclust\nFunction cutree() returns a flat clustering of the dataset\nWhat does height stand for?\nWhat does merge stand for?\nWhat does order stand for?\nHow different are the different method?"
  },
  {
    "objectID": "core/slides/slides-hclust.html#hierarchical-clustering-of-usarrests",
    "href": "core/slides/slides-hclust.html#hierarchical-clustering-of-usarrests",
    "title": "Hierarchical Clustering",
    "section": "Hierarchical clustering of USArrests\n",
    "text": "Hierarchical clustering of USArrests\n\n\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…"
  },
  {
    "objectID": "core/slides/slides-hclust.html#about-dendrograms-output-dendro_data",
    "href": "core/slides/slides-hclust.html#about-dendrograms-output-dendro_data",
    "title": "Hierarchical Clustering",
    "section": "About dendrograms (output dendro_data())",
    "text": "About dendrograms (output dendro_data())\nAn object of class dendro is a list of 4 objects:\n\nsegments\nlabels\nleaf_labels\nclass"
  },
  {
    "objectID": "core/slides/slides-hclust.html#questions",
    "href": "core/slides/slides-hclust.html#questions",
    "title": "Hierarchical Clustering",
    "section": "Questions",
    "text": "Questions\n\nHow to build the dendrogram?\nHow to choose the cut?"
  },
  {
    "objectID": "core/slides/slides-hclust.html#bird-eye-view-at-hierarchical-agglomerative-clustering-methods",
    "href": "core/slides/slides-hclust.html#bird-eye-view-at-hierarchical-agglomerative-clustering-methods",
    "title": "Hierarchical Clustering",
    "section": "Bird-eye view at hierarchical agglomerative clustering methods",
    "text": "Bird-eye view at hierarchical agglomerative clustering methods\nAll hierarchical agglomerative clustering methods (HACMs) can be described by the following general algorithm.\n\nAt each stage distances between clusters are recomputed by the Lance-Williams dissimilarity update formula according to the particular clustering method being used.\nIdentify the 2 closest points and combine them into a cluster (treating existing clusters as points too)\nIf more than one cluster remains, return to step 1."
  },
  {
    "objectID": "core/slides/slides-hclust.html#greed-is-good",
    "href": "core/slides/slides-hclust.html#greed-is-good",
    "title": "Hierarchical Clustering",
    "section": "Greed is good!",
    "text": "Greed is good!\n\nHierarchical agglomerative clustering methods are examples of greedy algorithms\n\nGreedy algorithms sometimes compute optimal solutions\n\nHuffmann coding (Information Theory)\nMinimum spanning tree (Graph algorithms)\n\n\n\nGreedy algorithms sometimes compute sub-optimal solutions\n\nSet cover (NP-hard problem)\n…\n\n\n\nEfficient greedy algorithms rely on ad hoc data structures\n\nPriority queues\nUnion-Find"
  },
  {
    "objectID": "core/slides/slides-hclust.html#algorithm-detailed",
    "href": "core/slides/slides-hclust.html#algorithm-detailed",
    "title": "Hierarchical Clustering",
    "section": "\n Algorithm (detailed)",
    "text": "Algorithm (detailed)\n\nStart with \\((\\mathcal{C}_{i}^{(0)})= (\\{ \\vec{X}_i \\})\\) the collection of all singletons.\n\nAt step \\(s\\), we have \\(n-s\\) clusters \\((\\mathcal{C}_{i}^{(s)})\\):\n\nFind the two most similar clusters according to a criterion \\(\\Delta\\): \\[(i,i') = \\operatorname{argmin}_{(j,j')} \\Delta(\\mathcal{C}_{j}^{(s)},\\mathcal{C}_{j'}^{(s)})\\]\nMerge \\(\\mathcal{C}_{i}^{(s)}\\) and \\(\\mathcal{C}_{i'}^{(s)}\\) into \\(\\mathcal{C}_{i}^{(s+1)}\\)\nKeep the \\(n-s-2\\) other clusters \\(\\mathcal{C}_{i''}^{(s+1)} = \\mathcal{C}_{i''}^{(s)}\\)\n\n\nRepeat until there is only one cluster left"
  },
  {
    "objectID": "core/slides/slides-hclust.html#analysis",
    "href": "core/slides/slides-hclust.html#analysis",
    "title": "Hierarchical Clustering",
    "section": "Analysis",
    "text": "Analysis\n\nComplexity: \\(O(n^3)\\) in general.\n\nCan be reduced to \\(O(n^2)\\) (sometimes to \\(O(n \\log n)\\))\n\nif the number of possible mergers for a given cluster is bounded.\nfor the most classical distances by maintaining a nearest neighbors list."
  },
  {
    "objectID": "core/slides/slides-hclust.html#merging-criterion-based-on-the-distance-between-points",
    "href": "core/slides/slides-hclust.html#merging-criterion-based-on-the-distance-between-points",
    "title": "Hierarchical Clustering",
    "section": "Merging criterion based on the distance between points",
    "text": "Merging criterion based on the distance between points\nMinimum linkage:\n\\[\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) =\\min_{\\vec{X}_i \\in \\mathcal{C}_i} \\min_{\\vec{X}_j \\in    \\mathcal{C}_j} d(\\vec{X}_i, \\vec{X}_j)\\]\nMaximum linkage:\n\\[\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) = \\max_{\\vec{X}_i \\in \\mathcal{C}_i} \\max_{\\vec{X}_j \\in    \\mathcal{C}_j} d(\\vec{X}_i, \\vec{X}_j)\\]\nAverage linkage:\n\\[\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) =\\frac{1}{|\\mathcal{C}_i||\\mathcal{C}_j|} \\sum_{\\vec{X}_i \\in    \\mathcal{C}_i}\\sum_{\\vec{X}_j \\in \\mathcal{C}_j} d(\\vec{X}_i, \\vec{X}_j)\\]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#wards-criterion-minimum-varianceinertia-criterion",
    "href": "core/slides/slides-hclust.html#wards-criterion-minimum-varianceinertia-criterion",
    "title": "Hierarchical Clustering",
    "section": "Ward’s criterion : minimum variance/inertia criterion",
    "text": "Ward’s criterion : minimum variance/inertia criterion\n\\(\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) = \\sum_{\\vec{X}_i \\in \\mathcal{C}_i} \\left( d^2(\\vec{X}_i, \\mu_{\\mathcal{C}_i \\cup \\mathcal{C}_j} ) - d^2(\\vec{X}_i, \\mu_{\\mathcal{C}_i}) \\right) +\\)\n\\(\\qquad\\qquad \\qquad \\sum_{\\vec{X}_j \\in \\mathcal{C}_j} \\left( d^2(\\vec{X}_j, \\mu_{\\mathcal{C}_i \\cup \\mathcal{C}_j} ) - d^2(\\vec{X}_j, \\mu_{\\mathcal{C}_j}) \\right)\\)\nIf \\(d\\) is the euclidean distance\n\\[\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) = \\frac{ |\\mathcal{C}_i||\\mathcal{C}_j|}{|\\mathcal{C}_i|+ |\\mathcal{C}_j|} d^2(\\mu_{\\mathcal{C}_i}, \\mu_{\\mathcal{C}_j})\\]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#lance-williams-update-formulae",
    "href": "core/slides/slides-hclust.html#lance-williams-update-formulae",
    "title": "Hierarchical Clustering",
    "section": "Lance-Williams update formulae",
    "text": "Lance-Williams update formulae\nSuppose that clusters \\(C_{i}\\) and \\(C_{j}\\) were next to be merged\nAt this point, all of the current pairwise cluster distances are known\nThe recursive update formula gives the updated cluster distances following the pending merge of clusters \\(C_{i}\\) and \\(C_{j}\\)\nLet\n\n\\(d_{ij}, d_{ik}\\), and \\(d_{jk}\\) be shortands for the pairwise distances between clusters \\(C_{i}, C_{j}\\) and \\(C_{k}\\)\n\\(d_{{(ij)k}}\\) be shortand for the distance between the new cluster \\(C_{i}\\cup C_{j}\\) and \\(C_{k}\\) ( \\(k\\not\\in \\{i,j\\}\\) )"
  },
  {
    "objectID": "core/slides/slides-hclust.html#lance-williams-update-formulae-continued",
    "href": "core/slides/slides-hclust.html#lance-williams-update-formulae-continued",
    "title": "Hierarchical Clustering",
    "section": "Lance-Williams update formulae (continued)",
    "text": "Lance-Williams update formulae (continued)\nAn algorithm belongs to the Lance-Williams family if the updated cluster distance \\(d_{{(ij)k}}\\) can be computed recursively by\n\\[d_{(ij)k} = \\alpha _{i}d_{ik}+ \\alpha _{j}d_{jk}+ \\beta d_{ij}+ \\gamma |d_{ik}-d_{jk}|\\]\nwhere \\(\\alpha_{i},\\alpha _{j},\\beta\\) , and \\(\\gamma\\) are parameters, which may depend on cluster sizes, that together with the cluster distance function \\(d_{ij}\\) determine the clustering algorithm.\nClustering algorithms such as\n\nsingle linkage,\ncomplete linkage, and\ngroup average\n\nmethod have a recursive formula of the above type"
  },
  {
    "objectID": "core/slides/slides-hclust.html#lance-williams-update-formula-for-wards-criterion",
    "href": "core/slides/slides-hclust.html#lance-williams-update-formula-for-wards-criterion",
    "title": "Hierarchical Clustering",
    "section": "Lance-Williams update formula for Ward’s criterion",
    "text": "Lance-Williams update formula for Ward’s criterion\n\\[\\begin{array}{rl}d\\left(C_i \\cup C_j, C_k\\right) & = \\frac{n_i+n_k}{n_i+n_j+n_k}d\\left(C_i, C_k\\right)  +\\frac{n_j+n_k}{n_i+n_j+n_k}d\\left(C_j, C_k\\right) \\\\ & \\phantom{==}- \\frac{n_k}{n_i+n_j+n_k} d\\left(C_i, C_j\\right)\\end{array}\\]\n\\[\\alpha_i = \\frac{n_i+n_k}{n_i+n_j+n_k} \\qquad \\alpha_j = \\frac{n_j+n_k}{n_i+n_j+n_k}\\qquad \\beta = \\frac{- n_k}{n_i+n_j+n_k}\\]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#an-unfair-quotation",
    "href": "core/slides/slides-hclust.html#an-unfair-quotation",
    "title": "Hierarchical Clustering",
    "section": "An unfair quotation",
    "text": "An unfair quotation\n\nWard’s minimum variance criterion minimizes the total within-cluster variance .fr[Wikipedia]\n\n\nIs that correct?\nIf corrected, what does it mean?\n\nIf we understand the statement as:\n\nfor any \\(k\\), the flat clustering obtained by cutting the dendrogram so as to obtain a \\(k\\)-clusters partition minimizes the total within-cluster variance/inertia amongst all \\(k\\)-clusterings\n\nthen, the statement is not proved. If it were proved, it would imply \\(\\mathsf{P}=\\mathsf{NP}\\)\nThe total within-cluster variance/inertia is the objective function in the \\(k\\)-means problem.\nThe statement is misleading"
  },
  {
    "objectID": "core/slides/slides-hclust.html#what-happens-in-wards-method",
    "href": "core/slides/slides-hclust.html#what-happens-in-wards-method",
    "title": "Hierarchical Clustering",
    "section": "What happens in Ward’s method?",
    "text": "What happens in Ward’s method?\n\nAt each step find the pair of clusters that leads to minimum increase in total within-cluster variance after merging .fr[Wikipedia]\n\n\nThis increase is a weighted squared distance between cluster centers .fr[Wikipedia]\n\n\nAt the initial step, all clusters are singletons (clusters containing a single point). To apply a recursive algorithm under this objective function, the initial distance between individual objects must be (proportional to) squared Euclidean distance."
  },
  {
    "objectID": "core/slides/slides-hclust.html#views-on-inertia",
    "href": "core/slides/slides-hclust.html#views-on-inertia",
    "title": "Hierarchical Clustering",
    "section": "Views on Inertia:",
    "text": "Views on Inertia:\n\\[I   = \\frac{1}{n} \\sum_{i=1}^n \\|\\vec{X}_i - \\vec{m} \\|^2\\]\nwhere \\(\\vec{m} = \\sum_{i=1}^n \\frac{1}{n}\\vec{X}_i\\)\n\\[I = \\frac{1}{2n^2} \\sum_{i,j} \\|\\vec{X}_i - \\vec{X}_j\\|^2\\]\nTwice the mean squared distance to the mean equals the mean squared distance between sample points\nRecall that for a real random variable \\(Z\\) with mean \\(\\mu\\)\n\\[\\operatorname{var}(Z) = \\mathbb{E}(Z -m)^2 = \\inf_a \\mathbb{E}(Z -a)^2\\]\nand\n\\[\\operatorname{var}(Z) = \\frac{1}{2} \\mathbb{E}(Z -Z')^2\\]\nwhere \\(Z'\\) is an independent copy of \\(Z\\)\nThe different formulae for inertia is just mirroring the different views at variance\nThe inertia is the trace of an empirical covariance matrix."
  },
  {
    "objectID": "core/slides/slides-hclust.html#decompositions-of-inertia-huyghens-formula",
    "href": "core/slides/slides-hclust.html#decompositions-of-inertia-huyghens-formula",
    "title": "Hierarchical Clustering",
    "section": "Decompositions of inertia (Huyghens formula)",
    "text": "Decompositions of inertia (Huyghens formula)\n\nSample \\(x_1,\\ldots, x_{n+m}\\) with mean \\(\\bar{X}_{n+m}\\) and variance \\(V\\)\nPartition \\(\\{1,\\ldots,n+m\\} = A \\cup B\\) with \\(|A|=n, |B|=m\\), \\(A \\cap B =\\emptyset\\)\nLet \\(\\bar{X}_n = \\frac{1}{n}\\sum_{i \\in A} X_i\\) and \\(\\bar{X}_m=\\frac{1}{m}\\sum_{i \\in B}X_i\\) \\[\\bar{X}_{n+m} =  \\frac{n}{n+m} \\bar{X}_{n}  +\\frac{m}{n+m} \\bar{X}_{m}\\]\nLet \\(V_A\\) be the variance of \\((x_i)_{i\\in A}\\), \\(V_B\\) be the variance of \\((x_i)_{i\\in B}\\)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#decompositions-of-inertia-huyghens-formula-1",
    "href": "core/slides/slides-hclust.html#decompositions-of-inertia-huyghens-formula-1",
    "title": "Hierarchical Clustering",
    "section": "Decompositions of inertia (Huyghens formula)",
    "text": "Decompositions of inertia (Huyghens formula)\n\nLet \\(V_{\\text{between}}\\) be the variance of a ghost sample with \\(n\\) copies of \\(\\bar{X}_n\\) and \\(m\\) copies of \\(\\bar{X}_m\\) \\[V_{\\text{between}} =  \\frac{n}{n+m} (\\bar{X}_n -\\bar{X}_{n+m})^2 + \\frac{m}{n+m} (\\bar{X}_m -\\bar{X}_{n+m})^2\\]\nLet \\(V_{\\text{within}}\\) be the weighted mean of variances within classes \\(A\\) and \\(B\\) \\[V_{\\text{within}}  = \\frac{n}{n+m}  V_A + \\frac{m}{n+m} V_B\\]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#decompositions-of-inertia",
    "href": "core/slides/slides-hclust.html#decompositions-of-inertia",
    "title": "Hierarchical Clustering",
    "section": "Decompositions of inertia",
    "text": "Decompositions of inertia\n\n\n\n\n\n\nProposition: Huyghens formula I\n\n\n\\[V = V_{\\text{within}} +  V_{\\text{between}}\\]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#huyghens-formula-can-be-extended-to-any-number-of-classes",
    "href": "core/slides/slides-hclust.html#huyghens-formula-can-be-extended-to-any-number-of-classes",
    "title": "Hierarchical Clustering",
    "section": "Huyghens formula can be extended to any number of classes",
    "text": "Huyghens formula can be extended to any number of classes\n\n\n\n\n\n\nProposition: Huyghens (II)\n\n\n\nSample \\(\\vec{x}_1, \\ldots,\\vec{x}_n\\) from \\(\\mathbb{R}^p\\) with mean \\(\\bar{X}_n\\), inertia \\(I\\).\nLet \\(A_1, A_2\\ldots, A_k\\) be a partition of \\(\\{1,\\ldots,n\\}\\).\nLet \\(I_\\ell\\) (resp. \\(\\bar{X}^\\ell\\)) be the inertia (resp. the mean) of sub-sample \\(\\vec{x}_i, i\\in A_\\ell\\)\nLet \\(I_{\\text{between}}\\) be the inertia of the ghost sample, formed by \\(|A_1|\\) copies of \\(\\bar{X}^1\\), \\(|A_2|\\) copies of \\(\\bar{X}^2\\), … \\(|A_k|\\) copies of \\(\\bar{X}^k\\)\nLet \\(I_{\\text{within}} =  \\sum_{\\ell=1}^k \\frac{|A_\\ell|}{n}  I_\\ell\\)\n\n\\[I =   I_{\\text{within}} +  I_{\\text{between}}\\]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cophenetic-disimilarity",
    "href": "core/slides/slides-hclust.html#cophenetic-disimilarity",
    "title": "Hierarchical Clustering",
    "section": "Cophenetic disimilarity",
    "text": "Cophenetic disimilarity\nGiven a dendrogram, the cophenetic disimilarity between two sample points \\(x, x'\\) is the intergroup disimilarity at which observations \\(x\\) and \\(x'\\) are first joined.\n\n\n\n\n\n\nProposition\n\n\nA cophenetic disimilarity has the ultrametric property\n\n\n\n\n\nAll triangles are isoceles and the unequal length should be no longer than the length of the two equal sides"
  },
  {
    "objectID": "core/slides/slides-hclust.html#cophenetic-correlation-coefficient",
    "href": "core/slides/slides-hclust.html#cophenetic-correlation-coefficient",
    "title": "Hierarchical Clustering",
    "section": "Cophenetic correlation coefficient",
    "text": "Cophenetic correlation coefficient\nThe cophenetic correlation coefficient measures how faithfully a dendrogram preserves the pairwise distances between the original unmodeled data points\n\n\nwikipedia]"
  },
  {
    "objectID": "core/slides/slides-hclust.html#computing-cophenetic-correlation-coefficient",
    "href": "core/slides/slides-hclust.html#computing-cophenetic-correlation-coefficient",
    "title": "Hierarchical Clustering",
    "section": "Computing cophenetic correlation coefficient",
    "text": "Computing cophenetic correlation coefficient\nIn  use the dendextend package\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsingle\ncomplete\naverage\nmcquitty\nward.D\ncentroid\nmedian\nward.D2\n\n\n0.86\n0.73\n0.88\n0.87\n0.86\n0.87\n0.86\n0.87"
  },
  {
    "objectID": "core/slides/slides-hclust.html#how-to-apply-general-algorithm",
    "href": "core/slides/slides-hclust.html#how-to-apply-general-algorithm",
    "title": "Hierarchical Clustering",
    "section": "How to apply general algorithm?",
    "text": "How to apply general algorithm?\n\nLance-Williams dissimilarity update formula calculates dissimilarities between a new cluster and existing points, based on the dissimilarities prior to forming the new cluster\nThis formula has 3 parameters\nEach HACM is characterized by its own set of Lance-Williams parameters"
  },
  {
    "objectID": "core/slides/slides-hclust.html#implementations-of-the-general-algorithm",
    "href": "core/slides/slides-hclust.html#implementations-of-the-general-algorithm",
    "title": "Hierarchical Clustering",
    "section": "Implementations of the general algorithm",
    "text": "Implementations of the general algorithm\nStored matrix approach\nUse matrix, and then apply Lance-Williams to recalculate dissimilarities between cluster centers. Storage \\(O(N^2)\\) and time at least \\(O(N^2)\\), but is \\(\\Theta(N^3)\\) if matrix is scanned linearly\nStored data approach\n\\(O(N)\\) space for data but recompute pairwise dissimilarities, needs \\(\\Theta(N^3)\\) time\nSorted matrix approach\n\\(O(N^2)\\) to calculate dissimilarity matrix, \\(O(N^2 \\log N)\\) to sort it, \\(O(N^2)\\) to construct hierarchy, but one need not store the data set, and the matrix can be processed linearly, which reduces disk accesses"
  },
  {
    "objectID": "core/slides/slides-hclust.html#agglomerative-clustering-heuristic",
    "href": "core/slides/slides-hclust.html#agglomerative-clustering-heuristic",
    "title": "Hierarchical Clustering",
    "section": "Agglomerative Clustering Heuristic",
    "text": "Agglomerative Clustering Heuristic\n\nStart with very small clusters (a sample point by cluster?)\nMerge iteratively the most similar clusters according to some greedy criterion \\(\\Delta\\).\nGenerates a hierarchy of clusterings instead of a single one.\nNeed to select the number of cluster afterwards.\nSeveral choice for the merging criterion\n\nExamples:\n\nMinimum Linkage: merge the closest cluster in term of the usual distance\nWard’s criterion: merge the two clusters yielding the less inner inertia loss (minimum variance criterion)"
  },
  {
    "objectID": "core/slides/slides-hclust.html#packages",
    "href": "core/slides/slides-hclust.html#packages",
    "title": "Hierarchical Clustering",
    "section": "Packages",
    "text": "Packages\n\n\n\nggdendro\ndendextend\ndendroextras\n\n\n\n\nscipy\nscikit-learn"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html",
    "href": "core/labs-solutions/lab-bivariate.html",
    "title": "Bivariate analysis",
    "section": "",
    "text": "M1 MIDS/MFA/LOGOS\n\n\nUniversité Paris Cité\n\n\nAnnée 2025\n\n\nCourse Homepage\n\n\nMoodle"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#setup",
    "href": "core/labs-solutions/lab-bivariate.html#setup",
    "title": "Bivariate analysis",
    "section": "Setup",
    "text": "Setup\n\nCodestopifnot(\n  require(glue),\n  require(magrittr),\n  require(lobstr),\n  require(arrow),\n  require(ggforce),\n  require(vcd),\n  require(ggmosaic),\n  require(httr),\n  require(patchwork),\n  require(corrr),\n  require(gapminder),\n  require(slider),\n  require(tidyverse), \n  require(gt)\n) \n\n\nBivariate techniques depend on the types of columns we are facing.\nFor numerical/numerical samples\n\nScatter plots\nSmoothed lineplots (for example linear regression)\n2-dimensional density plots\n\nFor categorical/categorical samples : mosaicplots and variants\nFor numerical/categorical samples\n\nBoxplots per group\nHistograms per group\nDensity plots per group"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#chi-square-independenceassociation-test",
    "href": "core/labs-solutions/lab-bivariate.html#chi-square-independenceassociation-test",
    "title": "Bivariate analysis",
    "section": "Chi-square independence/association test",
    "text": "Chi-square independence/association test\nhttps://statsfonda.github.io/site/content/ch4_2.html#test-dindépendance\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nCompute the chi-square association statistic between CATEGORIE and SEXE.\nDisplay the output of chisq.test() as a table, using broom::tidy()\n\n\n\n\n\n\n\n\n\n\n\nCodetest_1 &lt;- df |&gt;\n  select(CATEGORIE, SEXE) |&gt;\n  table() |&gt;\n  chisq.test()\n\n# test_1 \n\ntest_1 |&gt;\n  broom::tidy() |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(decimals=2)\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n140.67\n0.00\n5.00\nPearson's Chi-squared test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCompute the Chi-square statistics from the contingeny table\n\n\n\n\n\n\n\n\n\nCoderowcounts &lt;- apply(tb, MARGIN = 1, FUN = sum)\ncolcounts &lt;- apply(tb, MARGIN = 2, FUN = sum)\n\nexpected &lt;- (rowcounts %*% t(colcounts))/sum(colcounts)\n\n# norm((tb - expected) / sqrt(expected), type = \"F\")^2\n\ngt_e &lt;- expected |&gt;\n  tibble::as_tibble() |&gt; \n  mutate(CATEGORIE= levels(df$CATEGORIE)) |&gt;\n  relocate(CATEGORIE) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(decimals=2) |&gt;\n  gt::tab_caption(\"Expected counts CATEGORIE/SEXE\")\n\n\n\n\n\n\n\n\n\nContingency table in wide form\\n (2-way table)\n\nCATEGORIE\nF\nM\n\n\n\nBusiness, Management and Finance\n23\n23\n\n\nLiberal profession\n82\n51\n\n\nServices\n75\n50\n\n\nSelling\n30\n18\n\n\nAdministration\n72\n22\n\n\nPrimary-Secondary\n15\n138\n\n\n\n\n\n\n\n\n\n\n\n\nExpected counts CATEGORIE/SEXE\n\nCATEGORIE\nF\nM\n\n\n\nBusiness, Management and Finance\n22.81\n23.19\n\n\nLiberal profession\n65.94\n67.06\n\n\nServices\n61.98\n63.02\n\n\nSelling\n23.80\n24.20\n\n\nAdministration\n46.61\n47.39\n\n\nPrimary-Secondary\n75.86\n77.14\n\n\n\n\n\n\n\n\n\n\nCodenorm((tb - expected) / sqrt(expected), type = \"F\")^2\n\n[1] 140.6717"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#grouped-boxplots",
    "href": "core/labs-solutions/lab-bivariate.html#grouped-boxplots",
    "title": "Bivariate analysis",
    "section": "Grouped boxplots",
    "text": "Grouped boxplots\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPlot boxplots of AGE according to NIV_ETUDES\n\n\n\n\n\n\n\n\n\nCodedf |&gt;\n  ggplot() +\n  aes(x=NIV_ETUDES, y=AGE) +\n  geom_boxplot() +\n  rot_x_text\n\n\n\n\n\n\n\n\nCodedf |&gt;\n  ggplot() +\n  aes(x=fct_infreq(NIV_ETUDES), y=AGE) +\n  geom_boxplot(varwidth = T) +\n  rot_x_text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nDraw density plots of AGE, facet by NIV_ETUDES and SEXE\n\n\n\n\n\n\n\n\n\nCodep &lt;- df |&gt; \n  ggplot() +\n  aes(x=AGE) +\n  stat_density(fill=\"white\", color=\"black\") +\n  facet_grid(rows=vars(NIV_ETUDES), \n             cols=vars(SEXE))\n\np\n\nWarning: Groups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCollapse rare levels of NIV_ETUDES and replay.\n\n\n\n\n\n\n\n\n\nCodep %+% (\n  df |&gt; \n    mutate(NIV_ETUDES = fct_lump_min(NIV_ETUDES, min=30)) \n)"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#scatterplots",
    "href": "core/labs-solutions/lab-bivariate.html#scatterplots",
    "title": "Bivariate analysis",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\n\n\n\n\nNoteQuestion\n\n\n\nMake a scatterplot of SAL_HORwith respect to AGE\n\n\n\n\n\n\n\n\n\nCodedf |&gt; \n  ggplot() +\n  aes(x=AGE, y=SAL_HOR, color=SEXE) +\n  geom_point(alpha=.7)"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#linear-correlation-coefficient",
    "href": "core/labs-solutions/lab-bivariate.html#linear-correlation-coefficient",
    "title": "Bivariate analysis",
    "section": "Linear correlation coefficient",
    "text": "Linear correlation coefficient\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCompute the Pearson, Spearman and Kendall correlation coefficients between AGE and SAL_HOR using function cor() from base R\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodedf |&gt;\n  summarise(\n    pearson=cor(AGE, SAL_HOR), \n    spearman=cor(AGE, SAL_HOR, method = \"spearman\"),\n    kendall=cor(AGE, SAL_HOR, method=\"kendall\")) |&gt; \n  gt::gt() |&gt;\n  gt::fmt_number(decimals=2) |&gt;\n  gt::tab_caption(\n    \"Correlation coefficients between SAL_HOR and AGE\\nRecensement dataset\"\n    )\n\n\n\n\nCorrelation coefficients between SAL_HOR and AGE Recensement dataset\n\npearson\nspearman\nkendall\n\n\n0.25\n0.32\n0.22"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#rank-based-methods",
    "href": "core/labs-solutions/lab-bivariate.html#rank-based-methods",
    "title": "Bivariate analysis",
    "section": "Rank based methods",
    "text": "Rank based methods\nSpearman’s rho (𝜌) and Kendall’s tau (𝜏) are both non-parametric correlation coefficients used to measure the strength and direction of a monotonic relationship between two variables.\n\nSpearman’s rho (𝜌)\n\nBased on rank differences. Defined as the Pearson correlation coefficient between the ranked variables.\\[\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\\] where \\(d_i\\) is the difference between the ranks of each pair, and \\(n\\) is the number of observations.\n\nKendall’s tau (𝜏)\n\nBased on concordant and discordant pairs.  Measures the proportion of pairs that have the same order in both variables compared to the total number of pairs. \\[\\tau = \\frac{(C - D)}{\\frac{1}{2} n (n - 1)}\\] where \\(C\\) is the number of concordant pairs, and \\(D\\) is the number of discordant pairs.\n\n\nWhen to Use Which?\n\n\n\n\n\n\n\nFactor\nSpearman’s rho (𝜌)\nKendall’s tau (𝜏)\n\n\n\nLarge differences in ranks\nMore sensitive\nLess sensitive\n\n\nSmall sample sizes\nLess reliable\nMore reliable\n\n\nOutlier resistance\nModerate\nHigh\n\n\nComputational efficiency\nFaster\nSlower (due to pairwise comparisons)\n\n\nInterpretation\nSimilar to Pearson’s correlation\nMore intuitive (proportion of concordance)"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#chatterjees-correlation-coefficient-chatterjees-ξ",
    "href": "core/labs-solutions/lab-bivariate.html#chatterjees-correlation-coefficient-chatterjees-ξ",
    "title": "Bivariate analysis",
    "section": "Chatterjee’s correlation coefficient (Chatterjee’s \\(ξ\\))",
    "text": "Chatterjee’s correlation coefficient (Chatterjee’s \\(ξ\\))\n\nThe three most popular classical measures of statistical association are Pearson’s correlation coefficient, Spearman’s ρ, and Kendall’s τ . These coefficients are very powerful for detecting linear or monotone associations, and they have well-developed asymptotic theories for calculating P-values. However, the big problem is that they are not effective for detecting associations that are not monotonic, even in the complete absence of noise.\n\n\nLet \\((X, Y)\\) be a pair of random variables, where \\(Y\\) is not a constant. Let \\((X_1 , Y_1 ), \\ldots, (X_n , Y_n )\\) be i.i.d. pairs with the same law as \\((X, Y)\\), where \\(n ≥ 2\\). The new coefficient has a simpler formula if the \\(X_i\\)’s and the \\(Y_i\\) ’s have no ties. This simpler formula is presented first, and then the general case is given. Suppose that the \\(X_i\\)’s and the \\(Y_i\\) ’s have no ties. Rearrange the data as \\((X_{(1)} , Y_{(1)} ), . . . , (X_{(n)} , Y_{(n)} )\\) such that \\(X_{(1)} ≤ · · · ≤ X_{(n)}\\) . Since the \\(X_i\\)’s have no ties, there is a unique way of doing this. Let ri be the rank of \\(Y_{(i)}\\), that is, the number of \\(j\\) such that \\(Y_{(j)} ≤ Y_{(i)}\\). The new correlation coefficient is defined as\n\n\\[ξ_n(X, Y ) := 1 −  3\\sum_{i=1}^{n-1} \\frac{|r_{i+1} − r_i|}{n^2-1}\\]\n\nIn the presence of ties, \\(ξ_n\\) is defined as follows. If there are ties among the \\(X_i\\)’s, then choose an increasing rearrangement as above by breaking ties uniformly at random. Let \\(r_i\\) be as before, and additionally define \\(l_i\\) to be the number of \\(j\\) such that \\(Y_{(j)} ≥ Y_{(i)}\\). Then define\n\n\\[ξ_n(X, Y ) := 1 −  3n\\sum_{i=1}^{n-1} \\frac{|r_{i+1} − r_i|}{2 \\sum_{i=1}^n l_i (n − l_i )}\\]\n\nWhen there are no ties among the \\(Y_i\\) ’s, \\(l_1 , \\ldots , l_n\\) is just a permutation of \\(1, \\ldots , n\\), and so the denominator in the above expression is just \\(n(n^2 − 1)/3\\), which reduces this definition to the earlier expression.\n\nFrom Sourav Chatterjee: A new correlation coefficient\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWrite a dplyr pipeline from computing the \\(ξ\\) correlation coefficient between Y=lifeExp and X=gdpPercap in the gapminder dataset, per year and continent.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodetab_xi &lt;- gapminder::gapminder |&gt;  \n  group_by(year, continent) |&gt; \n  arrange(gdpPercap) |&gt; \n  mutate(rnk= row_number(lifeExp), \n         lnk=rank(desc(lifeExp), ties.method = \"max\"), \n         N=n()) |&gt; \n  mutate(fol=lead(rnk), dd=abs(fol-rnk)) |&gt; \n  summarise(Xi=1-n()*sum(dd, na.rm = T)/(2*sum(lnk*(N-lnk), na.rm = T))) \n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nCodetab_xi  |&gt; \n  print(n=10)  \n\n# A tibble: 60 × 3\n# Groups:   year [12]\n    year continent     Xi\n   &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;\n 1  1952 Africa    0.0377\n 2  1952 Americas  0.135 \n 3  1952 Asia      0.244 \n 4  1952 Europe    0.610 \n 5  1952 Oceania   0     \n 6  1957 Africa    0.0466\n 7  1957 Americas  0.240 \n 8  1957 Asia      0.330 \n 9  1957 Europe    0.516 \n10  1957 Oceania   0     \n# ℹ 50 more rows\n\n\n\nCodetab_xi |&gt;\n  pivot_wider(\n    id_cols=continent, \n    names_from=year,\n    values_from=Xi\n    ) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(decimals=2)\n\n\n\n\n\ncontinent\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\n\nAfrica\n0.04\n0.05\n−0.01\n0.05\n0.24\n0.27\n0.38\n0.27\n0.36\n0.31\n0.13\n0.17\n\n\nAmericas\n0.13\n0.24\n0.35\n0.17\n0.19\n0.13\n0.30\n0.43\n0.52\n0.33\n0.29\n0.23\n\n\nAsia\n0.24\n0.33\n0.38\n0.27\n0.32\n0.26\n0.29\n0.30\n0.43\n0.50\n0.50\n0.46\n\n\nEurope\n0.61\n0.52\n0.40\n0.38\n0.32\n0.38\n0.06\n0.29\n0.39\n0.51\n0.48\n0.51\n\n\nOceania\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html",
    "href": "core/labs-solutions/lab-tables.html",
    "title": "Tables manipulation II",
    "section": "",
    "text": "M1 MIDS/MFA/LOGOS\n\n\nUniversité Paris Cité\n\n\nAnnée 2025\n\n\nCourse Homepage\n\n\nMoodle"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#setup",
    "href": "core/labs-solutions/lab-tables.html#setup",
    "title": "Tables manipulation II",
    "section": "Setup",
    "text": "Setup\nWe will use the following packages. If needed, we install them.\n\nCodeold_theme &lt;- theme_set(theme_minimal())\n\n\nCheck nycflights13 for any explanation concerning the tables and their columns."
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#data-loading",
    "href": "core/labs-solutions/lab-tables.html#data-loading",
    "title": "Tables manipulation II",
    "section": "Data loading",
    "text": "Data loading\nWe will use data from package nycflights13. Those data consist of five tables.\n\nCodeflights &lt;- nycflights13::flights\nweather &lt;- nycflights13::weather\nairports &lt;- nycflights13::airports\nairlines &lt;- nycflights13::airlines\nplanes &lt;- nycflights13::planes\n\n\nThe five tables have been loaded as 5 data frames. They could also be considered as tables. Living in an in memory relational database operated by SQLite.\n\nCodecon &lt;- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\nflights_lite &lt;- copy_to(con, nycflights13::flights)\nairports_lite &lt;- copy_to(con, nycflights13::airports)\nplanes_lite &lt;-  copy_to(con, nycflights13::planes)\nweather_lite &lt;- copy_to(con, nycflights13::weather)\nairlines_lite &lt;- copy_to(con, nycflights13::airlines)\n\n\nThis allows us to handle the tables/data frames as tables living in a plain database. If you are familiar with SQL, this is well-known territory.\n\nCodeflights_lite |&gt;\n  select(contains(\"delay\")) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT `dep_delay`, `arr_delay`\nFROM `nycflights13::flights`\n\n\nAnyway you can view the data in spreadsheet style.\n\nCodeView(flights)\n\n\nAsk for help about table flights (use head and glimpse)\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCode?flights\n\n\n\nCodeflights |&gt; \n  glimpse()\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\n\nCodeairports |&gt; \n  glimpse()\n\nRows: 1,458\nColumns: 8\n$ faa   &lt;chr&gt; \"04G\", \"06A\", \"06C\", \"06N\", \"09J\", \"0A9\", \"0G6\", \"0G7\", \"0P2\", \"…\n$ name  &lt;chr&gt; \"Lansdowne Airport\", \"Moton Field Municipal Airport\", \"Schaumbur…\n$ lat   &lt;dbl&gt; 41.13047, 32.46057, 41.98934, 41.43191, 31.07447, 36.37122, 41.4…\n$ lon   &lt;dbl&gt; -80.61958, -85.68003, -88.10124, -74.39156, -81.42778, -82.17342…\n$ alt   &lt;dbl&gt; 1044, 264, 801, 523, 11, 1593, 730, 492, 1000, 108, 409, 875, 10…\n$ tz    &lt;dbl&gt; -5, -6, -6, -5, -5, -5, -5, -5, -5, -8, -5, -6, -5, -5, -5, -5, …\n$ dst   &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"U\", \"A\", \"A\", \"U\", \"A\",…\n$ tzone &lt;chr&gt; \"America/New_York\", \"America/Chicago\", \"America/Chicago\", \"Ameri…"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#first-queries-the-dplyr-way",
    "href": "core/labs-solutions/lab-tables.html#first-queries-the-dplyr-way",
    "title": "Tables manipulation II",
    "section": "First Queries (the dplyr way)",
    "text": "First Queries (the dplyr way)\nFind all flights that:\n\nHad an arrival delay of two or more hours\n\nHint: use verb filter.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  filter(arr_delay &gt;= 120) |&gt; \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1 10200\n\n\nCompare with SQL\n\nCodeflights_lite |&gt; \n  filter(arr_delay &gt;= 120) |&gt; \n  count()  |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT COUNT(*) AS `n`\nFROM (\n  SELECT `nycflights13::flights`.*\n  FROM `nycflights13::flights`\n  WHERE (`arr_delay` &gt;= 120.0)\n) AS `q01`\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nWe can translate the dplyr pipeline into an SQL query. We can even get some explanations\nflights_lite |&gt; \n  filter(arr_delay &gt;= 120) |&gt; \n  dplyr::explain()\n\n\n\n\nFlew to Houston (IAH or HOU)\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  filter(dest %in% c(\"HOU\", \"IAH\")) |&gt;  \n  nrow()\n\n[1] 9313\n\nCode# flights |&gt; \n#   filter(dest == \"HOU\" | dest == \"IAH\") |&gt; \n#   count()\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights_lite |&gt; \n  filter(dest %in% c(\"HOU\", \"IAH\")) |&gt;  \n  show_query()\n\n&lt;SQL&gt;\nSELECT `nycflights13::flights`.*\nFROM `nycflights13::flights`\nWHERE (`dest` IN ('HOU', 'IAH'))\n\n\n\nCode# flights_lite |&gt; \n#   filter(dest == \"HOU\" | dest == \"IAH\") |&gt; \n#   show_query()\n\n\n\n\n\n\nWere operated by United, American, or Delta\n\n\n\n\n\n\n\nPackage stringr could be useful.\n\nCodeairlines |&gt; \n  filter(stringr::str_starts(name, \"United\") |\n        stringr::str_starts(name, \"American\") |\n        stringr::str_starts(name, \"Delta\"))\n\n# A tibble: 3 × 2\n  carrier name                  \n  &lt;chr&gt;   &lt;chr&gt;                 \n1 AA      American Airlines Inc.\n2 DL      Delta Air Lines Inc.  \n3 UA      United Air Lines Inc. \n\nCodeairlines |&gt; \n  filter(stringr::str_detect(name, (\"United|American|Delta\"))) |&gt; \n  pull(carrier)\n\n[1] \"AA\" \"DL\" \"UA\"\n\n\nUnfortunately, the next code is not supported.\n#| eval: false\nairlines_lite |&gt; \n  filter(stringr::str_starts(name, \"United\") |\n         stringr::str_starts(name, \"American\") |\n         stringr::str_starts(name, \"Delta\")) |&gt; \n  show_query()\nIt would be the dplyr version of the following SQL code.\nSELECT *\nFROM `nycflights13::airlines`\nWHERE \"name\" LIKE 'United%' OR \n      \"name\" LIKE 'American%' OR \n      \"name\" LIKE 'Delta%' ;\nstringr is part of tidyverse. It offers an API for string manipulations.\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nWe build on the tip above to extract the matching airlines codes.\nWe may proceed using a subquery\n\nCodeflights |&gt; \n  filter(carrier %in% (\n        airlines |&gt; \n        filter(stringr::str_detect(name, (\"United|American|Delta\"))) |&gt; \n        pluck(\"carrier\")\n      )\n) |&gt; \nhead(6)\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1      517            515         2      830            819\n2  2013     1     1      533            529         4      850            830\n3  2013     1     1      542            540         2      923            850\n4  2013     1     1      554            600        -6      812            837\n5  2013     1     1      554            558        -4      740            728\n6  2013     1     1      558            600        -2      753            745\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nCodeflights_lite |&gt; \n  filter(carrier %in% c(\"AA\", \"DL\", \"UA\")) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `nycflights13::flights`.*\nFROM `nycflights13::flights`\nWHERE (`carrier` IN ('AA', 'DL', 'UA'))\n\n\nSELECT *\nFROM `nycflights13::flights`\nWHERE carrier IN (SELECT carrier \n  FROM `nycflights13::airlines`\n  WHERE \"name\" LIKE 'United%' OR \n        \"name\" LIKE 'American%' OR \n        \"name\" LIKE 'Delta%')\n) ; \nWe may also rely on a NATURAL JOIN.\nIn this case, the subquery can be mimicked using a join.\n\nCodeairlines |&gt; \n  filter(str_detect(name, \"United|Delta|American\")) |&gt; \n  inner_join(flights) |&gt; \n  head(6)\n\nJoining with `by = join_by(carrier)`\n\n\n# A tibble: 6 × 20\n  carrier name       year month   day dep_time sched_dep_time dep_delay arr_time\n  &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n1 AA      American…  2013     1     1      542            540         2      923\n2 AA      American…  2013     1     1      558            600        -2      753\n3 AA      American…  2013     1     1      559            600        -1      941\n4 AA      American…  2013     1     1      606            610        -4      858\n5 AA      American…  2013     1     1      623            610        13      920\n6 AA      American…  2013     1     1      628            630        -2     1137\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\nCode#  sample_n(size=6)    # random sampling \n\n\nThis code is not legal.\n#| eval: false\nairlines_lite |&gt; \n  filter(stringr::str_starts(name, \"United\") |\n        stringr::str_starts(name, \"American\") |\n        stringr::str_starts(name, \"Delta\")) |&gt; \n  inner_join(flights_lite) |&gt; \n  show_query()\n\n\n\n\nDeparted in summer (July, August, and September)\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nWe need to know the month numbers to answer this question.\n\nCodeflights |&gt; \n  filter(month %in% c(7,8,9)) |&gt; \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1 86326\n\n\nA more ambitious (and sustainable) approach relies on the date/time manipulation function from lubridate\n\nCodeflights |&gt; \n  filter(lubridate::month(time_hour) %in% 7:9) |&gt; \n  head()\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     7     1        1           2029       212      236           2359\n2  2013     7     1        2           2359         3      344            344\n3  2013     7     1       29           2245       104      151              1\n4  2013     7     1       43           2130       193      322             14\n5  2013     7     1       44           2150       174      300            100\n6  2013     7     1       46           2051       235      304           2358\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nor even\n\nCodemy_locale &lt;- 'en_US.UTF-8'\n\nflights |&gt; \n  filter(lubridate::month(time_hour, \n                          label=T, \n                          abbr=F, \n                          locale=my_locale) %in% \n                    c('July', 'August', 'September')) |&gt; \n  head()\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     7     1        1           2029       212      236           2359\n2  2013     7     1        2           2359         3      344            344\n3  2013     7     1       29           2245       104      151              1\n4  2013     7     1       43           2130       193      322             14\n5  2013     7     1       44           2150       174      300            100\n6  2013     7     1       46           2051       235      304           2358\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\n\n\n\n\n\nWhen manipulating temporal information (date, time, duration), keep an eye on what lubridate offers. The API closely parallels what Relational Database Management Systems (RDMS) and Python (datetime) offer.\n\n\n\n\nArrived more than two hours late, but didn’t leave late\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  filter(arr_delay &gt;= 120, dep_delay &lt;= 0) |&gt; \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    29\n\n\n\n\n\n\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  filter(dep_delay &gt; 60, \n         arr_delay &lt; dep_delay -30) |&gt; \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  1819\n\n\n\n\n\n\nDeparted between midnight and 6am (inclusive)\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  filter(dep_time&lt;=600, dep_time&gt;0) |&gt; \n  head()\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1      517            515         2      830            819\n2  2013     1     1      533            529         4      850            830\n3  2013     1     1      542            540         2      923            850\n4  2013     1     1      544            545        -1     1004           1022\n5  2013     1     1      554            600        -6      812            837\n6  2013     1     1      554            558        -4      740            728\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nCodeflights |&gt; \n  filter(lubridate::hour(time_hour)&lt;=5  | (\n         lubridate::hour(time_hour)==6 & minute(time_hour)==0)) |&gt; \n  head()\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1      517            515         2      830            819\n2  2013     1     1      533            529         4      850            830\n3  2013     1     1      542            540         2      923            850\n4  2013     1     1      544            545        -1     1004           1022\n5  2013     1     1      554            600        -6      812            837\n6  2013     1     1      554            558        -4      740            728\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\n\n\n\n\n\nRead filter() in R for Data Science 1st Ed\nRead Chapter Transform in R for Data Science 2nd Ed"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#missing-data",
    "href": "core/labs-solutions/lab-tables.html#missing-data",
    "title": "Tables manipulation II",
    "section": "Missing data",
    "text": "Missing data\n\nHow many flights per origin have a missing dep_time?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nR provide several functions to handle missing (NA) data\n\nCodeflights |&gt; \n  filter(is.na(dep_time)) |&gt; \n  count(by=origin)\n\n# A tibble: 3 × 2\n  by        n\n  &lt;chr&gt; &lt;int&gt;\n1 EWR    3239\n2 JFK    1863\n3 LGA    3153\n\n\n\nCodeflights_lite |&gt; \n  filter(is.na(dep_time)) |&gt; \n  count(by=origin) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `by`, COUNT(*) AS `n`\nFROM (\n  SELECT `nycflights13::flights`.*, `origin` AS `by`\n  FROM `nycflights13::flights`\n  WHERE ((`dep_time` IS NULL))\n) AS `q01`\nGROUP BY `by`\n\n\nNot far from a spontaneous answer! We could obtain the latter anyway.\n\nCodeflights_lite |&gt; \n  filter(is.na(dep_time)) |&gt; \n  group_by(origin) |&gt; \n  summarise(n=n()) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `origin`, COUNT(*) AS `n`\nFROM (\n  SELECT `nycflights13::flights`.*\n  FROM `nycflights13::flights`\n  WHERE ((`dep_time` IS NULL))\n) AS `q01`\nGROUP BY `origin`\n\n\nNote that combining dplyr verbs and pipes (|&gt; or |&gt;) provides a much more readable and modular approach than vanilla SQL.\nUsing dplyr and pipe, the order in which operation are executed is clear and obvious. In SQL, it is counterintuitive.\n\n\n\nIn table flights, 8255 (nrow(filter(flights, is.na(dep_time)))) rows have a missing dep_time field.\n\nWhat other variables are missing?\n\n\n\n\n\n\n\nThe introduction to tidyselect is a must read.\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThis is an opportunity to introduce\n\ntidy selection helpers like across()\n\ntidying functions like pivot_longer from package tidyr.\n\n\nCodeflights |&gt; \n  filter(is.na(dep_time)) |&gt;     # keep lines with missing `dep_time`\n  summarize(across(everything(), \\(x) all(is.na(x)))) |&gt;    # compute the number of NA cells in other columns\n  pivot_longer(cols = everything()) # |&gt;  # make the resulting table thin\n\n# A tibble: 19 × 2\n   name           value\n   &lt;chr&gt;          &lt;lgl&gt;\n 1 year           FALSE\n 2 month          FALSE\n 3 day            FALSE\n 4 dep_time       TRUE \n 5 sched_dep_time FALSE\n 6 dep_delay      TRUE \n 7 arr_time       TRUE \n 8 sched_arr_time FALSE\n 9 arr_delay      TRUE \n10 carrier        FALSE\n11 flight         FALSE\n12 tailnum        FALSE\n13 origin         FALSE\n14 dest           FALSE\n15 air_time       TRUE \n16 distance       FALSE\n17 hour           FALSE\n18 minute         FALSE\n19 time_hour      FALSE\n\nCode  # filter(value) |&gt;  \n  # select(name)\n\n\n\nCodeflights_lite |&gt; \n  filter(is.na(dep_time)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `nycflights13::flights`.*\nFROM `nycflights13::flights`\nWHERE ((`dep_time` IS NULL))\n\n\n\n\n\n\nWhat might these rows with missing data represent?\n\nAll the information you can only get if the flight did take off.\n\nCodenot_cancelled &lt;-  flights |&gt; \n  filter(!is.na(dep_time))\n\n\n\n\nMore questions: for each column in flight report the number of missing values.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  summarise(across(everything(), ~  sum(is.na(.)))) |&gt; \n  pivot_longer(cols = everything()) |&gt; \n  filter(value &gt; 0) |&gt; \n  arrange(desc(value))\n\n# A tibble: 6 × 2\n  name      value\n  &lt;chr&gt;     &lt;int&gt;\n1 arr_delay  9430\n2 air_time   9430\n3 arr_time   8713\n4 dep_time   8255\n5 dep_delay  8255\n6 tailnum    2512\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nFunction skim from package skimr delivers per column summaries.\n\nCodeflights |&gt; \n  skimr::skim()\n\n\nData summary\n\n\nName\nflights\n\n\nNumber of rows\n336776\n\n\nNumber of columns\n19\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n14\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\ncarrier\n0\n1.00\n2\n2\n0\n16\n0\n\n\ntailnum\n2512\n0.99\n5\n6\n0\n4043\n0\n\n\norigin\n0\n1.00\n3\n3\n0\n3\n0\n\n\ndest\n0\n1.00\n3\n3\n0\n105\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nyear\n0\n1.00\n2013.00\n0.00\n2013\n2013\n2013\n2013\n2013\n▁▁▇▁▁\n\n\nmonth\n0\n1.00\n6.55\n3.41\n1\n4\n7\n10\n12\n▇▆▆▆▇\n\n\nday\n0\n1.00\n15.71\n8.77\n1\n8\n16\n23\n31\n▇▇▇▇▆\n\n\ndep_time\n8255\n0.98\n1349.11\n488.28\n1\n907\n1401\n1744\n2400\n▁▇▆▇▃\n\n\nsched_dep_time\n0\n1.00\n1344.25\n467.34\n106\n906\n1359\n1729\n2359\n▁▇▇▇▃\n\n\ndep_delay\n8255\n0.98\n12.64\n40.21\n-43\n-5\n-2\n11\n1301\n▇▁▁▁▁\n\n\narr_time\n8713\n0.97\n1502.05\n533.26\n1\n1104\n1535\n1940\n2400\n▁▃▇▇▇\n\n\nsched_arr_time\n0\n1.00\n1536.38\n497.46\n1\n1124\n1556\n1945\n2359\n▁▃▇▇▇\n\n\narr_delay\n9430\n0.97\n6.90\n44.63\n-86\n-17\n-5\n14\n1272\n▇▁▁▁▁\n\n\nflight\n0\n1.00\n1971.92\n1632.47\n1\n553\n1496\n3465\n8500\n▇▃▃▁▁\n\n\nair_time\n9430\n0.97\n150.69\n93.69\n20\n82\n129\n192\n695\n▇▂▂▁▁\n\n\ndistance\n0\n1.00\n1039.91\n733.23\n17\n502\n872\n1389\n4983\n▇▃▂▁▁\n\n\nhour\n0\n1.00\n13.18\n4.66\n1\n9\n13\n17\n23\n▁▇▇▇▅\n\n\nminute\n0\n1.00\n26.23\n19.30\n0\n8\n29\n44\n59\n▇▃▆▃▅\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\ntime_hour\n0\n1\n2013-01-01 05:00:00\n2013-12-31 23:00:00\n2013-07-03 10:00:00\n6936"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#arrange",
    "href": "core/labs-solutions/lab-tables.html#arrange",
    "title": "Tables manipulation II",
    "section": "Arrange",
    "text": "Arrange\n\nHow could you use arrange() to sort all missing values to the start? (Hint: use is.na()).\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1       NA           1630        NA       NA           1815\n 2  2013     1     1       NA           1935        NA       NA           2240\n 3  2013     1     1       NA           1500        NA       NA           1825\n 4  2013     1     1       NA            600        NA       NA            901\n 5  2013     1     2       NA           1540        NA       NA           1747\n 6  2013     1     2       NA           1620        NA       NA           1746\n 7  2013     1     2       NA           1355        NA       NA           1459\n 8  2013     1     2       NA           1420        NA       NA           1644\n 9  2013     1     2       NA           1321        NA       NA           1536\n10  2013     1     2       NA           1545        NA       NA           1910\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\nSort flights to find the most delayed flights.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  arrange(desc(dep_delay)) |&gt; \n  select(dep_delay, arr_delay, everything())\n\n# A tibble: 336,776 × 19\n   dep_delay arr_delay  year month   day dep_time sched_dep_time arr_time\n       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;int&gt;\n 1      1301      1272  2013     1     9      641            900     1242\n 2      1137      1127  2013     6    15     1432           1935     1607\n 3      1126      1109  2013     1    10     1121           1635     1239\n 4      1014      1007  2013     9    20     1139           1845     1457\n 5      1005       989  2013     7    22      845           1600     1044\n 6       960       931  2013     4    10     1100           1900     1342\n 7       911       915  2013     3    17     2321            810      135\n 8       899       850  2013     6    27      959           1900     1236\n 9       898       895  2013     7    22     2257            759      121\n10       896       878  2013    12     5      756           1700     1058\n# ℹ 336,766 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\nPick the ten most delayed flights (with finite dep_delay)\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  slice_max(order_by=dep_delay, n = 10, na_rm = T)\n\n# A tibble: 10 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\nFind the flights that left earliest.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  arrange(dep_time) |&gt; \n  select(dest, origin, dep_time)\n\n# A tibble: 336,776 × 3\n   dest  origin dep_time\n   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;\n 1 SYR   JFK           1\n 2 MDW   LGA           1\n 3 SJU   JFK           1\n 4 BQN   JFK           1\n 5 SJU   JFK           1\n 6 SJU   JFK           1\n 7 BQN   JFK           1\n 8 MDW   LGA           1\n 9 PWM   JFK           1\n10 PSE   JFK           1\n# ℹ 336,766 more rows\n\n\n\n\n\n\nSort flights to find the fastest (highest speed) flights.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  filter(!is.na(air_time)) |&gt; \n  mutate(speed = distance/air_time) |&gt; \n  arrange(desc(speed)) |&gt; \n  select(speed, distance, air_time, origin, dest, everything())\n\n# A tibble: 327,346 × 20\n   speed distance air_time origin dest   year month   day dep_time\n   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n 1 11.7       762       65 LGA    ATL    2013     5    25     1709\n 2 10.8      1008       93 EWR    MSP    2013     7     2     1558\n 3 10.8       594       55 EWR    GSP    2013     5    13     2040\n 4 10.7       748       70 EWR    BNA    2013     3    23     1914\n 5  9.86     1035      105 LGA    PBI    2013     1    12     1559\n 6  9.4      1598      170 JFK    SJU    2013    11    17      650\n 7  9.29     1598      172 JFK    SJU    2013     2    21     2355\n 8  9.27     1623      175 JFK    STT    2013    11    17      759\n 9  9.24     1598      173 JFK    SJU    2013    11    16     2003\n10  9.24     1598      173 JFK    SJU    2013    11    16     2349\n# ℹ 327,336 more rows\n# ℹ 11 more variables: sched_dep_time &lt;int&gt;, dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;,\n#   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nThis is an overkill. We are sorting in order to perform maximum selection. Recall that sorting requires more comparisons than selection. The comparison between sorting and selecting with respect to data shuffling is even less favourable.\n\nCodeflights |&gt; \n  filter(!is.na(air_time)) |&gt; \n  mutate(speed = distance/air_time) |&gt;\n  slice_max(n=10, speed) |&gt;\n  select(speed, distance, air_time, origin, dest, everything())\n\n# A tibble: 15 × 20\n   speed distance air_time origin dest   year month   day dep_time\n   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n 1 11.7       762       65 LGA    ATL    2013     5    25     1709\n 2 10.8      1008       93 EWR    MSP    2013     7     2     1558\n 3 10.8       594       55 EWR    GSP    2013     5    13     2040\n 4 10.7       748       70 EWR    BNA    2013     3    23     1914\n 5  9.86     1035      105 LGA    PBI    2013     1    12     1559\n 6  9.4      1598      170 JFK    SJU    2013    11    17      650\n 7  9.29     1598      172 JFK    SJU    2013     2    21     2355\n 8  9.27     1623      175 JFK    STT    2013    11    17      759\n 9  9.24     1598      173 JFK    SJU    2013    11    16     2003\n10  9.24     1598      173 JFK    SJU    2013    11    16     2349\n11  9.24     1598      173 JFK    SJU    2013    11    17      851\n12  9.24     1598      173 JFK    SJU    2013    11    17     1926\n13  9.24     1598      173 JFK    SJU    2013    12     5     1858\n14  9.24     1598      173 JFK    SJU    2013     2    10     1658\n15  9.24     1598      173 JFK    SJU    2013     2    10     1958\n# ℹ 11 more variables: sched_dep_time &lt;int&gt;, dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;,\n#   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\nWhich flights traveled the farthest?\n\n\n\n\n\n\n\nThe database provides all we need with columns distance and air_time. Otherwise, with the positions of airports from table airports, we should be able to compute distances using :\n\n‘Haversine’ formula.\n\nhttps://en.wikipedia.org/wiki/Haversine_formula\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  arrange(desc(distance)) |&gt; \n  distinct(distance, dest, origin)\n\n# A tibble: 226 × 3\n   distance dest  origin\n      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1     4983 HNL   JFK   \n 2     4963 HNL   EWR   \n 3     3370 ANC   EWR   \n 4     2586 SFO   JFK   \n 5     2576 OAK   JFK   \n 6     2569 SJC   JFK   \n 7     2565 SFO   EWR   \n 8     2521 SMF   JFK   \n 9     2475 LAX   JFK   \n10     2465 LGB   JFK   \n# ℹ 216 more rows\n\n\n\n\n\n\nWhich traveled the shortest?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  arrange(distance) |&gt; \n  select(distance, dest, origin, everything())\n\n# A tibble: 336,776 × 19\n   distance dest  origin  year month   day dep_time sched_dep_time dep_delay\n      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1       17 LGA   EWR     2013     7    27       NA            106        NA\n 2       80 PHL   EWR     2013     1     3     2127           2129        -2\n 3       80 PHL   EWR     2013     1     4     1240           1200        40\n 4       80 PHL   EWR     2013     1     4     1829           1615       134\n 5       80 PHL   EWR     2013     1     4     2128           2129        -1\n 6       80 PHL   EWR     2013     1     5     1155           1200        -5\n 7       80 PHL   EWR     2013     1     6     2125           2129        -4\n 8       80 PHL   EWR     2013     1     7     2124           2129        -5\n 9       80 PHL   EWR     2013     1     8     2127           2130        -3\n10       80 PHL   EWR     2013     1     9     2126           2129        -3\n# ℹ 336,766 more rows\n# ℹ 10 more variables: arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;,\n#   carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, air_time &lt;dbl&gt;, hour &lt;dbl&gt;,\n#   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nAvoid an overkill …\n\nCodeflights |&gt;\n  distinct(distance, origin, dest) |&gt; \n  slice_min(distance, n=10)\n\n# A tibble: 11 × 3\n   distance origin dest \n      &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1       17 EWR    LGA  \n 2       80 EWR    PHL  \n 3       94 JFK    PHL  \n 4       96 LGA    PHL  \n 5      116 EWR    BDL  \n 6      143 EWR    ALB  \n 7      160 EWR    PVD  \n 8      169 EWR    BWI  \n 9      173 JFK    MVY  \n10      184 JFK    BWI  \n11      184 LGA    BOS"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#projection",
    "href": "core/labs-solutions/lab-tables.html#projection",
    "title": "Tables manipulation II",
    "section": "Projection",
    "text": "Projection\n\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nCodeflights |&gt; \n  select(dep_time, dep_delay, arr_time, arr_delay) |&gt; \n  head()\n\n# A tibble: 6 × 4\n  dep_time dep_delay arr_time arr_delay\n     &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1      517         2      830        11\n2      533         4      850        20\n3      542         2      923        33\n4      544        -1     1004       -18\n5      554        -6      812       -25\n6      554        -4      740        12\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nCodeflights |&gt; \n  select(starts_with(\"dep\"), starts_with(\"arr\")) |&gt;  \n  head()\n\n# A tibble: 6 × 4\n  dep_time dep_delay arr_time arr_delay\n     &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1      517         2      830        11\n2      533         4      850        20\n3      542         2      923        33\n4      544        -1     1004       -18\n5      554        -6      812       -25\n6      554        -4      740        12\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nCodeflights |&gt; \n  select(starts_with(\"dep_\") | starts_with(\"arr\")) |&gt; \n  head()\n\n# A tibble: 6 × 4\n  dep_time dep_delay arr_time arr_delay\n     &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1      517         2      830        11\n2      533         4      850        20\n3      542         2      923        33\n4      544        -1     1004       -18\n5      554        -6      812       -25\n6      554        -4      740        12\n\n\n\n\n\nWhat happens if you include the name of a variable multiple times in a select() call?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\nIt is included once in the result.\n\nCodeflights |&gt; \n  select(arr_time, starts_with(\"dep_\") | starts_with(\"arr\"), arr_time) |&gt; \n  head()\n\n# A tibble: 6 × 4\n  arr_time dep_time dep_delay arr_delay\n     &lt;int&gt;    &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1      830      517         2        11\n2      850      533         4        20\n3      923      542         2        33\n4     1004      544        -1       -18\n5      812      554        -6       -25\n6      740      554        -4        12\n\n\n\n\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\nvars &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodevars &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\nflights |&gt; \n  filter(across(any_of(vars), is.numeric))\n\nWarning: Using `across()` in `filter()` was deprecated in dplyr 1.0.8.\nℹ Please use `if_any()` or `if_all()` instead.\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nCodeflights |&gt; \n  filter(if_any(vars, \\(x) x&lt;0)) |&gt; \n  relocate(ends_with('delay')) |&gt; \n  head()\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(vars)\n\n  # Now:\n  data %&gt;% select(all_of(vars))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\n# A tibble: 6 × 19\n  dep_delay arr_delay  year month   day dep_time sched_dep_time arr_time\n      &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;int&gt;\n1        -1       -18  2013     1     1      544            545     1004\n2        -6       -25  2013     1     1      554            600      812\n3        -4        12  2013     1     1      554            558      740\n4        -5        19  2013     1     1      555            600      913\n5        -3       -14  2013     1     1      557            600      709\n6        -3        -8  2013     1     1      557            600      838\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\nDoes the result of running the following code surprise you?\n\n\nCodeselect(\n  flights, \n  contains(\"TIME\", ignore.case =TRUE))  |&gt; \n  head()\n\n# A tibble: 6 × 6\n  dep_time sched_dep_time arr_time sched_arr_time air_time time_hour          \n     &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dttm&gt;             \n1      517            515      830            819      227 2013-01-01 05:00:00\n2      533            529      850            830      227 2013-01-01 05:00:00\n3      542            540      923            850      160 2013-01-01 05:00:00\n4      544            545     1004           1022      183 2013-01-01 05:00:00\n5      554            600      812            837      116 2013-01-01 06:00:00\n6      554            558      740            728      150 2013-01-01 05:00:00\n\n\n\nHow do the select helpers deal with case by default?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nBy default, select helpers ignore case. This complies with a similar behavior in RDBMS\n\n\n\n\nHow can you change that default?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeselect(flights, \n  contains(\"TIME\", ignore.case=FALSE))  |&gt; \n  head()\n\n# A tibble: 6 × 0"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#mutations",
    "href": "core/labs-solutions/lab-tables.html#mutations",
    "title": "Tables manipulation II",
    "section": "Mutations",
    "text": "Mutations\n\nCurrently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  mutate(across(ends_with('dep_time') | ends_with('arr_time'), \n                (\\(x) 60L * (x %/% 100L) + (x %% 100L)),\n                .names=\"cor_{.col}\")) |&gt;\n  select(ends_with('dep_time'), ends_with('arr_time'), everything())\n\n# A tibble: 336,776 × 23\n   dep_time sched_dep_time cor_dep_time cor_sched_dep_time arr_time\n      &lt;int&gt;          &lt;int&gt;        &lt;int&gt;              &lt;int&gt;    &lt;int&gt;\n 1      517            515          317                315      830\n 2      533            529          333                329      850\n 3      542            540          342                340      923\n 4      544            545          344                345     1004\n 5      554            600          354                360      812\n 6      554            558          354                358      740\n 7      555            600          355                360      913\n 8      557            600          357                360      709\n 9      557            600          357                360      838\n10      558            600          358                360      753\n# ℹ 336,766 more rows\n# ℹ 18 more variables: sched_arr_time &lt;int&gt;, cor_arr_time &lt;int&gt;,\n#   cor_sched_arr_time &lt;int&gt;, year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;,\n#   dep_delay &lt;dbl&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\nCodedf &lt;- flights |&gt; \n  mutate(across(ends_with('dep_time') | ends_with('arr_time'), \n                (\\(x) 60L * (x %/% 100L) + (x %% 100L)),\n                .names=\"{.col}\"))\n\n\nWe should do something more here. We should recast the dep_time, arr_time, and so on columns as durations. This would allow us to perform time arithmetics with lubridate.\n\nCodelubridate::interval(start=Sys.time(), \n                    end= Sys.time() + \n                        lubridate::duration(num=70, units=\"minutes\"))\n\n[1] 2026-01-16 14:57:03 CET--2026-01-16 16:07:03 CET\n\n\n\n\n\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodedf |&gt; \n  mutate(est_arr_time = (dep_time + air_time) %% 1440) |&gt; \n  filter(abs(est_arr_time - arr_time)&lt;1000) |&gt; \n  arrange(desc(abs(est_arr_time - arr_time))) |&gt; \n  select(dest, arr_time, est_arr_time, ends_with(\"time\"), everything())\n\n# A tibble: 314,414 × 20\n   dest  arr_time est_arr_time dep_time sched_dep_time sched_arr_time air_time\n   &lt;chr&gt;    &lt;int&gt;        &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt;\n 1 HNL        881         1226      594            600            890      632\n 2 HNL        858         1203      594            600            910      609\n 3 HNL        865         1210      594            600            900      616\n 4 HNL       1084         1429      810            815           1097      619\n 5 HNL        855         1199      598            600            890      601\n 6 HNL       1090         1434      815            809           1093      619\n 7 HNL        842         1186      585            600            885      601\n 8 HNL       1062         1405      809            809           1093      596\n 9 HNL        889         1232      597            600            890      635\n10 HNL        900         1243      597            600            930      646\n# ℹ 314,404 more rows\n# ℹ 13 more variables: year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, dep_delay &lt;dbl&gt;,\n#   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodedf |&gt; \n  mutate(est_arr_time = (dep_time + air_time) %% 1440) |&gt; \n  filter(!is.na(arr_time), !is.na(est_arr_time)) |&gt; \n  ggplot() +\n  aes(x = arr_time, y = est_arr_time) +\n  geom_point(alpha=.1, size=.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodedf |&gt; \n  mutate(est_arr_time = (dep_time + air_time) %% 1440) |&gt; \n  filter(!is.na(arr_time), !is.na(est_arr_time)) |&gt; \n  filter(abs(arr_time -est_arr_time) &lt;600) |&gt; \n  ggplot() +\n  aes(x = (arr_time -est_arr_time)%%60) +\n  stat_density() +\n  scale_x_continuous(breaks=seq(0, 60, 10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\n\n\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\n\n\nWe expect dep_time == sched_dep_time + dep_delay %% 1440\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodedf |&gt; \n  filter(!is.na(dep_time)) |&gt; \n  mutate(est_dep_time = (sched_dep_time + dep_delay) %% 1440L) |&gt; \n  ggplot() +\n  aes(x=dep_time, y=est_dep_time) +\n  geom_point(alpha=.1, size=.1) + \n  coord_fixed() \n\n\n\n\n\n\n\n\n\n\nFor most of the flights, the discrepancy between dep_time and the computed value is 0.\nFor 29 flights it is positive. The discrepancy is then exactly equal to 24 hours.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodedf |&gt; \n  filter(!is.na(dep_time)) |&gt; \n  mutate(discr = dep_time - (sched_dep_time + dep_delay) %% 1440L) |&gt; \n  filter(discr &gt; 0) |&gt; \n  select(dest, origin, carrier, distance, discr) |&gt; \n  left_join(airports, by=c(\"dest\"=\"faa\")) |&gt; \n  arrange(carrier, desc(distance), dest) |&gt; \n  select(dest, origin, carrier, name, discr)\n\n# A tibble: 29 × 5\n   dest  origin carrier name               discr\n   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n 1 MIA   LGA    AA      Miami Intl          1440\n 2 ORD   LGA    AA      Chicago Ohare Intl  1440\n 3 PSE   JFK    B6      &lt;NA&gt;                1440\n 4 PSE   JFK    B6      &lt;NA&gt;                1440\n 5 PSE   JFK    B6      &lt;NA&gt;                1440\n 6 PSE   JFK    B6      &lt;NA&gt;                1440\n 7 PSE   JFK    B6      &lt;NA&gt;                1440\n 8 PSE   JFK    B6      &lt;NA&gt;                1440\n 9 SJU   JFK    B6      &lt;NA&gt;                1440\n10 SJU   JFK    B6      &lt;NA&gt;                1440\n# ℹ 19 more rows\n\n\n\n\n\n\nFind the 10 most delayed flights using a ranking function. How do you want to handle ties?\n\n\nCarefully read the documentation for min_rank().\nWindowed rank functions.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  filter(dense_rank(-arr_delay) &lt;=10) |&gt; \n  arrange(desc(arr_delay)) |&gt; \n  sample_n(10)\n\n# A tibble: 10 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     1    10     1121           1635      1126     1239           1810\n 3  2013     9    20     1139           1845      1014     1457           2210\n 4  2013     6    15     1432           1935      1137     1607           2120\n 5  2013    12     5      756           1700       896     1058           2020\n 6  2013     7    22     2257            759       898      121           1026\n 7  2013     4    10     1100           1900       960     1342           2211\n 8  2013     7    22      845           1600      1005     1044           1815\n 9  2013     5     3     1133           2055       878     1250           2215\n10  2013     3    17     2321            810       911      135           1020\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#aggregations",
    "href": "core/labs-solutions/lab-tables.html#aggregations",
    "title": "Tables manipulation II",
    "section": "Aggregations",
    "text": "Aggregations\n\nBrainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios:\n\nA flight is 15 minutes early 50% of the time, and 15 minutes late 10% of the time.\nA flight is always 10 minutes late.\nA flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time.\n99% of the time a flight is on time. 1% of the time it’s 2 hours late.\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights_gp &lt;- flights |&gt; \n  group_by(flight, carrier, dest, origin) \n\n\n\nCodeflights_gp |&gt; \n  summarize(q50 = quantile(arr_delay,.5, , na.rm=T), \n            q90 = quantile(arr_delay,.9, , na.rm=T),\n            .groups=\"drop\") |&gt; \n  filter(q50&lt;=-15, q90&gt;=15) |&gt; \n  head()\n\n# A tibble: 6 × 6\n  flight carrier dest  origin   q50   q90\n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1      5 AS      SEA   EWR      -21  26.4\n2      6 DL      SLC   JFK      -18  20  \n3     10 UA      IAH   LGA      -19  28.5\n4     11 AS      SEA   EWR      -15  26.4\n5     31 DL      SFO   JFK      -22  17.2\n6     59 AA      SFO   JFK      -16  20.2\n\n\nAnother approach builds on experimental reframe()\n\nCodequantile_df &lt;- function(x, probs = c(0.5, 0.9)) {\n  tibble(\n    val = quantile(x, probs, na.rm = TRUE),\n    quant = probs\n  )\n}\n\n\n\nCodetmp &lt;- flights_gp |&gt; \n  reframe(q= quantile_df(arr_delay)) |&gt; \n  unnest(cols=q)\n\n\nreframe() generates a list-column (a column is tibble-valued). The result has to be unnested.\n\nCodetmp |&gt; \n  pivot_wider(\n    id_cols=c(flight, carrier, dest, origin),\n    names_from= quant,\n    values_from= val,\n    names_prefix=\"q\"\n  ) |&gt; \n  head()\n\n# A tibble: 6 × 6\n  flight carrier dest  origin  q0.5   q0.9\n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1      1 AA      LAX   JFK    -13    19   \n2      1 B6      FLL   JFK      2    47.8 \n3      1 DL      SJU   JFK    -21    -6.40\n4      1 UA      ORD   EWR    -26   -26   \n5      1 UA      PBI   EWR    -19   -14.2 \n6      1 WN      MDW   LGA      2.5  33.6 \n\n\n\n\n\n\nCodeflights |&gt; \n  group_by(dest) |&gt; \n  summarise(n_cancelled = sum(is.na(dep_time)))\n\n# A tibble: 105 × 2\n   dest  n_cancelled\n   &lt;chr&gt;       &lt;int&gt;\n 1 ABQ             0\n 2 ACK             0\n 3 ALB            20\n 4 ANC             0\n 5 ATL           317\n 6 AUS            21\n 7 AVL            12\n 8 BDL            31\n 9 BGR            15\n10 BHM            25\n# ℹ 95 more rows\n\n\n\nCodeflights_lite |&gt; \n  group_by(dest) |&gt; \n  summarise(n_cancelled = sum(is.na(dep_time))) |&gt; \n  show_query()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n&lt;SQL&gt;\nSELECT `dest`, SUM((`dep_time` IS NULL)) AS `n_cancelled`\nFROM `nycflights13::flights`\nGROUP BY `dest`\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  group_by(flight, carrier, dest, origin) |&gt; \n  summarize(q = quantile(arr_delay,.5, na.rm=T), m=mean(arr_delay, na.rm=TRUE)) |&gt; \n  filter((q &lt; -15) | (q &gt; 15)) |&gt; \n  arrange(desc(m))\n\n`summarise()` has grouped output by 'flight', 'carrier', 'dest'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 4,161 × 6\n# Groups:   flight, carrier, dest [4,106]\n   flight carrier dest  origin     q     m\n    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1    372 UA      CLE   LGA      455   455\n 2    488 UA      DEN   LGA      359   359\n 3    521 WN      BNA   EWR      335   335\n 4   5017 EV      DTW   LGA      334   334\n 5    468 UA      MCO   EWR      323   323\n 6    390 DL      DEN   LGA      318   318\n 7    809 DL      SLC   EWR      299   299\n 8   3261 EV      CLE   EWR      292   292\n 9   3760 9E      RIC   JFK      288   288\n10   1510 UA      IAH   EWR      283   283\n# ℹ 4,151 more rows\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  group_by(jour=lubridate::wday(time_hour, label=T, abbr = F)) |&gt; \n  summarise(n_cancelled=sum(is.na(dep_time)), n_tot=n())  |&gt; \n  mutate(prop_cancelled = n_cancelled/n_tot)\n\n# A tibble: 7 × 4\n  jour     n_cancelled n_tot prop_cancelled\n  &lt;ord&gt;          &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;\n1 dimanche         714 46357         0.0154\n2 lundi           1220 50690         0.0241\n3 mardi           1152 50422         0.0228\n4 mercredi        1202 50060         0.0240\n5 jeudi           1562 50219         0.0311\n6 vendredi        1608 50308         0.0320\n7 samedi           797 38720         0.0206\n\nCode?lubridate::wday\n\n\n\n\n\n\nWhich is more important: arrival delay or departure delay?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt;\n  filter(!is.na(arr_delay)) |&gt;\n  ggplot() +\n    aes(x=dep_delay, y=arr_delay) +\n    geom_point(alpha=.1)\n\n\n\n\n\n\n\n\n\n\n\nCome up with another approach that will give you the same output as not_cancelled |&gt; count(dest) and (without usingcount()`).\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodenot_cancelled |&gt; \n  summarise(n_distinct(dest), n_distinct(tailnum))\n\n# A tibble: 1 × 2\n  `n_distinct(dest)` `n_distinct(tailnum)`\n               &lt;int&gt;                 &lt;int&gt;\n1                104                  4037\n\n\n\n\n\n\nOur definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay) ) is slightly suboptimal. Why? Which is the most important column?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  filter(xor(is.na(arr_delay), is.na(dep_delay))) |&gt; \n  group_by(is.na(arr_delay), is.na(dep_delay)) |&gt; \n  summarise(n())\n\n`summarise()` has grouped output by 'is.na(arr_delay)'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 1 × 3\n# Groups:   is.na(arr_delay) [1]\n  `is.na(arr_delay)` `is.na(dep_delay)` `n()`\n  &lt;lgl&gt;              &lt;lgl&gt;              &lt;int&gt;\n1 TRUE               FALSE               1175\n\n\n\n\n\n\nLook at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  mutate(day_of_week=wday(time_hour, label=TRUE)) |&gt; \n  group_by(day_of_week) |&gt; \n  summarise(n_cancelled= sum(is.na(arr_delay)), n(), prop= n_cancelled/n()) |&gt; \n  ggplot() +\n  aes(x=day_of_week, y=prop) +\n  geom_col(width=.5) +\n  labs(title=\"Proportion of cancelled flights with respect to day of week\") +\n  xlab(\"Day of Week\") +\n  ylab(\"Proportion\")\n\n\n\n\n\n\n\n\n\n\n\nWhich carrier has the worst delays?\n\nChallenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarise(n()))\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  filter(!is.na(arr_delay)) |&gt; \n  group_by(carrier, dest) |&gt; \n  summarise(m=mean(arr_delay), q9=quantile(arr_delay, (c(9)/10)), nn=n()) |&gt; \n  ungroup() |&gt; \n  filter(min_rank(-nn) &lt;50) |&gt; \n  ggplot() +\n  aes(x=carrier, y=dest) +\n  geom_point(aes(color=q9, size=nn)) +\n  scale_color_viridis_c() +\n  scale_size_area()\n\n`summarise()` has grouped output by 'carrier'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does the sort argument to count() do. When might you use it?\n\n\n\n\n\n\n\nTipSolution"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#miscellanea",
    "href": "core/labs-solutions/lab-tables.html#miscellanea",
    "title": "Tables manipulation II",
    "section": "Miscellanea",
    "text": "Miscellanea\n\nWhich carriers serve all destination airports (in the table) ?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  distinct(origin, dest) |&gt; \n  mutate(tot_dest = n_distinct(dest)) |&gt; \n  group_by(tot_dest, origin) |&gt; \n  summarise(n_dest=n(), .groups=\"drop\") |&gt; \n  filter(n_dest==tot_dest) \n\n# A tibble: 0 × 3\n# ℹ 3 variables: tot_dest &lt;int&gt;, origin &lt;chr&gt;, n_dest &lt;int&gt;\n\n\n Verb mutate() handles function n_distinct() as a window function with trivial window.\n\n\n\n\n\n\n\n\n\nThere is no need to perform grouping and aggregation to answer this question. Basic relational algebra is enough. This operation is called division in SQL language\n\nCodeR &lt;- flights |&gt;\n    distinct(carrier, dest)\n\nC &lt;- R |&gt; distinct(carrier) \nD &lt;- R |&gt; distinct(dest)\n\nsetdiff(C, crossing(C, D) |&gt; \n           setdiff(R) |&gt; \n           distinct(carrier))\n\n# A tibble: 0 × 1\n# ℹ 1 variable: carrier &lt;chr&gt;\n\n\nLet \\(\\mathcal{R}(A, B\\) be the schema of some table \\(R\\), then\\[\nR \\div \\pi_B(R) = \\pi_A(R) \\setminus \\left(\\pi_A \\left(\\pi_A(R) \\times \\pi_B(R) \\setminus R \\right)\\right)\n\\]\nMore generally if \\(S\\) has schema \\(\\mathcal{S}(B)\\) \\[\nR \\div S = \\pi_A(R) \\setminus \\left(\\pi_A \\left(\\pi_A(R) \\times \\pi_B(S) \\setminus R \\right)\\right)\n\\]\n\n\n\n\nRefer back to the lists of useful mutate and filtering functions.\nDescribe how each operation changes when you combine it with grouping.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\n\n\n\nWhich plane (tailnum) has the worst on-time record amongst planes with at least ten flights?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  filter(!is.na(arr_delay)) |&gt; \n  group_by(tailnum) |&gt; \n  summarise(m = mean(arr_delay), n_flights=n() ) |&gt; \n  filter(n_flights &gt; 10 ) |&gt; \n  filter(min_rank(-m) == 1) |&gt; \n  inner_join(distinct(flights, tailnum, carrier))\n\nJoining with `by = join_by(tailnum)`\n\n\n# A tibble: 1 × 4\n  tailnum     m n_flights carrier\n  &lt;chr&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;  \n1 N337AT   66.5        13 FL     \n\n\n\n\n\n\nWhat time of day should you fly if you want to avoid delays as much as possible?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nDefinitely before 9a.m.\n\nCodeflights |&gt; \n  filter(!is.na(arr_delay)) |&gt; \n  group_by(hour) |&gt; \n  summarise(m = mean(arr_delay, na.rm=T)) |&gt; \n  arrange(m) |&gt; \n  ggplot() + \n  aes(x=hour, y=m) +\n  geom_col(color=\"black\", fill=\"white\", alpha=.2) +\n  labs(title=\"Mean arrival delay as a function of departure hour\",\n       subtitle= \"nycflights13 data\")\n\n\n\n\n\n\n\n\n\n\n\nFor each destination, compute the total minutes of delay.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt;\n    filter(!is.na(arr_delay)) |&gt;\n    group_by(dest) |&gt;\n    summarise(m = sum(arr_delay, na.rm = T)) |&gt; \n    arrange(desc(m)) |&gt; \n    rename(faa=dest) |&gt;\n    dplyr::inner_join(airports) |&gt;\n    head() |&gt;\n    gt::gt()\n\nJoining with `by = join_by(faa)`\n\n\n\n\n\n\nfaa\nm\nname\nlat\nlon\nalt\ntz\ndst\ntzone\n\n\n\nATL\n190260\nHartsfield Jackson Atlanta Intl\n33.63672\n-84.42807\n1026\n-5\nA\nAmerica/New_York\n\n\nCLT\n100645\nCharlotte Douglas Intl\n35.21400\n-80.94314\n748\n-5\nA\nAmerica/New_York\n\n\nORD\n97352\nChicago Ohare Intl\n41.97860\n-87.90484\n668\n-6\nA\nAmerica/Chicago\n\n\nFLL\n96153\nFort Lauderdale Hollywood Intl\n26.07258\n-80.15275\n9\n-5\nA\nAmerica/New_York\n\n\nDCA\n82609\nRonald Reagan Washington Natl\n38.85208\n-77.03772\n15\n-5\nA\nAmerica/New_York\n\n\nRDU\n78107\nRaleigh Durham Intl\n35.87764\n-78.78747\n435\n-5\nA\nAmerica/New_York\n\n\n\n\n\n\n\n\n\n\nFor each flight, compute the proportion of the total positive arrival delays for its destination.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nWould be easy with ROLLUP (dest, flight)\nWITH R AS (\n  SELECT dest, flight, sum(arr_delay)\n  FROM flights\n  WHERE arr_delay &gt;0\n  GROUP BY ROLLUP(dest, flight)\n)\n\nSELECT r1.dest, r2.flight, r2.sum/r1.sum AS rap\nFROM \n  (SELECT dest, sum \n   FROM R \n   WHERE flight IS NULL) r1 \n  JOIN \n   R r2 \n  ON (r1.dest=r2.dest)\nWHERE r2.flight IS NOT NULL AND r2.sum/r1.sum &gt;.1\nORDER BY r1.dest, r2.flight ;\nWith WINDOW it is even easier\n\n\n\n\nUsing dplyr, it is easy. See A second look at group_by\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n   dplyr::filter(arr_delay &gt;0 ) |&gt; \n   group_by(dest, flight) |&gt; \n   summarise(total_delay_flight=sum(arr_delay)) |&gt; # result is a grouped tibble\n   mutate(total_delay_dest=sum(total_delay_flight), \n          delay_ratio= total_delay_flight/total_delay_dest)  |&gt; \n  filter(dest %in% c('LAX', 'ATL'), delay_ratio &gt; .02)   \n\n`summarise()` has grouped output by 'dest'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 20 × 5\n# Groups:   dest [2]\n   dest  flight total_delay_flight total_delay_dest delay_ratio\n   &lt;chr&gt;  &lt;int&gt;              &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt;\n 1 ATL      348               7000           300299      0.0233\n 2 ATL      926               9908           300299      0.0330\n 3 ATL      947               8670           300299      0.0289\n 4 ATL     1147               6968           300299      0.0232\n 5 ATL     1499               6155           300299      0.0205\n 6 ATL     1942               6224           300299      0.0207\n 7 ATL     2042              11583           300299      0.0386\n 8 ATL     4705               6458           300299      0.0215\n 9 LAX       21               8378           203226      0.0412\n10 LAX      119               4685           203226      0.0231\n11 LAX      133               5644           203226      0.0278\n12 LAX      169               6560           203226      0.0323\n13 LAX      181               8228           203226      0.0405\n14 LAX      185               5925           203226      0.0292\n15 LAX      413               6271           203226      0.0309\n16 LAX      415              10102           203226      0.0497\n17 LAX      523               5055           203226      0.0249\n18 LAX      535               6030           203226      0.0297\n19 LAX      771               6963           203226      0.0343\n20 LAX     2363               4273           203226      0.0210\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nWe have used the fact that the output of summarise(., total_delay_flight=sum(arr_delay)) is still a grouped dataframe/tibble (with grouping variable dest). At the following line, mutate performs aggregation with sum() in a groupwise manner, that is per group, and the result of the aggregation is appended to each row of the group, because we use mutate instead of summarize.\nWITH R AS (\n  SELECT dest, \n         flight, \n         SUM(arr_delay)  AS  total_delay_per_flight\n  FROM flights\n  WHERE arr_delay &gt; 0 \n  GROUP BY dest, flight\n) \n\nSELECT \n  dest, flight, \n  total_delay_per_flight, \n  SUM(total_delay_per_flight) OVER w_per_dest AS total_delay_per_dest \nFROM \n  R\nWINDOW w_per_dest AS (PARTITION BY dest) ;\n\n\n\n\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the delay of a flight is related to the delay of the immediately preceding flight.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n    group_by(origin)  |&gt; \n    arrange(year,month, day, sched_dep_time, .by_group=T) |&gt;   # .by_group -&gt; see documentation of arrange\n    mutate(prev_delay=lag(dep_delay, n=1L), prev_origin=lag(origin, n=1L)) |&gt; \n    slice_head(n=6L) |&gt;    # inspect the result \n    select(origin, prev_origin, dep_delay, prev_delay, everything())\n\n# A tibble: 18 × 21\n# Groups:   origin [3]\n   origin prev_origin dep_delay prev_delay  year month   day dep_time\n   &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n 1 EWR    &lt;NA&gt;                2         NA  2013     1     1      517\n 2 EWR    EWR                -4          2  2013     1     1      554\n 3 EWR    EWR                -5         -4  2013     1     1      555\n 4 EWR    EWR                -2         -5  2013     1     1      558\n 5 EWR    EWR                -1         -2  2013     1     1      559\n 6 EWR    EWR                 1         -1  2013     1     1      601\n 7 JFK    &lt;NA&gt;                2         NA  2013     1     1      542\n 8 JFK    JFK                -1          2  2013     1     1      544\n 9 JFK    JFK                 0         -1  2013     1     1      559\n10 JFK    JFK                -3          0  2013     1     1      557\n11 JFK    JFK                -2         -3  2013     1     1      558\n12 JFK    JFK                -2         -2  2013     1     1      558\n13 LGA    &lt;NA&gt;                4         NA  2013     1     1      533\n14 LGA    LGA                -6          4  2013     1     1      554\n15 LGA    LGA                -3         -6  2013     1     1      557\n16 LGA    LGA                -2         -3  2013     1     1      558\n17 LGA    LGA                -1         -2  2013     1     1      559\n18 LGA    LGA                 0         -1  2013     1     1      600\n# ℹ 13 more variables: sched_dep_time &lt;int&gt;, arr_time &lt;int&gt;,\n#   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n#   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\n\n\n\n\n\nlag() is an example of window function. If we were using SQL, we would define a WINDOW using an expression like\nWINDOW w As (PARTITION BY origin ORDER BY year, month, day, sched_dep_time)\nSomething still needs fixing here: some flights never took off (is.na(dep_time)). Should they be sided out? assigned an infinite departure delay?\n\n\n\n\nLook at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodedf &lt;- flights |&gt; \n    filter(!is.na(air_time), air_time &gt; 0) |&gt; \n    mutate(speed= (distance * 1.852)/ (air_time/60)) \n      \np1 &lt;- df |&gt; \n    ggplot() +\n        aes(x=origin, y=speed) +\n        geom_boxplot()\n\np2 &lt;- df |&gt; \n    ggplot() +\n        aes(x=speed) +\n        geom_histogram(color=\"black\", fill=\"white\", alpha=.2)\n\n\npatchwork::wrap_plots(p1 + p2 )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodep2 + geom_rug(alpha=.5) +\n    facet_zoom(xlim=c(900,1500))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nConsider all flights with average speed above \\(950\\text{km/h}\\) as suspicious.\nLet us visualize destinations and origins of the speedy flights.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodedf |&gt; \n    filter(speed&gt;950) |&gt; \n    mutate(dest=as_factor(dest)) |&gt; \n    ggplot() +\n        aes(x=fct_infreq(dest)) +\n        geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\nSJU, BQN are located in Puerto Rico. LAX is Los Angeles airport. STT is located in Virgin Islands.\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodedf |&gt; \n    filter(speed&gt;950, ! dest %in% c('SJU', 'BQN')) |&gt; \n    mutate(dest=as_factor(dest)) |&gt; \n    ggplot() +\n        aes(x=fct_infreq(dest), fill=origin) +\n        geom_bar() +\n        labs(x=\"Destination\", \n        title=\"Distribution of speedy flights\",\n        subtitle=\"Excluding SJU and BQN\")\n\n\n\n\n\n\n\n\n\n\n\nFind all destinations that are flown by at least two carriers. Use that information to rank the carriers.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\n\n\n\nFor each plane, count the number of flights before the first delay greater than 1 hour.\n\n\n\n\n\n\n\nAssume a plane is characterized by tailnum. Some flights have no tailnum. We ignore them.\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodeflights |&gt; \n  filter(!is.na(tailnum)) |&gt; \n  group_by(tailnum) |&gt; \n  arrange(year, month, day, sched_dep_time, .bygroup=T) |&gt; \n  mutate(rnk=row_number(), tot=n()) |&gt; \n  filter(is.na(arr_time) | arr_delay &gt;=60) |&gt; \n  slice_head(n=1)  |&gt; \n  mutate(rel_rnk = rnk/tot) |&gt; \n  select(tailnum, rnk, tot,  rel_rnk, carrier) |&gt; \n  arrange(desc(tot), desc(rel_rnk)) |&gt; \n  ungroup() \n\n# A tibble: 3,454 × 5\n   tailnum   rnk   tot rel_rnk carrier\n   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1 N725MQ     17   575 0.0296  MQ     \n 2 N722MQ     31   513 0.0604  MQ     \n 3 N723MQ     24   507 0.0473  MQ     \n 4 N711MQ     24   486 0.0494  MQ     \n 5 N713MQ      4   483 0.00828 MQ     \n 6 N258JB      8   427 0.0187  B6     \n 7 N298JB     17   407 0.0418  B6     \n 8 N353JB     34   404 0.0842  B6     \n 9 N351JB     10   402 0.0249  B6     \n10 N735MQ      3   396 0.00758 MQ     \n# ℹ 3,444 more rows"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#references",
    "href": "core/labs-solutions/lab-tables.html#references",
    "title": "Tables manipulation II",
    "section": "References",
    "text": "References\n\nData transformation cheatsheet\nR4Data Science Tidy\nBenchmarking\ndplyr and vctrs\nPosts on dplyr\nWindow functions on dplyr"
  },
  {
    "objectID": "core/labs-solutions/lab-ca.html",
    "href": "core/labs-solutions/lab-ca.html",
    "title": "LAB: Correspondance Analysis",
    "section": "",
    "text": "M1 MIDS/MFA/LOGOS\n\n\nUniversité Paris Cité\n\n\nAnnée 2025\n\n\nCourse Homepage\n\n\nMoodle\nBesides the usual packages (tidyverse, …), we shall require FactoMineR and related packages.\nCode\nstopifnot(\n  require(FactoMineR),\n  require(factoextra),\n  require(FactoInvestigate)\n)"
  },
  {
    "objectID": "core/labs-solutions/lab-ca.html#the-mortality-dataset",
    "href": "core/labs-solutions/lab-ca.html#the-mortality-dataset",
    "title": "LAB: Correspondance Analysis",
    "section": "The mortality dataset",
    "text": "The mortality dataset\nThe goal is to investigate a possible link between age group and Cause of death. We work with dataset mortality from package FactoMineR\n\n\nCode\ndata(\"mortality\", package = \"FactoMineR\")\n\n\n\nCode\n#help(mortality)\n\n\nA data frame with 62 rows (the different Causes of death) and 18 columns. Each column corresponds to an age interval (15-24, 25-34, 35-44, 45-54, 55-64, 65-74, 75-84, 85-94, 95 and more) in a year. The 9 first columns correspond to data in 1979 and the 9 last columns to data in 2006. In each cell, the counts of deaths for a Cause of death in an age interval (in a year) is given.\n\n\nSource\n\nCentre d’épidemiologie sur les Causes de décès médicales\n\n\nSee also EuroStat:\n\nCauses of death (hlth_cdeath) Reference Metadata in Single Integrated Metadata Structure (SIMS)\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nRead the documentation of the mortality dataset. Is this a sample? an aggregated dataset?\nIf you consider mortality as an agregated dataset, can you figure out the organization of the sample mortality was built from?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nThe mortality dataset is an aggregated dataset. It has been built from two samples. Each sample was built from the collection of death certificates from one calendar year in France (years 1999 and 2006). From each death certificate, two categorical pieces of information were extracted: age group of the deceased and a Cause of death. Each sample was then grouped by age group and Cause of death and counts were computed. This defines a two-ways contingency table in long form. The contingency table in wide form is obtained by pivoting: pick column names from column age group and values from counts. Column Cause of depth provide row names.\nThe final form of the dataset is obtained by concatenating the two contingency tables along the second axis.\n\n\nCode\nmortality &lt;- mortality |&gt; \n    mutate(Cause = rownames(mortality)) |&gt;\n    mutate(Cause = factor(Cause)) |&gt;\n    relocate(Cause)\n\n\n\n\nCode\nmy_gt &lt;- function(gt_tbl){\n  gt_tbl |&gt;\n  tab_style(\n    style = list(\n      \"font-variant: small-caps;\"\n    ),\n    locations = cells_body(columns = Cause)\n  ) |&gt;\n  gt::cols_align(\n    align=\"left\",\n    columns=Cause\n  ) \n\n}\n\n\n\n\nCode\nmortality |&gt;\n    select(Cause, ends_with('(06)')) |&gt; \n    sample_n(10) |&gt;\n    gt::gt() |&gt;\n    my_gt()\n\n\n\n\n\n\n\n\nCause\n15-24 (06)\n25-34 (06)\n35-44 (06)\n45-54 (06)\n55-64 (06)\n65-74 (06)\n75-84 (06)\n85-94 (06)\n95 and more (06)\n\n\n\n\nKidney and urethra disease\n3\n10\n35\n106\n266\n655\n2432\n2454\n545\n\n\nAddiction to prescription medication\n18\n77\n72\n15\n4\n2\n1\n0\n0\n\n\nBlood and hematopoietic disorders\n20\n22\n43\n94\n159\n259\n642\n670\n213\n\n\nMalignant tumour of the larynx, trachea, bronchus and lungs\n0\n38\n681\n4059\n7285\n8026\n7678\n1869\n128\n\n\nOther ill-defined symptoms and conditions\n42\n126\n346\n752\n1077\n1746\n4943\n7920\n4253\n\n\nUnknown or unspecified causes\n227\n476\n928\n1517\n1585\n1487\n2446\n2298\n872\n\n\nHomicides\n52\n83\n65\n63\n42\n27\n21\n6\n0\n\n\nViral hepatitis\n0\n9\n76\n117\n94\n143\n224\n61\n6\n\n\nMalignant ovarian tumour\n4\n12\n63\n302\n603\n863\n1054\n402\n38\n\n\nRhumatoid arthritis and osteoarthritis\n0\n1\n0\n4\n14\n99\n231\n176\n54"
  },
  {
    "objectID": "core/labs-solutions/lab-ca.html#elementary-statistics-and-table-wrangling",
    "href": "core/labs-solutions/lab-ca.html#elementary-statistics-and-table-wrangling",
    "title": "LAB: Correspondance Analysis",
    "section": "Elementary statistics and table wrangling",
    "text": "Elementary statistics and table wrangling\nBefore proceeding to Correspondence Analysis (CA), let us tidy up the table and draw some elementary plots.\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nStart by partially pivoting mortality, so as to obtain a tibble with columns Cause, year, while keeping all columns named after age groups (tidy up the data so as to obtain a tibble in partially long format).\nUse rowwise() and sum(c_cross()) so as to compute the total number of deaths per year and Cause in column total. This allows to mimic rowSums() inside a pipeline. Column grand_total is computed using a window function over grouping by Cause.\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nmortality_long &lt;- mortality  |&gt; \n  pivot_longer(\n    cols=-Cause,\n    cols_vary=\"slowest\",\n    names_to=c(\".value\", \"year\"),\n    names_pattern=\"([\\\\w\\\\- ]*) \\\\(([0-9]{2})\\\\)\"\n  )  |&gt; \n  mutate(year=ifelse(year=='06', 2006, 1979)) |&gt; \n  rowwise() |&gt; \n  mutate(total_year=sum(c_across(-c(Cause, year)))) |&gt; \n  group_by(Cause) |&gt; \n  mutate(grand_total = sum(total_year)) |&gt; \n  ungroup()\n\n\n\n\nCode\nmortality_long |&gt;\n slice_sample(n=10) |&gt;\n gt::gt() |&gt;\n my_gt() |&gt;\n gt::tab_caption(\"A sample of rows from Mortality table in long form\")\n\n\n\n\n\n\nA sample of rows from Mortality table in long form\n\n\nCause\nyear\n15-24\n25-34\n35-44\n45-54\n55-64\n65-74\n75-84\n85-94\n95 and more\ntotal_year\ngrand_total\n\n\n\n\nEvents of undetermined intention\n1979\n297\n382\n296\n329\n239\n279\n221\n66\n1\n2110\n2558\n\n\nGastroduodenal ulcer\n2006\n3\n1\n13\n38\n48\n99\n264\n244\n55\n765\n2981\n\n\nCerebrovascular disease\n2006\n35\n75\n311\n902\n1575\n3719\n12172\n11385\n2464\n32638\n98795\n\n\nMalignant tumour of the of the colon\n1979\n10\n39\n114\n556\n1130\n2898\n3541\n1257\n48\n9593\n21753\n\n\nMeningococal disease\n2006\n8\n0\n1\n1\n0\n4\n2\n0\n0\n16\n60\n\n\nDiabetes\n1979\n17\n43\n61\n232\n585\n2064\n3044\n1018\n47\n7111\n18083\n\n\nOther tumours\n2006\n39\n45\n141\n300\n564\n1010\n2192\n1713\n318\n6322\n10399\n\n\nMalignant tumour of the of the colon\n2006\n4\n17\n135\n538\n1421\n2572\n4641\n2486\n346\n12160\n21753\n\n\nOther ill-defined symptoms and conditions\n1979\n41\n75\n116\n276\n511\n1635\n5641\n7330\n1500\n17125\n38330\n\n\nHomicides\n1979\n92\n116\n115\n65\n50\n33\n35\n10\n0\n516\n875\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nA truly tidy version of the dataset can be obtained from further pivoting.\n\n\nCode\nmortality_tidy &lt;- mortality_long |&gt; \n  pivot_longer(\n    cols=-c(year,Cause,total_year, grand_total),\n    cols_vary=\"slowest\",\n    names_to=c(\"age_range\"),\n    values_to=c(\"#deaths\")\n) |&gt;\n  mutate(age_range = factor(age_range, levels=sort(unique(age_range)),ordered=T))\n\n\n\n\nCode\nmortality_tidy |&gt;\n  sample_n(5) |&gt;\n  gt::gt()\n\n\n\n\n\n\n\n\nCause\nyear\ntotal_year\ngrand_total\nage_range\n#deaths\n\n\n\n\nOther ill-defined symptoms and conditions\n2006\n21205\n38330\n25-34\n126\n\n\nMalignant ovarian tumour\n2006\n3341\n5661\n85-94\n402\n\n\nRoad accidents\n2006\n4648\n15255\n95 and more\n8\n\n\nMeningococal disease\n1979\n44\n60\n85-94\n0\n\n\nChronic liver disease\n2006\n7669\n23596\n35-44\n453\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nBuild a bar plot to display the importance of Causes of deaths in France in years 1979 and 2006\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nth &lt;- theme_get()\n(\nmortality_long |&gt; \n  mutate(Cause=fct_reorder(Cause, desc(grand_total))) |&gt;\n  mutate(year=as_factor(year)) |&gt;\n  ggplot() +\n  scale_fill_discrete() +\n  aes(x=Cause, \n      y=total_year, \n      fill=year) +\n  geom_col(position=position_dodge()) +\n  theme(\n    legend.position=\"none\",\n    axis.text.x=element_blank(), #remove x axis labels\n    axis.ticks.x=element_blank(), #remove x axis ticks\n  ) +\n  labs(\n    title = \"Causes of death, France, 1979, 2006\",\n    subtitle= \"Raw counts\"\n  ) +\n  xlab(label=NULL)\n) |&gt;\n  plotly::ggplotly()  \n\n\n\n\n\n\nCode\noth &lt;- theme_set(th)\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCompute and display the total number of deaths in France in years 1979 and 2006.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nmortality_long |&gt; \n  group_by(year) |&gt; \n  summarise(total_deaths = sum(total_year)) |&gt;\n  gt::gt() |&gt;\n  gt::cols_label(\n    year= \"Year\", \n    total_deaths = \"#Deaths\") |&gt;\n  gt::tab_caption(\"Mortality in France\")\n\n\n\n\n\n\nMortality in France\n\n\nYear\n#Deaths\n\n\n\n\n1979\n529974\n\n\n2006\n510921\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCompute the marginal counts for each year (1979, 2006). Compare.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nCounts have already been computed above.\n\n\nCode\nmortality_long |&gt; \n  select(Cause, year, total_year, grand_total) |&gt; \n  pivot_wider(\n    id_cols=c(Cause, grand_total), \n    names_from = year, \n    values_from = total_year) |&gt; \n  rename(Total=grand_total) |&gt; \n  arrange(desc(Total)) |&gt;\n  gt::gt() |&gt;\n  my_gt()\n\n\n\n\n\n\n\n\nCause\nTotal\n1979\n2006\n\n\n\n\nCerebrovascular disease\n98795\n66157\n32638\n\n\nOther heart disease\n97297\n54105\n43192\n\n\nIschemic cardiomyopathy\n88338\n49532\n38806\n\n\nOther illnesses relating to circulation\n61937\n31218\n30719\n\n\nMalignant tumour of the larynx, trachea, bronchus and lungs\n50604\n20840\n29764\n\n\nOther malignent tumours\n48809\n23262\n25547\n\n\nOther diseases of the nervous system and sensory organs\n38891\n12056\n26835\n\n\nOther ill-defined symptoms and conditions\n38330\n17125\n21205\n\n\nOther digestive conditions\n32697\n18092\n14605\n\n\nOther respiratory ailments\n26339\n14197\n12142\n\n\nUnknown or unspecified causes\n26192\n14356\n11836\n\n\nChronic liver disease\n23596\n15927\n7669\n\n\nOther accidents\n23353\n10921\n12432\n\n\nMalignant tumour of the of the colon\n21753\n9593\n12160\n\n\nSuicides\n20337\n9952\n10385\n\n\nMalignant tumour of the breast\n20236\n8605\n11631\n\n\nMalignant neplasm of the lymphatic and hematopoietic tissues\n20000\n7589\n12411\n\n\nDiabetes\n18083\n7111\n10972\n\n\nFalls\n17711\n12503\n5208\n\n\nOther chronic respiritory illnesses\n17331\n9680\n7651\n\n\nOther psychological and behavioural disorders\n17160\n3749\n13411\n\n\nMalignant tumour of the prostate\n15514\n6577\n8937\n\n\nRoad accidents\n15255\n10607\n4648\n\n\nPneumonia\n14700\n5057\n9643\n\n\nMalignant tumour of the stomach\n13783\n9020\n4763\n\n\nOther endocrinological, metabolic and nutritional conditions\n13665\n6030\n7635\n\n\nKidney and urethra disease\n13613\n7107\n6506\n\n\nMalignant tumour of the of the pancreas\n12851\n4588\n8263\n\n\nOther infectious diseases and parasites\n11466\n4045\n7421\n\n\nMalignant tumour of the liver and intrahepatic biliary tract\n11452\n4001\n7451\n\n\nOther tumours\n10399\n4077\n6322\n\n\nMalignant tumour of the lip, pharynx and mouth\n9722\n5606\n4116\n\n\nMalignant tumour of the oesophogus\n9267\n5430\n3837\n\n\nMalignant tumour of the rectum and anus\n9246\n4980\n4266\n\n\nMalignant tumour of the bladder\n8322\n3633\n4689\n\n\nAlcohol abuse and alcohol-related psychosis\n6327\n3371\n2956\n\n\nMalignant ovarian tumour\n5661\n2320\n3341\n\n\nMalignant tumour in other parts of the uterus\n5263\n2940\n2323\n\n\nMalignant tumour of the kidney\n5205\n2101\n3104\n\n\nOther genito-urinary diseases\n4387\n2552\n1835\n\n\nBlood and hematopoietic disorders\n4299\n2177\n2122\n\n\nOther diseases of the osteo-articular system + muscles and connecting tissue\n3935\n1025\n2910\n\n\nInfections of the skin and sub-cutaneous cellular tissue\n3439\n1649\n1790\n\n\nGastroduodenal ulcer\n2981\n2216\n765\n\n\nTuberculosis\n2797\n2070\n727\n\n\nOther external injury and poisoning\n2731\n1023\n1708\n\n\nEvents of undetermined intention\n2558\n2110\n448\n\n\nAsthma\n2488\n1495\n993\n\n\nMalignant melanoma\n2205\n658\n1547\n\n\nMalignant tumour of the cervix\n1527\n824\n703\n\n\nAccidental poisoning\n1510\n503\n1007\n\n\nRhumatoid arthritis and osteoarthritis\n1284\n705\n579\n\n\nInfluenza\n1166\n1051\n115\n\n\nViral hepatitis\n1059\n329\n730\n\n\nHomicides\n875\n516\n359\n\n\nOther congenital defects and chromosomal abnormalities\n592\n145\n447\n\n\nCongenital defects of the circulatory system\n540\n275\n265\n\n\nMeningitis\n481\n362\n119\n\n\nAddiction to prescription medication\n222\n33\n189\n\n\nComplications in pregnancy and childbirth\n150\n91\n59\n\n\nCongenital defects of the nervous system\n109\n61\n48\n\n\nMeningococal disease\n60\n44\n16"
  },
  {
    "objectID": "core/labs-solutions/lab-ca.html#correspondance-analysis",
    "href": "core/labs-solutions/lab-ca.html#correspondance-analysis",
    "title": "LAB: Correspondance Analysis",
    "section": "Correspondance Analysis",
    "text": "Correspondance Analysis\n\n\n\n\n\n\nImportantCA executive summary\n\n\n\n\nStart from a 2-way contingency table \\(X\\) with \\(\\sum_{i,j} X_{i,j}=N\\)\nNormalize \\(P = \\frac{1}{N}X\\) (correspondance matrix)\nLet \\(r\\) (resp. \\(c\\)) be the row (resp. column) wise sums vector\nLet \\(D_r=\\text{diag}(r)\\) denote the diagonal matrix with row sums of \\(P\\) as coefficients\nLet \\(D_c=\\text{diag}(c)\\) denote the diagonal matrix with column sums of \\(P\\) as coefficients\nThe row profiles matrix is \\(D_r^{-1} \\times P\\)\nThe standardized residuals matrix is \\(S = D_r^{-1/2} \\times \\left(P - r c^\\top\\right) \\times D_c^{-1/2}\\)\n\nCA consists in computing the SVD of the standardized residuals matrix \\(S =  U  \\times D \\times V^\\top\\)\nFrom the SVD, we get\n\n\\(D_r^{-1/2} \\times U\\) standardized coordinates of rows\n\\(D_c^{-1/2} \\times V\\) standardized coordinates of columns\n\\(D_r^{-1/2} \\times U \\times D\\) principal coordinates of rows\n\\(D_c^{-1/2} \\times V \\times D\\) principal coordinates of columns\nSquared singular values: the principal inertia\n\nWhen calling svd(.), the argument should be \\[D_r^{1/2}\\times \\left(D_r^{-1} \\times P \\times D_c^{-1}- \\mathbf{I}\\times \\mathbf{I}^\\top  \\right)\\times D_c^{1/2}= D_r^{-1/2}\\times \\left( P - r \\times c^\\top  \\right)\\times D_c^{-1/2}\\]\n\n\n\n\n\n\n\n\nImportantCA and extended SVD\n\n\n\nAs \\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^\\top = (D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^\\top\\]\n\\((D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^\\top\\) is the extended SVD of \\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^\\top\\] with respect to \\(D_r\\) and \\(D_c\\)\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPerform CA on the two contingency tables.\n\n\n\n\n\n\n\n\nYou may use FactoMineR::CA(). It is interesting to compute the correspondence analysis in your own way, by preparing the matrix that is handled to svd() and returning a named list containing all relevant information.\n\nDo the Jedi and Sith build their own light sabers? Jedi do. It’s a key part of the religion to have a kyber crystal close to you, to build the saber through the power of the force creating a blade unique and in tune with them\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nlst_ca &lt;- list()\n\nfor (y in c('79', '06')) {\n  lst_ca[[y]] &lt;- mortality |&gt; \n    select(ends_with(glue('({y})'))) |&gt; \n    FactoMineR::CA(ncp=8, graph = F)\n}\n\n\n\n\nCode\nlst &lt;- map(c('79', '06'), \n             \\(x) select(mortality, ends_with(glue('({x})'))) |&gt;\n             FactoMineR::CA(ncp=8, graph = F)\n           )\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nIf you did use FactoMineR::CA(), explain the organization of the result.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nThe result of FactoMineR::CA(...) is a named and nested list with five elements:\n\neig\n\na matrix/array containing enough information to build a screeplot.\n\ncall\n\na list of 9, containing the call to CA(), an object of type language, telling (in principle) the user how CA() was called. However, this is a quoted expression. Here we need to guess the value of y in the calling environment understand what’s going on.\n\n\n\n\nCode\nlst_ca[[1]]$call$call\n\n\nFactoMineR::CA(X = select(mortality, ends_with(glue(\"({y})\"))), \n    ncp = 8, graph = F)\n\n\nElement call also contains the table margin distributions marge.col and marge.row. The truncation rank ncp (number of components) can be assigned before computing the SVD (default value is 5). Element \\(X\\) stores the contingency table that was effectively used for computing Correpondence Analysis.\n\nrow\n\nInformation gathered from SVD to facilitate row profiles analysis.\n\ncol\n\na list structured in the same way as element row. Used for column profiles analysis\n\nsvd\n\na list of 3, just as the resuld of svd() containing the singular values, the left and right singular vectors of matrix \\(...\\)\n\n\n\n\n\n\n\n\n\n\nIn principle, all relevant information can be gathered from components svd, call.marge.row, and call.marge.col."
  },
  {
    "objectID": "core/labs-solutions/lab-ca.html#screeplots",
    "href": "core/labs-solutions/lab-ca.html#screeplots",
    "title": "LAB: Correspondance Analysis",
    "section": "Screeplots",
    "text": "Screeplots\n\n\n\n\n\n\nNoteQuestion\n\n\n\nDraw screeplots. Why are they useful? Comment briefly.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nca_79 &lt;- lst_ca[[1]]\n\nca_79$eig |&gt; \n  as_tibble() |&gt; \n  mutate(across(where(is.numeric), ~ round(.x, digits=2))) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n\neigenvalue\npercentage of variance\ncumulative percentage of variance\n\n\n\n\n0.29\n61.00\n61.00\n\n\n0.14\n28.98\n89.98\n\n\n0.03\n6.13\n96.12\n\n\n0.01\n2.86\n98.98\n\n\n0.00\n0.73\n99.70\n\n\n0.00\n0.17\n99.88\n\n\n0.00\n0.09\n99.97\n\n\n0.00\n0.03\n100.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nScreeplot\n\n\nCode\nca_79$eig |&gt; \n  as_tibble() |&gt; \n  rownames_to_column(var=\"PC\") |&gt; \n  rename(percent=eigenvalue, cumulative=`cumulative percentage of variance`) |&gt; \n  ggplot() + \n  aes(x=PC, y=percent, label=round(cumulative,2)) +\n  geom_text(angle=45, vjust=-1, hjust=-0.1) + \n  geom_col(fill=NA, colour=\"black\") +\n  ylab(\"Squared singular values\") +\n  ylim(c(0, .4)) +\n  labs(\n    title=\"Screeplot for CA\",\n    subtitle = \"Mortality 1979: Age Group versus Causes of Death\"\n  )"
  },
  {
    "objectID": "core/labs-solutions/lab-ca.html#row-profiles-analysis",
    "href": "core/labs-solutions/lab-ca.html#row-profiles-analysis",
    "title": "LAB: Correspondance Analysis",
    "section": "Row profiles analysis",
    "text": "Row profiles analysis\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPerform row profiles analysis.\nWhat are the classical plots? How can you build them from the output of FactoMiner::CA?\nBuild the table of row contributions (the so-called \\(\\cos^2\\))\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nAttribute row of objects of class CA (exported from FactoMineR) is the starting point of any row profiles analysis.\n\n\nCode\nca_79_row &lt;- ca_79$row\n\n\nAttribute row is a named list made of \\(4\\) components.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\ncoord\n\na matrix with named rows and columns. The number of rows of coord matches the number of rows of the contingency table (here, the number of possible death Causes). The number of columns matches the rank of the truncated SVD that underlies Correspondance Analysis. Here it is \\(5\\) which also the rank of the standardized contingency table.\n\n\n\nThe row principal coordinates are the principal coordinates of each row profile in terms of the principal component.\n\nThe columns of coord are pairwise orthogonal in the inner product space defined by diag(call$marge.row) (which embodies the marginal probabilities of the so-called Causes of deaths)\n\n\nCode\nx &lt;- ca_79$row$coord\nr &lt;- ca_79$call$marge.row\n\nA &lt;- round(t(x) %*% diag(r) %*% x, 2)\n\nis_diagonal &lt;- function (A, tol=1e-2){\n  norm(diag(diag(A))-A, type='F') &lt;= tol\n}\n\n# We expect A to be diagonal\nis_diagonal(A)\n\n\n[1] TRUE\n\n\nWe can recover row$coord from the left singular vectors and the singular values:\n\n\nCode\nwith(ca_79,   \n  norm(row$coord - with(svd, U %*% diag(vs[1:ca_79$call$ncp])), 'F')\n)\n\n\n[1] 0\n\n\n\n\nCode\nprep_rows &lt;- ca_79_row$coord |&gt; \n  as_tibble() |&gt; \n  mutate(name= rownames(ca_79_row$coord)) |&gt; \n  relocate(name) |&gt; \n  mutate(prop=r, inertia=ca_79_row$inertia) \n\nprep_rows |&gt; \n  mutate(across(where(is.numeric), \\(x) round(x,2))) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n\nname\nDim 1\nDim 2\nDim 3\nDim 4\nDim 5\nDim 6\nDim 7\nDim 8\nprop\ninertia\n\n\n\n\nAccidental poisoning\n1.53\n0.52\n0.04\n-0.17\n0.01\n-0.03\n-0.01\n-0.05\n0.00\n0.00\n\n\nAddiction to prescription medication\n1.75\n0.44\n-0.12\n0.27\n-0.12\n-0.24\n0.19\n0.22\n0.00\n0.00\n\n\nAlcohol abuse and alcohol-related psychosis\n0.65\n-0.74\n0.44\n-0.08\n-0.17\n0.09\n0.02\n-0.02\n0.01\n0.01\n\n\nAsthma\n-0.01\n-0.10\n-0.12\n-0.05\n0.02\n0.00\n0.05\n0.03\n0.00\n0.00\n\n\nBlood and hematopoietic disorders\n-0.08\n0.07\n-0.07\n-0.05\n-0.04\n-0.02\n-0.01\n0.03\n0.00\n0.00\n\n\nCerebrovascular disease\n-0.32\n0.17\n-0.06\n-0.04\n-0.06\n-0.01\n0.00\n0.00\n0.12\n0.02\n\n\nChronic liver disease\n0.49\n-0.83\n0.30\n0.09\n-0.08\n0.04\n-0.01\n-0.01\n0.03\n0.03\n\n\nComplications in pregnancy and childbirth\n3.53\n1.80\n0.28\n-2.09\n0.99\n-0.77\n0.15\n0.06\n0.00\n0.00\n\n\nCongenital defects of the circulatory system\n1.94\n0.54\n0.01\n-0.10\n0.11\n-0.01\n-0.08\n0.11\n0.00\n0.00\n\n\nCongenital defects of the nervous system\n2.36\n0.98\n-0.23\n0.47\n-0.07\n-0.38\n0.23\n-0.07\n0.00\n0.00\n\n\nDiabetes\n-0.21\n-0.05\n-0.24\n-0.07\n0.02\n0.02\n0.03\n0.01\n0.01\n0.00\n\n\nEvents of undetermined intention\n1.71\n0.35\n0.24\n-0.46\n0.05\n-0.03\n0.01\n-0.01\n0.00\n0.01\n\n\nFalls\n-0.34\n0.49\n0.27\n0.07\n-0.02\n-0.05\n-0.05\n-0.02\n0.02\n0.01\n\n\nGastroduodenal ulcer\n-0.11\n-0.14\n-0.08\n-0.03\n-0.03\n0.01\n0.01\n-0.01\n0.00\n0.00\n\n\nHomicides\n2.19\n0.62\n0.35\n-0.72\n0.02\n0.22\n-0.18\n0.08\n0.00\n0.01\n\n\nInfections of the skin and sub-cutaneous cellular tissue\n-0.41\n0.40\n0.11\n0.02\n-0.06\n-0.03\n-0.01\n-0.02\n0.00\n0.00\n\n\nInfluenza\n-0.41\n0.51\n0.28\n0.10\n0.06\n0.02\n-0.02\n-0.02\n0.00\n0.00\n\n\nIschemic cardiomyopathy\n-0.14\n-0.18\n-0.17\n-0.02\n0.02\n0.02\n0.01\n-0.01\n0.09\n0.01\n\n\nKidney and urethra disease\n-0.33\n0.23\n0.00\n-0.02\n-0.04\n-0.01\n-0.01\n0.02\n0.01\n0.00\n\n\nMalignant melanoma\n0.63\n-0.34\n0.25\n-0.32\n0.06\n0.07\n-0.06\n0.02\n0.00\n0.00\n\n\nMalignant neplasm of the lymphatic and hematopoietic tissues\n0.33\n-0.18\n-0.10\n-0.11\n0.07\n0.02\n0.02\n-0.03\n0.01\n0.00\n\n\nMalignant ovarian tumour\n0.28\n-0.59\n0.07\n0.09\n-0.01\n-0.03\n0.01\n-0.01\n0.00\n0.00\n\n\nMalignant tumour in other parts of the uterus\n0.08\n-0.41\n0.01\n0.06\n0.01\n0.00\n-0.01\n0.05\n0.01\n0.00\n\n\nMalignant tumour of the bladder\n-0.13\n-0.25\n-0.23\n0.01\n0.08\n0.01\n0.00\n0.02\n0.01\n0.00\n\n\nMalignant tumour of the breast\n0.21\n-0.49\n0.17\n0.03\n-0.06\n0.03\n-0.02\n0.02\n0.02\n0.01\n\n\nMalignant tumour of the cervix\n0.42\n-0.60\n0.29\n-0.07\n-0.16\n0.16\n-0.07\n0.05\n0.00\n0.00\n\n\nMalignant tumour of the kidney\n0.07\n-0.45\n-0.10\n0.07\n0.11\n-0.02\n-0.03\n0.01\n0.00\n0.00\n\n\nMalignant tumour of the larynx, trachea, bronchus and lungs\n0.21\n-0.69\n-0.01\n0.14\n0.08\n-0.04\n-0.01\n-0.01\n0.04\n0.02\n\n\nMalignant tumour of the lip, pharynx and mouth\n0.41\n-0.82\n0.31\n0.21\n-0.16\n-0.07\n0.09\n0.00\n0.01\n0.01\n\n\nMalignant tumour of the liver and intrahepatic biliary tract\n0.06\n-0.44\n-0.13\n0.05\n0.13\n-0.05\n-0.02\n0.01\n0.01\n0.00\n\n\nMalignant tumour of the oesophogus\n0.21\n-0.69\n0.07\n0.17\n0.01\n-0.06\n0.02\n0.03\n0.01\n0.01\n\n\nMalignant tumour of the of the colon\n-0.13\n-0.18\n-0.17\n-0.01\n0.03\n0.00\n0.00\n0.00\n0.02\n0.00\n\n\nMalignant tumour of the of the pancreas\n-0.01\n-0.37\n-0.15\n0.04\n0.10\n0.02\n-0.04\n0.01\n0.01\n0.00\n\n\nMalignant tumour of the prostate\n-0.29\n-0.01\n-0.35\n-0.09\n0.03\n0.05\n0.03\n-0.01\n0.01\n0.00\n\n\nMalignant tumour of the rectum and anus\n-0.08\n-0.27\n-0.16\n0.01\n0.09\n0.00\n-0.02\n-0.02\n0.01\n0.00\n\n\nMalignant tumour of the stomach\n-0.12\n-0.21\n-0.19\n-0.02\n0.05\n0.02\n-0.01\n0.01\n0.02\n0.00\n\n\nMeningitis\n0.63\n-0.22\n0.02\n-0.05\n0.03\n0.08\n-0.12\n0.04\n0.00\n0.00\n\n\nMeningococal disease\n1.93\n0.72\n-0.55\n0.77\n-0.09\n0.08\n-0.02\n0.05\n0.00\n0.00\n\n\nOther accidents\n1.18\n0.30\n0.01\n-0.02\n-0.02\n0.00\n0.01\n-0.01\n0.02\n0.03\n\n\nOther chronic respiritory illnesses\n-0.27\n0.06\n-0.11\n-0.01\n0.01\n0.01\n0.01\n0.01\n0.02\n0.00\n\n\nOther congenital defects and chromosomal abnormalities\n1.09\n0.19\n0.04\n-0.29\n0.13\n-0.27\n0.12\n0.01\n0.00\n0.00\n\n\nOther digestive conditions\n-0.13\n0.02\n-0.01\n-0.05\n-0.03\n0.00\n0.00\n0.00\n0.03\n0.00\n\n\nOther diseases of the nervous system and sensory organs\n-0.08\n0.12\n-0.10\n-0.04\n-0.04\n0.01\n0.01\n0.01\n0.02\n0.00\n\n\nOther diseases of the osteo-articular system + muscles and connecting tissue\n-0.03\n0.00\n-0.06\n0.00\n0.01\n0.02\n-0.01\n0.00\n0.00\n0.00\n\n\nOther endocrinological, metabolic and nutritional conditions\n-0.27\n0.23\n0.12\n0.03\n-0.02\n0.01\n-0.01\n0.00\n0.01\n0.00\n\n\nOther external injury and poisoning\n0.23\n-0.10\n-0.03\n-0.06\n0.04\n0.05\n-0.04\n-0.03\n0.00\n0.00\n\n\nOther genito-urinary diseases\n-0.37\n0.26\n-0.07\n-0.06\n-0.07\n0.00\n0.02\n0.02\n0.00\n0.00\n\n\nOther heart disease\n-0.36\n0.28\n0.07\n0.01\n-0.02\n-0.01\n-0.01\n0.00\n0.10\n0.02\n\n\nOther ill-defined symptoms and conditions\n-0.49\n0.59\n0.47\n0.18\n0.16\n0.05\n0.03\n0.01\n0.03\n0.03\n\n\nOther illnesses relating to circulation\n-0.23\n0.07\n-0.06\n-0.02\n-0.01\n0.00\n-0.01\n0.00\n0.06\n0.00\n\n\nOther infectious diseases and parasites\n-0.05\n0.01\n-0.07\n-0.04\n0.01\n0.00\n0.01\n0.01\n0.01\n0.00\n\n\nOther malignent tumours\n0.09\n-0.29\n-0.04\n0.02\n0.04\n-0.02\n-0.02\n0.00\n0.04\n0.00\n\n\nOther psychological and behavioural disorders\n-0.40\n0.36\n-0.03\n-0.07\n-0.11\n-0.02\n0.01\n-0.01\n0.01\n0.00\n\n\nOther respiratory ailments\n-0.26\n0.17\n0.04\n0.02\n0.02\n-0.01\n0.00\n-0.01\n0.03\n0.00\n\n\nOther tumours\n0.19\n-0.25\n0.01\n-0.01\n0.03\n0.00\n-0.03\n0.02\n0.01\n0.00\n\n\nPneumonia\n-0.38\n0.38\n0.17\n0.04\n-0.03\n0.00\n-0.01\n0.00\n0.01\n0.00\n\n\nRhumatoid arthritis and osteoarthritis\n-0.23\n0.02\n-0.22\n-0.08\n0.00\n0.00\n0.00\n-0.05\n0.00\n0.00\n\n\nRoad accidents\n2.47\n1.05\n-0.33\n0.36\n-0.03\n0.01\n0.00\n0.00\n0.02\n0.15\n\n\nSuicides\n1.39\n0.04\n0.31\n-0.48\n0.06\n-0.02\n0.01\n0.00\n0.02\n0.04\n\n\nTuberculosis\n0.13\n-0.34\n0.01\n-0.04\n0.01\n0.02\n-0.01\n-0.02\n0.00\n0.00\n\n\nUnknown or unspecified causes\n0.43\n0.05\n0.13\n-0.08\n0.01\n0.01\n0.01\n0.00\n0.03\n0.01\n\n\nViral hepatitis\n0.49\n-0.10\n-0.09\n0.04\n-0.02\n-0.02\n0.00\n0.00\n0.00\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\ninertia\n\na numerical vector with length matching the number of rows of coord, contrib and cos2.\n\n\n\nInertia is the way CA measures variation between row profiles. Total inertia is the \\(\\chi^2\\) statistic divided by sample size.\n\nRow inertia can be obtained by multiplying the row marginal probability by the squared Euclidean norm of the row in the principal coordinate matrix.\n\n\nCode\nwith (ca_79_row,\n  sum(abs(r* (rowSums(coord^2)) - inertia))\n)\n\n\n[1] 1.877449e-16\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\ncos2\n\nCoefficients of matrix cos2 are the share of row inertia from the corresponding cell in coord\n\n\n\n\nCode\nwith (ca_79_row,\n  norm((diag(r/inertia) %*% coord^2) - cos2, type='F')\n)\n\n\n[1] 5.432216e-16\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\ncontrib\n\n\n\n\nNot too surprisingly, coord, contrib, and cos2 share the same row names and column names.\n\n\nCode\nsum(ca_79$call$X)\n\n\n[1] 529974\n\n\nCode\nsum((rowSums(ca_79$call$X)/sum(ca_79$call$X) - r)^2)\n\n\n[1] 6.311339e-35\n\n\nThe Row Profiles are the rows of matrix R below\n\n\nCode\nP &lt;- as.matrix(with(ca_79$call, Xtot/N))\ncoord &lt;- ca_79_row$coord\ninertia &lt;- ca_79_row$inertia\n\nr &lt;- ca_79$call$marge.row\nc &lt;- colSums(P)\n\nn &lt;- nrow(P)\np &lt;- ncol(P)\n\nR &lt;- diag(r^(-1)) %*% P \n\nQ &lt;- R - matrix(1, nrow = n, ncol = n) %*% P\n\n\n\n\nCode\nM &lt;- diag(r^(-1)) %*% P %*% diag(c^(-1)) - matrix(1, nrow=n, ncol=p)\n\nn * norm(diag(r^(1/2)) %*% M %*% diag(c^(1/2)), type = \"F\")^2\n\n\n[1] 29.39279\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nWe can now display a scatterplot from component coord. This is called a Row Plot.\n\n\nCode\np_scat &lt;-  ( \n  prep_rows |&gt; \n    ggplot() +\n    aes(x=`Dim 1`, y=`Dim 2`, label=name) +\n    geom_point() +\n    coord_fixed() \n  ) \n\np_scat |&gt; plotly::ggplotly()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nWith little effort, it is possible to scale the points so as to tell the reader the relative numerical importance of each Cause of death. Coloring/filling the points using inertia also helps: high inertia rows match light-colored points.\n\n\nCode\nppp &lt;- prep_rows |&gt; \n    ggplot() +\n    aes(x=`Dim 1`, \n        y=`Dim 2`, \n        label=name, \n        size=prop, \n        fill=log10(inertia),\n        color=log10(inertia)) +\n    geom_point(alpha=0.75) +\n    scale_size_area() +\n    coord_fixed() +\n    scale_fill_viridis_c(aesthetics=c(\"fill\", \"color\"), \n                         guide=\"colorbar\", \n                         direction = 1) +\n    ggtitle(\n      \"Mortality France 1979: Row plot\"\n    )\n  \nppp |&gt; plotly::ggplotly()\n\n\n\n\n\n\nCode\n# (ca_79$row)$contrib\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPlot the result of row profile analysis using plot.CA from FactoMineR.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPerform column profiles analysis\n\n\n\n\nCode\nnames(ca_79_row)\n\n\n[1] \"coord\"   \"contrib\" \"cos2\"    \"inertia\"\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nage_group_names &lt;-  str_match(rownames(ca_79$col$coord), '([\\\\w \\\\-]*) \\\\(79\\\\)')[,2]\n\nprep_cols &lt;- ca_79$col$coord |&gt; \n  as_tibble() |&gt; \n  mutate(name= age_group_names) |&gt; \n  relocate(name) |&gt; \n  mutate(prop=c, inertia=ca_79$col$inertia)\n\n\n\n\nCode\n(\n  prep_cols |&gt; \n    ggplot() +\n    aes(x=`Dim 1`, \n        y=`Dim 2`, \n        label=name, \n        size=prop, \n        fill=log10(inertia),\n        color=log10(inertia)) +\n    geom_point(alpha=0.75) +\n    scale_size_area() +\n    coord_fixed() +\n    scale_fill_viridis_c(aesthetics=c(\"fill\", \"color\"),direction = 1) +\n    ggtitle(\n      \"Mortality France 1979: Col plot\"\n    )) |&gt; plotly::ggplotly()"
  },
  {
    "objectID": "core/labs-solutions/lab-ca.html#symmetric-plots",
    "href": "core/labs-solutions/lab-ca.html#symmetric-plots",
    "title": "LAB: Correspondance Analysis",
    "section": "Symmetric plots",
    "text": "Symmetric plots\n\n\n\n\n\n\nNoteQuestion\n\n\n\nBuild the symmetric plots (biplots) for correspondence analysis of Mortalitity data\n\n\n\nFrom the shelf\n\n\nCode\nplot.CA(ca_79)\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCode\n(\nprep_rows |&gt; \n    ggplot() +\n    aes(x=`Dim 1`, \n        y=`Dim 2`, \n        label=name, \n        size=prop, \n        fill=log10(inertia),\n        color=log10(inertia)) +\n    geom_point(alpha=0.75) +\n    scale_size_area() +\n    coord_fixed() +\n    scale_fill_viridis_c(aesthetics=c(\"fill\", \"color\"),direction = 1) +\n    geom_point(data = prep_cols,\n      aes(x=`Dim 1`, \n        y=`Dim 2`, \n        label=name, \n        size=prop, \n        fill=log10(inertia),\n        color=log10(inertia)\n      ),\n      shape=\"square\",\n      alpha=.5,      \n    )\n) |&gt; plotly::ggplotly()\n\n\nWarning in geom_point(data = prep_cols, aes(x = `Dim 1`, y = `Dim 2`, label =\nname, : Ignoring unknown aesthetics: label\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nIt is convenient to use distinct color scales for rows and columns.\n\n\nCode\n(\nprep_rows |&gt; \n    ggplot() +\n    scale_size_area() +\n    coord_fixed() +\n    aes(x=`Dim 1`, \n        y=`Dim 2`, \n        text=name, \n        size=prop, \n        fill=log10(inertia)) +\n    geom_point(alpha=0.75) +\n    scale_fill_viridis_c(option=\"D\") +\n    geom_point(data = prep_cols,\n      aes(x=`Dim 1`, \n        y=`Dim 2`, \n        text=name, \n        size=prop, \n        color=log10(inertia)\n      ),\n      shape=\"square\",\n      alpha=.5,      \n    ) +\n    scale_color_viridis_c(option=\"F\") +\n    theme_minimal(\n    )  \n)  |&gt; plotly::ggplotly()\n\n\nWarning in geom_point(data = prep_cols, aes(x = `Dim 1`, y = `Dim 2`, text =\nname, : Ignoring unknown aesthetics: text"
  },
  {
    "objectID": "core/labs-solutions/lab-ca.html#mosaicplots",
    "href": "core/labs-solutions/lab-ca.html#mosaicplots",
    "title": "LAB: Correspondance Analysis",
    "section": "Mosaicplots",
    "text": "Mosaicplots\n\n\n\n\n\n\nNoteQuestion\n\n\n\nMosaic plots provide an alternative way of exploring contingency tables. They are particularly handy when handling 2-way contingency tables.\nDraw mosaic plots for the two contingency tables living inside mortality datasets.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nmortality |&gt; \n  select(ends_with('(06)')) |&gt; \n  chisq.test() |&gt; \n  broom::glance()\n\n\nWarning in chisq.test(select(mortality, ends_with(\"(06)\"))): Chi-squared\napproximation may be incorrect\n\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1   229784.       0       488 Pearson's Chi-squared test\n\n\n\n\nCode\nmortality |&gt; \n  select(ends_with('(06)')) |&gt; \n  as.matrix() |&gt; \n  as.table() |&gt; \n  mosaicplot(color = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nAre you able to deliver an interpretation of this Correspondence Analysis?"
  },
  {
    "objectID": "core/labs-solutions/lab-ca.html#hierarchical-clusetring-of-row-profiles",
    "href": "core/labs-solutions/lab-ca.html#hierarchical-clusetring-of-row-profiles",
    "title": "LAB: Correspondance Analysis",
    "section": "Hierarchical clusetring of row profiles",
    "text": "Hierarchical clusetring of row profiles\n\n\n\n\n\n\nNoteQuestion\n\n\n\nBuild the standardized matrix for row profiles analysis. Compute the pairwise distance matrix using the \\(\\chi^2\\) distances. Should you work centered row profiles?\n\n\nWe use the weighted \\(\\ell_2\\) distances defined by the product of the two marginal distributions. The squared distance between the conditional probabilities defined by rows \\(a\\) and \\(a'\\) is \\[\\sum_{b}  \\frac{\\left( N_{a,b}/N_{a,.} - N_{a',b}/N_{a',.}\\right)^2}{N_{.,b}/N}\\]\nThe \\(\\ell_2\\) distance between the rows of the principal coordinates matrix row$coord coincides since they are all centered and normalized with respect to \\((N_{.,b}/N)\\).\n\n\nCode\ndist_Causes_79 &lt;- ca_79$row$coord[,1:8] |&gt; \n  dist()\n\n\n\n\nCode\nhc_79 &lt;- hclust(dist_Causes_79, method = \"single\")\n\n\n\n\nCode\nstopifnot(\n  require(ggdendro),\n  require(dendextend),\n  require(sloop)\n)\n\n\nThe instance of hclust is transformed into a an object of class dendro. Class dendro is equipped with a variety of functions/methods for analyzing, visualizing, and exploiting the result of hclust().\n\n\nCode\ndendro_79 &lt;- dendro_data(hc_79)\n\n\n\n\nCode\nclass(dendro_79)\n\n\n[1] \"dendro\"\n\n\n\n\nCode\n(\ndendro_79 |&gt; \n  ggdendrogram(\n    leaf_labels = T, \n    rotate = T) + \n  ggdendro::theme_dendro() +\n  scale_y_reverse()\n  ) |&gt; plotly::ggplotly()\n\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPerform hierarchical clustering of row profiles with method/linkage \"single\". Check the definition of the method. Did you know the underlying algorithm? If yes, in which context did you get acquainted with this algorithm?\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nChoose the number of classes (provide justification).\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCan you explain the size of the different classes in the partition?"
  },
  {
    "objectID": "core/labs-solutions/lab-ca.html#atypical-row-profiles",
    "href": "core/labs-solutions/lab-ca.html#atypical-row-profiles",
    "title": "LAB: Correspondance Analysis",
    "section": "Atypical row profiles",
    "text": "Atypical row profiles\n\n\n\n\n\n\nNoteQuestion\n\n\n\nRow profiles that do not belong to the majority class are called atypical.\n\nCompute the share of inertia of atypical row profiles.\nDraw a symmetric plot (biplot) outlining the atypical row profiles."
  },
  {
    "objectID": "core/labs-solutions/lab-ca.html#investigating-independenceassociation",
    "href": "core/labs-solutions/lab-ca.html#investigating-independenceassociation",
    "title": "LAB: Correspondance Analysis",
    "section": "Investigating independence/association",
    "text": "Investigating independence/association\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nCalculate the theoretical population table for deces. Do you possible to carry out a chi-squared test?\nPerform a hierarchical classification of the line profiles into two classes.\nMerge the rows of deces corresponding to the same class (you can use the the tapply function), and perform a chi-square test. chi-square test. What’s the conclusion?\nWhy is it more advantageous to carry out this grouping into two classes compared to arbitrarily grouping two classes, in order to prove the dependence between these two variables?"
  },
  {
    "objectID": "core/labs-solutions/lab-ca.html#about-the-average-profile",
    "href": "core/labs-solutions/lab-ca.html#about-the-average-profile",
    "title": "LAB: Correspondance Analysis",
    "section": "About the “average profile”",
    "text": "About the “average profile”\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nRepresent individuals from the majority class. Do they all seem to you to correspond to an average profile?\nTry to explain this phenomenon considering the way in which hierarchical classification uses the Single Linkage method.\n\n\n\n\n\n\n\n\n\nCautionCaveat\n\n\n\nThe mortality dataset should be taken with grain of salt. Assigning a single Cause to every death is not a trivial task. It is even questionable: if somebody dies from some infection beCause she could not be cured using an available drug due to another preexisting pathology, who is the culprit?"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html",
    "href": "core/labs-solutions/lab-mca.html",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "",
    "text": "M1 MIDS/MFA/LOGOS\n\n\nUniversité Paris Cité\n\n\nAnnée 2025\n\n\nCourse Homepage\n\n\nMoodle\nBesides the usual packages (tidyverse, …), we shall require FactoMineR and related packages.\nCodestopifnot(\n  require(FactoMineR),\n  require(factoextra),\n  require(FactoInvestigate)\n)"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#the-gss-dataset",
    "href": "core/labs-solutions/lab-mca.html#the-gss-dataset",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The GSS dataset",
    "text": "The GSS dataset\nWe will use data coming from the General Social Survey. The General Social Survey data can be retrieved using the gssr package. If needed, install the gssr package and its companion package gssrdoc.\n\nCodestopifnot(\n  require(\"gssr\"),\n  require(\"gssrdoc\")\n)\n\nLoading required package: gssr\n\n\nPackage loaded. To attach the GSS data, type data(gss_all) at the console.\nFor the codebook, type data(gss_dict).\nFor the panel data and documentation, type e.g. data(gss_panel08_long) and data(gss_panel_doc).\nFor help on a specific GSS variable, type ?varname at the console.\n\n\nLoading required package: gssrdoc\n\n\nThe data we will use are panel data made available by the GSS. In order to explore them, it’s very useful toload gss_dict and gss_panel_doc.\n\nCodedata(gss_dict)\ndata(gss_panel_doc)\n\n\nCheck the online help.\n\nCode?gss_panel_doc\n\n\ngss_dict and gss_panel_doc are dataframes that can be queried:\n\nCodegss_panel_doc |&gt; \n  dplyr::sample_n(5) |&gt; \n  select(id, text)  |&gt; \n  gt::gt()\n\n\n\n\n\nid\ntext\n\n\n\nrelhhd8\n1622. What is (PERSON)'s relationship to (HEAD OF HOUSEHOLD)?\n\n\ndenom16\n115a. If Protestant: What specific denomination is that, if any?\n\n\nconjudge\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? j. U.S. Supreme Court.\n\n\nwordh\n1612. We would like to know something about how people go about guessing words they do not know. On this card are listed some words--you may know some of them, and you may not know quite a few of them. On each line the first word is in capital letters like BEAST. Then there are five other words. Tell me the number of the word that comes closest to the meaning of the word in capital letters. For example, if the word in capital letters is BEAST, you would say \"4\" since \"animal\" come closer to BEAST than any of the other words. If you wish, I will read the words to you. These words are difficult for almost everyone just give me your best guess if you are not sure of the answer.\n\n\ncompwage\n1087. Do you believe your fixed annual wages in calendar year 2005 were higher or lower than those of employees with similar experience and job descriptions in other companies in your region? Please answer on a 1 to 5 scale.\n\n\n\n\n\n\nIn the panel questionnaire, some questions have race in the field id. Check again the online help.\n\nCode?race\n\n\nThe answer is something like that:\nQuestion 24. What race do you consider yourself?\nAnd it contains a brief summary of the answers given through the years.\n\n\nYear\nBlack\nOther\nWhite\niap (NA)\nTotal\n\n\n\n2010\n311\n183\n1550\n-\n2044\n\n\n2012\n301\n196\n1477\n-\n1974\n\n\n2014\n386\n262\n1890\n-\n2538\n\n\n\n(this is just an excerpt)\n\nCodegss_panel_doc |&gt; \n  filter(str_detect(id, \"race\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  select(id, description, text) |&gt; \n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\n\n\n\nrace\nRACE\n24. What race do you consider yourself?\n\n\nracecen3\nRACECEN3\n1602. What is your race? Indicate one or more races that you consider yourself to be.\n\n\nnatrace\nNATRACE\n68. We are faced with many problems in this country, none of some of these problems, and for each one I'd like you to tell me whether you think we're spending too much money on it, too little money, or about the right amount. h. Improving the conditions of Blacks.\n\n\nnatracey\nNATRACEY\n69. We are faced with many problems in this country, none of some of these problems, and for each one I'd like you to tell me whether you think we're spending too much money on it, too little money, or about the right amount. h. Assistance to blacks.\n\n\nintrace3\nINTRACE3\nWhat is your race? Indicate one or more races that you consider yourself to be.\n\n\n\n\n\n\nThe data set we will use comes from year 2010 panel data.\n\nCodedata(\"gss_panel10_long\")\n\ngss_panel10_long \n\n# A tibble: 6,132 × 1,200\n   firstid  wave oversamp sampcode  sample      form         formwt vpsu  vstrat\n   &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+&gt;\n 1 1           1        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 2 1           2        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 3 1           3        1 501       9 [2000 FP] 2 [ALTERNAT…      1 1     2240  \n 4 2           1        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 5 2           2        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 6 2           3        1 501       9 [2000 FP] 1 [STANDARD…      1 1     2240  \n 7 3           1        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n 8 3           2        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n 9 3           3        1 501       9 [2000 FP] 1 [STANDARD…      1 2     2240  \n10 4           1        1 501       9 [2000 FP] 2 [ALTERNAT…      1 2     2240  \n# ℹ 6,122 more rows\n# ℹ 1,191 more variables: samptype &lt;dbl+lbl&gt;, wtpan12 &lt;dbl+lbl&gt;,\n#   wtpan123 &lt;dbl+lbl&gt;, wtpannr12 &lt;dbl+lbl&gt;, wtpannr123 &lt;dbl+lbl&gt;,\n#   id &lt;dbl+lbl&gt;, mar1 &lt;dbl+lbl&gt;, mar2 &lt;dbl+lbl&gt;, mar3 &lt;dbl+lbl&gt;,\n#   mar4 &lt;dbl+lbl&gt;, mar5 &lt;dbl+lbl&gt;, mar6 &lt;dbl+lbl&gt;, mar7 &lt;dbl+lbl&gt;,\n#   mar8 &lt;dbl+lbl&gt;, mar9 &lt;dbl+lbl&gt;, mar10 &lt;dbl&gt;, mar11 &lt;dbl+lbl&gt;, mar12 &lt;dbl&gt;,\n#   mar13 &lt;dbl&gt;, mar14 &lt;dbl&gt;, abany &lt;dbl+lbl&gt;, abdefect &lt;dbl+lbl&gt;, …\n\n\nAt the beginning, the panel was made of roughly 2000 respondents. And the respondents were interviewed in 2010, 2012 and 2014 (the three waves).\n\nCodegss_panel10_long  |&gt;  \n  filter(wave==3, !is.na(id)) |&gt;\n  select(firstid, wave, id, sex)\n\n# A tibble: 1,304 × 4\n   firstid  wave id        sex       \n   &lt;fct&gt;   &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; \n 1 1           3 10001     1 [MALE]  \n 2 2           3 10002     2 [FEMALE]\n 3 3           3 10003     2 [FEMALE]\n 4 6           3 10004     1 [MALE]  \n 5 7           3 10005     2 [FEMALE]\n 6 9           3 10006     2 [FEMALE]\n 7 10          3 10007     2 [FEMALE]\n 8 11          3 10008     2 [FEMALE]\n 9 12          3 10009     1 [MALE]  \n10 13          3 10010     1 [MALE]  \n# ℹ 1,294 more rows\n\n\nSome respondents left the survey. Attrition can be monitored within the next query.\n\nCodegss_panel10_long |&gt; \n  select(wave, id) |&gt;\n  group_by(wave) |&gt;\n  summarize(observed = n_distinct(id),\n            missing = sum(is.na(id)))\n\n# A tibble: 3 × 3\n   wave observed missing\n  &lt;dbl&gt;    &lt;int&gt;   &lt;int&gt;\n1     1     2044       0\n2     2     1552     493\n3     3     1305     740"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#the-confidence-topic",
    "href": "core/labs-solutions/lab-mca.html#the-confidence-topic",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The confidence topic",
    "text": "The confidence topic\nDo GSS surveys are made of a huge number of questions. Not all questions were asked to the respondents. Indeed, each question was asked to two thirds of the respondents. Questions were related to demographic features (Age, Sex, Level of education, Employment and so on). Answers to these questions can be used to determine whether the panel sample is representative. Data can be compared with last census data (there is a census every ten years in the US).\nA number of questions are related to the confidence topic. Respondents were asked about the level of confidence they put in a wide array of institutions.\n\nCodegss_panel_doc |&gt; \n  filter(str_detect(text, \"confidence\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  select(id, description, text) |&gt; \n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\n\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\n\n\nconbus\nCONBUS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? b. Major companies.\n\n\nconclerg\nCONCLERG\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? c. Organized religion.\n\n\nconeduc\nCONEDUC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? d. Education.\n\n\nconfed\nCONFED\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? e. Executive branch of the federal government.\n\n\nconlabor\nCONLABOR\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? f. Organized labor.\n\n\nconpress\nCONPRESS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? g. Press.\n\n\nconmedic\nCONMEDIC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? h. Medicine.\n\n\ncontv\nCONTV\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? i. Television.\n\n\nconjudge\nCONJUDGE\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? j. U.S. Supreme Court.\n\n\nconsci\nCONSCI\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? k. Scientific community.\n\n\nconlegis\nCONLEGIS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? l. Congress\n\n\nconarmy\nCONARMY\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? m. Military\n\n\n\n\n\n\nFor institutions like Science or Medicine, or Executive branch of federal government, Respondents were asked whether they have: Great deal of confidence, Only some confidence, Hardly any confidence in the institution. The same Likert scale with 3 levels was used for all institutions.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nFrom the gss_panel10_long dataset, extract columns corresponding to questions from the confidence topic\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nWe first gather the id of the questions pertaining to the confidence topic.\n\nCodeconfidence_ids &lt;- gss_panel_doc |&gt; \n  filter(str_detect(text, \"confidence\")) |&gt;\n  slice_sample(n=1, by=text) |&gt; \n  pull(id)\n\n\nThen we protect the panel data on the questions pertaining to the confidence topic. We also keep the respondent key, sex, and the relevant wave. s ::: {.cell}\nCodegss_panel10_long |&gt; \n  filter(wave==1) |&gt;\n  select(firstid, wave, id, sex, contains(confidence_ids)) |&gt;\n  sample_n(10) |&gt;\n  gt::gt()\n\n\n\n\n\nfirstid\nwave\nid_1: id\nsex_1: RESPONDENTS SEX\nconfinan_1: CONFID IN BANKS & FINANCIAL INSTITUTIONS\nconbus_1: CONFIDENCE IN MAJOR COMPANIES\nconclerg_1: CONFIDENCE IN ORGANIZED RELIGION\nconeduc_1: CONFIDENCE IN EDUCATION\nconfed_1: CONFID. IN EXEC BRANCH OF FED GOVT\nconlabor_1: CONFIDENCE IN ORGANIZED LABOR\nconpress_1: CONFIDENCE IN PRESS\nconmedic_1: CONFIDENCE IN MEDICINE\ncontv_1: CONFIDENCE IN TELEVISION\nconjudge_1: CONFID. IN UNITED STATES SUPREME COURT\nconsci_1: CONFIDENCE IN SCIENTIFIC COMMUNITY\neconsci\nconlegis_1: CONFIDENCE IN CONGRESS\nconarmy_1: CONFIDENCE IN MILITARY\n\n\n\n1225\n1\n1225\n2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1511\n1\n1511\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1153\n1\n1153\n2\n3\n2\n2\n1\n2\n2\n2\n2\n1\n2\n2\nNA\n2\n1\n\n\n1585\n1\n1585\n2\n3\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\n1\n3\n\n\n940\n1\n940\n2\n3\n2\n3\n2\n3\n3\n2\n1\n2\n3\n2\nNA\n3\n2\n\n\n110\n1\n110\n2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n559\n1\n559\n2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1536\n1\n1536\n1\n3\n2\n2\n2\n2\n2\n3\n2\n3\n2\n2\nNA\n3\n1\n\n\n846\n1\n846\n2\n3\n2\n2\n3\n3\n2\n3\n1\n3\n2\n1\n4\n3\n2\n\n\n220\n1\n220\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nLet us go back to the documentation.\n\nCodegss_panel_doc |&gt;\n  filter(id %in% confidence_ids) |&gt; \n  select(-ends_with(\"2\"), -ends_with(\"3\")) |&gt;\n  gt::gt()\n\n\n\n\n\nid\ndescription\ntext\nproperties_1\nmarginals_1\n\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/265\"), c(\"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\")\nc(30.1, 55.2, 14.7, NA, NA, 100), c(\"395\", \"725\", \"193\", \"671\", \"16\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\", \"CONFINAN_1\")\n\n\nconbus\nCONBUS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? b. Major companies.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/247\"), c(\"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\")\nc(17.6, 62.3, 20.1, NA, NA, NA, 100), c(\"228\", \"806\", \"260\", \"671\", \"32\", \"3\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\", \"CONBUS_1\")\n\n\nconclerg\nCONCLERG\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? c. Organized religion.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/250\"), c(\"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\")\nc(25.1, 52.5, 22.4, NA, NA, NA, 100), c(\"324\", \"678\", \"289\", \"671\", \"35\", \"3\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\", \"CONCLERG_1\")\n\n\nconeduc\nCONEDUC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? d. Education.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/259\"), c(\"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\")\nc(27.1, 56.5, 16.4, NA, NA, 100), c(\"359\", \"747\", \"217\", \"671\", \"6\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\", \"CONEDUC_1\")\n\n\nconfed\nCONFED\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? e. Executive branch of the federal government.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/262\"), c(\"CONFED_1\", \"CONFED_1\", \"CONFED_1\")\nc(13.8, 45.4, 40.8, NA, NA, NA, 100), c(\"179\", \"590\", \"531\", \"671\", \"26\", \"3\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONFED_1\", \"CONFED_1\", \"CONFED_1\", \"CONFED_1\", \"CONFED_1\", \"CONFED_1\", \"CONFED_1\")\n\n\nconlabor\nCONLABOR\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? f. Organized labor.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/289\"), c(\"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\")\nc(12.3, 58.1, 29.6, NA, NA, 100), c(\"155\", \"731\", \"372\", \"671\", \"71\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\", \"CONLABOR_1\")\n\n\nconpress\nCONPRESS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? g. Press.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/298\"), c(\"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\")\nc(10, 48.5, 41.5, NA, NA, NA, 100), c(\"132\", \"638\", \"546\", \"671\", \"12\", \"1\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\", \"CONPRESS_1\")\n\n\nconmedic\nCONMEDIC\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? h. Medicine.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/295\"), c(\"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\")\nc(40.1, 49.3, 10.6, NA, NA, 100), c(\"530\", \"651\", \"140\", \"671\", \"8\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\", \"CONMEDIC_1\")\n\n\ncontv\nCONTV\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? i. Television.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/328\"), c(\"CONTV_1\", \"CONTV_1\", \"CONTV_1\")\nc(9.4, 48.2, 42.3, NA, NA, NA, 100), c(\"124\", \"634\", \"556\", \"671\", \"14\", \"1\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONTV_1\", \"CONTV_1\", \"CONTV_1\", \"CONTV_1\", \"CONTV_1\", \"CONTV_1\", \"CONTV_1\")\n\n\nconjudge\nCONJUDGE\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? j. U.S. Supreme Court.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/286\"), c(\"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\")\nc(31.8, 50.3, 17.8, NA, NA, NA, 100), c(\"410\", \"649\", \"230\", \"671\", \"38\", \"2\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\", \"CONJUDGE_1\")\n\n\nconsci\nCONSCI\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? k. Scientific community.\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/319\"), c(\"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\")\nc(41.6, 51, 7.3, NA, NA, NA, 100), c(\"527\", \"646\", \"93\", \"671\", \"62\", \"1\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\", \"CONSCI_1\")\n\n\nconlegis\nCONLEGIS\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? l. Congress\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/292\"), c(\"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\")\nc(10.8, 51.2, 37.9, NA, NA, NA, 100), c(\"141\", \"666\", \"493\", \"671\", \"28\", \"1\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", \"9\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", NA, \"Total\"), c(\"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\", \"CONLEGIS_1\")\n\n\nconarmy\nCONARMY\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? m. Military\nc(\"Data type\", \"Missing-data codes\", \"Record/column\"), c(\"numeric\", \"0,8,9\", \"1/244\"), c(\"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\")\nc(46.8, 39.8, 13.4, NA, NA, 100), c(\"613\", \"521\", \"175\", \"671\", \"20\", \"2,000\"), c(\"1\", \"2\", \"3\", \"0\", \"8\", NA), c(\"A GREAT DEAL\", \"ONLY SOME\", \"HARDLY ANY\", \"IAP\", \"DK\", \"Total\"), c(\"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\", \"CONARMY_1\")\n\n\n\n\n\n\nLevels labels for answers to confidence questions are:\n\n\nCodefoo &lt;- gss_panel_doc |&gt;\n   filter(id %in% confidence_ids) |&gt; \n   select(-ends_with(\"2\"), -ends_with(\"3\")) |&gt; \n   unnest(\n    cols=c(properties_1), names_repair = \"universal\") |&gt; \n   unnest(\n    cols=c(marginals_1), names_repair = \"universal\") |&gt; \n   head(6) \n\nNew names:\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...6`\n\nCodefoo |&gt; gt::gt()\n\n\n\n\n\nid...1\ndescription\ntext\nproperty\nvalue...5\nid...6\npercent\nn\nvalue...9\nlabel\nid...11\n\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\n30.1\n395\n1\nA GREAT DEAL\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\n55.2\n725\n2\nONLY SOME\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\n14.7\n193\n3\nHARDLY ANY\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\nNA\n671\n0\nIAP\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\nNA\n16\n8\nDK\nCONFINAN_1\n\n\nconfinan\nCONFINAN\nfar as the people running these institutions are concerned, would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in them? a. Banks and financial institutions.\nData type\nnumeric\nCONFINAN_1\n100.0\n2,000\nNA\nTotal\nCONFINAN_1\n\n\n\n\n\n\n\n\nIAP stands for respondent was not asked the question.\n\nDK stands for Don’t Know\n\n\n\nIf respondents were not given the opportunity to provide a response to a question because it was not included in the survey they were administered, then their responses are coded as “IAP.”\n\n:::::\n\nCodepanel_doc &lt;- gssrdoc::gss_panel_doc"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#table-wrangling",
    "href": "core/labs-solutions/lab-mca.html#table-wrangling",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Table wrangling",
    "text": "Table wrangling\nBefore proceeding to Multiple Correspondence Analysis (CA), let us select the set of active variables.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nProject gss_panel10_long on firstid, wave, id, sex, and columns with names in\n  confinan conbus conclerg coneduc confed conlabor conpress conmedic contv conjudge consci conlegis conarmy\nFilter so as to keep only wave 1.\nTransform all relevant columns into factors.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodeconfidence_panel10 &lt;- gss_panel10_long |&gt; \n  filter(wave==1) |&gt;\n  select(firstid, wave, id, sex, contains(confidence_ids)) |&gt;\n  mutate(across(starts_with(\"con\"), as_factor)) |&gt;\n  mutate(sex=as_factor(sex))\n\n\n\nCodeconfidence_10 &lt;- gss_panel_doc |&gt;\n   filter(id %in% confidence_ids) |&gt; \n   select(-ends_with(\"2\"), -ends_with(\"3\")) |&gt; \n   unnest(\n    cols=c(properties_1), names_repair = \"universal\") |&gt; \n   unnest(\n    cols=c(marginals_1), names_repair = \"universal\") |&gt;\n   mutate(text=str_squish(text)) |&gt;\n   mutate(text= str_match(text, \"\\\\? [\\\\w]\\\\.(.*)\")) |&gt; \n   mutate(text=text[,2]) |&gt; \n   filter(property == \"Missing-data codes\") |&gt;\n   select(-starts_with(\"id\"), -property, -`value...5`)\n\nNew names:\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...6`\n\n\n\nCodelevels_table &lt;- confidence_10 |&gt; \n  filter(label!='Total' | is.na(label)) |&gt;\n  distinct(`value...9`, label)"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#elementary-statistics",
    "href": "core/labs-solutions/lab-mca.html#elementary-statistics",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Elementary statistics",
    "text": "Elementary statistics\n\n\n\n\n\n\nNoteQuestion\n\n\n\nUse skimmr to summarize your dataset.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodeconfidence_panel10 |&gt;\n  skimr::skim(where(is.factor)) |&gt;\n  select(-factor.ordered, -skim_type)|&gt;\n  filter(str_starts(skim_variable,\"con\")) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(decimals=3) |&gt;\n  gt::tab_caption(\n    caption = \"GSS Panel data 2010 First wave, Topic confidence\"\n  )\n\n\n\n\nGSS Panel data 2010 First wave, Topic confidence\n\nskim_variable\nn_missing\ncomplete_rate\nfactor.n_unique\nfactor.top_counts\n\n\n\nconfinan\n680.000\n0.667\n3.000\nONL: 656, HAR: 564, A G: 144, IAP: 0\n\n\nconbus\n705.000\n0.655\n3.000\nONL: 848, HAR: 317, A G: 174, IAP: 0\n\n\nconclerg\n725.000\n0.645\n3.000\nONL: 720, HAR: 328, A G: 271, IAP: 0\n\n\nconeduc\n681.000\n0.667\n3.000\nONL: 793, A G: 367, HAR: 203, IAP: 0\n\n\nconfed\n698.000\n0.659\n3.000\nONL: 620, HAR: 499, A G: 227, IAP: 0\n\n\nconlabor\n744.000\n0.636\n3.000\nONL: 781, HAR: 373, A G: 146, IAP: 0\n\n\nconpress\n689.000\n0.663\n3.000\nONL: 621, HAR: 594, A G: 140, IAP: 0\n\n\nconmedic\n681.000\n0.667\n3.000\nONL: 647, A G: 558, HAR: 158, IAP: 0\n\n\ncontv\n681.000\n0.667\n3.000\nONL: 677, HAR: 521, A G: 165, IAP: 0\n\n\nconjudge\n709.000\n0.653\n3.000\nONL: 706, A G: 400, HAR: 229, IAP: 0\n\n\nconsci\n738.000\n0.639\n3.000\nONL: 683, A G: 544, HAR: 79, IAP: 0\n\n\nconlegis\n697.000\n0.659\n3.000\nONL: 635, HAR: 587, A G: 125, IAP: 0\n\n\nconarmy\n688.000\n0.663\n3.000\nA G: 720, ONL: 507, HAR: 129, IAP: 0\n\n\n\n\n\n\n\nCodeconfidence_panel10 |&gt;\n  filter(is.na(conarmy)) |&gt;\n  filter(is.na(conclerg)) |&gt;\n  filter(is.na(conpress))\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nThere are a lot of missing data in your data set.\nHow are missing cells related?\nDrop rows with missing data in the confidence questions.\nWhat are the dimensions of your data set?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodeconfidence_data &lt;- confidence_panel10 |&gt; \n  mutate(across(starts_with(\"con\"), .fns=\\(x) !is.na(x), .names=\"not_{.col}_isna\")) |&gt;\n  rowwise() |&gt;\n  filter(all(c_across(ends_with(\"isna\")))) |&gt; \n  ungroup() |&gt; \n  select(-ends_with(\"isna\"), -firstid, -wave, -econsci) |&gt;\n  mutate(across(starts_with(\"con\"), fct_drop), sex=as_factor(sex))\n\nconfidence_data |&gt; \n  slice_sample(n=10) |&gt; \n  gt::gt()\n\n\n\n\n\nid_1: id\nsex_1: RESPONDENTS SEX\nconfinan_1: CONFID IN BANKS & FINANCIAL INSTITUTIONS\nconbus_1: CONFIDENCE IN MAJOR COMPANIES\nconclerg_1: CONFIDENCE IN ORGANIZED RELIGION\nconeduc_1: CONFIDENCE IN EDUCATION\nconfed_1: CONFID. IN EXEC BRANCH OF FED GOVT\nconlabor_1: CONFIDENCE IN ORGANIZED LABOR\nconpress_1: CONFIDENCE IN PRESS\nconmedic_1: CONFIDENCE IN MEDICINE\ncontv_1: CONFIDENCE IN TELEVISION\nconjudge_1: CONFID. IN UNITED STATES SUPREME COURT\nconsci_1: CONFIDENCE IN SCIENTIFIC COMMUNITY\nconlegis_1: CONFIDENCE IN CONGRESS\nconarmy_1: CONFIDENCE IN MILITARY\n\n\n\n1894\nFEMALE\nONLY SOME\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nONLY SOME\n\n\n1639\nFEMALE\nONLY SOME\nONLY SOME\nONLY SOME\nA GREAT DEAL\nONLY SOME\nONLY SOME\nONLY SOME\nA GREAT DEAL\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nA GREAT DEAL\n\n\n718\nMALE\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nHARDLY ANY\n\n\n1101\nFEMALE\nONLY SOME\nONLY SOME\nHARDLY ANY\nONLY SOME\nA GREAT DEAL\nHARDLY ANY\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nONLY SOME\nA GREAT DEAL\n\n\n1120\nFEMALE\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nHARDLY ANY\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\n\n\n1417\nFEMALE\nONLY SOME\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nHARDLY ANY\nHARDLY ANY\nONLY SOME\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\n\n\n1454\nMALE\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\nHARDLY ANY\nA GREAT DEAL\nA GREAT DEAL\nHARDLY ANY\nA GREAT DEAL\n\n\n1598\nMALE\nHARDLY ANY\nONLY SOME\nONLY SOME\nA GREAT DEAL\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\n\n\n62\nFEMALE\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\n\n\n1157\nMALE\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nONLY SOME\nONLY SOME\nHARDLY ANY\nA GREAT DEAL\nONLY SOME\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\nA GREAT DEAL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nIn your explore possible associations between the answers to the different confidence questions?\nHow would you test possible independence between answer to confidence in science and confidence in the Army.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nTODO"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#the-case-for-using-mca",
    "href": "core/labs-solutions/lab-mca.html#the-case-for-using-mca",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "The case for using MCA",
    "text": "The case for using MCA\n\nIn order to construct a field of ideological and political attitudes, which will subsequently be used as a reference for locating the empirical typologies of response styles and survey compliance, we apply multiple correspondence analysis (MCA). MCA belongs to the family of techniques used in geometric data analysis (Le Roux and Rouanet 2004). It allows for the extraction of the most important dimensions in a set of categorical variables, and the graphical representation of variable categories and individuals relative to each other in a coordinate system. Distances between categories as well as individuals can be interpreted as a measure of (dis)similarity: If categories often co-appear in individual’s responses, they are located close together in the space produced by MCA. Rare co-appearances, accordingly, result in a larger distance between the respective categories. Furthermore, illustrative variables can passively be projected into the field, a technique that has been termed visual regression analysis (Lebart et al. 1984). Whereas the space is determined by the distances between the categories of active variables, passive variables do not alter the structure of the constructed field, but appear in their average and hence most likely position.\n\nFrom https://doi.org/10.1007/s11135-016-0458-3\nIn this lab, we won’t look at the field of ideological and political attitudes, but rather at the field of confidence level in a variety of institutions."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#multiple-correspondance-analysis",
    "href": "core/labs-solutions/lab-mca.html#multiple-correspondance-analysis",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Multiple Correspondance Analysis",
    "text": "Multiple Correspondance Analysis\n\n\n\n\n\n\nImportantMCA executive summary\n\n\n\nThe input of multiple correspondence analysis is a data frame \\(X\\) with \\(n\\) rows and \\(p\\) categorical columns. Multiple Correspondence Analysis starts by building the indicator matrix. The indicator matrix is built by one-hot encoding of each categorical variable.\n\nA categorical variable \\(V_j\\) (factor) with \\(q\\) levels is mapped to \\(q\\) \\(\\{0,1\\}\\) -valued variables \\(V_{j,r}\\) for \\(r \\leq q\\)\nIf levels are indexed by \\(\\{1, \\ldots, q\\}\\), if the value of the categorical variable \\(V_j\\) from row \\(i\\) is \\(k \\in \\{1, \\ldots, q\\}\\), the bina$ \\[k \\mapsto \\underbrace{0,\\ldots, 0}_{k-1}, 1, \\underbrace{0, \\ldots, 0}_{q-k}\\]\nThe indicator matrix has as many rows as the data matrix\nThe number of columns of the indicator matrix is the sum of the number of levels of the categorical variables/columns of the data matrix\nThe indicator matrix is a numerical matrix. It is suitable for factorial methodss\n\nRecall \\(X\\) is the data matrix with \\(n\\) rows (individuals) and \\(p\\) categorical columns (variables)\nFor \\(j \\in \\{1, \\ldots, p\\}\\), let \\(J_j\\) denote the number of levels(categories) of variable \\(j\\)\nLet \\(q = \\sum_{j\\leq p} J_j\\) be the sum of the number of levels throughout the variables\nLet \\(Z\\) be the incidence matrix with \\(n\\) rows and \\(q\\) columns\nFor \\(j\\leq p\\) and \\(k \\leq J_j\\), let \\(\\langle j, k\\rangle = \\sum_{j'&lt;j} J_{j'}+k\\)\nLet \\(N = n \\times p = \\sum_{i\\leq n} \\sum_{j \\leq p} X_{i,j}\\) and \\[P = \\frac{1}{N} Z\\]\n(the correspondence matrix for MCA)\n The row wise sums of correspondence matrix \\(P\\) are all equal to \\(1/n=p/N\\)\nThe column wise sum of the correspondence matrix \\(P\\) for the \\(k\\)th level of the \\(j\\)th variable of \\(X\\) ( \\(j \\leq p\\) ) is \\[N_{\\langle j,k\\rangle}/N = f_{\\langle j,k\\rangle}/p\\]\nwhere \\(f_{\\langle j,k\\rangle}\\) stands for the relative frequency of level \\(k\\) of the \\(j\\)th variable\n\\[D_r = \\frac{1}{n}\\text{Id}_n\\qquad D_c =\\text{diag}\\left(\\frac{f_{\\langle j,k\\rangle}}{p}\\right)_{j \\leq p, k\\leq J_j}\\]\nIn MCA, we compute the SVD \\(U \\times D \\times V^\\top\\) of the standardized residuals matrix:\n\\[S = D_r^{-1/2}\\times \\left(P - r\\times c^\\top\\right) \\times D_c^{-1/2} = \\sqrt{n}\\left(P - r\\times c^\\top\\right) \\times D_c^{-1/2}\\]\nCoefficient \\(i, \\langle j, k\\rangle\\) of \\(S\\) is \\[\\frac{\\mathbb{I}_{i, \\langle j, k\\rangle}- f_{\\langle j,k\\rangle}}{\\sqrt{n f_{\\langle j,k\\rangle}/p}}\\]\nMCA consists in computing the SVD of the standardized residuals matrix \\(S =  U  \\times D \\times V^\\top\\)\nFrom the SVD, we get\n\n\n\\(D_r^{-1/2} \\times U\\) standardized coordinates of rows\n\n\n\\(D_c^{-1/2} \\times V\\) standardized coordinates of columns\n\n\n\\(D_r^{-1/2} \\times U \\times D\\) principal coordinates of rows\n\n\n\\(D_c^{-1/2} \\times V \\times D\\) principal coordinates of columns\n\nSquared singular values: the principal inertia\n\n\nWhen calling svd(.), the argument should be \\[D_r^{1/2}\\times \\left(D_r^{-1} \\times P \\times D_c^{-1}- \\mathbf{I}\\times \\mathbf{I}^\\top  \\right)\\times D_c^{1/2}= D_r^{-1/2}\\times \\left( P - r \\times c^\\top  \\right)\\times D_c^{-1/2}\\]\n\n\n\n\n\n\n\n\nImportantMCA and extended SVD\n\n\n\nAs\n\\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^\\top = (D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^\\top\\]\n\\[(D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^\\top\\]\nis the extended SVD of\n\\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^\\top\\]\nwith respect to \\(D_r\\) and \\(D_c\\)\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPerform MCA on the indicator matrix.\n\n\n\n\n\n\n\n\nYou may use FactoMineR::MCA(). It is interesting to compute the correspondence analysis in your own way, by preparing the matrix that is handled to svd() and returning a named list containing all relevant information.\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nWe lazyly rely on FactoMineR::MCA.\n\nCoderes.mca &lt;- confidence_data |&gt; \n  select(starts_with(\"con\")) |&gt; \n  FactoMineR::MCA(graph=F)\n\n\nThe result is an object of S3 class MCA.\n\n\ns\n\n\n\n\n\n\nNoteQuestion\n\n\n\nIf you did use FactoMineR::MCA(), explain the organization of the result.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nThe result of FactoMineR::MCA(...) is a named and nested list with five elements:\n\neig\n\na matrix/array containing enough information to build a screeplot.\n\ncall\n\na list of 9, containing the call to CA(), an object of type language, telling (in principle) the user how CA() was called. However, this is a quoted expression. Here we need to guess the value of y in the calling environment understand what’s going on.\n\n\nElement call also contains the table margin distributions marge.col and marge.row. The truncation rank ncp (number of components) can be assigned before computing the SVD (default value is 5). Element \\(X\\) stores the contingency table that was effectively used for computing Correpondence Analysis.\n\nind\n\nInformation gathered from SVD to facilitate individulals (respndents) analysis.\n\nvar\n\na list structured in the same way as element ind. Used for variables/categories profiles analysis\n\nsvd\n\na list of 3, just as the resuld of svd() containing the singular values, the left and right singular vectors of matrix \\(...\\)\n\n\n\n\n\n\n\n\n\n\nTipSolution, output of print(res.mca)\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n1\n$eig\n“eigenvalues”\n\n\n2\n$var\n“results for the variables (categories)”\n\n\n3\n$var$coord\n“coord. of the categories”\n\n\n4\n$var$cos2\n“cos2 for the categories”\n\n\n5\n$var$contrib\n“contributions of the categories”\n\n\n6\n$var$v.test\n“v-test for the categories”\n\n\n7\n$ind\n“results for the individuals”\n\n\n8\n$ind$coord\n“coord. for the individuals”\n\n\n9\n$ind$cos2\n“cos2 for the individuals”\n\n\n10\n$ind$contrib\n“contributions of the individuals”\n\n\n11\n$quali.sup\n“results for the supplementary categorical variables”\n\n\n12\n$quali.sup$coord\n“coord. for the supplementary categories”\n\n\n13\n$quali.sup$cos2\n“cos2 for the supplementary categories”\n\n\n14\n$quali.sup$v.test\n“v-test for the supplementary categories”\n\n\n15\n$call\n“intermediate results”\n\n\n16\n$call$marge.col\n“weights of columns”\n\n\n17\n$call$marge.li\n“weights of rows”\n\n\n\n res.mca$svd is not part of the output!\n\neig is computed from the singular values in res.mca$svd\nvar contains material for plotting information about categories and variables on factorial planes\nind conatins material for plotting information about individuals on on factorial planes\n\n\n\n\n\n\n\n\n\nIn principle, all relevant information can be gathered from components svd, call.marge.li, and call.marge.col."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#screeplots",
    "href": "core/labs-solutions/lab-mca.html#screeplots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Screeplots",
    "text": "Screeplots\n\n\n\n\n\n\nNoteQuestion\n\n\n\nDraw screeplots. Why are they useful? Comment briefly.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nWe first define a tidying function for preparing the screeplot. It is designed along the lines of method tidy for objects of class prcomp. It consists in building a data frame from component eig of the MCA object.\n\nCodetidy_mca_scree &lt;- function(res_mca){\n  res_mca$eig |&gt; \n    as_tibble(rownames=\"Dim\") |&gt;\n    rename(\n      `% Inertia` = `percentage of variance`,\n      `Cumulative % Inertia` = `cumulative percentage of variance`\n    ) |&gt; \n    mutate(\n      PC = as.integer(str_match(Dim, 'dim ([0-9]*)')[,2])\n    )\n}\n\n\nWe can now build a column plot.\n\nCoderes.mca |&gt; \n  tidy_mca_scree() |&gt;\n  ggplot() +\n  aes(x=PC, y=eigenvalue, label=glue(\"{round(`% Inertia`,2)} %\")) +\n  geom_col() +\n  geom_text(angle=45, vjust=-1, hjust=-.1) \n\n\n\n\n\n\n\nNote that the share of cumulated inertia doesn’t grow very fast with the number of components. Situation is different from what we met when performing Correspondence Analysis or Principal Component Analysis. This tells us that we will hqve to look beyond the first two axis. This is commonplace with multiple correspondence analysis. Look at the reference below where most of the interesting information can be found on the 3rd and 4th axis."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#individuals",
    "href": "core/labs-solutions/lab-mca.html#individuals",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Individuals",
    "text": "Individuals\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPerform Individual profiles analysis.\nWhat are the classical plots? How can you build them from the output of FactoMiner::MCA?\nBuild the table of row contributions (the so-called \\(\\cos^2\\))\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nAttribute ind of objects of class MCA (exported from FactoMineR) is the starting point of any row profiles analysis.\nWe design an augment method. For objects of class MCA. This is like what is done for objects of class prcomp and different from what we did with objects of class CA. Indeed, MCA on the indicator matrix handles a sample just like PCA and not a contingency table like CA.\n\nCodeaugment_mca &lt;- function(res_mca, data=NULL){\n  \n  result &lt;- res_mca$ind$coord |&gt; \n    as_tibble()  \n  \n  if (!is.null(data)) {\n    result &lt;- result |&gt;\n      bind_cols(confidence_data)\n      # inner_join(confidence_data, by=c(\"id\")) \n  }\n  \n  return(result)\n}\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nAttribute ind is a named list made of \\(4\\) components.\n\ncoord\n\na matrix with named rows and columns. The number of rows of coord matches the number of rows of the table fed to MCA (here, the number of respondents in the panel). The number of columns matches the rank of the truncated SVD that underlies Multiple Correspondance Analysis. Here it is the default truncation \\(5\\).\n\n\n\nThe principal coordinates are the principal coordinates of each individual in terms of the principal component.\n\nThe columns of coord are pairwise orthogonal in the usual inner product space.\nFunction augment_mca Bind the columns of coord with the columns of the original dataset.\n\nCoderes.mca |&gt; \n#| label: augment_mca_glimpse\n  augment_mca(confidence_data) |&gt; \n  glimpse()\n\nRows: 1,183\nColumns: 20\n$ `Dim 1`  &lt;dbl&gt; 1.35465733, -0.10861408, -0.33683136, 0.72450001, 0.72525336,…\n$ `Dim 2`  &lt;dbl&gt; 0.41190167, -0.90245225, -0.45183706, 0.25425970, 0.10225521,…\n$ `Dim 3`  &lt;dbl&gt; 0.161371575, 0.010268886, 0.107542556, -0.009273224, -0.38139…\n$ `Dim 4`  &lt;dbl&gt; 0.141129674, 0.310980059, -0.097880877, -0.085996832, 0.36106…\n$ `Dim 5`  &lt;dbl&gt; 0.227285260, 0.086562193, 0.016680275, -0.115998402, -0.06681…\n$ id       &lt;dbl+lbl&gt;  4,  6,  7,  9, 11, 15, 19, 20, 21, 24, 26, 28, 31, 41, 4…\n$ sex      &lt;dbl+lbl&gt; 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, …\n$ confinan &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, HARDLY ANY, ONLY SOME, ON…\n$ conbus   &lt;fct&gt; ONLY SOME, ONLY SOME, ONLY SOME, ONLY SOME, ONLY SOME, ONLY S…\n$ conclerg &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEA…\n$ coneduc  &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, ONLY SOME, A GREAT DEAL, …\n$ confed   &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEA…\n$ conlabor &lt;fct&gt; A GREAT DEAL, ONLY SOME, HARDLY ANY, ONLY SOME, HARDLY ANY, O…\n$ conpress &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, HARDLY ANY, ONLY SOME, HA…\n$ conmedic &lt;fct&gt; A GREAT DEAL, ONLY SOME, HARDLY ANY, A GREAT DEAL, ONLY SOME,…\n$ contv    &lt;fct&gt; ONLY SOME, ONLY SOME, HARDLY ANY, ONLY SOME, ONLY SOME, ONLY …\n$ conjudge &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEA…\n$ consci   &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, ONLY SOME, …\n$ conlegis &lt;fct&gt; ONLY SOME, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEAL, …\n$ conarmy  &lt;fct&gt; A GREAT DEAL, ONLY SOME, ONLY SOME, A GREAT DEAL, A GREAT DEA…\n\n\nNote that, we can recover ind$coord from the left singular vectors and the singular values.\nThe output of augment_mca is the basis of the individual plots. Individual plots are defined by picking a pair of principal axes. Him use the 3rd and the 4th axis, and we map the sex column from the original dataset to the fill aesthetics.\n\nCoderes.mca |&gt; \n  augment_mca(confidence_data) |&gt; \n  ggplot() +\n  aes(x=`Dim 3`, y=`Dim 4`, fill=sex) +\n  geom_point(alpha=.2, size=.2) +\n  coord_fixed() +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\ncos2\n\nCoefficients of matrix cos2 are the share of row inertia from the corresponding cell in coord\n\n\n\nCoderes.mca |&gt; \n  augment_mca(confidence_data) |&gt; \n  ggplot() +\n  aes(x=`Dim 1`, y=`Dim 2`, fill=sex) +\n  geom_point(alpha=.2, size=.2) +\n  coord_fixed() +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\ncontrib\n\n\n\n\nNot too surprisingly, coord, contrib, and cos2 share the same row names and column names."
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#variablescategories",
    "href": "core/labs-solutions/lab-mca.html#variablescategories",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Variables/Categories",
    "text": "Variables/Categories\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPerform column profiles (categories) analysis. You may use factoextra::fviz_mca_var()\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nThe var component. Of an MCA object is related with the right singular vectors of the residual matrix.\n\nCodep &lt;- res.mca |&gt; \n  factoextra::fviz_mca_var() +\n  coord_fixed() \n  \np +\n  facet_zoom(\n    xy=`Dim 1`&gt;=0 & `Dim 2`&gt;=0,\n    xlim=c(0,2),ylim=c(0,2))\n\n\n\n\n\n\n\nFollowing what has been done for objects of class prcomp, we design a tidy_mca_var function that turns the var component into a dataframe. Row names which correspond to categories, are turned into a column we call levels.\n\nCodetidy_mca_var &lt;- function(res_mca){\n  res_mca$var$coord |&gt; \n  as_tibble(rownames=\"levels\")\n}\n\n\nA value in column levels indicate a variable., that is a topic (science, religion, army, …) and a confidence level (A value on a Likert scale). We enrich the data frame by extracting the variable (topic) and the value (sentiment).\nWe are then ready to plot an analog of the correlation circle.\n\nCodetidied_mca_var &lt;- tidy_mca_var(res.mca) |&gt;   \n  mutate(sentiment=str_split(as.character(levels),'_')) |&gt;\n  mutate(topic=map_chr(sentiment, \\(x)x[1]), sentiment=map_chr(sentiment, \\(x)x[2])) \n\np &lt;- tidied_mca_var |&gt; \n  ggplot() +\n    aes(x=`Dim 1`, y=`Dim 2`, label=levels, fill=topic, shape=sentiment) +\n    geom_point(alpha=.5, show.legend = F) +\n    coord_fixed() +\n    scale_color_viridis_d() +\n    theme(legend.position=\"none\") +\n    labs(\n      title = \"MCA: Variables categories\",\n      subtitle = \"Color: topic, Shape: confidence level\",\n      caption = \"GSS data, panel 2010, wave 1\"\n    )\n\np |&gt;\n  plotly::ggplotly() \n\n\n\n\n\nResorting to plotly allows us to mitigate the overplotting effect.\nThe plot on the first two axes seems easy to interpret. As we move along the first axis from the left, we see growing confidence.\nOn the positive part of the second axis, we have extreme confidence levels, either high or low. This suggests that individuals have trends: either we trust all institutions; or none of them; or finally a little bit all of them.\nHow could we comfort such a narrative?\n\nCode(\n p + \n    aes(x=`Dim 3`, y=`Dim 4`, label=levels, color=sentiment) \n) |&gt;   plotly::ggplotly() \n\n\n\n\n\n\nCode(\n  p +\n    aes(x=`Dim 4`, y=`Dim 5`, label=levels) \n) |&gt;  plotly::ggplotly() \n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the v.test component of the var component of an MCA object?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nHere is a chatbot answer:\n\nIn Multiple Correspondence Analysis (MCA), the v-test (also known as the test-value or t-test value) is a statistical measure used to assess the significance of the association between a category and a dimension (factor). It helps determine which categories contribute the most to the definition of a given dimension.\n\nDefinition and Calculation\nThe v-test is computed as:\n\\[v = \\frac{p - P}{\\sqrt{P(1 - P)/n}}\\]\nwhere: - \\(p\\) = proportion of individuals in a given category for the specific dimension, - \\(P\\) = overall proportion of individuals in that category across all dimensions, - \\(n\\) = total number of individuals.\nAlternatively, the v-test can also be expressed as:\n\\[v = \\frac{\\text{category coordinate}}{\\text{standard deviation of the coordinate}}\\]\nThis essentially provides a z-score, which follows a standard normal distribution under the null hypothesis.\nInterpretation\n\nA high absolute v-test value (typically above 2 or below -2) indicates that the category is strongly associated with the given dimension.\n\nPositive v-test: The category is overrepresented in that dimension.\n\nNegative v-test: The category is underrepresented in that dimension.\nIf the absolute value of the v-test is below 2, the association is not statistically significant (at a 5% significance level).\nWhy is it useful?\n\nIt helps interpret MCA results quantitatively rather than relying only on visual inspection.\nIt aids in identifying key categories that structure the data on a given axis.\nIt provides statistical validation for category contributions to dimensions.\n\nWould you like a practical example or interpretation of a specific MCA result?"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#symmetric-plots",
    "href": "core/labs-solutions/lab-mca.html#symmetric-plots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Symmetric plots",
    "text": "Symmetric plots\n\n\n\n\n\n\nNoteQuestion\n\n\n\nBuild the symmetric plots (biplots) for multiple correspondence analysis.\n\n\nFrom the shelf\nWe resort to factoextra. In principle, the biplot can be constructed from the coord matrices in the var and the ind components of the MCA object.\n\nCodep &lt;- res.mca |&gt; \n  factoextra::fviz_mca_biplot(\n    label= \"var\",\n    alpha.ind=.5\n  )\n\np\n\n\n\n\n\n\nCode# p |&gt;  plotly::ggplotly()\n\n\n\n\n\n\n\n\nTipSolution"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#mosaicplots",
    "href": "core/labs-solutions/lab-mca.html#mosaicplots",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Mosaicplots",
    "text": "Mosaicplots\nMCA can be complemented by Mosaicplots, Double Decker plots, Chi-square tests, and Correspondence analyses between pair of variables.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nDraw a mosaic plot to visualize association between confidence levesl in Science and confidence level in Medicine.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodeconfidence_data |&gt; \n  select(consci, conmedic) |&gt;\n  table() |&gt; \n  mosaicplot(color=T)\n\n\n\n\n\n\n\n\nCodeconfidence_data |&gt; \n  select(consci, conmedic) |&gt; \n  table() |&gt;\n  chisq.test() |&gt; \n  broom::glance() |&gt; \n  gt::gt() |&gt; \n  gt::tab_caption(\"Chi2 independence test for Confidence in Science and Confidence in Medicine\")\n\n\n\n\nChi2 independence test for Confidence in Science and Confidence in Medicine\n\nstatistic\np.value\nparameter\nmethod\n\n\n137.8878\n7.994493e-29\n4\nPearson's Chi-squared test\n\n\n\n\n\n\nCodeconfidence_data |&gt; \n  select(consci, conclerg) |&gt; \n  table() |&gt;\n  chisq.test() |&gt; \n  broom::glance() |&gt; \n  gt::gt() |&gt; \n  gt::tab_caption(\"Chi2 independence test for Confidence in Science and Confidence in Clergy\")\n\n\n\n\nChi2 independence test for Confidence in Science and Confidence in Clergy\n\nstatistic\np.value\nparameter\nmethod\n\n\n39.27785\n6.103953e-08\n4\nPearson's Chi-squared test\n\n\n\n\n\n\nCodet_sex_clerg_sci &lt;- confidence_data |&gt; \n  select(sex, conclerg, consci) |&gt; \n  table()\n  \nt_sex_clerg_sci  |&gt;\n  vcd::doubledecker( )\n\n\n\n\n\n\n\n\nCodet_sex_clerg_sci |&gt; \n  mosaicplot(color=T, main=\"Confidence in Clergy and Science\")"
  },
  {
    "objectID": "core/labs-solutions/lab-mca.html#further-references",
    "href": "core/labs-solutions/lab-mca.html#further-references",
    "title": "LAB: Multiple Correspondence Analysis (MCA)",
    "section": "Further references",
    "text": "Further references\nBarth, Alice and Schmitz, Andreas. 2018. Response quality and ideological dispositions: an integrative approach using geometric and classifying techniques. Quality & Quantity\n\nWhen analyzing survey data, response quality has consequential implications for substantial conclusions. Differences in response quality are usually explained by personality, or socio-demographic or cognitive characteristics. Little, however, is known about how respondents’ political attitudes, values, and opinions impact on quality aspects. This is a striking analytical omission, as potential associations between political values and various forms of response biases and artefacts call into question surveys’ ability to represent ‘public opinion’. In this contribution, response quality is traced back to respondents’ political and ideological dispositions. For this purpose, a relational understanding of response quality is applied that takes into account different aspects of response behaviors, as well as the interrelations between these indicators. Using data from the US General Social Survey (2010–2014), an empirical typology of response quality is created via finite mixture analysis. The resulting classes are then related to positions in the US field of ideological dispositions constructed via multiple correspondence analysis. The analyses reveal that there are (1) different combinations of response patterns and thus different empirical response types, and (2) that these types of response quality systematically vary with regard to the respondents’ political and ideological (dis)positions. Implications of the findings for public opinion surveys are discussed."
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html",
    "href": "core/labs-solutions/lab-hclust.html",
    "title": "Lab: Hierarchical Clustering",
    "section": "",
    "text": "M1 MIDS/MFA/LOGOS\n\n\nUniversité Paris Cité\n\n\nAnnée 2025\n\n\nCourse Homepage\n\n\nMoodle\nCodestopifnot(\n  require(gt),\n  require(skimr),\n  require(GGally),\n  require(patchwork),\n  require(ggforce),\n  require(glue),\n  require(ggfortify),\n  require(ggvoronoi),\n  require(magrittr),\n  require(broom),\n  require(ggdendro),\n  require(dendextend),\n  require(plotly),\n  require(tidyverse)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(\n  theme_minimal(base_size=9, \n                base_family = \"Helvetica\")\n)\nCodeknitr::opts_chunk$set(\n  message = FALSE,\n  warning = FALSE,\n  comment=NA,\n  prompt=FALSE,\n  cache=FALSE,\n  echo=TRUE,\n  results='asis'\n)\nCodegc &lt;- options(ggplot2.discrete.colour=\"viridis\")\ngc &lt;- options(ggplot2.discrete.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.colour=\"viridis\")"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#preamble",
    "href": "core/labs-solutions/lab-hclust.html#preamble",
    "title": "Lab: Hierarchical Clustering",
    "section": "Preamble",
    "text": "Preamble\nHierarchical clustering builds dendrograms\nExplore the data structure: dendrograms (objects of class dendrogram) are represented by lists of lists with attributes (not by tibbles).\nThe dendrograms created from objects of class hclust represent planar binary trees.\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nHow do you define abstractly planar binary trees?\nIn dendrograms created from objects of class hclust, what do the leaf nodes represent?\nIn dendrograms created from objects of class hclust, what do the internal nodes represent ?\n\n\n\n\n\n\n\n\n\nKeep an eye on Introduction to dendextend by the package author Tal Galili."
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#playing-with-a-toy-dendrogram",
    "href": "core/labs-solutions/lab-hclust.html#playing-with-a-toy-dendrogram",
    "title": "Lab: Hierarchical Clustering",
    "section": "Playing with a toy dendrogram",
    "text": "Playing with a toy dendrogram\n\nCodedend &lt;- 1:5 %&gt;% \n  dist %&gt;% \n  hclust(method=\"ward.D2\") %&gt;% \n  as.dendrogram\n\n\nNodes are identified by their prefix order index (note that this depend on the chosen rotation).\n\nCodedend %&gt;%  \n  rotate(c(1,2,4,5,3)) %&gt;% \n  get_nodes_attr(\"members\", \n                 id = c(1, 2, 5, 7)) \n\n[1] 5 2 3 1\n\n\n\nCodecophenetic(rotate(dend, c(1,2,4,5,3)))\n\n         1        2        3        4\n2 1.000000                           \n3 3.872983 3.872983                  \n4 3.872983 3.872983 1.000000         \n5 3.872983 3.872983 1.732051 1.732051\n\nCodecophenetic(dend)\n\n         1        2        5        3\n2 1.000000                           \n5 3.872983 3.872983                  \n3 3.872983 3.872983 1.732051         \n4 3.872983 3.872983 1.732051 1.000000\n\n\n\nCodedend %&gt;% \n  rotate(c(1,2,4,5,3)) %&gt;% \n  get_nodes_attr(\"height\") \n\n[1] 3.872983 1.000000 0.000000 0.000000 1.732051 1.000000 0.000000 0.000000\n[9] 0.000000\n\n\n\nCodeas.ggdend(rev(dend))\n\n\n\n\n\n\n\n\nCode# kmeans(tibble(x=1:5), centers = 2)\n\n\n\nCode# Get various attributes\ndend %&gt;% \n  get_nodes_attr(\"height\") # node's height\n\n[1] 3.872983 1.000000 0.000000 0.000000 1.732051 0.000000 1.000000 0.000000\n[9] 0.000000\n\n\nHow is attributed height computed? What is its purpose?\nWhat kind of tree traversal is used by get_nodes_... helpers?\n\nCodedend %&gt;% \n  get_nodes_attr(\"members\")\n\n[1] 5 2 1 1 3 1 2 1 1"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#tweaking-a-dendrogram",
    "href": "core/labs-solutions/lab-hclust.html#tweaking-a-dendrogram",
    "title": "Lab: Hierarchical Clustering",
    "section": "Tweaking a dendrogram",
    "text": "Tweaking a dendrogram\nWhy should we do that?\nHow should we do that?"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#usarrests",
    "href": "core/labs-solutions/lab-hclust.html#usarrests",
    "title": "Lab: Hierarchical Clustering",
    "section": "USArrests",
    "text": "USArrests\nWe work on USArrests dataset. We want to classify the 50 (united) states on the basis of the arrests profile and the urbanization rate. We rely on hierarchical, bottom-up classification.\n\nCodedata(\"USArrests\")\n\nUSArrests &lt;- USArrests %&gt;% \n    tibble::rownames_to_column(var=\"region\")\n\nUSArrests &lt;- USArrests %&gt;%\n    mutate(region = tolower(region))\n\nrownames(USArrests) &lt;- USArrests$region\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 5\n$ region   &lt;chr&gt; \"alabama\", \"alaska\", \"arizona\", \"arkansas\", \"california\", \"co…\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\n\nCodesource(\"./UTILS/make_biotiful.R\")\n\n\n\nCodemake_biotifoul(USArrests, .f=is.numeric)\n\n\n\n\n\n\n\nThe function dist is used to calculate pairwise distances between individuals.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCompute pairwise distances between rows of USArrests (with and without scaling)\n\n\n\nCodedist.1 &lt;-  USArrests %&gt;% \n    select(where(is.numeric)) %&gt;% \n    dist()\n\n\n\nCodedist.2 &lt;- USArrests %&gt;% \n    select(where(is.numeric)) %&gt;% \n    scale %&gt;% \n    dist()\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPerform hierarchical clustering on unscaled and scaled dataset.\n\n\n\nCodehcl.1 &lt;- hclust(dist.1, method = \"ward.D2\")\nhcl.2 &lt;- hclust(dist.2, method = \"ward.D2\") # scaled\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\n\n\nCodemutate(USArrests, \n       .cluster = factor(cutree(hcl.1, 5))) %&gt;% \n  inner_join(map_data(\"state\"), by = \"region\") %&gt;%\n  ggplot() +\n  aes(x=long, y=lat, group=region, fill=.cluster) +\n  geom_polygon() +\n  scale_fill_viridis_d() +\n  ggtitle(\"Components of arrest data\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#the-dendrogram-class",
    "href": "core/labs-solutions/lab-hclust.html#the-dendrogram-class",
    "title": "Lab: Hierarchical Clustering",
    "section": "The dendrogram class",
    "text": "The dendrogram class\n\n\n\n\n\n\nNoteQuestion\n\n\n\nExploration of results of hierarchical clustering (objects of class hclust) is facilitated by converting to class dendrogram.\n\n\n\nCodedend.1 &lt;- hcl.1 %&gt;% \n  as.dendrogram()\n\nlength(dend.1[[1]][[2]])\n\n[1] 2\n\n\n\nCodeclass(dend.1)\n\n[1] \"dendrogram\"\n\nCodeclass(unclass(dend.1))\n\n[1] \"list\"\n\nCodemethods(class=class(dend.1)) %&gt;% head()\n\n[1] \"[[.dendrogram\"            \"all.equal.dendrogram\"    \n[3] \"as.dendrogram.dendrogram\" \"as.ggdend.dendrogram\"    \n[5] \"as.hclust.dendrogram\"     \"click_rotate.dendrogram\" \n\n\n\nCodedend.1 %&gt;% head()\n\n--[dendrogram w/ 2 branches and 50 members at h = 701]\n  |--[dendrogram w/ 2 branches and 16 members at h = 141]\n  |  |--[dendrogram w/ 2 branches and 10 members at h = 69.3]\n  |  |  |--[dendrogram w/ 2 branches and 3 members at h = 30.1] ..\n  |  |  `--[dendrogram w/ 2 branches and 7 members at h = 43.4] ..\n  |  `--[dendrogram w/ 2 branches and 6 members at h = 82.3]\n  |     |--[dendrogram w/ 2 branches and 4 members at h = 33.4] ..\n  |     `--[dendrogram w/ 2 branches and 2 members at h = 38.5] ..\n  `--[dendrogram w/ 2 branches and 34 members at h = 353]\n     |--[dendrogram w/ 2 branches and 14 members at h = 106]\n     |  |--[dendrogram w/ 2 branches and 6 members at h = 42.5] ..\n     |  `--[dendrogram w/ 2 branches and 8 members at h = 44.8] ..\n     `--[dendrogram w/ 2 branches and 20 members at h = 163]\n        |--[dendrogram w/ 2 branches and 10 members at h = 38.5] ..\n        `--[dendrogram w/ 2 branches and 10 members at h = 66] ..\netc... \n\n\n\nCodedend.1 %&gt;% \n  ggdendrogram(rotate = TRUE,labels = T) +\n  ggtitle(\"Dendrogram for USArrests\") +\n  ggdendro::theme_dendro() +\n  scale_y_reverse(expand = c(0.2, 0))\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\n\n\nCode# label(dend.1)\n\ndend.2 &lt;-  as.dendrogram(hcl.1)\n# order it the closest we can to the order of the observations:\ndend.2 &lt;- rotate(dend.2, 1:50)\n# Color the branches based on the clusters:\ndend.2 &lt;- color_branches(dend.2, k=3) #, groupLabels=iris_species)\n# Manually match the labels, as much as possible, to the real classification of the flowers:\n# labels_colors(dend.2) &lt;-\n#    rainbow_hcl(3)[sort_levels_values(\n#       as.numeric(iris[,5])[order.dendrogram(dend.2)]\n#    )]"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#ward-method",
    "href": "core/labs-solutions/lab-hclust.html#ward-method",
    "title": "Lab: Hierarchical Clustering",
    "section": "Ward method",
    "text": "Ward method\nThe meth=ward.D2 option allows you to aggregate individuals according to the method of Ward, that is, according to the variance.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the distance used? Describe the method of classification by variance?\n\n\nThe output clas$height gives the jump height of the dendrogram to each new iteration. In the case of Ward’s method, she is proportional to the loss of inter-class variance.\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nHow many groups are there at step 0? at the last step?\nHow many iterations are there?\nRecall the definition of inter-class variance.\nWhat is the inter-class variance at step 0? at the last step? How is it going according to the number of groups (or according to the number of iterations)?\nBy comparing the total inertia and the `clas$height’ output, find the coefficient of proportionality between the loss of inter-class variance and height of jumps."
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#choice-of-the-number-of-classes",
    "href": "core/labs-solutions/lab-hclust.html#choice-of-the-number-of-classes",
    "title": "Lab: Hierarchical Clustering",
    "section": "Choice of the number of classes",
    "text": "Choice of the number of classes\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nPlot the curve corresponding to the loss of variance inter in as a function of the number of iterations :\nSelect the “optimal” number of classes.\nVerify that, for the number of classes chosen, the number by class is sufficient (we can use the cutree function).\nThese classes can be represented using a dendrogram\nYou can also colour the leaves of the tree corresponding to a class. To do this, install and load the package `dendextend’."
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#link-with-pca",
    "href": "core/labs-solutions/lab-hclust.html#link-with-pca",
    "title": "Lab: Hierarchical Clustering",
    "section": "Link with PCA",
    "text": "Link with PCA\nWe will represent the classes obtained in the factorial design(s) obtained by the PCA. This will make it possible to represent the classes and describe them according to the variables initials.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nRepresent the coordinates of the individuals in each group in the first factorial plane (with one color for each class). The vector generated by `cutree’ can be used to form a color vector. Interpretation.\n\n\n\nCodemutate(USArrests, \n       .cluster = factor(cutree(hcl.1, 10))) %&gt;% \n  inner_join(map_data(\"state\"), by = \"region\") %&gt;%\n  ggplot() +\n  aes(x=long, y=lat, group=region, fill=.cluster) +\n  geom_polygon() +\n  scale_fill_viridis_d() +\n  ggtitle(\"Components of arrest data\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nCode#data(france)\n\n\n\nCode(map_data(\"state\") %&gt;%\n  ggplot() +\n  aes(x=long, \n      y=lat, \n      label=factor(region), \n      fill = factor(region)) +\n  geom_polygon() +\n  scale_fill_viridis_d() +\n  ggtitle(\"USA\") +\n  theme(legend.position = \"none\")) |&gt; \n    ggplotly()"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#cophenetic-distance",
    "href": "core/labs-solutions/lab-hclust.html#cophenetic-distance",
    "title": "Lab: Hierarchical Clustering",
    "section": "Cophenetic distance",
    "text": "Cophenetic distance\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\n\n\nCodedist.coph.1 &lt;- cophenetic(dend.1)"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#cophenetic-distance-between-dendrograms",
    "href": "core/labs-solutions/lab-hclust.html#cophenetic-distance-between-dendrograms",
    "title": "Lab: Hierarchical Clustering",
    "section": "Cophenetic distance between dendrograms",
    "text": "Cophenetic distance between dendrograms\n\nCodedata(\"iris\")\n\nhcl.iris &lt;- iris %&gt;% \n  select(where(is.numeric)) %&gt;% \n  scale() %&gt;% \n  dist() %&gt;% \n  hclust(meth=\"ward.D2\")\n\ndend.iris &lt;-  dendro_data(hcl.iris)\n\ndend.iris %$% (\n  ggplot() +\n  geom_segment(data = segments,\n               aes(x = x, y = y,\n                   xend = xend, yend = yend)\n  ) +\n  geom_text(data = labels,\n            aes(x = x, y = y,\n                label = label, hjust = 0),\n            size = 3\n  ) +\n  coord_flip() +\n  scale_y_reverse(expand = c(0.2, 0))\n)"
  },
  {
    "objectID": "core/labs-solutions/lab-hclust.html#references",
    "href": "core/labs-solutions/lab-hclust.html#references",
    "title": "Lab: Hierarchical Clustering",
    "section": "References",
    "text": "References\nggdendro\ndendroextra\nhier_clust, tidyclust"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-categorical.html",
    "href": "core/labs-solutions/lab-univariate-categorical.html",
    "title": "Univariate Categorical Analysis",
    "section": "",
    "text": "M1 MIDS/MFA/LOGOS\n\n\nUniversité Paris Cité\n\n\nAnnée 2025\n\n\nCourse Homepage\n\n\nMoodle"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-categorical.html#counting",
    "href": "core/labs-solutions/lab-univariate-categorical.html#counting",
    "title": "Univariate Categorical Analysis",
    "section": "Counting",
    "text": "Counting\n\n\n\n\n\n\nNoteQuestion\n\n\n\nUse table, prop.table from base R to compute the frequencies and proportions of the different levels. In statistics, the result of table() is a (one-way) contingency table.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodedf |&gt;\n  pull(REV_FOYER) |&gt;\n  table()\n\n\n      [0-5000)    [5000-7500)   [7500-10000)  [10000-12500)  [12500-15000) \n             9              5              5              9              7 \n [15000-17500)  [17500-20000)  [20000-25000)  [25000-30000)  [30000-35000) \n            19             26             38             30             35 \n [35000-40000)  [40000-50000)  [50000-60000)  [60000-75000)  [75000-1e+05) \n            61             70             71             89             77 \n[1e+05-150000) \n            48 \n\n\n\nCodedf |&gt;\n  count(REV_FOYER)\n\n# A tibble: 16 × 2\n   REV_FOYER          n\n   &lt;fct&gt;          &lt;int&gt;\n 1 [0-5000)           9\n 2 [5000-7500)        5\n 3 [7500-10000)       5\n 4 [10000-12500)      9\n 5 [12500-15000)      7\n 6 [15000-17500)     19\n 7 [17500-20000)     26\n 8 [20000-25000)     38\n 9 [25000-30000)     30\n10 [30000-35000)     35\n11 [35000-40000)     61\n12 [40000-50000)     70\n13 [50000-60000)     71\n14 [60000-75000)     89\n15 [75000-1e+05)     77\n16 [1e+05-150000)    48\n\n\n\n\n\nWhat is the class of the object generated by table? Is it a vector, a list, a matrix, an array ?\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodeta &lt;- df %&gt;% \n  pull(REV_FOYER) %&gt;% \n  table() \n\nl &lt;- list(\n  is.vector=is.vector, \n  is.list=is.list, \n  is.matrix=is.matrix, \n  is.array=is.array\n)\n\nmap_lgl(l, ~ .x(ta))\n\nis.vector   is.list is.matrix  is.array \n    FALSE     FALSE     FALSE      TRUE \n\n\n\n\n\n\n\n\n\n\n\nas.data.frame() (or as_tibble) can transform a table object into a dataframe.\n\nCodeta &lt;-  rename(as.data.frame(ta), REV_FOYER=`.`)\n\nta\n\n        REV_FOYER Freq\n1        [0-5000)    9\n2     [5000-7500)    5\n3    [7500-10000)    5\n4   [10000-12500)    9\n5   [12500-15000)    7\n6   [15000-17500)   19\n7   [17500-20000)   26\n8   [20000-25000)   38\n9   [25000-30000)   30\n10  [30000-35000)   35\n11  [35000-40000)   61\n12  [40000-50000)   70\n13  [50000-60000)   71\n14  [60000-75000)   89\n15  [75000-1e+05)   77\n16 [1e+05-150000)   48\n\n\n\n\n\nYou may use knitr::kabble(), possibly knitr::kable(., format=\"markdown\") to tweak the output.\nIf you are more ambitious, use gt::....\nIn order to feed ggplot with a contingency table, it is useful to build contingency tables as dataframes. Use dplyr::count() to do this.\n\n\n\n\n\n\nskimr::skim() allows us to perform univariate categorical analysis all at once.\n\nCodedf %&gt;% \n  skimr::skim(where(is.factor)) %&gt;% \n  print(n=50)\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Piped data\nNumber of rows             599       \nNumber of columns          11        \n_______________________              \nColumn type frequency:               \n  factor                   9         \n________________________             \nGroup variables            None      \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 SEXE                  0             1 FALSE          2\n2 REGION                0             1 FALSE          4\n3 STAT_MARI             0             1 FALSE          5\n4 SYNDICAT              0             1 FALSE          2\n5 CATEGORIE             0             1 FALSE         10\n6 NIV_ETUDES            0             1 FALSE         15\n7 NB_PERS               0             1 FALSE          9\n8 NB_ENF                0             1 FALSE          7\n9 REV_FOYER             0             1 FALSE         16\n  top_counts                           \n1 M: 302, F: 297                       \n2 S: 200, W: 148, NE: 129, NW: 122     \n3 M: 325, C: 193, D: 61, S: 14         \n4 non: 496, oui: 103                   \n5 Lib: 133, Ser: 125, Adm: 94, Sel: 48 \n6 12 : 187, Col: 148, Bac: 114, Ass: 45\n7 2: 196, 4: 130, 3: 122, 1: 63        \n8 0: 413, 1: 86, 2: 76, 3: 18          \n9 [60: 89, [75: 77, [50: 71, [40: 70   \n\n\nThe output can be tailored to your specific objectives and fed to functions that are geared to displaying large tables (see packages knitr, DT, and gt)"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-categorical.html#using-a-for-loop",
    "href": "core/labs-solutions/lab-univariate-categorical.html#using-a-for-loop",
    "title": "Univariate Categorical Analysis",
    "section": "Using a for loop",
    "text": "Using a for loop\nWe have to build a barplot for each categorical variable. Here, we just have nine of them. We could do this using cut and paste, and some editing. In doing so, we would not comply with the DRY (Don’t Repeat Yourself) principle.\nIn order to remain DRY, we will attempt to abstract the recipe we used to build our first barplot.\nThis recipe is pretty simple:\n\nBuild a ggplot object with df as the data layer.\nAdd an aesthetic mapping a categorical column to axis x\n\nAdd a geometry using geom_bar\n\nAdd labels explaining the reader which column is under scrutiny\n\nWe first need to gather the names of the categorical columns. The following chunk does this in a simple way.\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodecol_names &lt;- df %&gt;% \n  select(where(is.factor))%&gt;%\n  names()\n\n\n\n\n\nIn the next chunk, we shall build a named list of ggplot objects consisting of barplots. The for loop body is almost obtained by cutting and pasting the recipe for the first barplot.\n\n\n\n\n\n\nNote an important difference: instead of something aes(x=col) where col denotes a column in the dataframe, we shall write aes(x=.data[[col]]) where col is a string that matches a column name. Writing aes(x=col) would not work.\nThe loop variable col iterates over the column names, not over the columns themselves.\nWhen using ggplot in interactive computations, we write aes(x=col), and, under the hood, the interpreter uses the tidy evaluation mechanism that underpins R to map df$col to the x axis.\nggplot functions like aes() use data masking to alleviate the burden of the working Statistician.\nWithin the context of ggplot programming, pronoun .data refers to the data layer of the graphical object.\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodelist_plots &lt;- list()\n\nfor (col in col_names){\n  p &lt;- df %&gt;% \n  ggplot() +\n  aes(x=.data[[col]]) +     # mind the .data pronoun\n  geom_bar() +\n  labs(\n    title=\"Census data\",\n    subtitle = col\n  )\n\n  list_plots[[col]] &lt;- p  # add the ggplot object to the list\n} \n\n\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nInspect the individual plots. ::: {.cell}\nCodep_temp &lt;- list_plots[[\"REV_FOYER\"]] +\n  aes(x=forcats::fct_infreq(.data[[\"REV_FOYER\"]]))\n\np_temp\n\n\n\n\n\n\n\n\n\n:::::\nIf the labels on the x-axis are not readable, we need to tweak them. This amounts to modifying the theme layer in the ggplot object, and more specifically the axis.text.x attribute.\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodep_temp +\n  theme(axis.text.x = element_text(angle = 45))"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-categorical.html#using-functional-programming-lapply-purrr...",
    "href": "core/labs-solutions/lab-univariate-categorical.html#using-functional-programming-lapply-purrr...",
    "title": "Univariate Categorical Analysis",
    "section": "Using functional programming (lapply, purrr::...)",
    "text": "Using functional programming (lapply, purrr::...)\nAnother way to compute the list of graphical objects replaces the for loop by calling a functional programming tool. This mechanism relies on the fact that in R, functions are first-class objects.\n\n\n\n\n\n\nPackage purrr offers a large range of tools with a clean API. Base R offers lapply().\n\n\n\nWe shall first define a function that takes as arguments a datafame, a column name, and a title. We do not perform any defensive programming. Call your function foo.\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodefoo &lt;- function(df, col, .title= \"WE NEED A TITLE!!!\"){\n  p &lt;- df %&gt;% \n  ggplot() +\n  aes(x=fct_infreq(.data[[col]])) +\n  geom_bar() +\n  labs(\n    title=.title,\n    subtitle = col\n  ) +\n  theme(axis.text.x = element_text(angle = 45)) \n  return(p)\n}\n\n\n\n\n\nFunctional programmming makes code easier to understand.\nUse foo, lapply or purrr::map() to build the list of graphical objects.\nWith purrr::map(), you may use either a formula or an anonymous function. With lapply use an anonymous function.\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodell &lt;- map(col_names, ~ foo(df, .x, \"Census data\"))\n\n\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodemap(col_names, \\(x) foo(df, x, \"Census data\"))\n\n\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodelapply(col_names, \\(x)  foo(df, x, \"Census data\"))\n\n\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nThis is essentially like executing\n\nCodell &lt;- list()\n\nfor (.x in col_names){\n  ll[[.x]] &lt;- foo(df, .x, \"Census data\")\n}\n\n\n\n\n\nPackage patchwork offers functions for displaying collections of related plots.\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodepatchwork::wrap_plots(ll, ncol=3)"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html",
    "href": "core/labs-solutions/lab-kmeans.html",
    "title": "Lab: Clustering k-means",
    "section": "",
    "text": "M1 MIDS/MFA/LOGOS\n\n\nUniversité Paris Cité\n\n\nAnnée 2025\n\n\nCourse Homepage\n\n\nMoodle\nCodestopifnot(\n  require(gt),\n  require(sloop),\n  require(lobstr), \n  require(skimr),\n  require(GGally),\n  require(patchwork),\n  require(ggforce),\n  require(glue),\n  require(ggfortify),\n  require(ggvoronoi),\n  require(magrittr),\n  require(broom),\n  require(tidyclust),\n  require(tidyverse)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(\n  theme_minimal(base_size=9, \n                base_family = \"Helvetica\")\n)\nCodeknitr::opts_chunk$set(\n  message = FALSE,\n  warning = FALSE,\n  comment=NA,\n  prompt=FALSE,\n  cache=FALSE,\n  echo=TRUE,\n  results='asis'\n)\nCodegc &lt;- options(ggplot2.discrete.colour=\"viridis\")\ngc &lt;- options(ggplot2.discrete.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.colour=\"viridis\")"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#foreword",
    "href": "core/labs-solutions/lab-kmeans.html#foreword",
    "title": "Lab: Clustering k-means",
    "section": "Foreword",
    "text": "Foreword\nThis lab is dedicated to the k-means clustering method. In words, k-means takes as input a collection of points in \\(\\mathbb{R}^d\\) (a numerical dataset) and a positive integer \\(k\\). It returns a collection of \\(k\\) points (the centers) from \\(\\mathbb{R}^d\\). The centers define a Voronoï tesselation/partition/diagran of \\(\\mathbb{R}^d\\). The Voronoï cells define a clustering of the original dataset."
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#voronoi-tesselationpartitiondiagram",
    "href": "core/labs-solutions/lab-kmeans.html#voronoi-tesselationpartitiondiagram",
    "title": "Lab: Clustering k-means",
    "section": "Voronoi tesselation/partition/diagram",
    "text": "Voronoi tesselation/partition/diagram\nWikipedia on Voronoï diagrams\nIn the next chunk, we generate a Voronoï diagram on \\(\\mathbb{R}^2\\) with \\(100\\) cells defined from \\(100\\) random points drawn from the uniform distribution on a square. Function stat_voronoi() comes from ggvoronoi\n\nCodeset.seed(45056)\n\npoints &lt;- tibble(\n  x=runif(100, 0, 1),\n  y=runif(100, 0, 1),\n  distance = sqrt((x-100)^2 + (y-100)^2)\n) \n\np &lt;- ggplot(points) +\n    aes(x=x, y=y) +\n    geom_point(size=.2) +\n    coord_fixed() +\n    xlim(c(-.25, 2.25)) +\n    ylim(c(-.25, 2.25)) \n\np + (p + stat_voronoi(geom=\"path\")) +\n  patchwork::plot_annotation(\n    title=\"Voronoi tesselation\",\n    subtitle = \"Left: 100 random points\\nRight: Voronoï diagram\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo adjacent Voronoï cells are separated by a (possibly semi-infinite) line segment\nLet the so-called centers be denoted by \\(c_1, \\ldots, c_n\\). They form the codebook \\(\\mathcal{C}\\).\n\nThe Voronoï cell with center \\(c_i\\) is defined by \\[\\left\\{x : x \\in \\mathbb{R}^d, \\qquad \\| x- c_i \\|_2 = \\min_{j \\leq n} \\|x -c_j\\|_2\\right\\}\\]\n\nThe center of a Voronoï cell is usually not its barycenter"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#k-means-objective-function",
    "href": "core/labs-solutions/lab-kmeans.html#k-means-objective-function",
    "title": "Lab: Clustering k-means",
    "section": "\nk-means objective function",
    "text": "k-means objective function\n\n\n\n\n\n\nNoteDefinition\n\n\n\nThe \\(k\\)-means algorithm aims at building a codebook \\(\\mathcal{C}\\) that minimizes\n\\[\\mathcal{C} \\mapsto \\sum_{i=1}^n \\min_{c \\in \\mathcal{C}}  \\Vert X_i - c\\Vert_2^2\\]\nover all codebooks with given cardinality\nIf \\(c \\in \\mathcal{C}\\) is the closest centroid to \\(X \\in \\mathbb{R}^p\\),\n\\[\\|c - X\\|^2\\]\nis the quantization/reconstruction error suffered when using codebook \\(\\mathcal{C}\\) to approximate \\(X\\)\n\n\n\n\n\n\n\n\n If there are no restrictions on the dimension of the input space, on the number of centroids, or on sample size, computing an optimal codebook is a \\(\\mathsf{NP}\\) -hard problem"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#the-kmeans-function",
    "href": "core/labs-solutions/lab-kmeans.html#the-kmeans-function",
    "title": "Lab: Clustering k-means",
    "section": "The kmeans() function",
    "text": "The kmeans() function\nkmeans() is a wrapper for a collection of Algorithms that look like the Lloyd algorithm\n\nInitialize\n\nChoose \\(k\\) centroids\n\n\nIterations: Two phases\n\nMovement\n\nAssign each sample point to the closest centroid (Assign each sample point to a class in the Voronoi partition defined by the centroids)\n\nUpdate\n\nFor each class in the current Voronoi partition, update the centroid so as to minimize the Within Cluster Sum of Squared distances.\n\n\n No guarantee to converge to a global optimum!\nProceeed by trial and error.\nRepeat the algorithm and keep the best result."
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#iris-data",
    "href": "core/labs-solutions/lab-kmeans.html#iris-data",
    "title": "Lab: Clustering k-means",
    "section": "Iris data",
    "text": "Iris data\n\n\n\n\n\n\nNoteQuestion\n\n\n\nRun kmeans() on the projection of the Iris dataset (data(iris)) on the Petal plane.\nCheck ?iris and https://en.wikipedia.org/wiki/Iris_flower_data_set for more on this (historical) dataset.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nWe look for a partition into three cells.\n\nCodedata(iris)\n\nkms &lt;- iris |&gt; \n  select(starts_with(\"Petal\")) |&gt;\n  kmeans(3)\n\nclass(kms)\n\n[1] \"kmeans\"\n\nCodesloop::otype(kms)\n\n[1] \"S3\"\n\n\nThe result is an object of class kmeans. The class is equiped with broom methods.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nCheck the attributes of object kms\n\nUnclass the object and check the attributes again.\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodeattributes(kms)\n\n$names\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n$class\n[1] \"kmeans\"\n\nCodeukms &lt;- unclass(kms)\nattributes(ukms)\n\n$names\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nCodeclass(kms)\n\n[1] \"kmeans\"\n\nCodekms_bornagain &lt;- structure(\n  ukms, \n  class=\"kmeans\")\n\nsloop::otype(kms_bornagain)\n\n[1] \"S3\"\n\nCodeclass(kms_bornagain)\n\n[1] \"kmeans\"\n\n\nObjects of class kmeans have two attributes names and class. Because of the class attribute, objects of class kmeans are not just lists with named elements."
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#summarizing-a-clustering",
    "href": "core/labs-solutions/lab-kmeans.html#summarizing-a-clustering",
    "title": "Lab: Clustering k-means",
    "section": "Summarizing a clustering",
    "text": "Summarizing a clustering\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCheck the structure of objects of class kmeans and use broom::tidy() to get a summary.\nCompare with summary() from base R\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodedf_centers &lt;- select(iris, starts_with(\"Petal\")) |&gt;\n  kmeans(centers = 3) |&gt; \n  broom::tidy() \n\ndf_centers |&gt;  \n  gt::gt() |&gt;\n  gt::fmt_number(decimals = 2) |&gt;\n  gt::tab_caption(\"Iris clustering in the Petal plane, kmeans with 3 clusters\")\n\n\n\n\nIris clustering in the Petal plane, kmeans with 3 clusters\n\nPetal.Length\nPetal.Width\nsize\nwithinss\ncluster\n\n\n\n4.27\n1.34\n52.00\n13.06\n1\n\n\n1.46\n0.25\n50.00\n2.02\n2\n\n\n5.60\n2.04\n48.00\n16.29\n3\n\n\n\n\n\n\n\nHow are the rows related to clusters?\nWhat are the coordinates good for?\nWhat does the size column mean?\n\nwithinss stands for Within Sum of Squares. How is it computed? Why is it useful?"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#visualizing-a-clustering",
    "href": "core/labs-solutions/lab-kmeans.html#visualizing-a-clustering",
    "title": "Lab: Clustering k-means",
    "section": "Visualizing a clustering",
    "text": "Visualizing a clustering\n\n\n\n\n\n\nNoteQuestion\n\n\n\nUse broom::augment() and broom::tidy() to prepare two dataframes that will allow you to overlay a scatterplot of the dataset and a Voronoï diagram defined by the centers output by kmeans().\nCompare the result with plot()\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodeq &lt;- kms |&gt; \n  augment(iris) |&gt;\n  ggplot() +\n  aes(x=Petal.Length, \n      y=Petal.Width\n      ) +\n  geom_point(aes(shape=Species), size=1, show.legend = F) +\n  coord_fixed()\n\nqq &lt;-  (q + geom_point(aes(shape=Species, \n                           colour=.cluster), \n                       size=1))+\n  stat_voronoi(data = df_centers,   #&lt;&lt;\n               geom=\"path\",\n               outline=data.frame(x=c(0, 7, 7, 0), \n                                  y=c(0, 0, 3, 3))\n               ) +\n  geom_point(data = df_centers,   #&lt;&lt;\n             colour = \"black\",\n             shape=\"+\",\n             size=5)  \n\nq / qq +\n  plot_annotation(title = \"Kmeans over Iris dataset, k=3\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodegeom_sugar &lt;- list(\n    stat_voronoi(data = df_centers,\n                 geom=\"path\",\n                 alpha=.5,\n                 outline = tribble(~x, ~y,\n                                   0., 0.,\n                                   7., 0.,\n                                   7., 3,\n                                   0., 3) \n                 ),\n    geom_point(data = df_centers,   \n               colour = \"black\",\n               shape=\"+\",\n               size=5),\n    coord_fixed(),\n    labs(col=\"Voronoï cells\")\n)\n\n\n\nCodebroom::augment(kms, iris) |&gt;\n  ggplot(aes(x=Petal.Length, y=Petal.Width)) +\n  geom_point(aes(shape=Species, color=.cluster)) +\n  geom_sugar \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodekms |&gt; \n  autoplot(data=iris)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nRedo the same operations but choose the Sepal.xxx dimension.\nDesign a function to avoid repetitive coding.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodeplot_km_centroids &lt;- function(augmented_km, centroids, col1, col2){\n\n  outline &lt;- augmented_km |&gt;\n    dplyr::select({{col1}}, {{col2}}) |&gt;\n    dplyr::rename(x={{col1}}, y={{col2}}) |&gt; \n    summarise(across(everything(), .fns=list(\"min\"= min, \"max\"=max), .names=\"{.col}_{.fn}\"))\n\n  tb_outline &lt;- tibble(\n    x = with(outline, c(x_min-1.0, x_max+1.0, x_max+1.0, x_min-1.0)),\n    y = with(outline, rep(c(y_min-1, y_max+1), each=2)),\n    group=rep(1, 4)\n  )\n\n  p &lt;- augmented_km |&gt; \n  ggplot() +\n  aes(\n    x={{col1}}, \n    y={{col2}}) +\n  geom_point(aes(colour=.cluster))  +\n  stat_voronoi(data = centroids,\n               geom=\"path\",\n               outline= tb_outline\n  ) +\n  geom_point(data = centroids,\n             colour = \"black\",\n             shape=\"+\",\n             size=5) +\n  coord_fixed() +\n  theme_minimal()\n\n  if (has_rownames(augmented_km)) {\n    p &lt;- p +\n      ggrepel::geom_label_repel(\n        aes(colour=.cluster, \n            label=`.rownames`))\n  }\n\n  return(p)\n}\n\n\n\n\n\nCodekms &lt;- kmeans(select(iris, Sepal.Length, Sepal.Width), 3)\n\n\nplot_km_centroids(\n  augment(kms, iris), \n  tidy(kms), \n  Sepal.Length, \n  Sepal.Width\n)"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#playing-with-k",
    "href": "core/labs-solutions/lab-kmeans.html#playing-with-k",
    "title": "Lab: Clustering k-means",
    "section": "Playing with \\(k\\)\n",
    "text": "Playing with \\(k\\)\n\nThe number of cells/clusters may not be given a priori. Conducting clustering using a method like kmeans requires picking a reasonable choice for k.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPerform kmeans clustering with \\(k=2\\). Use glance, tidy, augment to discuss the result.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodedf &lt;- select(iris, \n             starts_with(\"Sepal\"))\n\nkms &lt;- kmeans(df, 2)\n\n\n\nCodeplot_km_centroids(\n  augment(kms, iris),\n  tidy(kms),\n  Sepal.Length,\n  Sepal.Width\n)\n\n\n\n\n\n\n\nWe can compare the spread between inner and outer sum of squares for clusterings with \\(k \\in 2, 3\\).\n\nCodebind_rows(glance(kms),\n          glance(kmeans(df, centers=3,\n              nstart = 32L))) |&gt; \n  mutate(k=c(2, 3))  |&gt;\n  gt::gt()\n\n\n\n\n\ntotss\ntot.withinss\nbetweenss\niter\nk\n\n\n\n130.4753\n58.2150\n72.26027\n1\n2\n\n\n130.4753\n37.0507\n93.42456\n3\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPerform k-means for \\(k=2, ... 10\\), plot within sum of squares as function of \\(k\\). Comment.\n\n\n\nCodetmp &lt;-map_dfr(2:10, ~ glance(kmeans(df, \n                                   centers=.,\n                                   nstart = 32L))) |&gt; \n  rowid_to_column(var=\"k\") |&gt; \n  mutate(k=k+1, across(where(is.numeric), ~ signif(.x, 3))) \n\n\n\nCodetmp |&gt;\n  ggplot(aes(x=forcats::as_factor(k), y=tot.withinss/totss)) +\n  geom_col(width=.25) +\n  ggtitle(\"Iris data\", \"WithinSS/Total Sum of Squares as a function of k\") +\n  xlab(\"k\") +\n  ylab(\"Within Clusters Sum of Squares (relative)\") +\n  scale_x_discrete(breaks=as.character(2:10), labels=as.character(2:10))\n\n\n\n\n\n\n\n\nCodekms &lt;- kmeans(df, 4)\niris4 &lt;- broom::augment(kms, iris)\n\n\nplot_km_centroids(\n  augment(kms, iris),\n  tidy(kms),\n  Sepal.Length,\n  Sepal.Width\n)  +\n ggtitle(label=\"Kmeans Iris data\",\n         subtitle=\"k=4\") +\n labs(col=\"Clusters\")\n\n\n\n\n\n\n\n\nCodebroom::tidy(kmeans(df, 4)) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(decimals = 2)\n\n\n\n\n\nSepal.Length\nSepal.Width\nsize\nwithinss\ncluster\n\n\n\n5.52\n2.61\n33.00\n5.97\n1\n\n\n5.02\n3.45\n49.00\n11.57\n2\n\n\n6.27\n2.91\n41.00\n4.85\n3\n\n\n7.10\n3.11\n27.00\n6.02\n4"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#lloyds-iterations",
    "href": "core/labs-solutions/lab-kmeans.html#lloyds-iterations",
    "title": "Lab: Clustering k-means",
    "section": "Lloyd’s iterations",
    "text": "Lloyd’s iterations\nThe kmeans function does not minimize the kmeans cost. It offers a collection of iterative algorithms that aim at approximately minimizing the cost.\n\n\n\n\n\n\n\nInitialize\n\nChoose \\(k\\) centroids\n\n\nIterations: Two phases\n\nMovement\n\nAssign each sample point to the closest centroid (Assign each sample point to a class in the Voronoi partition defined by the centroids)\n\nUpdate\n\nFor each class in the current Voronoi partition, update the centroid so as to minimize the Within Cluster Sum of Squared distances.\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodekm &lt;- list(centers=df[1:3, ]) # stupid initialization\n\nsequence &lt;- list()\n\nfor (i in 1:20) {\n  km &lt;- kmeans(df,\n               km$centers,\n               algorithm = \"Lloyd\",\n               iter.max = 1)\n  sequence[[length(sequence)+1]] &lt;- force(km)\n}\n\n\n\nCodeadd_voronoi &lt;- function(p, kmscenters, marker){\n  p +\n    geom_point(data=data.frame(kmscenters),         #&lt;&lt;\n               mapping=aes(x=Sepal.Length, y=Sepal.Width),\n               shape=marker,\n               col=\"black\",\n               size=5) +\n    stat_voronoi(data = as.data.frame(kmscenters),  #&lt;&lt;\n                 aes(x=Sepal.Length,y=Sepal.Width),\n                 geom=\"path\",\n                 outline=data.frame(x=c(4, 8, 8, 4),\n                                    y=c(2, 2, 4.5, 4.5)))\n}\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodei &lt;- 2\n\np &lt;- broom::augment(sequence[[i]], iris) |&gt;\n  ggplot() +\n  coord_fixed(ratio=1) +\n  geom_point(aes(x=Sepal.Length, y=Sepal.Width, shape=Species, col=.cluster)) +\n  ggtitle(\"Kmeans Lloyd's algorithm\", \"Iris data\")\n\np |&gt;\n  add_voronoi(sequence[[i]]$centers, marker=\"o\") +   #&lt;&lt;\n  labs(colour=paste(\"Cluster, step \", i- 1))\n\n\n\n\n\n\n\n\nCodei &lt;- 2\n\n(p %+%\n  broom::augment(sequence[[i]], iris)) |&gt;\n  add_voronoi(sequence[[i]]$centers, marker='+') +   #&lt;&lt;\n  geom_point(data=data.frame(sequence[[2]]$centers),   #&lt;&lt;\n             mapping=aes(x=Sepal.Length, y=Sepal.Width),\n             shape=\"o\", col=\"black\", size=5) +\n  labs(colour=paste(\"Cluster, step \", i- 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodei &lt;- 3\n\n(p %+%\n  broom::augment(sequence[[i]], iris)) |&gt;\n  add_voronoi(sequence[[i]]$centers, marker='+') +   #&lt;&lt;\n  geom_point(data=data.frame(sequence[[2]]$centers),   #&lt;&lt;\n             mapping=aes(x=Sepal.Length, y=Sepal.Width),\n             shape=\"o\", col=\"black\", size=5) +\n  labs(colour=paste(\"Cluster, step \", i- 1))\n\n\n\n\n\n\n\n\nCodei &lt;- 5\n\n(p %+%\n  broom::augment(sequence[[i]], iris)) |&gt;\n  add_voronoi(sequence[[i]]$centers, marker='*') +   #&lt;&lt;\n  geom_point(data=data.frame(sequence[[2]]$centers),   #&lt;&lt;\n             mapping=aes(x=Sepal.Length, y=Sepal.Width),\n             shape=\"o\", col=\"black\",size=5) +\n  labs(colour=paste(\"Cluster, step \", i- 1))"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#revisiting-the-swiss-fertility-data",
    "href": "core/labs-solutions/lab-kmeans.html#revisiting-the-swiss-fertility-data",
    "title": "Lab: Clustering k-means",
    "section": "Revisiting the swiss fertility data",
    "text": "Revisiting the swiss fertility data\n\n\n\n\n\n\nNoteQuestion\n\n\n\nRecall the dataset used in Lab PCA\nPerform kmeans clustering in original coordinates and kmeans clustering in the first principal coordinates plane\nCompare the results\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCodedata(swiss)\n\nswiss_scaled &lt;- swiss |&gt;  \n  select(-Fertility) |&gt; \n  scale() \n\nkm.2.swiss &lt;- swiss_scaled |&gt; \n  kmeans(centers = 2,  nstart = 10L)\n\ndf_centers.2 &lt;- broom::tidy(km.2.swiss)\n\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCodekm.2.swiss.pca &lt;-  swiss_scaled |&gt;\n  prcomp() |&gt; \n  augment(data=swiss)|&gt; \n  dplyr::select(starts_with(\".fittedPC\")) |&gt; \n  kmeans(centers=2, nstart = 10L)\n\ndf_centers.2.pca &lt;- tidy(km.2.swiss.pca)\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nCodedf_centers.2 |&gt;\n  gt::gt() |&gt; \n  gt::fmt_number(decimals = 2)\n\n\n\n\n\nAgriculture\nExamination\nEducation\nCatholic\nInfant.Mortality\nsize\nwithinss\ncluster\n\n\n\n−0.46\n0.57\n0.36\n−0.67\n−0.17\n28.00\n101.66\n1\n\n\n0.68\n−0.83\n−0.53\n0.99\n0.25\n19.00\n49.26\n2\n\n\n\n\n\n\n\nCodekm.2.swiss |&gt; \n  broom::glance() |&gt;\n  gt::gt() |&gt; \n  gt::fmt_number(decimals = 2)\n\n\n\n\n\ntotss\ntot.withinss\nbetweenss\niter\n\n\n230.00\n150.91\n79.09\n1.00\n\n\n\n\n\n\n\nCodedf_centers.2.pca |&gt;\n  gt::gt() |&gt; \n  gt::fmt_number(decimals = 2)\n\n\n\n\n\n.fittedPC1\n.fittedPC2\n.fittedPC3\n.fittedPC4\n.fittedPC5\nsize\nwithinss\ncluster\n\n\n\n1.50\n0.32\n−0.36\n−0.02\n−0.07\n19.00\n49.26\n1\n\n\n−1.02\n−0.21\n0.25\n0.02\n0.05\n28.00\n101.66\n2\n\n\n\n\n\n\n\nCodekm.2.swiss.pca |&gt; \n  broom::glance() |&gt;\n  gt::gt() |&gt; \n  gt::fmt_number(decimals = 2)\n\n\n\n\n\ntotss\ntot.withinss\nbetweenss\niter\n\n\n230.00\n150.91\n79.09\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCodeplot_km_centroids(\n  broom::augment(km.2.swiss, scale(swiss)),\n  broom::tidy(km.2.swiss), \n  Education, \n  Infant.Mortality\n) +\n  labs(\n    title= \"Kmeans over Swiss dataset, k=2\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodeplot_km_centroids(\n  augment(km.2.swiss.pca,\n         broom::augment(prcomp(swiss_scaled), data=swiss)),\n  tidy(km.2.swiss.pca),\n  .fittedPC1,\n  .fittedPC2\n) + labs(\n      title=\"Kmeans over Swiss dataset, k=2\",\n      subtitle=\"Clustering over the Principal components\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodekm.4.swiss.pca &lt;-  swiss_scaled |&gt;\n  prcomp() |&gt; \n  broom::augment(data=swiss)|&gt; \n  dplyr::select(starts_with(\".fittedPC\")) |&gt; \n  kmeans(centers=4, nstart = 10L)\n\ndf_centers.4.pca &lt;- broom::tidy(km.4.swiss.pca)\n\n\n\nCodeplot_km_centroids(\n  augment(km.4.swiss.pca,\n         broom::augment(prcomp(swiss_scaled), data=swiss)),\n  tidy(km.4.swiss.pca),\n  .fittedPC1,\n  .fittedPC2\n) + labs(\n      title=\"Kmeans over Swiss dataset, k=4\",\n      subtitle=\"Clustering over the Principal components\"\n  )"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#revisiting-the-mortality-dataset",
    "href": "core/labs-solutions/lab-kmeans.html#revisiting-the-mortality-dataset",
    "title": "Lab: Clustering k-means",
    "section": "Revisiting the mortality dataset",
    "text": "Revisiting the mortality dataset\n\n\n\n\n\n\nNoteQuestion\n\n\n\nRecall the dataset used in Lab CA\nPerform kmeans clustering of categories in the row principal coordinates and the column principal coordinates"
  },
  {
    "objectID": "core/labs-solutions/lab-kmeans.html#references",
    "href": "core/labs-solutions/lab-kmeans.html#references",
    "title": "Lab: Clustering k-means",
    "section": "References",
    "text": "References\nVignette k_means from tidyclust"
  },
  {
    "objectID": "core/labs-solutions/lab-cca.html",
    "href": "core/labs-solutions/lab-cca.html",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "",
    "text": "M1 MIDS/MFA/LOGOS\n\n\nUniversité Paris Cité\n\n\nAnnée 2025\n\n\nCourse Homepage\n\n\nMoodle"
  },
  {
    "objectID": "core/labs-solutions/lab-cca.html#canonical-correlation-analysis",
    "href": "core/labs-solutions/lab-cca.html#canonical-correlation-analysis",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "Canonical Correlation Analysis",
    "text": "Canonical Correlation Analysis\n\\[C(X,Y) = \\mathbb{E}\\left[X Y^\\top\\right]\\]\n\\[\\begin{bmatrix} C_{xx} & C_{xy} \\\\ C_{xy}^{\\top} & C_{yy}\\end{bmatrix}\\]\nThe first canonical components are the solution of the next problem\n\n\n\n\n\n\nNoteOptimization problem\n\n\n\n\\[\\begin{array}{lll}\\text{Maximize} & & u^\\top C_{xy}  v \\\\\\text{subject to} & & u^\\top C_{xx} uv=1 =v^\\top C_{yy} v     \\end{array}\\]\n\n\n\n\n\n\n\n\nNoteProposition\n\n\n\nLet\n\\[U \\times D \\times V^\\top\\]\nbe a SVD of\n\\[C_{xx}^{-1/2} \\times C_{xy} \\times C_{yy}^{-1/2}\\]\nThe solution to the optimization problem above is\n\\[a = C_{xx}^{-1/2} u_1 \\qquad \\text{and} \\qquad b= S_{yy}^{-1/2} v_1\\]\nwhere \\(u_1\\) and \\(v_1\\) are the leading left and right singular vectors of \\(C_{xx}^{-1/2} \\times C_{xy} \\times C_{yy}^{-1/2}\\), that is the first column vectors of \\(U\\) and \\(V\\).\n\n\n\nProof: \n\n\n\n\n\n\nNoteProposition\n\n\n\nA sequence of canonical components of \\(C_{xy}\\) can be obtained from the sequence of (extended) left and right singular vectors of \\(C_{xy}\\) with respect to \\(C_{xx}\\) and \\(C_{yy}\\)\n\n\nProof: \n\n\n\n\n\n\nNoteProposition\n\n\n\nLet \\(H_X\\) (resp. \\(H_Y\\)) be orthorgonal projection matrix on the linear space spanned by the columns of \\(X\\) (resp. \\(Y\\)).\nCanonical correlations \\(ρ_1 \\geq \\ldots \\geq \\rho_s, \\ldots\\) are the positive square roots of the eigenvalues \\(\\lambda_1, \\ldots \\geq \\lambda_s, \\ldots\\) of \\(H_X \\times H_Y\\) (which are the same as \\(H_Y \\times H_X\\)): \\(ρ_s = λ_s\\)  Vectors \\(U^1, \\ldots, U^{p_1}\\) are the standardized eigenvectors corresponding to the decreasing eigenvalues \\(λ_1 \\geq \\ldots \\geq \\lambda_{p_1}\\) of \\(H_X  \\times H_Y\\)\nVectors \\(V^1, \\ldots, V^{p_2}\\) are the standardized eigenvectors corresponding to the decreasing eigenvalues \\(λ_1 \\geq \\ldots \\geq \\lambda_{p_2}\\) of \\(H_X  \\times H_Y\\)"
  },
  {
    "objectID": "core/labs-solutions/lab-cca.html#canonical-correlation-analysis-cca-in-r",
    "href": "core/labs-solutions/lab-cca.html#canonical-correlation-analysis-cca-in-r",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "Canonical Correlation Analysis (CCA) in R\n",
    "text": "Canonical Correlation Analysis (CCA) in R\n\n\ncancor() from base package R\n\nFunction cancor(x, y, xcenter=T, ycenter=T) computes the canonical correlations between two data matrices x and y. Henceforth we assume that the columns of x and y are centered. Matrices x and y have the same number \\(n\\) of rows. x (resp. y) has p1 (resp. p2) columns.\n\nThe canonical correlation analysis seeks linear combinations of the y variables which are well explained by linear combinations of the x variables. The relationship is symmetric as well explained is measured by correlations.\n\nThe result is a list of five components\n\n\ncor correlations.\n\nxcoef estimated coefficients for the x variables.\n\nycoef estimated coefficients for the y variables.\n\nOur assumption above allows us to assume xcenter and ycenter are zeros.\nThe next example is taken from the documentation. Use ?LiveCycleSavings to get more information on the dataset.\n\nCodeLifeCycleSavings |&gt; \n  as_tibble() |&gt;\n  slice_sample(n=5)\n\n# A tibble: 5 × 5\n     sr pop15 pop75   dpi  ddpi\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  12.1  23.3  4.41 1508.  3.93\n2  12.9  42.2  0.83  728.  4.56\n3  18.6  45.2  0.56  138.  5.14\n4  12.8  46.3  1.12  152.  2   \n5  13.2  23.8  4.43 2108.  3.82\n\n\n\nCodefm1 &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = LifeCycleSavings)\n \nsummary(fm1)\n\n\nCall:\nlm(formula = sr ~ pop15 + pop75 + dpi + ddpi, data = LifeCycleSavings)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.2422 -2.6857 -0.2488  2.4280  9.7509 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 28.5660865  7.3545161   3.884 0.000334 ***\npop15       -0.4611931  0.1446422  -3.189 0.002603 ** \npop75       -1.6914977  1.0835989  -1.561 0.125530    \ndpi         -0.0003369  0.0009311  -0.362 0.719173    \nddpi         0.4096949  0.1961971   2.088 0.042471 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.803 on 45 degrees of freedom\nMultiple R-squared:  0.3385,    Adjusted R-squared:  0.2797 \nF-statistic: 5.756 on 4 and 45 DF,  p-value: 0.0007904\n\n\n\nCodepop &lt;- LifeCycleSavings |&gt; \n  dplyr::select(starts_with('pop'))\noec &lt;- LifeCycleSavings |&gt; \n  dplyr::select(-starts_with('pop'))\n  \nres.cca &lt;- cancor(pop, oec)\n\nres.cca$cor\n\n[1] 0.8247966 0.3652762\n\n\nThis tells us that highest possible linear correlation beween a linear combination of pop15, pop75 and a linear combination of sr, dpi, ddpi is res.cca$cor[1]. The coefficients of the corresponding linear combinations can be found on the rows of components xcoef and ycoef\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCheck that the different components of the output of cancor() satisfy all properties they should satisfy.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodecc &lt;- cor(\n  as.matrix(pop) %*% as.matrix(res.cca$xcoef), \n  as.matrix(oec) %*% as.matrix(res.cca$ycoef)\n  )\n\ndiag(cc) - res.cca$cor\n\n[1]  2.220446e-16 -5.551115e-17\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nDesign a suite of tests (using testthat) that any contender of the implementation provided by package stats should pass.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nTODO\n\n\nPackage CCA\n\nAbstract of CCA: An R Package to Extend Canonical Correlation Analysis\n\nCanonical correlations analysis (CCA) is an exploratory statistical method to highlight correlations between two data sets acquired on the same experimental units. The cancor() function in R (R Development Core Team 2007) performs the core of computations but further work was required to provide the user with additional tools to facilitate the interpretation of the results.\n\nAs in PCA, CA, MCA, several kinds of graphical representations can be displayed from the results of CCA:\n\na barplot of the squared canonical correlations (which tells us about the low rank approximations of \\(H_X \\times H_Y\\))\nscatter plots for the initial variables \\(X^j\\) and \\(Y^k\\) (ako correlation circles)\nscatter plots for the individuals (rows)\nbiplots"
  },
  {
    "objectID": "core/labs-solutions/lab-cca.html#applications",
    "href": "core/labs-solutions/lab-cca.html#applications",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "Applications",
    "text": "Applications\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nLoad nutrimouse dataset from CCA.\nInsert the 4 elements of list nutrimouse in the global environment (see list2env())\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodestopifnot(\n  require(CCA)\n)\n\ndata(\"nutrimouse\")\n\ne &lt;- list2env(nutrimouse, .GlobalEnv)\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nCompute the cross correlation matrix between gene and lipid\n\nVisualize the cross correlation matrix\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodeY &lt;- as.matrix(gene)   # 40 x 120\nX &lt;- as.matrix(lipid)  # 40 x 20\n\n\n\nCodec_XY = corrr::correlate(cbind(X, Y))\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n\nCodec_XY_long &lt;- c_XY |&gt;\n  tidyr::pivot_longer(cols=-c(term), names_to=\"term2\", values_to=\"cor\")  \n\n\n\nCodep  &lt;- c_XY_long |&gt;\n  dplyr::filter(\n    term %in% names(gene),\n    term2 %in% names(lipid)\n  ) |&gt;\n  ggplot() +\n  aes(x=term2, y=term, fill=abs(cor)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  xlab(\"diet\") +\n  ylab(\"gene\")\n\np |&gt; plotly::ggplotly()\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nCompute the canonical correlations between gene and lipid, save the result in res.cca\n\nCheck the canonical correlations.\nComment\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCoderes.cca &lt;- cancor(as.matrix(gene), as.matrix(lipid))\n\n\n\nCodesum(res.cca$cor)\n\n[1] 21\n\n\n\\(H_X\\times H_Y\\) has 21 eigenvalues equal to \\(1\\). As the subspaces defined by the columns in gene and lipid have dimensions at most 21 and 40, \\(H_X\\times H_Y\\) equals the projection of \\(\\mathbb{R}^{40}\\) over the smallest subspace.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nSample 10 columns from gene and lipid and repeat the operation\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCodeset.seed(42)\nn &lt;- 10\n\nss_gene &lt;- gene |&gt;\n  dplyr::select(sample(names(gene),  n)) |&gt;\n  scale()\n\nss_lipid&lt;- lipid |&gt;\n  dplyr::select(sample(names(lipid),  n)) |&gt;\n  scale()\n\nres.cca &lt;- cancor(as.matrix(ss_gene), as.matrix(ss_lipid)) \n\nres.cc &lt;- cc(X=as.matrix(ss_gene), Y=as.matrix(ss_lipid))\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nScreeplot\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCoderes.cca$cor |&gt; \n  as_tibble() |&gt;\n  gt::gt() |&gt;\n  gt::fmt_scientific() |&gt;\n  gt::tab_caption(\"Canonical correlations between `gene` columns of nutrimouse and  `lipid` columns\")\n\n\n\n\nCanonical correlations between `gene` columns of nutrimouse and `lipid` columns\n\nvalue\n\n\n\n9.62 × 10−1\n\n\n\n8.82 × 10−1\n\n\n\n7.90 × 10−1\n\n\n\n7.35 × 10−1\n\n\n\n6.96 × 10−1\n\n\n\n5.66 × 10−1\n\n\n\n5.09 × 10−1\n\n\n\n2.67 × 10−1\n\n\n\n1.58 × 10−1\n\n\n\n6.62 × 10−2\n\n\n\n\n\n\n\n\nCoderes.cca$cor |&gt; \n  as_tibble() |&gt;\n  mutate(PC=as.factor(1:n), eig=value^2, percent=eig, cumulative=cumsum(eig)) |&gt;\n  ggplot() +\n    aes(x=PC, y=eig, label=eig) +\n    geom_col(fill=\"white\", color=\"black\") +\n    theme_minimal() +\n    labs(\n      title=\"Squared Canonical Correlations\",\n      subtitle=\"sample of 10 genes and 10 lipids\", \n      caption=\"nutrimouse data\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nBuild a correlation circle\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\nWe use function plt.cc from CCA.\n\nCodeCCA::plt.cc(\n  res.cc, \n  var.label=T,\n  ind.names = str_c(genotype, diet, sep=\"-\")\n)"
  },
  {
    "objectID": "core/labs-solutions/lab-cca.html#references",
    "href": "core/labs-solutions/lab-cca.html#references",
    "title": "Lab: Canonical Correlation Analysis",
    "section": "References",
    "text": "References\nhttps://www.jstatsoft.org/article/view/v023i12"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html",
    "href": "core/labs-solutions/lab-pca.html",
    "title": "LAB: Principal Component Analysis",
    "section": "",
    "text": "Code\n# We will use the following packages. \n# If needed, install them : pak::pkg_install(). \nstopifnot(\n  require(\"corrr\"),\n  require(\"magrittr\"),\n  require(\"lobstr\"),\n  require(\"ggforce\"),\n  require(\"gt\"),\n  require(\"glue\"),\n  require(\"skimr\"),\n  require(\"patchwork\"), \n  require(\"tidyverse\"),\n  require(\"ggfortify\")\n  # require(\"autoplotly\")\n)\nCode\nold_theme &lt;- theme_set(theme_minimal())\n\noptions(ggplot2.discrete.colour=\"viridis\")\noptions(ggplot2.discrete.fill=\"viridis\")\noptions(ggplot2.continuous.fill=\"viridis\")\noptions(ggplot2.continuous.colour=\"viridis\")"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#swiss-fertility-data",
    "href": "core/labs-solutions/lab-pca.html#swiss-fertility-data",
    "title": "LAB: Principal Component Analysis",
    "section": "Swiss fertility data",
    "text": "Swiss fertility data\nDataset swiss from datasets::swiss connect fertility and social, economic data within 47 French-speaking districts in Switzerland.\n\nFertility : fertility index\nAgriculture : jobs in agricultural sector\nExamination : literacy index (military examination)\nEducation : proportion of people with successful secondary education\nCatholic : proportion of Catholics\nInfant.Mortality : mortality quotient at age 0\n\nFertility index (Fertility) is considered as the response variable\nThe social and economic variables are covariates (explanatory variables).\nSee European Fertility Project for more on this dataset.\n\nPCA (Principal Component Analysis) is concerned with covariates.\n\n\nCode\ndata(\"swiss\")\n\nswiss %&gt;% \n  glimpse(50)\n\n\nRows: 47\nColumns: 6\n$ Fertility        &lt;dbl&gt; 80.2, 83.1, 92.5, 85.8,…\n$ Agriculture      &lt;dbl&gt; 17.0, 45.1, 39.7, 36.5,…\n$ Examination      &lt;int&gt; 15, 6, 5, 12, 17, 9, 16…\n$ Education        &lt;int&gt; 12, 9, 5, 7, 15, 7, 7, …\n$ Catholic         &lt;dbl&gt; 9.96, 84.84, 93.40, 33.…\n$ Infant.Mortality &lt;dbl&gt; 22.2, 22.2, 20.2, 20.3,…\n\n\nHave a look at the documentation of the dataset"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#describe-the-dataset",
    "href": "core/labs-solutions/lab-pca.html#describe-the-dataset",
    "title": "LAB: Principal Component Analysis",
    "section": "Describe the dataset",
    "text": "Describe the dataset\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCompute summary for each variable\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nIt is enough to call summary() on each column of swiss. This can be done in a functional programming style using package purrr. The collections of summaries can be rearranged so as to build a dataframe that is fit for reporting.\n\n\nCode\ntt &lt;- map_dfr(swiss, summary, .id = \"var\")  \n\n\n\n\nCode\ntt |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=1)\n\n\n\n\n\n\n\n\nvar\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n\n\n\n\nFertility\n35.00\n64.700\n70.40\n70.14255\n78.450\n92.5\n\n\nAgriculture\n1.20\n35.900\n54.10\n50.65957\n67.650\n89.7\n\n\nExamination\n3.00\n12.000\n16.00\n16.48936\n22.000\n37.0\n\n\nEducation\n1.00\n6.000\n8.00\n10.97872\n12.000\n53.0\n\n\nCatholic\n2.15\n5.195\n15.14\n41.14383\n93.125\n100.0\n\n\nInfant.Mortality\n10.80\n18.150\n20.00\n19.94255\n21.700\n26.6\n\n\n\n\n\n\n\nFunction skim from skimr delivers all univariate summaries in suitable form.\n\n\nCode\nfoo &lt;- swiss %&gt;% \n  select(-Fertility) %&gt;% \n  skim()  \n\n\n\n\nCode\nfoobar &lt;- foo %&gt;%  \n  filter(skim_type==\"numeric\") %&gt;% \n  rename(variable=skim_variable)  %&gt;% \n    mutate(across(where(is.numeric), ~ round(.x, digits=1))) \n\n\n\n\nCode\nfoobar %&gt;% \n  gt::gt() \n\n\n\n\n\n\n\n\nskim_type\nvariable\nn_missing\ncomplete_rate\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nnumeric\nAgriculture\n0\n1\n50.7\n22.7\n1.2\n35.9\n54.1\n67.7\n89.7\n▃▃▆▇▅\n\n\nnumeric\nExamination\n0\n1\n16.5\n8.0\n3.0\n12.0\n16.0\n22.0\n37.0\n▅▇▆▂▂\n\n\nnumeric\nEducation\n0\n1\n11.0\n9.6\n1.0\n6.0\n8.0\n12.0\n53.0\n▇▃▁▁▁\n\n\nnumeric\nCatholic\n0\n1\n41.1\n41.7\n2.1\n5.2\n15.1\n93.1\n100.0\n▇▁▁▁▅\n\n\nnumeric\nInfant.Mortality\n0\n1\n19.9\n2.9\n10.8\n18.1\n20.0\n21.7\n26.6\n▁▂▇▆▂\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nDisplay graphic summary for each variable.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nWe have to pick some graphical summary of the data. Boxplots and violine plots could be used if we look for concision.\nWe use histograms to get more details about each column.\nNot that covariates have different meanings: Agriculture, Catholic, Examination, and Education are percentages with values between \\(0\\) and \\(100\\).\nWe have no details about the standardized fertility index Fertility\nInfant.Mortality is also a rate:\n\nInfant mortality is the death of an infant before his or her first birthday. The infant mortality rate is the number of infant deaths for every 1,000 live births. In addition to giving us key information about maternal and infant health, the infant mortality rate is an important marker of the overall health of a society.\n\nsee Center for Desease Control\nWe reuse the function we have already developped during previous sessions.\n\n\nCode\nmake_biotifoul(swiss, .f = is.numeric)\n\n\n\n\n\n\n\n\n\nHistograms reveal that our covariates have very different distributions.\nReligious affiliation (Catholic) tells us that there two types of districts, which is reminiscent of the old principle Cujus regio, ejus religio , see Old Swiss Confederacy.\nAgriculture shows that in most districts, agriculture was still a very important activity.\nEducation reveals that in all but a few districts, most children did not receive secondary education. Examination shows that some districts lag behind the bulk of districts. Even less exhibit a superior performance.\nThe two demographic variables Fertility and Infant.Mortality look roughly unimodal with a few extreme districts."
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#investigate-pairwise-correlations",
    "href": "core/labs-solutions/lab-pca.html#investigate-pairwise-correlations",
    "title": "LAB: Principal Component Analysis",
    "section": "Investigate pairwise correlations",
    "text": "Investigate pairwise correlations\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nCompute, display and comment the sample correlation matrix\nDisplay jointplots for each pair of variables\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nPackage corrr, functions correlate and rplot provide a convenient tool.\nNote that corrr::rplot() creates a graphical object of class ggplot. We can endow it with more layers.\n\n\nCode\nswiss |&gt; \n    corrr::correlate(use=\"pairwise.complete.obs\",method=\"pearson\", quiet=T) |&gt; \n  corrr::shave() |&gt; \n  corrr::rplot() + \n  labs(title=\"Correlation plot for Swiss Fertility data\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe high positive linear correlation between Education and Examination is moderately surprising. The negative correlation between the proportion of people involved in Agriculture and Education and Examinationis also not too surprising. Secondary schooling required pupils from rural areas to move to cities.\nA more intriguing observation concerns the pairs Catholic and Examination (negative correlation) and Catholic and Education (little correlation).\nThe response variable Fertility looks negatively correlated with Examination an Education. These correlations are worth being further explored. In Demography, the decline of Fertility is often associated with the the rise of women education. Note that Examination is about males, and that Education does not give details about the way women complete primary education."
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#singular-value-decomposition-svd",
    "href": "core/labs-solutions/lab-pca.html#singular-value-decomposition-svd",
    "title": "LAB: Principal Component Analysis",
    "section": "Singular Value Decomposition (SVD)",
    "text": "Singular Value Decomposition (SVD)\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nProject the swiss dataset on the covariates (all columns but Fertility)\nCenter the projected data using matrix manipulation\nCenter the projected data using dplyr verbs\nCompare the results with the output of scale() with various optional arguments\nCall the centered matrix Y\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nHand-made centering of the dataframe emphasises the fact that centering is a linear operation. As a matter of fact, it consists in projecting the data frame on the linear space orthogonal to the constant vector.\n\n\nCode\nX &lt;- select(swiss, -Fertility) |&gt; \n    as.matrix()\n\nn &lt;- nrow(X)\nones &lt;-  matrix(1, nrow = n, ncol=1) \n\nY &lt;-  X - (1/n)* (ones  %*%  t(ones) %*% X) \n\n\nWe can also perform centering using dplyr verbs. This can be viewed as computing a window function over a trivial partition.\n\n\nCode\nswiss |&gt; \n  select(-Fertility) |&gt;\n  mutate(across(everything(), \\(x) x-mean(x)))  \n\n\n\n\n\n\n\n\n\n\nAnyway, function scale(X, scale=F) from base R does the job.\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCheck that the ouput of svd(Y) actually defines a Singular Value Decomposition.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nsvd(Y) is a list with \\(3\\) elements (u,d,v).\n\\[Y = U \\times D \\times V^\\top\\]\n\n\nCode\nsvd_Y &lt;-  svd(Y)\n\n1svd_Y %$%\n  (Y - u %*% diag(d) %*% t(v)) %&gt;% \n2  norm(type = \"F\")\n\nnorm( \n  diag(1, ncol(Y)) - \n  (svd_Y %$% (t(v) %*% v)), \n  'F'\n)  # &lt;3&gt;. \n\n\n\n1\n\nExposing pipe from magrittr\n\n2\n\nChecking the factorization\n\n\n\n\n[1] 1.04354e-13\n[1] 1.137847e-15\n\n\n\n\n\n\n\n\n\n\nNote that we used the exposing pipe %$% from magrittr to unpack svd_Y which is a list with class svd and members named u, d and v.\nWe could have used with(,) from base R.\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nRelate the SVD of \\(Y\\) and the eigen decomposition of \\(Y^\\top \\times Y\\)\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nThe matrix \\(1/n Y^\\top \\times Y\\) is the covariance matrix of the covariates.\nThe spectral decomposition of the symmetric Semi Definite Positive (SDP) matrix \\(1/n Y^\\top \\times Y\\) is related with the SVD factorization of \\(Y\\).\nThe spectral/eigen decomposition of \\(Y^\\top \\times Y\\) can be obtained using eigen().\nThe eigenspaces of \\(Y^\\top \\times Y\\) are the right eigenspaces of \\(Y\\).\n\n\nCode\n(t(eigen(t(Y) %*% Y )$vectors) %*% svd_Y$v ) %&gt;% \n  round(digits=2)\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    0    0    0\n[2,]    0   -1    0    0    0\n[3,]    0    0    1    0    0\n[4,]    0    0    0    1    0\n[5,]    0    0    0    0    1\n\n\nThe eigenvalues of \\(Y^\\top \\times Y\\) are the squared singular values of \\(Y\\)\n\n\nCode\neigen(t(Y) %*% Y )$values - (svd_Y$d)^2\n\n\n[1] -1.309672e-10 -1.455192e-11  8.640200e-12  8.526513e-12 -3.410605e-13\n\n\n\n\n\n\n\n\n\n\nHere, the eigenvectors of \\(Y^\\top \\times Y\\) coincide with the right singular vectors of \\(Y\\) corresponding to non-zero singular values. Up to sign changes, it is always true when the non-zero singular values are pairwise distinct."
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#perform-pca-on-covariates",
    "href": "core/labs-solutions/lab-pca.html#perform-pca-on-covariates",
    "title": "LAB: Principal Component Analysis",
    "section": "Perform PCA on covariates",
    "text": "Perform PCA on covariates\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPairwise analysis did not provide us with a clear and simple picture of the French-speaking districts.\nPCA (Principal Component Analysis) aims at exploring the variations of multivariate datasets around their mean (center of inertia). In the sequel, we will perform PCA on the matrix of centered covariates, with and without standardizing the centered columns.\nBase R offers prcomp(). Call prcomp() on the centered covariates\nNote that R also offers princomp\n\n\nWe first call prcomp() with the default arguments for centering and scaling, that is, we center columns and do not attempt to standardize columns. Name the output pco.\nWhat is the result made of?\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nCode\npco &lt;- swiss |&gt; \n  select(-Fertility) |&gt; \n  scale(scale = F) |&gt; \n  prcomp(scale. = F)\n\n\npco is a list with 5 members. It as a class attribute prcomp. It is an object of class prcomp (function prcomp() acts as a constructor for class pco just as lm() acts as a constructor for class lm). Class pco is an S3 class\n\n\nCode\nrlang::is_list(pco)\n\n\n[1] TRUE\n\n\nCode\nattributes(pco)\n\n\n$names\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n$class\n[1] \"prcomp\"\n\n\nCode\nsloop::s3_class(pco)\n\n\n[1] \"prcomp\"\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCheck that prcomp() is indeed a wrapper for svd().\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nWe first check that the matrix can be recovered from the product of the components of the prcomp object.\n\n\nCode\n(Y - pco$x %*% t(pco$rotation )) %&gt;% \n  round(digits = 2)  %&gt;% \n  head()\n\n\n             Agriculture Examination Education Catholic Infant.Mortality\nCourtelary             0           0         0        0                0\nDelemont               0           0         0        0                0\nFranches-Mnt           0           0         0        0                0\nMoutier                0           0         0        0                0\nNeuveville             0           0         0        0                0\nPorrentruy             0           0         0        0                0\n\n\nWe now check that the rotation component is indeed made of the right singular vectors (the \\(V\\) factor) ::: {.cell}\n\nCode\n(svd_Y$v %*% t(pco$rotation )) %&gt;% \n  round(2) \n\n\n     Agriculture Examination Education Catholic Infant.Mortality\n[1,]           1           0         0        0                0\n[2,]           0           1         0        0                0\n[3,]           0           0         1        0                0\n[4,]           0           0         0        1                0\n[5,]           0           0         0        0                1\n\n\n\nThe column vectors of component \\(x\\) are pairwise orthogonal. ::: {.cell}\n\nCode\n(t(pco$x) %*% pco$x) %&gt;% \n  round(2)  \n\n\n         PC1      PC2     PC3    PC4    PC5\nPC1 86484.49     0.00    0.00   0.00   0.00\nPC2     0.00 21127.44    0.00   0.00   0.00\nPC3     0.00     0.00 2706.14   0.00   0.00\nPC4     0.00     0.00    0.00 639.22   0.00\nPC5     0.00     0.00    0.00   0.00 348.01\n\nThe x component of the prcomp object is the product of the \\(U\\) and \\(D\\) factors.\n\n\nCode\nnorm(pco$x - svd_Y$u %*% diag(svd_Y$d),type=\"F\")\n\n\n[1] 1.962527e-13\n\n\nCode\nnorm(as.matrix(pco$rotation) -svd_Y$v, type=\"F\")\n\n\n[1] 2.04934e-15\n\n\nThe connection between \\(pco\\)sdev$ and \\(svd_Y\\)d$ is somewhat less transparent. ::: {.cell}\n\nCode\nsum(abs(apply(pco$x, 2, sd) - pco$sdev))\n\n\n[1] 1.865175e-14\n\n:::\nThe components of pco$sdev are the standard deviations of the columns of \\(U \\times D\\). \n\n\nCode\npco$sdev - svd_Y$d/sqrt(nrow(Y)-1)\n\n\n[1] -7.105427e-15  0.000000e+00  8.881784e-16 -1.332268e-15 -1.332268e-15\n\n\n:::::\n:::\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCheck that rows and columns of component rotation of the result of prcomp() have unit norm.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nCheck that rows and columns of matrix rotation have unit norm. ::: {.cell}\n\nCode\napply(pco$rotation, 2, \\(x) norm(x, \"2\"))\n\n\nPC1 PC2 PC3 PC4 PC5 \n  1   1   1   1   1 \n\n\nCode\napply(pco$rotation, 1, \\(x) norm(x, \"2\"))\n\n\n     Agriculture      Examination        Education         Catholic \n               1                1                1                1 \nInfant.Mortality \n               1 \n\n\n\n:::\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCheck Orthogonality of \\(V\\) (component rotation of the prcomp object)\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nCode\n# checking that pco$rotation is an orthogonal matrix \nt(pco$rotation) %*% pco$rotation\n\n\n              PC1           PC2           PC3           PC4           PC5\nPC1  1.000000e+00 -1.003429e-16  8.239937e-18 -1.097213e-16  6.938894e-18\nPC2 -1.003429e-16  1.000000e+00  1.181780e-16  5.074066e-17  4.857226e-17\nPC3  8.239937e-18  1.181780e-16  1.000000e+00  1.717376e-16 -6.938894e-17\nPC4 -1.097213e-16  5.074066e-17  1.717376e-16  1.000000e+00 -1.804112e-16\nPC5  6.938894e-18  4.857226e-17 -6.938894e-17 -1.804112e-16  1.000000e+00\n\n\nCode\npco$rotation %*% t(pco$rotation)\n\n\n                   Agriculture   Examination     Education      Catholic\nAgriculture       1.000000e+00  3.642919e-17 -1.153591e-16  1.689187e-16\nExamination       3.642919e-17  1.000000e+00 -8.630249e-17  2.244298e-17\nEducation        -1.153591e-16 -8.630249e-17  1.000000e+00 -1.127570e-16\nCatholic          1.689187e-16  2.244298e-17 -1.127570e-16  1.000000e+00\nInfant.Mortality  2.081668e-17 -1.734723e-16  8.326673e-17 -2.081668e-17\n                 Infant.Mortality\nAgriculture          2.081668e-17\nExamination         -1.734723e-16\nEducation            8.326673e-17\nCatholic            -2.081668e-17\nInfant.Mortality     1.000000e+00\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nMake a scatterplot from the first two columns of the \\(x\\) component of the prcomp object.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nObjects of class prcomp can be handled by generic functions like plot() or better autoplot(). Namely, method prcomp for generic S3 function autoplot() from ggplot2 delivers one of classical SVD plots.\n\n\n\n\nCode\nres &lt;- autoplot(pco) +\n  coord_fixed() +\n  theme_minimal()\n\nts &lt;- theme_set(theme_minimal())\n\nres\n\n\n\n\n\n\n\n\n\n\nautoplot(pco) is a scatterplot for the dataframe defined by matrix \\(U \\times D\\) projected on its first two principal components (first two columns).\n\n\nAs autoplot(pco) is an instance of class ggplot, it can be annotated, decorated as any other ggplot object. ::: {.cell}\n\nCode\n(\n  res + aes(color=Catholic) + theme_minimal()\n) +\n(  \n  res + aes(color=Education) + theme_minimal()\n) +\n  patchwork::plot_annotation(\n    subtitle = \"Scatterplot on the first two principal components (no column scaling)\",\n    title= \"Share of catholics can almost be determined from the sign of the first PC\",\n    caption = \"Swiss  Fertility data from R datasets\"\n  ) \n\n\n\n\n\n\n\n\n\n\n:::\n\n\n\n\n\n\nNoteQuestion\n\n\n\nDefine a graphical pipeline for the screeplot.\nHint: use function tidy() from broom, to get the data in the right form from an instance of prcomp.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nThe screeplot is a bar plot where each bar corresponds to a singular value. The bar height is proportional to the square of the corresponding singular value.\n\n\nCode\n1p_screeplot &lt;- . %&gt;%\n  broom::tidy(matrix=\"pcs\") %&gt;% { \n  ggplot(.) +\n3  aes(x=PC, y=percent, label=pct_format(1.-cumulative)) +\n  geom_text(angle=45, vjust=-1, hjust=-.1) + \n2  geom_col(fill=NA, colour=\"black\") +\n  theme_minimal()\n  } \n\n\n\n1\n\nDefine a pipeline for building a screeplot\n\n2\n\nMind the braces on the right side of the first pipe\n\n3\n\n1- cumulative tell the reader about the relative Frobenious error achieved by keeping the first components of the SVD expansion.\n\n\n\n\n\n\nCode\npco %&gt;% \n  p_screeplot() +\n  ylab('Relative squared Frobenius error/Relative squared error') +\n  labs(\n    title=\"Screeplot for swiss fertility data\",\n    subtitle=\"Keeping the first two components is enough to achieve relative Froebenius relative error 3.3%\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe screeplot is a visualization of the Eckart-Young-Mirsky Theorem. It tells us about the relative errors incurent when approximating the data matrix (with centered columns) by the low rank approximations defined by the truncated SVDs.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nDefine a function that replicates autoplot.prcomp()\nProject the dataset on the first two principal components (perform dimension reduction) and build a scatterplot. Colour the points according to the value of original covariates.\nHint: use generic function augment from broom.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nCode\np &lt;-  pco %&gt;%\n  broom::augment(swiss) %&gt;% \n  ggplot() +\n  aes(x=.fittedPC1, y=.fittedPC2, label=.rownames) +\n  geom_point() +\n  coord_fixed() +\n  ggrepel::geom_text_repel()  +\n  theme_minimal()\n\n(p + \n  aes(color=Infant.Mortality)) +\n(p + \n   aes(color=Education)) +\n(p + \n   aes(color=Examination)) +\n(p + \n   aes(color=Catholic)) +\n(p + \n   aes(color=Agriculture)) +\n(p + \n   aes(color=Fertility)) +  \nplot_layout(ncol = 2) +\nplot_annotation(title=\"Swiss data on first two PCs\" , \n                subtitle = \"centered, unscaled\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nApply broom::tidy() with optional argument matrix=\"v\" or matrix=\"loadings\" to the prcomp object.\nComment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nWe can extract factor \\(V\\) from the SVD factorization using generic function tidy from package broom ::: {.cell}\n\nCode\npco %&gt;% \n  broom::tidy(matrix=\"v\") %&gt;% \n  sample_n(5) |&gt;\n  gt::gt()\n\n\n\n\n\n\n\n\ncolumn\nPC\nvalue\n\n\n\n\nInfant.Mortality\n5\n-0.99110977\n\n\nEducation\n1\n-0.05841770\n\n\nAgriculture\n5\n-0.04863543\n\n\nCatholic\n4\n-0.07149914\n\n\nInfant.Mortality\n3\n0.09852527\n\n\n\n\n\n\n\n\nThe result is a tibble in long form. It is worth pivoting the dataframe into wide form. This gives back the rotation matrix.\n\n\nCode\nom &lt;- pco %&gt;% \n  broom::tidy(matrix=\"v\") %&gt;% \n  tidyr::pivot_wider(id_cols =column, \n              names_from = PC, \n              values_from = value) |&gt; \n  select(-1) |&gt;\n  as.matrix()\n\nnorm((om %*% t(om))-diag(1,5), \"F\")        \n\n\n[1] 8.196585e-16\n\n\n:::\n\n\n\n\n\n\nNoteQuestion\n\n\n\nBuild the third SVD plot, the so called correlation circle.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nThe correlation circle is built from the loadings, that is, from the rotation component of the prcomp object.\nWe define a preprocessing function to transform the rotation object into a proper tibble form. ::: {.cell}\n\nCode\nprep_co_circle &lt;- function(pco) {\n  r &lt;- pco$rotation\n  as_tibble(r) |&gt; \n    rename_with(.fn = \\(x) gsub('PC', '', x), .cols=everything()) |&gt;\n    mutate(row_id=rownames(r))\n}\n\n\n\nThe The next virtual graphical object will be our key tool to build the correlation circle.\n\n\nCode\nco_circle_ppl &lt;- (\n    pco %&gt;% \n    prep_co_circle() %&gt;% \n    filter(F)\n    ) %&gt;% \n  ggplot() +\n  aes(x=`1`, y=`2`, label=row_id) +\n  geom_segment(aes(xend=0, yend=0), arrow = grid::arrow(ends = \"first\")) +\n  ggrepel::geom_text_repel() +\n1  coord_fixed() +\n  xlim(c(-1.1, 1.1)) + ylim(c(-1.1, 1.1))  + \n  ggforce::geom_circle(aes(x0=0, y0=0, r=1), linetype=\"dashed\") +\n  theme_minimal()\n\n\n\n1\n\nimportant\n\n\n\n\n\n\n\n\nCode\nco_circle_ppl %+% (\n  pco %&gt;% \n  prep_co_circle()\n  )  +\n  labs(title=\"Correlation circle\", \n          subtitle = \"centered, unscaled\",\n          caption= \"Swiss Fertility dataset\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nThe length of each arrow is the length of the projection of the corresponding column of the data matrix over the plane generated by the first two rescaled left singular vectors (rescaling by the reciprocal of the singular values).\nThe first two principal componants (left singular vectors) are highly correlated with columns Agriculture and Catholic.\n\n\n:::\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCompute PCA after standardizing the columns, draw the correlation circle.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\n\nCode\npco2 &lt;- select(swiss, -Fertility) |&gt; \n prcomp(scale. = T)\n\nco_circle_ppl %+% (\n  pco2 %&gt;% \n  prep_co_circle()\n  )  +\n  labs(\n    title=\"Correlation circle\", \n    subtitle = \"centered, scaled\",\n    caption=\"Swiss fertility dataset\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nScaling columns seriously modify the correlation circle."
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#compare-standardized-and-non-standardized-pca",
    "href": "core/labs-solutions/lab-pca.html#compare-standardized-and-non-standardized-pca",
    "title": "LAB: Principal Component Analysis",
    "section": "Compare standardized and non-standardized PCA",
    "text": "Compare standardized and non-standardized PCA\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPay attention to the correlation circles.\n\nHow well are variables represented?\nWhich variables contribute to the first axis?\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nCode\npco_c &lt;- swiss %&gt;% \n  select(-Fertility) %&gt;% \n  prcomp()\n\npco_cs &lt;- swiss %&gt;% \n  select(-Fertility) %&gt;% \n  prcomp(scale.=T, center=T)\n\n\n\n\nCode\n(\n  co_circle_ppl %+% \n  prep_co_circle(pco_c)  +\n  labs(\n    subtitle = \"centered, unscaled\"\n  ) + \n  theme_minimal()  \n) +\n(\n  co_circle_ppl %+%  \n  prep_co_circle(pco_cs) +\n  labs(\n    subtitle = \"centered, scaled\"\n  ) +\n  theme_minimal()  \n) +\n  patchwork::plot_annotation(\n        title=\"Swiss, correlation circle\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nExplain the contrast between the two correlation circles.\n\n\nIn the sequel we focus on standardized PCA.\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nCode\nq &lt;-  autoplot(pco_cs, data=swiss) +\n  theme_minimal()\n\nts &lt;- theme_set(theme_minimal())\n\n(q + \n  aes(color=Infant.Mortality)) +\n(q + \n   aes(color=Education)) +\n(q + \n   aes(color=Examination)) +\n(q + \n   aes(color=Catholic)) +\n(q + \n   aes(color=Agriculture)) +\n(q + \n   aes(color=Fertility)) +  \npatchwork::plot_layout(ncol = 2) +\npatchwork::plot_annotation(\n    title=\"Scatterplot on first two PCs\", \n    subtitle = \"centered, scaled PCA\",\n    caption = \"Swiss Fertility dataset\")"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#provide-an-interpretation-of-the-first-two-principal-axes",
    "href": "core/labs-solutions/lab-pca.html#provide-an-interpretation-of-the-first-two-principal-axes",
    "title": "LAB: Principal Component Analysis",
    "section": "Provide an interpretation of the first two principal axes",
    "text": "Provide an interpretation of the first two principal axes\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhich variables contribute to the two first principal axes?\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nThis comes from the correlation circle. We rely on function prep_co_circle and on the graphical pipeline co_circle_ppl.\n\n\nCode\n(\n  co_circle_ppl %+% \n    prep_co_circle(pco_cs) +\n    ggtitle(\"Swiss, correlation circle\", \n            subtitle = \"centered, scaled\") +\n  theme_minimal()  \n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nAnalyze the signs of correlations between variables and axes?\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nCode\nswiss |&gt;   # ggrepel::geom_text_repel(data=df_cocirc, \n  #                          aes(x= 4* `1`,\n  #                              y= 4 * `2`, \n  #                              label=column), \n  #                          color=\"red\")\n  select(-Fertility) |&gt; \n  corrr::correlate(use=\"pairwise.complete.obs\",method=\"pearson\", quiet=T) |&gt; \n  corrr::shave() |&gt; \n  corrr::rplot(print_cor = T) +\n  theme_minimal()"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#add-the-fertility-variable",
    "href": "core/labs-solutions/lab-pca.html#add-the-fertility-variable",
    "title": "LAB: Principal Component Analysis",
    "section": "Add the Fertility variable",
    "text": "Add the Fertility variable\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPlot again the correlation circle using the same principal axes as before, but add the Fertility variable.\nHow does Fertility relate with covariates? with principal axes?\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nWe use \\[D^{-1} \\times U^\\top \\times X = V^\\top\\]\nIt is enough to multipliy the data matrix by \\(D^{-1} \\times U^\\top\\) and to pipe the result into the coorelation circle graphical pipeline.\n\n\nCode\nfoo &lt;- t(diag(svd_Y$d^(-1)) %*% t(svd_Y$u) %*% as.matrix(scale(swiss, scale=F)))  \n \nco_circle_ppl  %+% (\n  as_tibble(foo) |&gt;\n  rename_with(.fn = \\(x) gsub('V', '', x), .cols=everything()) |&gt;\n  mutate(row_id=rownames(foo))\n) +\n  theme_minimal()  \n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`."
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#biplot",
    "href": "core/labs-solutions/lab-pca.html#biplot",
    "title": "LAB: Principal Component Analysis",
    "section": "Biplot",
    "text": "Biplot\n\n\n\n\n\n\nNoteQuestion\n\n\n\nThe last svd plot (biplot) consists of overlaying the scatter plot of component x of the prcomp object and the correlation circle.\nSo the biplot is a graphical object built on two dataframes derived on components x and rotation of the prcomp objects.\nDesign a graphical pipeline.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nCode\npco &lt;- swiss %&gt;% \n  select(-Fertility) %&gt;% \n  prcomp(scale.=T)\n\ndf_cocirc &lt;- pco %&gt;% \n  broom::tidy(matrix=\"v\") %&gt;% \n  tidyr::pivot_wider(id_cols =column, \n              names_from = PC, \n              values_from = value) \n\nbroom::augment(pco, data=swiss) %&gt;% \n  ggplot() + \n  geom_point(aes(x=.fittedPC1, \n                 y=.fittedPC2, \n                 color=Fertility, label=.rownames)) +\n  coord_fixed() + \n  ggrepel::geom_text_repel(aes(x=.fittedPC1, \n                               y=.fittedPC2,\n                               color=Infant.Mortality,\n                               label=.rownames)) + \n  geom_segment(data=df_cocirc,  \n               mapping=aes(x= 4* `1`, \n                           y= 4 * `2`, \n                           linetype=factor(column),\n                           label=column,\n                           xend=0, \n                           yend=0), \n               arrow = grid::arrow(ends = \"first\",\n                                    unit(.1, \"inches\")\n                                  )) + \n  scale_color_viridis_c() +\n  xlim(c(-5,5)) + \n  ylim(c(-5,5)) +\n  theme_minimal()\n\n\nWarning in geom_point(aes(x = .fittedPC1, y = .fittedPC2, color = Fertility, :\nIgnoring unknown aesthetics: label\n\n\nWarning in geom_segment(data = df_cocirc, mapping = aes(x = 4 * `1`, y = 4 * :\nIgnoring unknown aesthetics: label\n\n\nWarning: ggrepel: 21 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nautoplot.prcomp() has optional arguments. If set to True, logical argument loadings overlays the scatterplot defined by the principal components with the correlation circle.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nCode\nbip &lt;- autoplot(pco_cs, \n         data=swiss, \n         color=\"Fertility\", \n         loadings = TRUE, \n         loadings.colour = 'blue',\n         loadings.label = TRUE) +\n  coord_fixed() +\n  labs(\n    title = \"Biplot\",\n    subtitle = \"PCA after centering and scaling\",\n    caption = \"Swiss Fertility dataset\"\n  ) +\n  theme_minimal()\n\nts &lt;- theme_set(theme_minimal())\n\nbip\n\n\n\n\n\n\n\n\n\n\n\nCode\nbip_plotly &lt;- autoplot(pco_cs, \n         data=rownames_to_column(swiss, var=\"district\"), \n         color=\"Fertility\", \n         text=\"district\", \n         loadings = TRUE, \n         loadings.colour = 'blue',\n         loadings.label = TRUE) + \n  aes(text=district) +\n  coord_fixed() +\n  labs(\n    title = \"Biplot\",\n    subtitle = \"PCA after centering and scaling\",\n    caption = \"Swiss Fertility dataset\"\n  ) +\n  theme_minimal()\n\nbip |&gt; plotly::ggplotly()\n\n\n\n\n\n\n\n\nCode\n(\nbroom::augment(pco, data=rownames_to_column(swiss, var=\"district\")) |&gt; \n  ggplot() + \n  geom_point(aes(x=.fittedPC1, \n                 y=.fittedPC2, \n                 color=Fertility,\n                 label=district)) +\n  coord_fixed() + \n  geom_segment(data=df_cocirc,  \n               mapping=aes(x= 4* `1`, \n                           y= 4 * `2`, \n                           linetype=factor(column),\n                           label=column,\n                           xend=0, \n                           yend=0), \n               arrow = grid::arrow(ends = \"first\",\n                                    unit(.1, \"inches\")\n                                  )) + \n  scale_color_viridis_c() +\n  xlim(c(-5,5)) + \n  ylim(c(-5,5)) +\n  labs(\n    title = \"Biplot\",\n    subtitle = \"PCA after centering and scaling\",\n    caption = \"Swiss Fertility dataset\"\n  ) +\n  theme_minimal() \n) |&gt; plotly::ggplotly()"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#generics",
    "href": "core/labs-solutions/lab-pca.html#generics",
    "title": "LAB: Principal Component Analysis",
    "section": "Generics",
    "text": "Generics\nautoplot() is an example of S3 generic function. Let us examine this function using sloop\n\n\n\n\n\n\nUse sloop::s3_dispatch() to compare autoplot(prcomp(swiss)) and autoplot(lm(Fertility ~ ., swiss))\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nCode\nsloop::ftype(autoplot)\n\n\n[1] \"S3\"      \"generic\"\n\n\nCode\nsloop::s3_dispatch(autoplot(prcomp(swiss)))\n\n\n=&gt; autoplot.prcomp\n * autoplot.default\n\n\nCode\nsloop::s3_dispatch(autoplot(lm(Fertility ~ ., swiss)))\n\n\n=&gt; autoplot.lm\n * autoplot.default\n\n\n\n\n\n\n\n\n\n\nUse sloop::s3_getmethod() to see the body of autoplot.prcomp\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nCode\nsloop::s3_get_method(autoplot.prcomp)\n## function (object, data = NULL, scale = 1, x = 1, y = 2, variance_percentage = TRUE, \n##     ...) \n## {\n##     plot.data &lt;- ggplot2::fortify(object, data = data)\n##     plot.data$rownames &lt;- rownames(plot.data)\n##     if (is_derived_from(object, \"prcomp\")) {\n##         ve &lt;- object$sdev^2/sum(object$sdev^2)\n##         PC &lt;- paste0(\"PC\", c(x, y))\n##         x.column &lt;- PC[1]\n##         y.column &lt;- PC[2]\n##         loadings.column &lt;- \"rotation\"\n##         lam &lt;- object$sdev[c(x, y)]\n##         lam &lt;- lam * sqrt(nrow(plot.data))\n##     }\n##     else if (is_derived_from(object, \"princomp\")) {\n##         ve &lt;- object$sdev^2/sum(object$sdev^2)\n##         PC &lt;- paste0(\"Comp.\", c(x, y))\n##         x.column &lt;- PC[1]\n##         y.column &lt;- PC[2]\n##         loadings.column &lt;- \"loadings\"\n##         lam &lt;- object$sdev[c(x, y)]\n##         lam &lt;- lam * sqrt(nrow(plot.data))\n##     }\n##     else if (is_derived_from(object, \"factanal\")) {\n##         if (is.null(attr(object, \"covariance\"))) {\n##             p &lt;- nrow(object$loading)\n##             ve &lt;- colSums(object$loading^2)/p\n##         }\n##         else ve &lt;- NULL\n##         PC &lt;- paste0(\"Factor\", c(x, y))\n##         x.column &lt;- PC[1]\n##         y.column &lt;- PC[2]\n##         scale &lt;- 0\n##         loadings.column &lt;- \"loadings\"\n##     }\n##     else if (is_derived_from(object, \"lfda\")) {\n##         ve &lt;- NULL\n##         PC &lt;- paste0(\"PC\", c(x, y))\n##         x.column &lt;- PC[1]\n##         y.column &lt;- PC[2]\n##         scale &lt;- 0\n##         loadings.column &lt;- NULL\n##     }\n##     else {\n##         stop(paste0(\"Unsupported class for autoplot.pca_common: \", \n##             class(object)))\n##     }\n##     if (scale != 0) {\n##         lam &lt;- lam^scale\n##         plot.data[, c(x.column, y.column)] &lt;- t(t(plot.data[, \n##             c(x.column, y.column)])/lam)\n##     }\n##     plot.columns &lt;- unique(c(x.column, y.column, colnames(plot.data)))\n##     plot.data &lt;- plot.data[, plot.columns]\n##     if (!is.null(loadings.column)) {\n##         loadings.data &lt;- as.data.frame(object[[loadings.column]][, \n##             ])\n##         loadings.data$rownames &lt;- rownames(loadings.data)\n##         loadings.columns &lt;- unique(c(x.column, y.column, colnames(loadings.data)))\n##         loadings.data &lt;- loadings.data[, loadings.columns]\n##     }\n##     else {\n##         loadings.data &lt;- NULL\n##     }\n##     if (is.null(ve) | !variance_percentage) {\n##         labs &lt;- PC\n##     }\n##     else {\n##         ve &lt;- ve[c(x, y)]\n##         labs &lt;- paste0(PC, \" (\", round(ve * 100, 2), \"%)\")\n##     }\n##     xlab &lt;- labs[1]\n##     ylab &lt;- labs[2]\n##     p &lt;- ggbiplot(plot.data = plot.data, loadings.data = loadings.data, \n##         xlab = xlab, ylab = ylab, ...)\n##     return(p)\n## }\n## &lt;bytecode: 0x5808d15583c0&gt;\n## &lt;environment: namespace:ggfortify&gt;"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html",
    "href": "core/labs-solutions/lab-r-intro.html",
    "title": "R language: a tour",
    "section": "",
    "text": "M1 MIDS/MFA/LOGOS\n\n\nUniversité Paris Cité\n\n\nAnnée 2025\n\n\nCourse Homepage\n\n\nMoodle"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#packages",
    "href": "core/labs-solutions/lab-r-intro.html#packages",
    "title": "R language: a tour",
    "section": "Packages",
    "text": "Packages\nBase R can do a lot. But the full power of R comes from a fast growing collection of packages.\nPackages are first installed (that is downloaded from cran and copied somewhere on the hard drive), and if needed, loaded during a session.\n\nInstallation can usually be performed using command install.packages(). In some circumstances, ad hoc installation commands (often from packages devtools) are needed\nPackage pak offers an interesting alternative to base R install.packages()\n\nOnce a package has been installed/downloaded on your hard drive\n\nif you want all objects exported by the package to be available in your session, you should load the package, using library() or require() (what’s the difference?). Technically, this loads the NameSpace defined by the package.\nif you just want to pick some objects exported from the package, you can use qualified names like package_name::object_name to access the object (function, dataset, …).\n\n\n\nFor example, when we write\ngapminder &lt;- gapminder::gapminder\nwe assign dataframe/tibble gapminder from package gapminder to identifier \"gapminder\" in global environment .\nFunction p_load() from pacman (package manager) blends installation and loading: if the package named in the argument of p_load() is not installed (not among the installed.packages()), p_load() attempts to install the package. If installation is successful, the package is loaded.\n\nif (! require(pak)){\n  install.packages(\"pak\")\n}\n\n\nstopifnot(\n  require(\"tidyverse\"), \n  require(\"lobstr\"),\n  require(\"ggforce\"),\n  require(\"nycflights13\"),\n  require(\"patchwork\"), \n  require(\"viridis\"),\n  require(\"MASS\"),\n  require(\"gapminder\"),\n  require(\"pryr\"),\n  require(\"pak\")\n)\n\n\n\n\n\n\n\nNoteOptional arguments\n\n\n\nA very nice feature of R is that functions from base R as well as from packages have optional arguments with sensible default values. Look for example at documentation of require() using expression ?require.\nOptional settings may concern individual functions or the collection of functions exported by some packages. In the next chunk, we reset the default color scales used by graphical functions from ggplot2.\n\nopts &lt;- options()  # save old options\n\noptions(ggplot2.discrete.colour=\"viridis\")\noptions(ggplot2.continuous.colour=\"viridis\")\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou shall not confuse installing (on your hard-drive) and loading (in session) a package.\n\n\n\n\n\n\n\n\nNoteQuestion for Pythonistas\n\n\n\n\nIn  what is the analogue of install.packages()?\nIn  what is the analogue of require()/library()?\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nIn , you can install a package pck using pip install pck or conda install pck (for example).\nIn , the analogue of require(pck) could be\nfrom pck import *\nNote that in R, once a package in installed on the hard drive, you do not need to write something like\nimport pck\nto be able to use objects exported by pck using qualified names (like pck.ze_object), you just need to use R qualified names:\npck::ze_object"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#vector-creation-and-assignment",
    "href": "core/labs-solutions/lab-r-intro.html#vector-creation-and-assignment",
    "title": "R language: a tour",
    "section": "Vector creation and assignment",
    "text": "Vector creation and assignment\nThe next three lines create three numerical atomic vectors.\nIn IDE Rstudio, have a look at the environment pane on the right before running the chunk, and after.\nUse ls() to investigate the environment before and after the execution of the three assignments.\n\nls()\nx &lt;- c(1, 2, 12)\ny &lt;- 5:7\nz &lt;- 10:1\nx ; y ; z \nls()\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nWhat are the identifiers known in the global environment before execution of lines 2-4?\nWhat are the identifiers known in the global environment after execution of lines 2-4?\nWhich objects are attached to identifiers x, y, and z?\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nls()\n\n[1] \"opts\"\n\nx &lt;- c(1, 2, 12)\ny &lt;- 5:7\nz &lt;- 10:1\nx ; y ; z \n\n[1]  1  2 12\n\n\n[1] 5 6 7\n\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\nls()\n\n[1] \"opts\" \"x\"    \"y\"    \"z\"   \n\n\nThe chunks adds three identifiers x,y,z to the global environment. Identifiers are bound to R objects which turn out to be numerical vectors.\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat does the next chunk?\n\nls()\nw &lt;- y\nls()\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe chunk inserts a new identifier w in the global environment. This identifier is associated with the same object as y.\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nIs the content of object denoted by y copied to a new object bound to w?\nInterpret the result of w == y.\nInterpret the result of identical(w,y) (use help(\"identical\") if needed).\n\n\nw == y \nidentical(w,y)\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nPackage lobstr lets us explore low-level aspects of R (and much more). Function lobstr::obj_addr() returns the address of the object denoted by the argument.\n\nlobstr::obj_addr(w)\n\n[1] \"0x5c50fc6ca238\"\n\nlobstr::obj_addr(y)\n\n[1] \"0x5c50fc6ca238\"\n\n\nNow, if we modify either y or w\n\ny &lt;- y + 1\nidentical(y, w)\n\n[1] FALSE\n\nc(lobstr::obj_addr(w), lobstr::obj_addr(y))\n\n[1] \"0x5c50fc6ca238\" \"0x5c50fd0a1098\"\n\n\nThe address associated with y has changed!\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe meaning of assignment in R differs from its counterpart in Python. In Python, assignment is shallow. In R, assignment creates a new identifier bound to the same object as the right-hand side of the assignment. If either side of the assignment is modified, it is copied to a new object before modification. This is called copy-on-modify."
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#indexation-slicing-modification",
    "href": "core/labs-solutions/lab-r-intro.html#indexation-slicing-modification",
    "title": "R language: a tour",
    "section": "Indexation, slicing, modification",
    "text": "Indexation, slicing, modification\nSlicing a vector can be done in two ways:\n\nproviding a vector of indices to be selected. Indices need not be consecutive.\nproviding a Boolean mask, that is a logical vector to select a set of positions.\n\n\nx &lt;- c(1, 2, 12) ; y &lt;- 5:7 ; z &lt;- 10:1\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nExplain the next lines\n\nz[1]   # slice of length 1\nz[0]   # What did you expect?\nz[x]   # slice of length ??? index error ?\nz[y]\nz[x %% 2]   # what happens with x[0] ?\nz[0 == (x %% 2)] # masking\nz[c(2, 1, 1)]\n\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\nIndices start at 1 (not like in C, Java, or Python)\n\n z[0] does not return an Error message. It returns an empty vector with the same basetype as x\n\n\nz[x] returns a vector made of z[x[1]], z[x[2]] and z[x[3]]==z[12]. Note again that z[12] does not raise an exception. It is simply not available (NA).\n\nx %% 2 returns 1 0 0 as %% stands for mod. z[x %% 2] returns the same thing as z[1]\n\n\nc( ) stands for combine, or concatenate.\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nIf the length of mask and and the length of the sliced vector do not coincide, what happens?\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\nNo error is signaled, the returned sequence is as long as the number of truthies in the mask.\nOut of bound truthies show up as NA\n\nz[rep(c(TRUE, FALSE), 6)]\n\n[1] 10  8  6  4  2 NA\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA scalar is just a vector of length \\(1\\)!\n\nclass(z)\n\n[1] \"integer\"\n\nclass(z[1])\n\n[1] \"integer\"\n\nclass(z[c(2,1)])\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nExplain the next lines\n\ny[2:3] &lt;- z[2:3]\ny == z[-10]\n\nz[-11]\n\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\nWe can assign a slice of a vector to a slice of identical size of another vector.\nWhat is the result of z[-11], z[-c(11:7)]?\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nExplain the next line\n\nz[-(1:5)]\n\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\nWe pick all positions in z but the ones in 1:5, that is 6, 7, 8, 9, 10\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nHow would you select the last element from a vector (say z)?\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\nz[length(z)]\n\n[1] 1\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\n is not  (reminder)!\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nReverse the entries of a vector. Find two ways to do that.\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\nz[seq(length(z), 1, by=-1)]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nz[length(z):1]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nrev(z)   # the simplest way, once you know rev()\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\nIn statistics, machine learning, we are often faced with the task of building grids of regularly spaced elements (these elements can be numeric or not). R offers a collection of tools to perform this. The most basic tool is rep().\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nRepeat a vector \\(2\\) times\nRepeat each element of a vector twice\n\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\nw &lt;- c(1, 7, 9)\nrep(w, 2)\n\n[1] 1 7 9 1 7 9\n\nrep(w, rep(2, length(w)))\n\n[1] 1 1 7 7 9 9\n\n\nNow, we can try something more fancy.\n\nrep(w, 1:3)\n\n[1] 1 7 7 9 9 9\n\n\nWhat are the requirements on the second (times) argument?\n\n\n\nLet us remove objects from the global environment.\n\nrm(w, x, y ,z)"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#numbers",
    "href": "core/labs-solutions/lab-r-intro.html#numbers",
    "title": "R language: a tour",
    "section": "Numbers",
    "text": "Numbers\nSo far, we told about numeric vectors. Numeric vectors are vectors of floating point numbers. R distinguishes several kinds of numbers.\n\nIntegers\nFloating point numbers (double)\n\nTo check whether a vector is made of numeric or of integer, use is.numeric() or is.integer(). Use as.integer, as.numeric() to enforce type conversion.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nExplain the outcome of the next chunks\n\nclass(113L) ; class(113) ; class(113L + 113) ; class(2 * 113L) ; class(pi) ; as.integer(pi)\n\n[1] \"integer\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] 3\n\n\n\nfloor(pi) ; class(floor(pi)) # mind the floor\n\n[1] 3\n\n\n[1] \"numeric\""
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#integer-arithmetic",
    "href": "core/labs-solutions/lab-r-intro.html#integer-arithmetic",
    "title": "R language: a tour",
    "section": "Integer arithmetic",
    "text": "Integer arithmetic\n\n29L * 31L ; 899L %/% 32L ; 899L %% 30L\n\n[1] 899\n\n\n[1] 28\n\n\n[1] 29\n\n\n\n\n\n\n\n\nR integers are not the natural numbers from Mathematics\nR numerics are not the real numbers from Mathematics\n\n.Machine$double.eps\n\n[1] 2.220446e-16\n\n.Machine$double.xmax\n\n[1] 1.797693e+308\n\n.Machine$sizeof.longlong\n\n[1] 8\n\nu &lt;- double(19L)\nv &lt;- numeric(5L)\nw &lt;- integer(7L)\nlapply(list(u, v, w), typeof)\n\n[[1]]\n[1] \"double\"\n\n[[2]]\n[1] \"double\"\n\n[[3]]\n[1] \"integer\"\n\nlength(c(u, v, w))\n\n[1] 31\n\ntypeof(c(u, v, w))\n\n[1] \"double\"\n\n\n\n\n\nR is (sometimes) able to make sensible use of Infinite.\n\nlog(0)\n\n[1] -Inf\n\nlog(Inf)\n\n[1] Inf\n\n1/0\n\n[1] Inf\n\n0/0\n\n[1] NaN\n\nmax(c( 0/0,1,10))\n\n[1] NaN\n\nmax(c(NA,1,10))\n\n[1] NA\n\nmax(c(-Inf,1,10))\n\n[1] 10\n\nis.finite(c(-Inf,1,10))\n\n[1] FALSE  TRUE  TRUE\n\nis.na(c(NA,1,10))\n\n[1]  TRUE FALSE FALSE\n\nis.nan(c(NaN,1,10))\n\n[1]  TRUE FALSE FALSE"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#computing-with-vectors",
    "href": "core/labs-solutions/lab-r-intro.html#computing-with-vectors",
    "title": "R language: a tour",
    "section": "Computing with vectors",
    "text": "Computing with vectors\nSumming, scalar multiplication\n\nx &lt;- 1:3\ny &lt;- 9:7\n\nsum(x) ; prod(x)\n\n[1] 6\n\n\n[1] 6\n\nz &lt;- cumsum(1:3)\nw &lt;- cumprod(3:5)\n\nx + y\n\n[1] 10 10 10\n\nx + z\n\n[1] 2 5 9\n\n2 * w\n\n[1]   6  24 120\n\n2 + w\n\n[1]  5 14 62\n\nw / 2\n\n[1]  1.5  6.0 30.0\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nHow would you compute a factorial?\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\nn &lt;- 10\ncumprod(1:n)\n\n [1]       1       2       6      24     120     720    5040   40320  362880\n[10] 3628800\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nApproximate \\(\\sum_{n=1}^\\infty 1/n^2\\) within \\(10^{-3}\\)?\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\\[\\sum_{n &gt; N} \\frac{1}{n^2} &lt; \\sum_{n &gt; N} \\frac{1}{n(n-1)} = \\sum_{n &gt; N} \\left(\\frac{1}{n-1}-\\frac{1}{n}\\right) = \\frac{1}{N}\\] So we may pick \\(N=1000\\).\n\nsum(x*y) # inner product\n\n[1] 46\n\nprod(1:5) # factorial(n) as prod(1:n)\n\n[1] 120\n\nN &lt;- 1000L\nsum(1/((1:N)^2)) ; pi^2/6 # grand truth\n\n[1] 1.643935\n\n\n[1] 1.644934\n\n(pi^2/6 - sum(1/((1:N)^2))) &lt; 1e-3\n\n[1] TRUE\n\n# N &lt;- 999L\n# (pi^2/6 - sum(1/((1:N)^2))) &lt; 1e-3\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nHow would you compute the inner product between two (atomic numeric) vectors?\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\nInner product between two vectors can be computed as a matrix product between a row vector and a column vector using %*%. Is this a good idea?\n\nmatrix(w, ncol=3) %*% matrix(y, nrow=3) == sum(w * y)\n\n     [,1]\n[1,] TRUE\n\n\n\n\n\n\n\n\n\n\n\nWhat we have called vectors so far are indeed atomic vectors.\n\nRead Chapter on Vectors in R advanced Programming\n\nKeep an eye on package vctrs for getting insights into the R vectors."
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#creation-transposition-and-reshaping",
    "href": "core/labs-solutions/lab-r-intro.html#creation-transposition-and-reshaping",
    "title": "R language: a tour",
    "section": "Creation, transposition and reshaping",
    "text": "Creation, transposition and reshaping\nA vector can be turned into a column matrix.\n\nv &lt;- as.matrix(1:5)\nv\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[4,]    4\n[5,]    5\n\n\nA matrix can be transposed\n\nt(v)  # transpose \n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n\ncat(dim(v), ' ', dim(t(v)), '\\n')\n\n5 1   1 5 \n\n\n\nA &lt;- matrix(1, nrow=5, ncol=2) ; A\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n[3,]    1    1\n[4,]    1    1\n[5,]    1    1\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nlobstr::mem_used() allows us to keep track of the amount of memory used by our R session. lobstr::obj_size() tells us the amount of memory used by the representation of an object.\nComment the next chunk\n\nm1 &lt;-lobstr::mem_used()\nA &lt;- matrix(rnorm(100000L), nrow=1000L)\nm2 &lt;- lobstr::mem_used()\nlobstr::obj_size(A)\n\n800.22 kB\n\nB &lt;- t(A)\nlobstr::obj_size(B)\n\n800.22 kB\n\nm3 &lt;- lobstr::mem_used()\nm2-m1 ; m3-m2\n\n807.81 kB\n\n\n1.09 MB\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nIs there a difference between the next two assignments?\nHow would you assign value to all entries of a matrix?\n\n\nA &lt;- matrix(rnorm(16), nrow=4)\nA[] &lt;- 0 ; A\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n\nA   &lt;- 0 ; A\n\n[1] 0\n\n\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\nThere is!\nThe first assignment assigns 0 to every entry in A.\nThe second assignment binds 0 to name A\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the final shape of A?\n\nA &lt;- matrix(1, nrow=5, ncol=2) \nA\nA[] &lt;- 1:15 \nA\n\n\n\nWe can easily generate diagonal matrices and constant matrices.\n\ndiag(1, 3)  # building identity matrix\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\nmatrix(0, 3, 3) # building null matrix\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nIs there any difference between the next two assignments?\n\nB &lt;- A[]\nB ; A\n\n[1] 0\n\n\n[1] 0\n\nlobstr::obj_addr(B) ; lobstr::obj_addr(A)\n\n[1] \"0x5c50fce2b060\"\n\n\n[1] \"0x5c50fda62838\"\n\nB &lt;- A\nlobstr::obj_addr(B) ; lobstr::obj_addr(A)\n\n[1] \"0x5c50fda62838\"\n\n\n[1] \"0x5c50fda62838\""
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#indexation-slicing-modification-1",
    "href": "core/labs-solutions/lab-r-intro.html#indexation-slicing-modification-1",
    "title": "R language: a tour",
    "section": "Indexation, slicing, modification",
    "text": "Indexation, slicing, modification\nIndexation consists in getting one item from a vector/list/matrix/array/dataframe.\nSlicing and subsetting consists in picking a substructure:\n\nsubsetting a vector returns a vector\nsubsetting a list returns a list\nsubsetting a matrix/array returns a matrix/array (beware of implicit simplifications and dimension dropping)\nsubsetting a dataframe returns a dataframe or a vector (again, beware of implicit simplifications).\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nExplain the next results\n\nA &lt;- matrix(1, nrow=5, ncol=2)\n\ndim(A[sample(5, 3), -1])\ndim(A[sample(5, 3), 1])\nlength(A[sample(5, 3), 1])\nis.vector(A[sample(5, 3), 1])\nA[10:15]\nA[60]\ndim(A[])\n\n\n\n\n\n\n\n\n\n\n\nNULL\n\n\nNULL\n\n\n[1] 3\n\n\n[1] TRUE\n\n\n[1]  1 NA NA NA NA NA\n\n\n[1] NA\n\n\n[1] 5 2\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nHow would you create a fresh copy of a matrix?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nA &lt;- matrix(rnorm(10), ncol=2L)\nB &lt;- matrix(0, nrow=5L, ncol=2L)\n\nB[] &lt;- A\nall(B==A) ; identical(A, B) ; lobstr::obj_addrs(list(A, B))\n\n[1] TRUE\n\n\n[1] TRUE\n\n\n[1] \"0x5c50fb4e0e78\" \"0x5c50fb4e1558\""
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#computing-with-matrices",
    "href": "core/labs-solutions/lab-r-intro.html#computing-with-matrices",
    "title": "R language: a tour",
    "section": "Computing with matrices",
    "text": "Computing with matrices\n\n\n* versus %*%\n\n\n%*% stands for matrix multiplication. In order to use it, the two matrices should have conformant dimensions.\n\n\n\nt(v) %*% A\n\n          [,1]     [,2]\n[1,] -4.124554 8.253829\n\n\nThere are a variety of reasonable products around. Some of them are available in R.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nHow would you compute the Hilbert-Schmidt inner product between two matrices?\n\\[\\langle A, B\\rangle_{\\text{HS}} = \\text{Trace} \\big(A \\times B^\\top\\big)\\]\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\nIn R, trace() does not return the trace of a matrix! Function is used for debugging.\nJust remember that the trace of a matrix is the sum of its diagonal elements.\n\nA &lt;- matrix(runif(6), 2, 3)\nB &lt;- matrix(runif(6), 2, 3)\nfoo &lt;- sum(diag(A %*% t(B)))\nbar &lt;- sum(A * B)\nfoo ; bar\n\n[1] 1.387838\n\n\n[1] 1.387838\n\n\nAre you surprised?\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nHow can you invert a square (invertible) matrix?\n\n\nUse solve(A) which is a shorthand for solve(A, diag(1, nrow(3)))."
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#handling-three-valued-logic",
    "href": "core/labs-solutions/lab-r-intro.html#handling-three-valued-logic",
    "title": "R language: a tour",
    "section": "Handling three-valued logic",
    "text": "Handling three-valued logic\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nTRUE &  (1&gt; (0/0))\n(1&gt; (0/0)) | TRUE\n(1&gt; (0/0)) | FALSE\nTRUE || (1&gt; (0/0))\nTRUE |  (1&gt; (0/0))\nTRUE || stopifnot(4&lt;3)\n# TRUE |  stopifnot(4&lt;3)  \nFALSE && stopifnot(4&lt;3)\n# FALSE & stopifnot(4&lt;3)\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nTRUE &  (1&gt; (0/0))\n\n[1] NA\n\n(1&gt; (0/0)) | TRUE\n\n[1] TRUE\n\n(1&gt; (0/0)) | FALSE\n\n[1] NA\n\nTRUE || (1&gt; (0/0))\n\n[1] TRUE\n\nTRUE |  (1&gt; (0/0))\n\n[1] TRUE\n\nTRUE || stopifnot(4&lt;3)\n\n[1] TRUE\n\n# TRUE |  stopifnot(4&lt;3)  \nFALSE && stopifnot(4&lt;3)\n\n[1] FALSE\n\n# FALSE & stopifnot(4&lt;3)\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the difference between logical operators || and | ?\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n|| is lazy. It does not evaluate its second argument if the first one evaluates to TRUE.\n&& is also lazy.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemark: favor &, | over &&, ||."
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#all-and-any",
    "href": "core/labs-solutions/lab-r-intro.html#all-and-any",
    "title": "R language: a tour",
    "section": "\nall and any\n",
    "text": "all and any\n\nLook at the definition of all and any.\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nHow would you check that a square matrix is symmetric?\nHow would you check that a matrix is diagonal?\n\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\nA square matrix is symmetric iff it is equal to its transpose. Recall that t(A) denotes the transpose of matrix A.\n\nA &lt;- matrix(rnorm(9), nrow=3, ncol=3) # a.s. non-symmetric\nall(A == t(A))\n\n[1] FALSE\n\nA &lt;- A %*% t(A)  # build a symmetric matrix, A + t(A) would work also\nall(A == t(A))\n\n[1] TRUE\n\n\nA == t(A) returns a matrix a logical matrix, whose entries are all TRUE iff A is symmetric.\nall() works for matrices as well as for vectors. This is sensible as matrices can be considered as vectors with some additional structure."
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#if-then-else",
    "href": "core/labs-solutions/lab-r-intro.html#if-then-else",
    "title": "R language: a tour",
    "section": "If () then {} else {}",
    "text": "If () then {} else {}\nIf expressions yes_expr and no_expr are complicated it makes sense to use the if (...) {...} else {...} construct\nThere is also a conditional statement with an optional else {}\n#| label: if-else\n#| eval: false\n#| collapse: false\nif (condition) {\n  ...\n} else {\n  ...\n}\n\n\n\n\n\n\nNoteQuestion\n\n\n\nIs there an elif construct in R?\n\n\nNope!\n R also offers a switch\n#| label: switch\nswitch (object,\n  case1 = {action1}, \n  case2 = {action2}, \n  ...\n)\n\n\n\n\n\n\nThere exists a selection function ifelse(test, yes_expr, no_expr).\n\nifelse(test, yes, no)\n\nNote that ifelse(...) is vectorized.\n\nx &lt;-  1L:6L\ny &lt;-  rep(\"odd\", 6)\nz &lt;- rep(\"even\", 6)\n\nifelse(x %% 2L, y, z)\n\n[1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\n This is a vectorized function"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#iterations-for-it-in-iterable-...",
    "href": "core/labs-solutions/lab-r-intro.html#iterations-for-it-in-iterable-...",
    "title": "R language: a tour",
    "section": "Iterations for (it in iterable) {...}\n",
    "text": "Iterations for (it in iterable) {...}\n\nHave a look at Iteration section in R for Data Science\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCreate a lower triangular matrix which represents the 5 first lines of the Pascal triangle.\n\n\nRecall\n\\[\\binom{n}{k} = \\binom{n-1}{k-1} + \\binom{n-1}{k}\\]\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\nT &lt;- matrix(0L, nrow=6, ncol=6)\nT[1,1] &lt;- 1L\n\nfor (i in 2:ncol(T))\n  T[i, 1:i] &lt;- c(0L, T[i-1, 2:i-1]) + T[i-1, 1:i]\n\ncolnames(T) &lt;- 0L:5L\nrownames(T) &lt;- 0L:5L\n\nT\n\n  0 1  2  3 4 5\n0 1 0  0  0 0 0\n1 1 1  0  0 0 0\n2 1 2  1  0 0 0\n3 1 3  3  1 0 0\n4 1 4  6  4 1 0\n5 1 5 10 10 5 1\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nLocate the smallest element in a numerical vector\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\nv &lt;- sample(1:100, 100)\nv[1:10]\n\n [1] 31 74 66 19 29 94 41 48 97 51\n\npmin &lt;- 1\n\n# q: what is the purpose of the following loop?\n# for (i in 2:length(v)) {\n#   if (v[i]&lt;v[pmin]) {\n#     pmin &lt;- i\n#   }\n# }\nfor (i in seq_along(v)) {\n  if (v[i]&lt;v[pmin]) {\n    pmin &lt;- i\n  }\n}\n\nprint(stringr::str_c('minimum is at ', pmin, ', it is equal to ', v[pmin]))\n\n[1] \"minimum is at 32, it is equal to 1\"\n\n\nThere are some redundant braces {}"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#while-condition",
    "href": "core/labs-solutions/lab-r-intro.html#while-condition",
    "title": "R language: a tour",
    "section": "While (condition) {…}",
    "text": "While (condition) {…}\n\n\n\n\n\n\nNoteQuestion\n\n\n\nFind the location of the minimum in a vector v\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\nv &lt;- sample(100, 100)\n\npmin &lt;- 1   # Minimum in v[1:1]\ni &lt;- 2\n\n# q: find le location of the minimum in  vector v\n\nwhile (i &lt;= length(v)) {\n  # loop invariant: v[pmin] == min(v[1:i])\n  if (v[i]&lt;v[pmin]) {\n    pmin &lt;- i\n  }\n  i &lt;- i + 1\n}\n\nprint(stringr::str_c('minimum is at ', pmin, ', it is equal to ', v[pmin]))\n\n[1] \"minimum is at 97, it is equal to 1\"\n\nwhich.min(v); v[which.min(v)]\n\n[1] 97\n\n\n[1] 1\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWrite a loop that checks whether vector v is non-decreasing.\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\nresult &lt;- TRUE\n\nfor (i in 2:length(v))\n  if (v[i] &lt; v[i-1]) {\n    result &lt;- FALSE\n    break\n  }\n\nif (result) {\n  print(\"non-decreasing\")\n} else {\n  print(\"not non-decreasing\")\n}\n\n[1] \"not non-decreasing\""
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#operators-purrrmap_",
    "href": "core/labs-solutions/lab-r-intro.html#operators-purrrmap_",
    "title": "R language: a tour",
    "section": "Operators purrr::map_???\n",
    "text": "Operators purrr::map_???\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWrite truth tables for &, |, &&, ||, ! and xor\nHint: use purrr::map, function outer()\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\nvals &lt;- c(TRUE, FALSE, NA)\nops &lt;- c(`&`, `|`, `xor`)\n\ntruth &lt;- purrr::map(ops, \\(x) outer(vals,vals, x))\n\nnames(truth) &lt;- (ops)\ntruth\n\n$`.Primitive(\"&\")`\n      [,1]  [,2]  [,3]\n[1,]  TRUE FALSE    NA\n[2,] FALSE FALSE FALSE\n[3,]    NA FALSE    NA\n\n$`.Primitive(\"|\")`\n     [,1]  [,2] [,3]\n[1,] TRUE  TRUE TRUE\n[2,] TRUE FALSE   NA\n[3,] TRUE    NA   NA\n\n$`function (x, y) \\n{\\n    (x | y) & !(x & y)\\n}`\n      [,1]  [,2] [,3]\n[1,] FALSE  TRUE   NA\n[2,]  TRUE FALSE   NA\n[3,]    NA    NA   NA\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWrite a function that takes as input a square matrix and returns TRUE if it is lower triangular.\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\nlt &lt;- function(A){\n  n &lt;- nrow(A)\n  all(purrr::map_lgl(1:(n-1), \\(x) all(0 == A[x, (x+1):n])))\n}\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nUse map , choose and proper use of pronouns to deliver the n first lines of the Pascal triangle using one line of code.\nAs far as the total number of operations is concerned, would you recommend this way of computing the Pascal triangle?\n\n\n\n\n\n\n\n\nNoneSolution\n\n\n\n\n\n\nn &lt;- 5\n\ntp5 &lt;- matrix(unlist(map(0:n,\n           \\(x) c(choose(x, 0:x), rep(0L, n-x)))),\n       nrow=n+1,\n       byrow=T)\n\nrownames(tp5) &lt;- 0:n\n\ncolnames(tp5) &lt;- 0:n\n\ntp5\n\n  0 1  2  3 4 5\n0 1 0  0  0 0 0\n1 1 1  0  0 0 0\n2 1 2  1  0 0 0\n3 1 3  3  1 0 0\n4 1 4  6  4 1 0\n5 1 5 10 10 5 1\n\n\nNo. Using map and choose, we do not reuse previous computations. The total number of arithmetic operations is \\(\\Omega(n^3)\\), it should be \\(O(n^2)\\).\n\n\n\n\n\n\n\n\n\nRead Chapter on Functional Programming in Advanced R"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html",
    "href": "core/labs-solutions/lab-gapminder.html",
    "title": "Data visualization",
    "section": "",
    "text": "M1 MIDS/MFA/LOGOS\n\n\nUniversité Paris Cité\n\n\nAnnée 2025\n\n\nCourse Homepage\n\n\nMoodle"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#grammar-of-graphics",
    "href": "core/labs-solutions/lab-gapminder.html#grammar-of-graphics",
    "title": "Data visualization",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nWe will use the Grammar of Graphics approach to visualization\nThe expression Grammar of Graphics was coined by Leiland Wilkinson to describe a principled approach to visualization in Data Analysis (EDA)\nA plot is organized around tabular data (a table with rows (observations) and columns (variables))\nA plot is a graphical object that can be built layer by layer\nBuilding a graphical object consists in chaining elementary operations\nThe acclaimed TED presentation by Hans Rosling illustrates the Grammar of Graphics approach\n\nWe will reproduce the animated demonstration using\n\n\nggplot2: an implementation of grammar of graphics in `R\n\nplotly: a bridge between R and the javascript library D3.js\n\nUsing plotly, opting for html ouput, brings the possibility of interactivity and animation"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#setup",
    "href": "core/labs-solutions/lab-gapminder.html#setup",
    "title": "Data visualization",
    "section": "Setup",
    "text": "Setup\nWe will use the following packages. If needed, we install them.\n\nCodestopifnot(\n  require(tidyverse), \n  require(patchwork), \n  require(glue), \n  require(ggforce), \n  require(plotly),\n  require(ggthemes),\n  require(gapminder),\n  require(ggrepel)\n)\n\n\nThe data we will use can be obtained by loading package gapminder\n\n\n\n\n\n\nTipTip\n\n\n\nIf the packages have not yet been installed on your hard drive, install them.\nYou can do that using base R install.packages() function:\ninstall.packages(\"tidyverse\")\nIt is often faster and more convenient to use functions from package pak\ninstall.packages(\"pak\")\npak::pkg_install(\"tidyverse\")\n\n\nYou need to understand the difference between installing and loading a package\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nHow do we get the list of installed packages?\nHow do we get the list of loaded packages?\nWhich objects are made available by a package?\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nThe (usually very long) list of installed packages can be obtained by a simple function call.\n\nCodedf &lt;- installed.packages()\nhead(df)\n##             Package       LibPath                                            \n## abind       \"abind\"       \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## ape         \"ape\"         \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## arkhe       \"arkhe\"       \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## arrow       \"arrow\"       \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## ash         \"ash\"         \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## AsioHeaders \"AsioHeaders\" \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n##             Version    Priority Depends       \n## abind       \"1.4-5\"    NA       \"R (&gt;= 1.5.0)\"\n## ape         \"5.8-1\"    NA       \"R (&gt;= 3.2.0)\"\n## arkhe       \"1.6.0\"    NA       \"R (&gt;= 3.5)\"  \n## arrow       \"16.1.0\"   NA       \"R (&gt;= 4.0)\"  \n## ash         \"1.0-15\"   NA       NA            \n## AsioHeaders \"1.22.1-2\" NA       NA            \n##             Imports                                                                                                                \n## abind       \"methods, utils\"                                                                                                       \n## ape         \"nlme, lattice, graphics, methods, stats, utils, parallel, Rcpp\\n(&gt;= 0.12.0), digest\"                                  \n## arkhe       \"graphics, methods, stats, utils\"                                                                                      \n## arrow       \"assertthat, bit64 (&gt;= 0.9-7), glue, methods, purrr, R6, rlang\\n(&gt;= 1.0.0), stats, tidyselect (&gt;= 1.0.0), utils, vctrs\"\n## ash         NA                                                                                                                     \n## AsioHeaders NA                                                                                                                     \n##             LinkingTo         \n## abind       NA                \n## ape         \"Rcpp\"            \n## arkhe       NA                \n## arrow       \"cpp11 (&gt;= 0.4.2)\"\n## ash         NA                \n## AsioHeaders NA                \n##             Suggests                                                                                                                                                                                                            \n## abind       NA                                                                                                                                                                                                                  \n## ape         \"gee, expm, igraph, phangorn, xml2\"                                                                                                                                                                                 \n## arkhe       \"tinytest\"                                                                                                                                                                                                          \n## arrow       \"blob, curl, cli, DBI, dbplyr, decor, distro, dplyr, duckdb\\n(&gt;= 0.2.8), hms, jsonlite, knitr, lubridate, pillar, pkgload,\\nreticulate, rmarkdown, stringi, stringr, sys, testthat (&gt;=\\n3.1.0), tibble, tzdb, withr\"\n## ash         NA                                                                                                                                                                                                                  \n## AsioHeaders NA                                                                                                                                                                                                                  \n##             Enhances License                   License_is_FOSS\n## abind       NA       \"LGPL (&gt;= 2)\"             NA             \n## ape         NA       \"GPL-2 | GPL-3\"           NA             \n## arkhe       NA       \"GPL (&gt;= 3)\"              NA             \n## arrow       NA       \"Apache License (&gt;= 2.0)\" NA             \n## ash         NA       \"GPL (&gt;= 2)\"              NA             \n## AsioHeaders NA       \"BSL-1.0\"                 NA             \n##             License_restricts_use OS_type MD5sum NeedsCompilation Built  \n## abind       NA                    NA      NA     \"no\"             \"4.4.0\"\n## ape         NA                    NA      NA     \"yes\"            \"4.4.1\"\n## arkhe       NA                    NA      NA     \"no\"             \"4.4.0\"\n## arrow       NA                    NA      NA     \"yes\"            \"4.4.0\"\n## ash         NA                    NA      NA     \"yes\"            \"4.4.0\"\n## AsioHeaders NA                    NA      NA     \"no\"             \"4.4.0\"\n\n\nNote that the output is tabular (it is a matrix and an array) that contains much more than the names of installed packages. If we just want the names of the installed packages, we can extract the column named Package.\n\nCodedf[1:5, c(\"Package\", \"Version\") ]\n##       Package Version \n## abind \"abind\" \"1.4-5\" \n## ape   \"ape\"   \"5.8-1\" \n## arkhe \"arkhe\" \"1.6.0\" \n## arrow \"arrow\" \"16.1.0\"\n## ash   \"ash\"   \"1.0-15\"\n\n\nMatrices and arrays represent mathematical object and are fit for computations. They are not so convenient as far as querying is concerned. Dataframes which are also tabular objects can be queried like tables in a relational database.\nLoading a package amounts to make a number of objects available in the current session. The objects are made available though Namespaces.\n\nCodeloadedNamespaces()\n##  [1] \"methods\"     \"graphics\"    \"plotly\"      \"generics\"    \"tidyr\"      \n##  [6] \"stringi\"     \"hms\"         \"digest\"      \"magrittr\"    \"evaluate\"   \n## [11] \"grid\"        \"timechange\"  \"grDevices\"   \"fastmap\"     \"jsonlite\"   \n## [16] \"ggrepel\"     \"tidyverse\"   \"ggthemes\"    \"httr\"        \"purrr\"      \n## [21] \"viridisLite\" \"scales\"      \"tweenr\"      \"codetools\"   \"lazyeval\"   \n## [26] \"cli\"         \"rlang\"       \"polyclip\"    \"munsell\"     \"withr\"      \n## [31] \"utils\"       \"yaml\"        \"stats\"       \"tools\"       \"base\"       \n## [36] \"tzdb\"        \"dplyr\"       \"colorspace\"  \"ggplot2\"     \"forcats\"    \n## [41] \"vctrs\"       \"R6\"          \"lifecycle\"   \"lubridate\"   \"stringr\"    \n## [46] \"htmlwidgets\" \"MASS\"        \"pkgconfig\"   \"pillar\"      \"gtable\"     \n## [51] \"glue\"        \"data.table\"  \"Rcpp\"        \"ggforce\"     \"xfun\"       \n## [56] \"tibble\"      \"tidyselect\"  \"knitr\"       \"farver\"      \"datasets\"   \n## [61] \"gapminder\"   \"htmltools\"   \"patchwork\"   \"rmarkdown\"   \"readr\"      \n## [66] \"compiler\"\n\n\nNote that we did not load explicitly some of the loadedNamespaces. Many of the loaded packages were loaded while loading other packages, for example metapackages like tidyverse."
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#have-a-look-at-gapminder-dataset",
    "href": "core/labs-solutions/lab-gapminder.html#have-a-look-at-gapminder-dataset",
    "title": "Data visualization",
    "section": "Have a look at gapminder dataset",
    "text": "Have a look at gapminder dataset\nThe gapminder table can be found at gapminder::gapminder\n\nA table has a schema: a list of named columns, each with a given type\nA table has a content: rows. Each row is a collection of items, corresponding to the columns\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nExplore gapminder::gapminder, using glimpse() and head()\n\n\nglimpse() allows to see the schema and the first rows\n\nhead() allows to see the first rows\nUse the pipe |&gt; to chain operations\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nDataframes\n\nCodegapminder &lt;- gapminder::gapminder\n\nglimpse(gapminder)\n## Rows: 1,704\n## Columns: 6\n## $ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n## $ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n## $ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n## $ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\ngapminder |&gt;  \n  glimpse()\n## Rows: 1,704\n## Columns: 6\n## $ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n## $ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n## $ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n## $ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\ngapminder |&gt; \n  head()\n## # A tibble: 6 × 6\n##   country     continent  year lifeExp      pop gdpPercap\n##   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n## 1 Afghanistan Asia       1952    28.8  8425333      779.\n## 2 Afghanistan Asia       1957    30.3  9240934      821.\n## 3 Afghanistan Asia       1962    32.0 10267083      853.\n## 4 Afghanistan Asia       1967    34.0 11537966      836.\n## 5 Afghanistan Asia       1972    36.1 13079460      740.\n## 6 Afghanistan Asia       1977    38.4 14880372      786.\n\n\nEven an empty dataframe has a scheme:\n\nCodegapminder |&gt; \n  head(0) |&gt; \n  glimpse()\n\nRows: 0\nColumns: 6\n$ country   &lt;fct&gt; \n$ continent &lt;fct&gt; \n$ year      &lt;int&gt; \n$ lifeExp   &lt;dbl&gt; \n$ pop       &lt;int&gt; \n$ gdpPercap &lt;dbl&gt; \n\nCode# glimpse(head(gapminder, 0))\n\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nThe schema of a dataframe/tibble is the list of column names and classes. The content of a dataframe is made of the rows. A dataframe may have null content\n\nCodegapminder |&gt; \n  filter(FALSE) |&gt; \n  glimpse()\n## Rows: 0\n## Columns: 6\n## $ country   &lt;fct&gt; \n## $ continent &lt;fct&gt; \n## $ year      &lt;int&gt; \n## $ lifeExp   &lt;dbl&gt; \n## $ pop       &lt;int&gt; \n## $ gdpPercap &lt;dbl&gt;"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#get-a-feeling-of-the-dataset",
    "href": "core/labs-solutions/lab-gapminder.html#get-a-feeling-of-the-dataset",
    "title": "Data visualization",
    "section": "Get a feeling of the dataset",
    "text": "Get a feeling of the dataset\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPick two random rows for each continent using slice_sample()\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nTo pick a slice at random, we can use function slice_sample. We can even perform sampling within groups defined by the value of a column.\n\nCodegapminder |&gt; \n  slice_sample(n=2, by=continent)\n\n# A tibble: 10 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Israel      Asia       2002    79.7  6029529    21906.\n 2 Nepal       Asia       1982    49.6 15796314      718.\n 3 Albania     Europe     1972    67.7  2263554     3313.\n 4 Hungary     Europe     1952    64.0  9504000     5264.\n 5 Congo, Rep. Africa     1987    57.5  2064095     4201.\n 6 Djibouti    Africa     1992    51.6   384156     2377.\n 7 Guatemala   Americas   1987    60.8  7326406     4246.\n 8 Haiti       Americas   1987    53.6  5756203     1823.\n 9 Australia   Oceania    1952    69.1  8691212    10040.\n10 New Zealand Oceania    2007    80.2  4115771    25185.\n\nCode#&lt; or equivalently \n# gapminder |&gt; \n#   group_by(continent) |&gt; \n#   slice_sample(n=2)\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat makes a table tidy?\n\n\n\n\n\n\n\n\nTipTip\n\n\n\nHave a look at Data tidying in R for Data Science (2nd ed.)\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nIs the gapminder table redundant?\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\ngapminder is redundant: column country completely determines the content of column continent. In database parlance, we have a functional dependancy: country → continent whereas the key of the table is made of columns country, year.\nTable gapminder is not in Boyce-Codd Normal Form (BCNF), not even in Third Normal Form (3NF)."
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#gapminder-tibble-extract",
    "href": "core/labs-solutions/lab-gapminder.html#gapminder-tibble-extract",
    "title": "Data visualization",
    "section": "Gapminder tibble (extract)",
    "text": "Gapminder tibble (extract)\n\n\n\n\n\n\nNoteQuestion\n\n\n\nExtract/filter a subset of rows using dplyr::filter(...)\n\nAll rows concerning a given country\nAll rows concerning a year\nAll rows concerning a given continnent and a year\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCode# q: in gapminder table extract all raws concerning France\n\ngapminder |&gt; \n  filter(country=='France') |&gt; \n  head()\n\n# A tibble: 6 × 6\n  country continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 France  Europe     1952    67.4 42459667     7030.\n2 France  Europe     1957    68.9 44310863     8663.\n3 France  Europe     1962    70.5 47124000    10560.\n4 France  Europe     1967    71.6 49569000    13000.\n5 France  Europe     1972    72.4 51732000    16107.\n6 France  Europe     1977    73.8 53165019    18293.\n\n\n\n\n\n\n\n\n\n\nEquality testing is performed using ==, not = (which is used to implement assignment)"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#filtering-selection-σ-from-database-theory-picking-one-year-of-data",
    "href": "core/labs-solutions/lab-gapminder.html#filtering-selection-σ-from-database-theory-picking-one-year-of-data",
    "title": "Data visualization",
    "section": "Filtering (selection \\(σ\\) from database theory) : Picking one year of data",
    "text": "Filtering (selection \\(σ\\) from database theory) : Picking one year of data\nThere is simple way to filter rows satisfying some condition. It consists in mimicking indexation in a matrix, leaving the colum index empty, replacing the row index by a condition statement (a logical expression) also called a mask.\n\nCode# q: in gapminder table extract all raws concerning year 2002\n\ngapminder_2002 &lt;- gapminder |&gt;\n  filter(year==2002)  # \n\ngapminder_2002 &lt;- gapminder[gapminder$year==2002,]\n\n\nHave a look at\n\nCodegapminder$year==2002\n\n\nWhat is the type/class of this expression?\nThis is possible in base R and very often convenient.\nNevertheless, this way of performing row filtering does not emphasize the connection between the dataframe and the condition. Any logical vector with the right length could be used as a mask. Moreover, this way of performing filtering is not very functional.\n\n\n\n\n\n\nIn the parlance of Relational Algebra, filter performs a selection of rows. Relational expression\n\\[σ_{\\text{condition}}(\\text{Table})\\]\ntranslates to\n\nCodefilter(Table, condition)\n\n\nwhere \\(\\text{condition}\\) is a boolean expression that can be evaluated on each row of \\(\\text{Table}\\). In SQL, the relational expression would translate into\n\nCodeSELECT \n  *\nFROM \n  Table\nWHERE \n  condition\n\n\nCheck Package dplyr docs\nThe posit cheatsheet on dplyr is an unvaluable resource for table manipulation.\n\n\n\nUse dplyr::filter() to perform row filtering\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCode# filter(gapminder, year==2002)\n\ngapminder |&gt; \n  filter(year==2002)\n\n# A tibble: 142 × 6\n   country     continent  year lifeExp       pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       2002    42.1  25268405      727.\n 2 Albania     Europe     2002    75.7   3508512     4604.\n 3 Algeria     Africa     2002    71.0  31287142     5288.\n 4 Angola      Africa     2002    41.0  10866106     2773.\n 5 Argentina   Americas   2002    74.3  38331121     8798.\n 6 Australia   Oceania    2002    80.4  19546792    30688.\n 7 Austria     Europe     2002    79.0   8148312    32418.\n 8 Bahrain     Asia       2002    74.8    656397    23404.\n 9 Bangladesh  Asia       2002    62.0 135656790     1136.\n10 Belgium     Europe     2002    78.3  10311970    30486.\n# ℹ 132 more rows\n\n\n\n\n\n\n\n\n\n\nImportantData masking\n\n\n\nNote that in stating the condition, we simply write year==2002 even though year is not the name of an object in our current session. This is possible because filter( ) uses data masking, year is meant to denote a column in gapminder. SQL interpreters use the same mechanism.\nThe ability to use data masking is one of the great strengths of the R programming language."
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#static-plotting-first-attempt",
    "href": "core/labs-solutions/lab-gapminder.html#static-plotting-first-attempt",
    "title": "Data visualization",
    "section": "Static plotting: First attempt",
    "text": "Static plotting: First attempt\n\n\n\n\n\n\nNoteQuestion\n\n\n\nDefine a plot with respect to gapminder_2002 along the lines suggested by Rosling’s presentation.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCodep &lt;- gapminder_2002 |&gt;\n  ggplot() \n\n\n\n\n\n\n\n\n\n\nYou should define a ggplot object with data layer gapminder_2022 and call this object p for further reuse.\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nMap variables gdpPercap and lifeExp to axes x and y. Define the axes. In ggplot2 parlance, this is called aesthetic mapping. Use aes().\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCode# q: Map variables gdpPercap and lifeExp to axes x and y. Define the axes.\np &lt;- p +\n  aes(x=gdpPercap, y=lifeExp)\n\np \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse ggplot object p and add a global aesthetic mapping gdpPercap and lifeExp to axes x and y (using + from ggplot2) .\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nFor each row, draw a point at coordinates defined by the mapping. You need to add a geom_ layer to your ggplot object, in this case geom_point() will do.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nWe add another layer to our graphical object.\n\nCodep &lt;- p +\n  geom_point()\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat’s up?\n\n\n\nWe are building a graphical object (a ggplot object) around a data frame (gapminder)\nWe supply aesthetic mappings (aes()) that can be either global or specifically bound to some geometries (geom_point()) or statistics\nThe global aesthetic mapping defines which columns (variables) are\n\nmapped to position (which columns are mapped to axes),\npossibly mapped to colours, linetypes, shapes, …\n\nGeometries and Statistics describe the building blocks of graphics\n\n\nWhat’s missing here?\nwhen comparing to the Gapminder demonstration, we can spot that\n\ncolors are missing\nbubble sizes are all the same. They should reflect the population size of the country\ntitles and legends are missing. This means the graphic object is useless.\n\nWe will add other layers to the graphical object to complete the plot"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#second-attempt-display-more-information",
    "href": "core/labs-solutions/lab-gapminder.html#second-attempt-display-more-information",
    "title": "Data visualization",
    "section": "Second attempt: display more information",
    "text": "Second attempt: display more information\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nMap continent to color (use aes())\nMap pop to bubble size (use aes())\nMake point transparent by tuning alpha (inside geom_point() avoid overplotting)\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCodep &lt;- p +\n  aes(color=continent, size=pop) +\n  geom_point(alpha=.5) \n\np\n\n\n\n\n\n\n\nNote that we only use global aesthetic mappings. This makes sense since we do not need to taylor aesthetics to specific geometries. Indeed we only have one geometry in our graphical object.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nIn this enrichment of the graphical object, guides have been automatically added for two aesthetics: color and size. Those two guides are deemed necessary since the reader has no way to guess the mapping from the five levels of continent to color (the color scale), and the reader needs help to connect population size and bubble size.\nggplot2 provides us with helpers to fine tune guides.\nThe scalings on the x and y axis do not deserve guides: the ticks along the coordinate axes provide enough information."
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#scaling",
    "href": "core/labs-solutions/lab-gapminder.html#scaling",
    "title": "Data visualization",
    "section": "Scaling",
    "text": "Scaling\nTo pay tribute to Hans Rosling, we need to take care of two scaling issues:\n\nthe gdp per capita axis should be logarithmic scale_x_log10()\n\nthe area of the point should be proportional to the population scale_size_area()\n\n\n\n\n\n\n\n\nNoteComplete the graphical object accordingly\n\n\n\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCode# q: use logarithmic scale for both axes\np &lt;- p +\n  scale_x_log10() +\n##  scale_size_area() +\n  ggtitle(\"Gapminder 2002, scaled\")\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nMotivate the proposed scalings.\n\nWhy is it important to use logarithmic scaling for gdp per capita?\nWhen is it important to use logarithmic scaling on some axis (in other contexts)?\nWhy is it important to specify scale_size_area() ?\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nTo see why using scale_size_area() is important, we can check what happens when we use scale_size() instead.\n\nCodepop_range &lt;- c(0, max(gapminder_2002$pop))\n\np +\n  scale_radius(limits = pop_range) + \n  ggtitle(\"scale_radius\")\n\n\n\n\n\n\n\nWith scale_size_area(), the area of the point is proportional to the value of the variable mapped to size. With scale_size(), the radius of the point is proportional to the value of the variable mapped to size, so the area is proportional to the square of the value of the variable. This tends to exaggerate the differences between the sizes of the points. This is a way of lying with statistics.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nWe use package patchwork to collect and present several graphical objects.\n\nCodeptchwrk &lt;- (\n  p + \n  scale_size(limits = pop_range) + \n  ggtitle(\"scale_size\")) + \n  (p + \n  scale_radius(limits = pop_range) + \n  ggtitle(\"scale_radius\")) \n\nptchwrk + plot_annotation(\n  title='Comparing scale_size and scale_radius', \n  caption='In the current setting, scale_size() should be favored'\n)\n\n\n\n\n\n\n\nAccording to the documentation, scale_size_area() ensures that a value of \\(0\\) is mapped to a size of \\(0\\). This is not the case with scale_size().\n\nCodeptchwrk &lt;- (\n  p + \n  scale_size(limits = pop_range) + \n  ggtitle(\"scale_size\")) + \n  (p + \n  scale_size_area() + \n  ggtitle(\"scale_size_area\")) \n\nptchwrk + plot_annotation(\n  title='Comparing scale_size and scale_size_area', \n  caption='In the current setting, scale_size_area() should be favored'\n)\n\n\n\n\n\n\n\n\nCodep &lt;- p + \n  scale_size_area()"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#in-perspective",
    "href": "core/labs-solutions/lab-gapminder.html#in-perspective",
    "title": "Data visualization",
    "section": "In perspective",
    "text": "In perspective\n\n\n\n\n\n\nNoteQuestion\n\n\n\nUsing copilots completions, we can summarize the construcion of the graphical object in a series of questions.\n# q: Define a plot with respect to table gapminder_2002 along the lines suggested by Rosling's TED presentation\n# q: Map variables gdpPercap and lifeExp to axes x and y. Define the axes. \n# q: For each row, draw a point at coordinates defined by the mapping.\n# q: Map continent to color\n# q: Map pop to bubble size\n# q: Make point transparent by tuning alpha (inside geom_point() avoid overplotting)\n# q: Add a plot title\n# q: Make axes titles explicit and readable\n# q: Use labs(...)  \n# q: Use scale_x_log10() and scale_size_area()\n# q: Fine tune the guides: replace pop by Population and titlecase continent\n# q: Use theme_minimal()\n# q: Use scale_color_manual(...) to fine tune the color aesthetic mapping.\n# q: Use facet_zoom() from package ggforce\n# q: Add labels to points. This can be done by aesthetic mapping. Use aes(label=..)\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCodeyoi &lt;- 2002\n\np &lt;-  p + \n  labs(\n    title=glue('The world in year {yoi}'),\n    x=\"Gross Domestic Product per capita (US$ 2009, corrected for PPP)\",\n    y=\"Life expectancy at birth\"\n  )\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nWe should also fine tune the guides: replace pop by Population and titlecase continent.\n\nCode# q: fine tune the guides: replace `pop` by `Population` and titlecase `continent`.\np &lt;- p +\n  guides(color = guide_legend(title = \"Continent\",\n                              override.aes = list(size = 5),\n                              order = 1),\n         size = guide_legend(title = \"Population\",\n                             order = 2))\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat should be the respective purposes of Title, Subtitle, Caption, … ?\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nThe title should be explicit and concise. It should summarize the content of the graphic object. Our title here “The world in year 2002” is concise but not explicit enough. The world may signify widely different things. Here, we mean world countries\nThe subtitle should provide additional information: “Public health does not boil down to GDP per capita”\nThe caption should provide additional information. Here we could explain the meaning of the axes, the color scale, the size scale, … provided guides are not enough. Here we could spot the source(s) of the data: UNO, WHO, World Bank, …, Gapminder foundation.\n\n\nCodep &lt;- p +\n  labs(\n    subtitle=\"Public health does not boil down to GDP per capita\",\n    caption=\"Source: Gapminder Foundation through Gapminder package\"\n  )\np"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#theming-using-ggthemes-or-not",
    "href": "core/labs-solutions/lab-gapminder.html#theming-using-ggthemes-or-not",
    "title": "Data visualization",
    "section": "Theming using ggthemes (or not)",
    "text": "Theming using ggthemes (or not)\n\nCodestopifnot(\n  require(\"ggthemes\")\n)\n\n\nA theme defines the look and feel of plots\nWithin a single document, we should use only one theme\nSee Getting the theme for a gallery of available themes\n\nCodep +\n  theme_economist()"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#tuning-scales",
    "href": "core/labs-solutions/lab-gapminder.html#tuning-scales",
    "title": "Data visualization",
    "section": "Tuning scales",
    "text": "Tuning scales\n\n\n\n\n\n\nNoteQuestion\n\n\n\nUse scale_color_manual(...) to fine tune the color aesthetic mapping.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\nCode```{r}\n#| label: theme_scale\nneat_color_scale &lt;-\n      c(\"Africa\" = \"#01d4e5\",\n        \"Americas\" = \"#7dea01\" ,\n        \"Asia\" = \"#fc5173\",\n        \"Europe\" = \"#fde803\",\n        \"Oceania\" = \"#536227\")\n```\n\n\n\nCodep &lt;- p +\n  scale_size_area(max_size = 15) + #&lt;&lt;\n  scale_color_manual(values = neat_color_scale) #&lt;&lt;\n\nScale for size is already present.\nAdding another scale for size, which will replace the existing scale.\n\nCodep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTip\n\n\n\n\n\nChoosing a color scale is a difficult task\nviridis is often a good pick.\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\nMimnimalist themes are often a good pick.\n\nCodeold_theme &lt;- theme_set(theme_minimal())\n\n\n\nCodep &lt;- p +\n   scale_size_area(max_size = 15,\n                  labels= scales::label_number(scale=1/1e6,\n                                               suffix=\" M\")) +\n   scale_color_manual(values = neat_color_scale) +\n    labs(title= glue(\"Gapminder  {min(gapminder$year)}-{max(gapminder$year)}\"),\n         x = \"Yearly Income per Capita\",\n         y = \"Life Expectancy\",\n       caption=\"From sick  and poor (bottom left) to healthy and rich (top right)\")   \n\nScale for size is already present.\nAdding another scale for size, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\nCodep + theme(legend.position = \"none\")"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#zooming-on-a-continent",
    "href": "core/labs-solutions/lab-gapminder.html#zooming-on-a-continent",
    "title": "Data visualization",
    "section": "Zooming on a continent",
    "text": "Zooming on a continent\n\nCodezoom_continent &lt;- 'Europe'  # choose another continent at your convenience \n\n\n\n\n\n\n\n\nUse facet_zoom() from package ggforce\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCodestopifnot(\n  require(\"ggforce\") #&lt;&lt;\n)\n\np_zoom_continent &lt;- p + \n  facet_zoom( #&lt;&lt;\n    xy= continent==zoom_continent, #&lt;&lt;\n    zoom.data= continent==zoom_continent #&lt;&lt;\n    ) #&lt;&lt;\n\np_zoom_continent"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#adding-labels",
    "href": "core/labs-solutions/lab-gapminder.html#adding-labels",
    "title": "Data visualization",
    "section": "Adding labels",
    "text": "Adding labels\n\n\n\n\n\n\nNoteQuestion\n\n\n\nAdd labels to points. This can be done by aesthetic mapping. Use aes(label=..)\nTo avoid text cluttering, package ggrepel offers interesting tools.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCodestopifnot(\nrequire(ggrepel) #&lt;&lt;\n)\n\np +\n   aes(label=country) + #&lt;&lt;\n   ggrepel::geom_label_repel(max.overlaps = 5) + #&lt;&lt;\n   scale_size_area(max_size = 15,\n                  labels= scales::label_number(scale=1/1e6,\n                                               suffix=\" M\"))  #+\n\n\n\nGapminder 2002 layer by layer\n\n\nCode  # scale_color_manual(values = neat_color_scale) +\n  # theme(legend.position = \"none\") +\n    # labs(title= glue(\"Gapminder  {min(gapminder$year)}-{max(gapminder$year)}\"),\n    #      x = \"Yearly Income per Capita\",\n    #      y = \"Life Expectancy\",\n    #    caption=\"From sick  and poor (bottom left) to healthy and rich (top right)\")"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#facetting",
    "href": "core/labs-solutions/lab-gapminder.html#facetting",
    "title": "Data visualization",
    "section": "Facetting",
    "text": "Facetting\nSo far we have only presented one year of data (2002)\nRosling used an animation to display the flow of time\nIf we have to deliver a printable report, we cannot rely on animation, but we can rely on facetting\nFacets are collections of small plots constructed in the same way on subsets of the data\n\n\n\n\n\n\nNoteQuestion\n\n\n\nAdd a layer to the graphical object using facet_wrap()\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCodep &lt;- p +\n  aes(text=country) +\n  guides(color = guide_legend(title = \"Continent\",\n                              override.aes = list(size = 5),\n                              order = 1),\n         size = guide_legend(title = \"Population\",\n                             order = 2)) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1)) +\n  facet_wrap(vars(year), ncol=6) +\n  ggtitle(\"Gapminder 1952-2007\")\n\np\n\n\n\n\n\n\n\n\n\n\nAs all rows in gapminder_2002 are all related to year 2002, we need to rebuild the graphical object along the same lines (using the same graphical pipeline) but starting from the whole gapminder dataset.\nShould we do this using cut and paste?\n No!!!\n\n\n\n\n\n\n\nImportantDon’t Repeat Yoursel (DRY)\n\n\n\n\nAbide to the DRY principle using operator %+%: the ggplot2 object p can be fed with another dataframe and all you need is proper facetting.\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCodep %+% gapminder"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#animate-for-free-with-plotly",
    "href": "core/labs-solutions/lab-gapminder.html#animate-for-free-with-plotly",
    "title": "Data visualization",
    "section": "Animate for free with plotly\n",
    "text": "Animate for free with plotly\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nUse plotly::ggplotly() to create a Rosling like animation.\nUse frame aesthetics.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCode```{r}\n#| label: animate\n#| eval: !expr knitr::is_html_output()\n#| code-annotations: hover\n\nq &lt;- filter(gapminder, FALSE) |&gt;\n   ggplot() +\n   aes(x = gdpPercap) +\n   aes(y = lifeExp) +\n   aes(size = pop) +\n   aes(text = country) +                   #\n   aes(fill = continent) +\n   # aes(frame = year) +                     #\n  geom_point(alpha=.5, colour='black') +\n  scale_x_log10() +\n  scale_size_area(max_size = 15,\n                  labels= scales::label_number(scale=1/1e6,\n                                               suffix=\" M\")) +\n  scale_fill_manual(values = neat_color_scale) +\n  theme(legend.position = \"none\") +\n  labs(title= glue(\"Gapminder  {min(gapminder$year)}-{max(gapminder$year)}\"),\n       x = \"Yearly Income per Capita\",\n       y = \"Life Expectancy\",\n       caption=\"From sick  and poor (bottom left) to healthy and rich (top right)\")\n\n\n(q %+% gapminder) |&gt;\n  plotly::ggplotly(height = 500, width=750) \n```\n\n\n\n\n\n\n\ntext will be used while hovering\n\n\nframe is used by plotly to drive the animation. One frame per year\n\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\nCode```{r}\n#| eval: !expr knitr::is_html_output()\n\n(\n  (q %+% gapminder) +\n  facet_null() +\n  aes(frame=year)\n) |&gt;\n plotly::ggplotly(height = 500, width=750)\n```\n\n\n\n\n\n\nCode(\n  (q %+% filter(gapminder, continent=='Europe')) +\n  facet_null() +\n  aes(frame=year)\n) |&gt;\n plotly::ggplotly(height = 500, width=750)"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#suggestions",
    "href": "core/labs-solutions/lab-gapminder.html#suggestions",
    "title": "Data visualization",
    "section": "Suggestions",
    "text": "Suggestions\nThink about ways to visualize specific aspects of the gapminder data.\n\nHow could you overlay the world in 1952 and 2007?\nHow could you visualize the evolution of life expectancy and population across the different countries?\nVisualize the evolution of former colonies and their colonizers.\nVisualize the evolution of countries from the former Soviet Union, Warsaw Pact, and Yugoslavia.\nVisualize the evolution of countries from the former British Empire."
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#more-material",
    "href": "core/labs-solutions/lab-gapminder.html#more-material",
    "title": "Data visualization",
    "section": "More material",
    "text": "More material\n\nRead Visualization in R for Data Science"
  },
  {
    "objectID": "core/labs-solutions/lab-progr.html",
    "href": "core/labs-solutions/lab-progr.html",
    "title": "R programming: generics",
    "section": "",
    "text": "M1 MIDS/MFA/LOGOS\n\n\nUniversité Paris Cité\n\n\nAnnée 2025\n\n\nCourse Homepage\n\n\nMoodle\nCode\nstopifnot(\n  require(Hmisc),\n  require(skimr),\n  require(patchwork),\n  require(ggforce),\n  require(glue),\n  require(ggfortify),\n  require(broom),\n  require(tidyverse)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(\n  theme_minimal(base_size=9, \n                base_family = \"Helvetica\")\n)\nCode\ngc &lt;- options(ggplot2.discrete.colour=\"viridis\")\ngc &lt;- options(ggplot2.discrete.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.colour=\"viridis\")"
  },
  {
    "objectID": "core/labs-solutions/lab-progr.html#objectives",
    "href": "core/labs-solutions/lab-progr.html#objectives",
    "title": "R programming: generics",
    "section": "Objectives",
    "text": "Objectives\n\n\nCode\nstopifnot(\n  require(rlang),\n  require(lobstr),\n  require(sloop),\n  require(devtools),\n  require(usethis),\n  require(testthat),\n  require(generics)\n)\n\n\nLoading required package: rlang\n\n\nLoading required package: lobstr\n\n\nLoading required package: sloop\n\n\nLoading required package: devtools\n\n\nLoading required package: usethis\n\n\nLoading required package: testthat\n\n\nLoading required package: generics"
  },
  {
    "objectID": "core/labs-solutions/lab-progr.html#generics-and-s3-classes",
    "href": "core/labs-solutions/lab-progr.html#generics-and-s3-classes",
    "title": "R programming: generics",
    "section": "Generics and S3 classes",
    "text": "Generics and S3 classes\nLet us first create an instance of class lm.\n\n\nCode\nlm0  &lt;- lm(Gas ~ Insul * Temp, MASS::whiteside)\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nWhat does function class() do?\nIs it possible to belong to type list and to class lm simultaneously?\nIn R what is an attribute?\nHow do we set and get attributes?\n\nWhat does function inherits() do?\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nclass(lm0)\n\n\n[1] \"lm\"\n\n\nCode\nis.list(lm0)\n\n\n[1] TRUE\n\n\nCode\nattributes(lm0)\n\n\n$names\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"contrasts\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n\n$class\n[1] \"lm\"\n\n\nCode\ninherits(lm0,\"list\")\n\n\n[1] FALSE\n\n\nCode\ninherits(lm0, \"lm\")\n\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nLoad package sloop.\n\nWhat does sloop::otype() do? Apply it to an object of class lm.\nWhat happens when we first unclass() the object?\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nsloop::otype(lm0)\n\n\n[1] \"S3\"\n\n\nCode\nsloop::otype(unclass(lm0))\n\n\n[1] \"base\"\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nsloop exports functions s3_class() and s3_get_method()\n\nApply s3_class() to all members of lm0\nWhat is the otype of autoplot() applied to an object of class lm?\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nsloop::s3_class(lm0)\n\n\n[1] \"lm\"\n\n\n\n\nCode\nsloop::s3_get_method(autoplot.lm)\n\n\nfunction (object, which = c(1:3, 5), data = NULL, colour = \"#444444\", \n    size = NULL, linetype = NULL, alpha = NULL, fill = NULL, \n    shape = NULL, label = TRUE, label.label = \".label\", label.colour = \"#000000\", \n    label.alpha = NULL, label.size = NULL, label.angle = NULL, \n    label.family = NULL, label.fontface = NULL, label.lineheight = NULL, \n    label.hjust = NULL, label.vjust = NULL, label.repel = FALSE, \n    label.n = 3, smooth.colour = \"#0000FF\", smooth.linetype = \"solid\", \n    ad.colour = \"#888888\", ad.linetype = \"dashed\", ad.size = 0.2, \n    nrow = NULL, ncol = NULL, ...) \n{\n    p1 &lt;- p2 &lt;- p3 &lt;- p4 &lt;- p5 &lt;- p6 &lt;- NULL\n    dropInf &lt;- function(x, h) {\n        if (any(isInf &lt;- h &gt;= 1)) {\n            warning(gettextf(\"not plotting observations with leverage one:\\n  %s\", \n                paste(which(isInf), collapse = \", \")), call. = FALSE, \n                domain = NA)\n            x[isInf] &lt;- NaN\n        }\n        x\n    }\n    show &lt;- rep(FALSE, 6)\n    show[which] &lt;- TRUE\n    class(object$residuals) &lt;- NULL\n    if (is.null(data)) {\n        plot.data &lt;- ggplot2::fortify(object)\n    }\n    else {\n        plot.data &lt;- ggplot2::fortify(object, data = data)\n    }\n    n &lt;- nrow(plot.data)\n    plot.data$.index &lt;- 1:n\n    plot.data$.label &lt;- rownames(plot.data)\n    is_glm &lt;- inherits(object, \"glm\")\n    r &lt;- residuals(object)\n    w &lt;- weights(object)\n    if (any(show[2L:6L])) {\n        s &lt;- if (inherits(object, \"rlm\")) {\n            object$s\n        }\n        else if (is_glm) {\n            sqrt(summary(object)$dispersion)\n        }\n        else {\n            sqrt(stats::deviance(object)/stats::df.residual(object))\n        }\n        hii &lt;- stats::lm.influence(object, do.coef = FALSE)$hat\n        r.hat &lt;- range(hii, na.rm = TRUE)\n        is_const_lev &lt;- all(r.hat == 0) || all(diff(r.hat) &lt; \n            1e-10 * mean(hii, na.rm = TRUE))\n        fs &lt;- dplyr::select_if(plot.data, function(x) is.character(x) | \n            is.factor(x))\n        fs[[\".label\"]] &lt;- NULL\n        if (is_const_lev & ncol(fs) &gt; 0) {\n            plot.data$.nf &lt;- stringr::str_wrap(interaction(fs, \n                sep = \":\"), width = 10)\n        }\n        if (any(show[2L:3L])) {\n            plot.data$.wresid &lt;- if (is.null(w)) {\n                r\n            }\n            else {\n                sqrt(w) * r\n            }\n            plot.data$.wstdresid &lt;- plot.data$.wresid/(s * sqrt(1 - \n                hii))\n        }\n        if (show[2L]) {\n            ylim &lt;- range(plot.data$.wstdresid, na.rm = TRUE)\n            ylim[2L] &lt;- ylim[2L] + diff(ylim) * 0.075\n            qn &lt;- stats::qqnorm(plot.data$.wstdresid, ylim = ylim, \n                plot.it = FALSE)\n            plot.data$.qqx &lt;- qn$x\n            plot.data$.qqy &lt;- qn$y\n        }\n    }\n    label.fitted &lt;- ifelse(is_glm, \"Predicted values\", \"Fitted values\")\n    label.y23 &lt;- ifelse(is_glm, \"Std. deviance resid.\", \"Standardized residuals\")\n    if (is.logical(shape) && !shape) {\n        if (missing(label)) {\n            label &lt;- TRUE\n        }\n        if (missing(label.n)) {\n            label.n &lt;- nrow(plot.data)\n        }\n    }\n    plot.data &lt;- flatten(plot.data)\n    if (label.n &gt; 0L) {\n        if (show[1L]) {\n            r.data &lt;- dplyr::arrange(plot.data, dplyr::desc(abs(.resid)))\n            r.data &lt;- utils::head(r.data, label.n)\n        }\n        if (\".wresid\" %in% colnames(plot.data)) {\n            wr.data &lt;- dplyr::arrange(plot.data, dplyr::desc(abs(.wresid)))\n            wr.data &lt;- utils::head(wr.data, label.n)\n        }\n        if (any(show[4L:6L])) {\n            cd.data &lt;- dplyr::arrange(plot.data, dplyr::desc(abs(.cooksd)))\n            cd.data &lt;- utils::head(cd.data, label.n)\n        }\n    }\n    .smooth &lt;- function(x, y) {\n        stats::lowess(x, y, f = 2/3, iter = 3)\n    }\n    .decorate.label &lt;- function(p, data) {\n        if (label & label.n &gt; 0) {\n            p &lt;- plot_label(p = p, data = data, label = label, \n                label.label = label.label, label.colour = label.colour, \n                label.alpha = label.alpha, label.size = label.size, \n                label.angle = label.angle, label.family = label.family, \n                label.fontface = label.fontface, label.lineheight = label.lineheight, \n                label.hjust = label.hjust, label.vjust = label.vjust, \n                label.repel = label.repel)\n        }\n        p\n    }\n    .decorate.plot &lt;- function(p, xlab = NULL, ylab = NULL, title = NULL) {\n        p + ggplot2::xlab(xlab) + ggplot2::ylab(ylab) + ggplot2::ggtitle(title)\n    }\n    smoother_m &lt;- ggplot2::aes_string(x = \"x\", y = \"y\")\n    if (show[1L]) {\n        t1 &lt;- \"Residuals vs Fitted\"\n        mapping &lt;- ggplot2::aes_string(x = \".fitted\", y = \".resid\")\n        smoother &lt;- .smooth(plot.data$.fitted, plot.data$.resid)\n        smoother &lt;- as.data.frame(smoother)\n        p1 &lt;- ggplot2::ggplot(data = plot.data, mapping = mapping)\n        if (!is.logical(shape) || shape) {\n            p1 &lt;- p1 + geom_factory(geom_point, plot.data, colour = colour, \n                size = size, linetype = linetype, alpha = alpha, \n                fill = fill, shape = shape)\n        }\n        p1 &lt;- p1 + ggplot2::geom_line(data = smoother, mapping = smoother_m, \n            colour = smooth.colour, linetype = smooth.linetype) + \n            ggplot2::geom_hline(yintercept = 0L, linetype = ad.linetype, \n                size = ad.size, colour = ad.colour)\n        p1 &lt;- .decorate.label(p1, r.data)\n        p1 &lt;- .decorate.plot(p1, xlab = label.fitted, ylab = \"Residuals\", \n            title = t1)\n    }\n    if (show[2L]) {\n        t2 &lt;- \"Normal Q-Q\"\n        qprobs &lt;- c(0.25, 0.75)\n        qy &lt;- stats::quantile(plot.data$.wstdresid, probs = qprobs, \n            names = FALSE, type = 7, na.rm = TRUE)\n        qx &lt;- stats::qnorm(qprobs)\n        slope &lt;- diff(qy)/diff(qx)\n        int &lt;- qy[1L] - slope * qx[1L]\n        mapping &lt;- ggplot2::aes_string(x = \".qqx\", y = \".qqy\")\n        p2 &lt;- ggplot2::ggplot(data = plot.data, mapping = mapping)\n        if (!is.logical(shape) || shape) {\n            p2 &lt;- p2 + geom_factory(geom_point, plot.data, colour = colour, \n                size = size, linetype = linetype, alpha = alpha, \n                fill = fill, shape = shape)\n        }\n        p2 &lt;- p2 + ggplot2::geom_abline(intercept = int, slope = slope, \n            linetype = ad.linetype, size = ad.size, colour = ad.colour)\n        p2 &lt;- .decorate.label(p2, wr.data)\n        p2 &lt;- .decorate.plot(p2, xlab = \"Theoretical Quantiles\", \n            ylab = label.y23, title = t2)\n    }\n    if (show[3L]) {\n        t3 &lt;- \"Scale-Location\"\n        mapping &lt;- ggplot2::aes_string(x = \".fitted\", y = \"sqrt(abs(.wstdresid))\")\n        smoother &lt;- .smooth(plot.data$.fitted, sqrt(abs(plot.data$.wstdresid)))\n        smoother &lt;- as.data.frame(smoother)\n        p3 &lt;- ggplot2::ggplot(data = plot.data, mapping = mapping)\n        if (!is.logical(shape) || shape) {\n            p3 &lt;- p3 + geom_factory(geom_point, plot.data, colour = colour, \n                size = size, linetype = linetype, alpha = alpha, \n                fill = fill, shape = shape)\n        }\n        p3 &lt;- p3 + ggplot2::geom_line(data = smoother, mapping = smoother_m, \n            colour = smooth.colour, linetype = smooth.linetype)\n        p3 &lt;- .decorate.label(p3, wr.data)\n        label.y3 &lt;- ifelse(is_glm, expression(sqrt(abs(`Std. deviance resid.`))), \n            expression(sqrt(abs(`Standardized residuals`))))\n        p3 &lt;- .decorate.plot(p3, xlab = label.fitted, ylab = label.y3, \n            title = t3)\n    }\n    if (show[4L]) {\n        t4 &lt;- \"Cook's distance\"\n        mapping &lt;- ggplot2::aes_string(x = \".index\", y = \".cooksd\", \n            ymin = 0, ymax = \".cooksd\")\n        p4 &lt;- ggplot2::ggplot(data = plot.data, mapping = mapping)\n        if (!is.logical(shape) || shape) {\n            p4 &lt;- p4 + geom_factory(geom_linerange, plot.data, \n                colour = colour, size = size, linetype = linetype, \n                alpha = alpha, fill = fill, shape = shape)\n        }\n        p4 &lt;- .decorate.label(p4, cd.data)\n        p4 &lt;- .decorate.plot(p4, xlab = \"Obs. Number\", ylab = \"Cook's distance\", \n            title = t4)\n    }\n    if (show[5L]) {\n        if (is_const_lev & ncol(fs) &gt; 0) {\n            t5 &lt;- \"Constant Leverage:\\nResiduals vs Factor Levels\"\n            mapping &lt;- ggplot2::aes_string(x = \".nf\", y = \".stdresid\")\n            p5 &lt;- ggplot2::ggplot(data = plot.data, mapping = mapping)\n            if (!is.logical(shape) || shape) {\n                p5 &lt;- p5 + geom_factory(geom_point, plot.data, \n                  colour = colour, size = size, linetype = linetype, \n                  alpha = alpha, fill = fill, shape = shape)\n            }\n            p5 &lt;- p5 + ggplot2::geom_hline(yintercept = 0L, linetype = ad.linetype, \n                size = ad.size, colour = ad.colour) + ggplot2::expand_limits(x = 0)\n            p5 &lt;- .decorate.label(p5, cd.data)\n            label.y5 &lt;- ifelse(is_glm, \"Std. Pearson resid.\", \n                \"Standardized Residuals\")\n            p5 &lt;- .decorate.plot(p5, xlab = \"Factor Level Combination\", \n                ylab = label.y5, title = t5)\n        }\n        else {\n            t5 &lt;- \"Residuals vs Leverage\"\n            mapping &lt;- ggplot2::aes_string(x = \".hat\", y = \".stdresid\")\n            smoother &lt;- .smooth(plot.data$.hat, plot.data$.stdresid)\n            smoother &lt;- as.data.frame(smoother)\n            p5 &lt;- ggplot2::ggplot(data = plot.data, mapping = mapping)\n            if (!is.logical(shape) || shape) {\n                p5 &lt;- p5 + geom_factory(geom_point, plot.data, \n                  colour = colour, size = size, linetype = linetype, \n                  alpha = alpha, fill = fill, shape = shape)\n            }\n            p5 &lt;- p5 + ggplot2::geom_line(data = smoother, mapping = smoother_m, \n                colour = smooth.colour, linetype = smooth.linetype) + \n                ggplot2::geom_hline(yintercept = 0L, linetype = ad.linetype, \n                  size = ad.size, colour = ad.colour) + ggplot2::expand_limits(x = 0)\n            p5 &lt;- .decorate.label(p5, cd.data)\n            label.y5 &lt;- ifelse(is_glm, \"Std. Pearson resid.\", \n                \"Standardized Residuals\")\n            p5 &lt;- .decorate.plot(p5, xlab = \"Leverage\", ylab = label.y5, \n                title = t5)\n        }\n    }\n    if (show[6L]) {\n        t6 &lt;- \"Cook's dist vs Leverage\"\n        mapping &lt;- ggplot2::aes_string(x = \".hat\", y = \".cooksd\")\n        smoother &lt;- .smooth(plot.data$.hat, plot.data$.cooksd)\n        smoother &lt;- as.data.frame(smoother)\n        p6 &lt;- ggplot2::ggplot(data = plot.data, mapping = mapping)\n        if (!is.logical(shape) || shape) {\n            p6 &lt;- p6 + geom_factory(geom_point, plot.data, colour = colour, \n                size = size, linetype = linetype, alpha = alpha, \n                fill = fill, shape = shape)\n        }\n        p6 &lt;- p6 + ggplot2::geom_line(data = smoother, mapping = smoother_m, \n            colour = smooth.colour, linetype = smooth.linetype) + \n            ggplot2::expand_limits(x = 0, y = 0)\n        p6 &lt;- .decorate.label(p6, cd.data)\n        p6 &lt;- .decorate.plot(p6, xlab = \"Leverage\", ylab = \"Cook's distance\", \n            title = t6)\n        g &lt;- dropInf(hii/(1 - hii), hii)\n        p &lt;- length(stats::coef(object))\n        bval &lt;- pretty(sqrt(p * plot.data$.cooksd/g), 5)\n        for (i in seq_along(bval)) {\n            bi2 &lt;- bval[i]^2\n            p6 &lt;- p6 + ggplot2::geom_abline(intercept = 0, slope = bi2, \n                linetype = ad.linetype, size = ad.size, colour = ad.colour)\n        }\n    }\n    if (is.null(ncol)) {\n        ncol &lt;- 0\n    }\n    if (is.null(nrow)) {\n        nrow &lt;- 0\n    }\n    plot.list &lt;- list(p1, p2, p3, p4, p5, p6)[which]\n    new(\"ggmultiplot\", plots = plot.list, nrow = nrow, ncol = ncol)\n}\n&lt;bytecode: 0x609dfc00dcb8&gt;\n&lt;environment: namespace:ggfortify&gt;\n\n\n\n\nCode\nsloop::s3_get_method(augment.lm)\n\n\nfunction (x, data = model.frame(x), newdata = NULL, se_fit = FALSE, \n    interval = c(\"none\", \"confidence\", \"prediction\"), conf.level = 0.95, \n    ...) \n{\n    warn_on_subclass(x, \"augment\")\n    check_ellipses(\"level\", \"augment\", \"lm\", ...)\n    interval &lt;- match.arg(interval)\n    df &lt;- augment_newdata(x, data, newdata, se_fit, interval, \n        level = conf.level)\n    if (is.null(newdata)) {\n        tryCatch({\n            infl &lt;- influence(x, do.coef = FALSE)\n            df &lt;- add_hat_sigma_cols(df, x, infl)\n        }, error = data_error)\n    }\n    df\n}\n&lt;bytecode: 0x609dfba87d00&gt;\n&lt;environment: namespace:broom&gt;\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nsloop::s3_methods_class(\"prcomp\") |&gt;\n  gt::gt() |&gt;\n  gt::tab_header(\"Generics of class prcomp\")\n\n\n\n\n\n\n\n\nGenerics of class prcomp\n\n\ngeneric\nclass\nvisible\nsource\n\n\n\n\naugment\nprcomp\nFALSE\nregistered S3method\n\n\nautoplot\nprcomp\nFALSE\nregistered S3method\n\n\nbiplot\nprcomp\nFALSE\nregistered S3method\n\n\nfortify\nprcomp\nFALSE\nregistered S3method\n\n\nplot\nprcomp\nFALSE\nregistered S3method\n\n\npredict\nprcomp\nFALSE\nregistered S3method\n\n\nprint\nprcomp\nFALSE\nregistered S3method\n\n\nsummary\nprcomp\nFALSE\nregistered S3method\n\n\ntidy\nprcomp\nFALSE\nregistered S3method\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nsloop::s3_dispatch(autoplot(lm0))\n\n\n=&gt; autoplot.lm\n * autoplot.default\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\ndata(UCBAdmissions)\nclass(UCBAdmissions)\n\n\n[1] \"table\"\n\n\nCode\notype(UCBAdmissions)\n\n\n[1] \"S3\"\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nsloop::s3_class(UCBAdmissions)\n\n\n[1] \"table\"\n\n\nCode\n# sloop::s3_get_method(autoplot.table)\n# sloop::s3_get_method(augment.table)\n\n\n\n\n\nOO in Advanced R Programming 1st Edition\nS3 classes Advanced R 2nd Edition"
  },
  {
    "objectID": "core/labs-solutions/lab-progr.html#programming-with-dplyr-and-ggplot2",
    "href": "core/labs-solutions/lab-progr.html#programming-with-dplyr-and-ggplot2",
    "title": "R programming: generics",
    "section": "Programming with dplyr and ggplot2",
    "text": "Programming with dplyr and ggplot2\nWe first aim at programming a function that takes as input a dataframe df, a column name col, and that, depending on the type of the column denoted by col, plots a histogram (for numerical column), a barplot (for factors), or raise an error of the column is neither categorical, nor numerical.\nThe function should return a ggplot object.\nHere is a first attempt.\nLet us first build a toy tibble.\n\n\nCode\ntb &lt;- tibble( \n  col_num = rnorm(100), \n  col_fac = as_factor(sample(letters, 100, replace = T)), \n  col_ts = Sys.time() + duration(sample(1:20, 100, replace=T),units=\"days\")\n) \n\ntb |&gt; \n  head()\n\n\n# A tibble: 6 × 3\n  col_num col_fac col_ts             \n    &lt;dbl&gt; &lt;fct&gt;   &lt;dttm&gt;             \n1  -0.425 z       2025-04-04 00:11:08\n2   0.129 l       2025-03-23 23:11:08\n3  -0.593 i       2025-03-23 23:11:08\n4   0.554 s       2025-04-02 00:11:08\n5  -1.21  e       2025-03-29 23:11:08\n6   0.273 k       2025-03-25 23:11:08\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\ngg_obj &lt;-  function(df, col){\n  \n1  vct &lt;- df[[col]]\n  tp &lt;- class(vct)\n\n  if (tp != \"numeric\" & tp !=\"factor\") {\n    stop(paste0(col, \" is of wrong type!\"))\n  }\n\n  p &lt;- ggplot(df) + \n2    aes(x=.data[[col]])\n\n  if (tp==\"numeric\") {\n    p &lt;- p + geom_histogram()\n  } else {\n    p &lt;- p + geom_bar()\n  }\n\n  p  \n}\n\n\n\n1\n\nList component is accessed by name. col is a string.\n\n2\n\n.data is a pronoun for the dataframe component of the ggplot object\n\n\n\n\n\n\nCode\n(\n  gg_obj(tb, \"col_num\") +\n    labs(\n      title= \"Histogram\", \n      subtitle= \"Numerical column\")\n) + (\n  gg_obj(tb, \"col_fac\") +\n    labs(\n      title= \"Barplot\",\n      subtitle= \"Factor column\"\n    )\n)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\nPass more optional arguments to geom_... (use ellipsis ...)\nAvoid quoting the column name\n\n\n\n\n\nCode\ngg_obj_2 &lt;-  function(df, col, ...){\n  params &lt;- list(...)\n  vct &lt;- pull(df, {{col}})\n  tp &lt;- class(vct)[1]\n\n  if (tp != \"numeric\" & tp !=\"factor\") {\n    stop(\"column is of wrong type!\")\n    return\n  }\n\n  p &lt;- ggplot(df) + \n1    aes(x={{col}})\n\n  if (tp==\"numeric\") {\n2    p &lt;- p + geom_histogram(...)\n  } else {\n    p &lt;- p + geom_bar(...)\n  }\n\n  p  \n}\n\n\n\n1\n\nstandard technique for programming with tidy-evaluation: embracing.\n\n2\n\nusing the ellipsis\n\n\n\n\n\n\nCode\n(\n  gg_obj(tb, \"col_num\") +\n    labs(\n      title= \"Histogram\", \n      subtitle= \"Numerical column\")\n) +\n(\n  gg_obj_2(tb, col_num, fill=\"red\") +\n    labs(\n      title= \"Histogram, tuning color\", \n      subtitle= \"Numerical column\")\n)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nHow could you add a geom_point() layer to each element of the following list?\n\n\nCode\nplots &lt;- list(\n  ggplot(mpg, aes(displ, hwy)),\n  ggplot(diamonds, aes(carat, price)),\n  ggplot(faithfuld, aes(waiting, eruptions, size = density))\n)\n\n\nFrom R Advanced Programming\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCode\nplots |&gt; \n  map(\\(x) x + geom_point()) |&gt;\n  patchwork::wrap_plots()"
  },
  {
    "objectID": "core/labs-solutions/lab-progr.html#inside-lm",
    "href": "core/labs-solutions/lab-progr.html#inside-lm",
    "title": "R programming: generics",
    "section": "Inside lm()",
    "text": "Inside lm()\n\n\n\n\n\n\nNoteQuestion\n\n\n\nIn classes like lm, prcomp, … we have a member called call. What does it represent? How is it constructed?\n First, read the code of lm.\n\n\n&gt; lm \nfunction (formula, data, subset, weights, na.action, method = \"qr\", \n    model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, \n    contrasts = NULL, offset, ...) \n{\n    ret.x &lt;- x\n    ret.y &lt;- y\n    cl &lt;- match.call()\n    mf &lt;- match.call(expand.dots = FALSE)\n    m &lt;- match(c(\"formula\", \"data\", \"subset\", \"weights\", \"na.action\", \n        \"offset\"), names(mf), 0L)\n    mf &lt;- mf[c(1L, m)]\n    mf$drop.unused.levels &lt;- TRUE\n    mf[[1L]] &lt;- quote(stats::model.frame)\n    mf &lt;- eval(mf, parent.frame())\n    if (method == \"model.frame\") \n        return(mf)\n    else if (method != \"qr\") \n        warning(gettextf(\"method = '%s' is not supported. Using 'qr'\", \n            method), domain = NA)\n    mt &lt;- attr(mf, \"terms\")\n    y &lt;- model.response(mf, \"numeric\")\n    w &lt;- as.vector(model.weights(mf))\n    if (!is.null(w) && !is.numeric(w)) \n        stop(\"'weights' must be a numeric vector\")\n    offset &lt;- model.offset(mf)\n    mlm &lt;- is.matrix(y)\n    ny &lt;- if (mlm) \n        nrow(y)\n    else length(y)\n    if (!is.null(offset)) {\n        if (!mlm) \n            offset &lt;- as.vector(offset)\n        if (NROW(offset) != ny) \n            stop(gettextf(\"number of offsets is %d, should equal %d (number of observations)\", \n                NROW(offset), ny), domain = NA)\n    }\n    if (is.empty.model(mt)) {\n        x &lt;- NULL\n        z &lt;- list(coefficients = if (mlm) matrix(NA_real_, 0, \n            ncol(y)) else numeric(), \n                  residuals = y, \n                  fitted.values = 0 * y, \n                  weights = w, \n                  rank = 0L, \n                  df.residual = if (!is.null(w)) sum(w != 0) else ny\n              )\n        if (!is.null(offset)) {\n            z$fitted.values &lt;- offset\n            z$residuals &lt;- y - offset\n        }\n    }\n    else {\n        x &lt;- model.matrix(mt, mf, contrasts)\n        z &lt;- if (is.null(w)) \n            lm.fit(x, y, offset = offset, singular.ok = singular.ok, \n                ...)\n        else lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, \n            ...)\n    }\n    class(z) &lt;- c(if (mlm) \"mlm\", \"lm\")\n    z$na.action &lt;- attr(mf, \"na.action\")\n    z$offset &lt;- offset\n    z$contrasts &lt;- attr(x, \"contrasts\")\n    z$xlevels &lt;- .getXlevels(mt, mf)\n    z$call &lt;- cl\n    z$terms &lt;- mt\n    if (model) \n        z$model &lt;- mf\n    if (ret.x) \n        z$x &lt;- x\n    if (ret.y) \n        z$y &lt;- y\n    if (!qr) \n        z$qr &lt;- NULL\n    z\n}\n&lt;bytecode: 0x55564224e930&gt;\n&lt;environment: namespace:stats&gt;\n\n\n\n\n\n\nNoteQuestion\n\n\n\nHave a look at function match.call()\n\n\n\n\nCode\ne &lt;- match.call(get, call(\"get\", \"abc\", i = FALSE, p = 3))\n\nis_expression(e)\n\n\n[1] TRUE\n\n\nCode\nis_call(e)\n\n\n[1] TRUE\n\n\nCode\nlobstr::ast(match.call(get, call(\"get\", \"abc\", i = FALSE, p = 3)))\n\n\n█─match.call \n├─get \n└─█─call \n  ├─\"get\" \n  ├─\"abc\" \n  ├─i = FALSE \n  └─p = 3 \n\n\nLet us explore the next toy example.\n\n\nCode\nfun &lt;- function(x, lower = 0, upper = 1) {\n  structure((x - lower) / (upper - lower), CALL = match.call())\n}\n\n\nw &lt;- fun(4 * atan(1), u = pi)\n\ntype_of(w)\n\n\nWarning: `type_of()` is deprecated as of rlang 0.4.0.\nPlease use `typeof()` or your own version instead.\nThis warning is displayed once every 8 hours.\n\n\n[1] \"double\"\n\n\nCode\nw\n\n\n[1] 1\nattr(,\"CALL\")\nfun(x = 4 * atan(1), upper = pi)\n\n\nCode\nstr(w)\n\n\n num 1\n - attr(*, \"CALL\")= language fun(x = 4 * atan(1), upper = pi)\n\n\nCode\nx &lt;- attr(w, 'CALL')\n\ntypeof(x)\n\n\n[1] \"language\"\n\n\nCode\nis_call(x)\n\n\n[1] TRUE\n\n\nCode\neval(x)  \n\n\n[1] 1\nattr(,\"CALL\")\nfun(x = 4 * atan(1), upper = pi)\n\n\nCode\n# try also eval_tidy(), eval_bare"
  },
  {
    "objectID": "core/labs-solutions/lab-progr.html#data-masking-and-environments",
    "href": "core/labs-solutions/lab-progr.html#data-masking-and-environments",
    "title": "R programming: generics",
    "section": "Data masking and environments",
    "text": "Data masking and environments\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution"
  },
  {
    "objectID": "core/labs-solutions/lab-progr.html#tidy-evaluation",
    "href": "core/labs-solutions/lab-progr.html#tidy-evaluation",
    "title": "R programming: generics",
    "section": "Tidy evaluation",
    "text": "Tidy evaluation\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is quasi-quotation?\nKeep the rlang cheatsheet around.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nExplain the difference between an expression and a quosure\n\n\n\n\n\n\n\n\nNoteQuestion"
  },
  {
    "objectID": "core/labs-solutions/lab-progr.html#references",
    "href": "core/labs-solutions/lab-progr.html#references",
    "title": "R programming: generics",
    "section": "References",
    "text": "References\nProgramming with ggplot"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html",
    "href": "core/labs-solutions/lab-univariate-numeric.html",
    "title": "LAB: Univariate analysis",
    "section": "",
    "text": "M1 MIDS/MFA/LOGOS\n\n\nUniversité Paris Cité\n\n\nAnnée 2025\n\n\nCourse Homepage\n\n\nMoodle"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html#numerical-summary",
    "href": "core/labs-solutions/lab-univariate-numeric.html#numerical-summary",
    "title": "LAB: Univariate analysis",
    "section": "Numerical summary",
    "text": "Numerical summary\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodedf |&gt; \n    pull(AGE) |&gt; \n    summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  16.00   29.00   42.00   41.85   53.50   80.00 \n\nCodesd(df$AGE) ; IQR(df$AGE) ; mad(df$AGE)\n\n[1] 14.11648\n\n\n[1] 24.5\n\n\n[1] 17.7912\n\n\n\n\n\nUse skimr::skim()\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodedf |&gt; \n    pull(AGE) |&gt; \n    skimr::skim()\n\n\nData summary\n\n\nName\npull(df, AGE)\n\n\nNumber of rows\n599\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\ndata\n0\n1\n41.85\n14.12\n16\n29\n42\n53.5\n80\n▆▇▇▆▁\n\n\n\n\n\nCodeskm &lt;- df |&gt; \n  skimr::skim(AGE)\n\nclass(skm)\n\n[1] \"skim_df\"    \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nCodeattributes(skm)\n\n$class\n[1] \"skim_df\"    \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n[1] 1\n\n$names\n [1] \"skim_type\"     \"skim_variable\" \"n_missing\"     \"complete_rate\"\n [5] \"numeric.mean\"  \"numeric.sd\"    \"numeric.p0\"    \"numeric.p25\"  \n [9] \"numeric.p50\"   \"numeric.p75\"   \"numeric.p100\"  \"numeric.hist\" \n\n$data_rows\n[1] 599\n\n$data_cols\n[1] 11\n\n$df_name\n[1] \"`df`\"\n\n$dt_key\n[1] NA\n\n$groups\ncharacter(0)\n\n$base_skimmers\n[1] \"n_missing\"     \"complete_rate\"\n\n$skimmers_used\n$skimmers_used$numeric\n[1] \"mean\" \"sd\"   \"p0\"   \"p25\"  \"p50\"  \"p75\"  \"p100\" \"hist\"\n\nCodetibble::as_tibble(skm)\n\n# A tibble: 1 × 12\n  skim_type skim_variable n_missing complete_rate numeric.mean numeric.sd\n  &lt;chr&gt;     &lt;chr&gt;             &lt;int&gt;         &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 numeric   AGE                   0             1         41.8       14.1\n# ℹ 6 more variables: numeric.p0 &lt;dbl&gt;, numeric.p25 &lt;dbl&gt;, numeric.p50 &lt;dbl&gt;,\n#   numeric.p75 &lt;dbl&gt;, numeric.p100 &lt;dbl&gt;, numeric.hist &lt;chr&gt;\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCompare mean and median, sd and IQR.\nAre mean and median systematically related?\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nAsk the bot.\nThere is at least one relation between median and mean for square-integrable distributoins: \\[|\\text{Median} - \\text{Mean}| \\leq \\text{sd}\\] Lévy’s inequality.\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nAre standard deviation and IQR systematically related ?\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nAsk the bot.\nYes."
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html#boxplots",
    "href": "core/labs-solutions/lab-univariate-numeric.html#boxplots",
    "title": "LAB: Univariate analysis",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n\n\n\n\nNoteQuestion\n\n\n\nDraw a boxplot of the Age distribution\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodep &lt;- df |&gt; \n    ggplot() +\n    aes(x=1L, y=AGE) +\n    geom_boxplot() +\n    labs(\n        title=\"Age distribution\",\n        subtitle = \"Census data\"\n    )\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nHow would you get rid of the useless ticks on the x-axis?\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nAsk the bot (In a boxplot built using ggplot2, how can I get rid from the ticks on the x axis?).\nYes. This is a matter of theme.\n\nCodep +\n  theme(\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank()\n  )"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html#histograms",
    "href": "core/labs-solutions/lab-univariate-numeric.html#histograms",
    "title": "LAB: Univariate analysis",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPlot a histogram of the empirical distribution of the AGE column\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodep &lt;- df |&gt; \n  ggplot() +\n  aes(x=AGE) +\n  labs(\n    title = \"Age distribution\",\n    subtitle = \"Census data\",\n    x = \"Age (Years)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\np +\n  geom_histogram(aes(y=after_stat(density)),\n                 bins=15,\n                 fill=\"white\",\n                 color=\"black\") +\n  labs(\n    caption = \"Histogram\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nTry different values for the bins parameter of geom_histogram()"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html#density-estimates",
    "href": "core/labs-solutions/lab-univariate-numeric.html#density-estimates",
    "title": "LAB: Univariate analysis",
    "section": "Density estimates",
    "text": "Density estimates\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPlot a density estimate of the AGE column (use stat_density.\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodep +\n  stat_density(\n              fill=\"white\",\n              color=\"black\") +\n  labs(\n    caption = \"Kernel Density Estimate\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPlay with parameters bw, kernel and adjust.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nOverlay the two plots (histogram and density).\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodep +\n  stat_density(\n              fill=\"white\",\n              color=\"black\") +\n  geom_histogram(aes(y=after_stat(density)),\n                 bins=15,\n                 fill=\"white\",\n                 color=\"black\",\n                 alpha=.5) +\n  labs(\n    caption = \"Overlayed Density Estimates\"\n  )"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html#ecdf",
    "href": "core/labs-solutions/lab-univariate-numeric.html#ecdf",
    "title": "LAB: Univariate analysis",
    "section": "ECDF",
    "text": "ECDF\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPlot the Empirical CDF of the AGE distribution\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\n\nCodep +\n    stat_ecdf()  +\n    labs(\n        caption = \"Empirical CDF\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCan you read the quartiles from the ECDF pplot?\n\n\n\n\n\n\n\n\nTipsolution\n\n\n\n\n\nOf course. Yes, we can."
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html#quantile-function",
    "href": "core/labs-solutions/lab-univariate-numeric.html#quantile-function",
    "title": "LAB: Univariate analysis",
    "section": "Quantile function",
    "text": "Quantile function\n\n\n\n\n\n\nNoteQuestion\n\n\n\nPlot the quantile function of the AGE distribution."
  },
  {
    "objectID": "labs-listings.html",
    "href": "labs-listings.html",
    "title": "Labs",
    "section": "",
    "text": "Note\n\n\n\nSessions are organized around labs. Feel free to look at the lab before sessions. Do not rush to solutions proposed here\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Tags\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nTags\n\n\n\n\n\n\n\n\nJan 12, 2026\n\n\nIntroduction and Visualization\n\n\nVisualization, Public Statistics\n\n\n\n\n\n\nJan 12, 2026\n\n\nBrush up your R\n\n\nR language, Tidyverse, IDE\n\n\n\n\n\n\nJan 19, 2026\n\n\nTable wranglig\n\n\nR language, dplyr, tabula data\n\n\n\n\n\n\nJan 26, 2026\n\n\nUnivariate categorical data\n\n\nUnivariate data, GSS\n\n\n\n\n\n\nJan 26, 2026\n\n\nUnivariate numeric data\n\n\nUnivariate data, GSS\n\n\n\n\n\n\nFeb 2, 2026\n\n\nBivariate data\n\n\nbivariate data, mosaicplots, scatterplots, simple linear regression\n\n\n\n\n\n\nFeb 16, 2026\n\n\nLinear Regression I\n\n\nLinear regression, OLS, lm\n\n\n\n\n\n\nMar 2, 2026\n\n\nLinear regression II\n\n\nLinear regression, OLS, lm, diagnostics, variable selection\n\n\n\n\n\n\nMar 9, 2026\n\n\nSVD and PCA\n\n\nSVD, PCA\n\n\n\n\n\n\nMar 16, 2026\n\n\nCorrespondence Analysis\n\n\nSVD, CA, Correspondence Analysis\n\n\n\n\n\n\nMar 23, 2026\n\n\nSVD, MCA, CCA\n\n\nSVD, PCA, CA, MCA, CCA\n\n\n\n\n\n\nMar 30, 2026\n\n\nClustering: kmeans\n\n\nClustering, k-means\n\n\n\n\n\n\nApr 6, 2026\n\n\nClustering: Hierarchical\n\n\nClustering, Hierarchical\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nTipTip\n\n\n\nBefore working out a lab, make sure the relevant packages are installed in your environment.",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "labs-solutions-listings.html",
    "href": "labs-solutions-listings.html",
    "title": "Labs Solutions",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Tags\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nTags\n\n\n\n\n\n\n\n\nJan 12, 2026\n\n\nIntroduction and Visualization\n\n\nVisualization, Public Statistics\n\n\n\n\n\n\nJan 12, 2026\n\n\nBrush up your R\n\n\nR language, Tidyverse, IDE\n\n\n\n\n\n\nJan 19, 2026\n\n\nTable wranglig\n\n\nR language, dplyr, tabula data\n\n\n\n\n\n\nJan 26, 2026\n\n\nUnivariate categorical data\n\n\nUnivariate data, GSS\n\n\n\n\n\n\nJan 26, 2026\n\n\nUnivariate numeric data\n\n\nUnivariate data, GSS\n\n\n\n\n\n\nFeb 2, 2026\n\n\nBivariate data\n\n\nbivariate data, mosaicplots, scatterplots, simple linear regression\n\n\n\n\n\n\nFeb 16, 2026\n\n\nLinear regression I\n\n\nLinear regression, OLS, lm\n\n\n\n\n\n\nMar 2, 2026\n\n\nLinear regression II\n\n\nLinear regression, OLS, lm, diagnostics, variable selection\n\n\n\n\n\n\nMar 9, 2026\n\n\nSVD and PCA\n\n\nSVD, PCA\n\n\n\n\n\n\nMar 16, 2026\n\n\nCorrespondence Analysis\n\n\nSVD, CA, Correspondence Analysis\n\n\n\n\n\n\nMar 23, 2026\n\n\nSVD, MCA, CCA\n\n\nSVD, PCA, CA, MCA, CCA\n\n\n\n\n\n\nMar 30, 2026\n\n\nClustering: kmeans\n\n\nClustering, k-means\n\n\n\n\n\n\nApr 6, 2026\n\n\nClustering: Hierarchical\n\n\nClustering, Hierarchical\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Solutions"
    ]
  },
  {
    "objectID": "projects-listings.html",
    "href": "projects-listings.html",
    "title": "Projects",
    "section": "",
    "text": "Note\n\n\n\nCourse evaluation is based on Projects \n\n Find a friend : all work done by pairs of students\n Create a single private GitHub repository for each project and each pair of students. You may use your github account to do that.\n Name the repository MA7BY020-2026-xx where xx is the project number\n Grant me access to these repositories\n In the README.md file at the root of your repository, list the name (last name, first number, and student id number) of each team member\n All work is transmitted through your private repository and nowhere else\n No emails for project submission\n All projects deliverables (report and presentation support) consist of Quarto notebooks. I should be able to fork your repository and use quarto render to generate the deliverable.\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Description\n      \n      \n        Tags\n      \n      \n        Due date - Oldest\n      \n      \n        Due date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDue date\n\n\n\nTitle\n\n\n\nDescription\n\n\n\nTags\n\n\n\n\n\n\n\n\nMar 1, 2026\n\n\nData wrangling and Visualization\n\n\nData extraction, wrangling, visualization, reproducible data science\n\n\nOECD, Visualization, Quarto, tidyverse, plotly\n\n\n\n\n\n\nMay 1, 2026\n\n\nRegression SVD methods and Voting data\n\n\nRegression, SVD, Clustering,\n\n\nRegression”, PCA, Clustering, Quarto\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nWarningEvaluation guidelines\n\n\n\nGrading criteria are given per project. They involve the following aspects:\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nNarrative, spelling and syntax\n\nEnglish/French \n\n\nFAIRness\n\nFAIR principles at  Wikipedia\n\n\nPlots correction\n\nchoice of aesthetics, geom, scale … \n\n\nPlots style\n\nTitles, legends, labels, breaks … \n\n\nTable wrangling\n\nETL, SQL like manipulations \n\n\nComputing Statistics\n\nAggregations, LR, PCA, CA, … \n\n\nDRY compliance\n\nDRY principle at  Wikipedia\n\n\nReport organization\n\n\n\n\nCode organization",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "slides-listings.html",
    "href": "slides-listings.html",
    "title": "Slides",
    "section": "",
    "text": "Slides summarize the lectures. Feel free to watch them before and after the lectures.\n point to material to be developped on blackboard.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Tags\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nTags\n\n\n\n\n\n\n\n\nJan 12, 2026\n\n\nHandling tables with dplyr\n\n\nSQL Tables, Relational Algebra, tidyverse\n\n\n\n\n\n\nMar 30, 2026\n\n\nHierarchical clustering\n\n\nDendrogram, Hierarchical clustering\n\n\n\n\n\n\nMar 30, 2026\n\n\nClustering kmeans\n\n\nkmeans, clustering\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nNoteMode d’emploi\n\n\n\nSlides use libraries revealjs or remark from . They are displayed in your browser.\nTo get help, press",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "weeks-listings.html",
    "href": "weeks-listings.html",
    "title": "Journal",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Tags\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nTags\n\n\n\n\n\n\n\n\nJan 12, 2026\n\n\nWeek 1\n\n\nVisualization, Public Statistics, R language, Tidyverse\n\n\n\n\n\n\nJan 19, 2026\n\n\nWeek 2\n\n\nR language, Tidyverse, Table manipulations\n\n\n\n\n\n\nJan 26, 2026\n\n\nWeek 3\n\n\nUnivariate data\n\n\n\n\n\n\nFeb 2, 2026\n\n\nWeek 4\n\n\nBivariate data\n\n\n\n\n\n\nFeb 9, 2026\n\n\nWeek 5\n\n\nLinear regression, OLS, lm, Model diagnostics\n\n\n\n\n\n\nFeb 16, 2026\n\n\nWeek 6\n\n\nLinear regression, OLS, lm, Model diagnostics\n\n\n\n\n\n\nFeb 23, 2026\n\n\nWeek 7\n\n\nPCA, SVD\n\n\n\n\n\n\nMar 9, 2026\n\n\nWeek 8\n\n\nCA, SVD\n\n\n\n\n\n\nMar 16, 2026\n\n\nWeek 9\n\n\nCCA, MCA, SVD\n\n\n\n\n\n\nMar 23, 2026\n\n\nWeek 10\n\n\nClusterings\n\n\n\n\n\n\nMar 30, 2026\n\n\nWeek 11\n\n\nClusterings\n\n\n\n\n\n\nApr 13, 2026\n\n\nWeek 12\n\n\nTBA\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Journal"
    ]
  }
]