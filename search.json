[
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (255) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-7.html#blackboard",
    "href": "weeks/week-7.html#blackboard",
    "title": "Week 7",
    "section": " Blackboard",
    "text": "Blackboard\n\nOrdinary Least Squares\nQR factorizatio\nRidge regression (Regularized Least Squares)\nPseudo-Inversion\nLinear regrssion with R"
  },
  {
    "objectID": "weeks/week-7.html#labs",
    "href": "weeks/week-7.html#labs",
    "title": "Week 7",
    "section": " Labs",
    "text": "Labs\n\nLab 6 - Linear Regression I  –&gt;\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables"
  },
  {
    "objectID": "weeks/week-7.html#further-work",
    "href": "weeks/week-7.html#further-work",
    "title": "Week 7",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-7.html#further-reading",
    "href": "weeks/week-7.html#further-reading",
    "title": "Week 7",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes\n\nCours de Statistique Fondamentale\n\nModèles linéaires\nSherman–Morrison–Woodbury formula\n\nSourav Chatterjee: A new correlation coefficient"
  },
  {
    "objectID": "weeks/week-7.html#logistics",
    "href": "weeks/week-7.html#logistics",
    "title": "Week 7",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (RH04A) Wednesday 13h30-15h\nSession Ibis Sophie Germain (014) Thursday 10h45-12h45\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-3.html#blackboard",
    "href": "weeks/week-3.html#blackboard",
    "title": "Week 3",
    "section": " Blackboard",
    "text": "Blackboard\nWe reviewed the toolkit of univariate analysis\n\nLexicon\nCategorical samples\n\nCounts, Contingency tables, barplots, colplots\n\nNumeric samples\n\nNumerical summaries for location and scale\nCumulative distribution functions and Quantiles functions\nQuantile-quantile plots\nHistograms"
  },
  {
    "objectID": "weeks/week-3.html#labs",
    "href": "weeks/week-3.html#labs",
    "title": "Week 3",
    "section": " Labs",
    "text": "Labs\nWe went (briefly) through the two labs\n\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\n\nWe relied on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr"
  },
  {
    "objectID": "weeks/week-3.html#further-work",
    "href": "weeks/week-3.html#further-work",
    "title": "Week 3",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-3.html#further-reading",
    "href": "weeks/week-3.html#further-reading",
    "title": "Week 3",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\nCours de Statistique Fondamentale\n\nChap Estimation de densité\nChap Test de Kolmogorov-Smirnov"
  },
  {
    "objectID": "weeks/week-3.html#logistics",
    "href": "weeks/week-3.html#logistics",
    "title": "Week 3",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (163) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-1.html#labs",
    "href": "weeks/week-1.html#labs",
    "title": "Week 1",
    "section": " Labs",
    "text": "Labs\nWe worked on the next two labs (solutions )\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\n\nWe used Rstudio, created and initialized a dedicated R project (without git and renv). We installed packages tidyverse, pak and gapminder.\nWe reconstructed the gapminder demo using ggplot2 and plotly.\nWe went through several aspects of R following Intro to R. This is an incentive to install packages lobstr and rlang. They can be very hepful when visualizing the data structures that underpin vectors, matrices, lists, and data frames."
  },
  {
    "objectID": "weeks/week-1.html#further-work",
    "href": "weeks/week-1.html#further-work",
    "title": "Week 1",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-1.html#further-reading",
    "href": "weeks/week-1.html#further-reading",
    "title": "Week 1",
    "section": " Further reading",
    "text": "Further reading\n\nThe ggplot book\nR for data science\n\nData visualization\nTransform\n\nLogical vectors\nNumbers\nFactors\nMissing values\nFunctions\nIteration\nA field guide to base R\n\n\nAdvanced R\n\nNames and values\nVectors\nSubsetting"
  },
  {
    "objectID": "weeks/week-1.html#logistics",
    "href": "weeks/week-1.html#logistics",
    "title": "Week 1",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "slides-listings.html",
    "href": "slides-listings.html",
    "title": "Slides",
    "section": "",
    "text": "Slides summarize the lectures. Feel free to watch them before and after the lectures.\n point to material to be developped on blackboard.\n\n\n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nMode d’emploi\n\n\n\nSlides use libraries revealjs or remark from . They are displayed in your browser.\nTo get help, press",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "quarto-format.html#a-translator",
    "href": "quarto-format.html#a-translator",
    "title": "Quarto",
    "section": "A translator",
    "text": "A translator",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "quarto-format.html#quarto-and-rstudio",
    "href": "quarto-format.html#quarto-and-rstudio",
    "title": "Quarto",
    "section": "Quarto and rstudio",
    "text": "Quarto and rstudio",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "quarto-format.html#quarto-and-vs-code",
    "href": "quarto-format.html#quarto-and-vs-code",
    "title": "Quarto",
    "section": "Quarto and vs code",
    "text": "Quarto and vs code",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "quarto-format.html#command-line-tool",
    "href": "quarto-format.html#command-line-tool",
    "title": "Quarto",
    "section": "Command Line Tool",
    "text": "Command Line Tool",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "posit-cloud.html",
    "href": "posit-cloud.html",
    "title": "Positcloud",
    "section": "",
    "text": "Website",
    "crumbs": [
      "Support",
      "Posit cloud"
    ]
  },
  {
    "objectID": "labs-listings.html",
    "href": "labs-listings.html",
    "title": "Labs",
    "section": "",
    "text": "Note\n\n\n\nSessions are organized around labs. Feel free to look at the lab before sessions. Do not rush to solutions proposed here\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 15, 2025\n\n\nIntroduction and Visualization\n\n\nVisualization, Public Statistics\n\n\n\n\nJan 15, 2025\n\n\nBrush up your R\n\n\nR language, Tidyverse, IDE\n\n\n\n\nJan 22, 2025\n\n\nTable wranglig\n\n\nR language, dplyr, tabula data\n\n\n\n\nJan 29, 2025\n\n\nUnivariate categorical data\n\n\nUnivariate data, GSS\n\n\n\n\nJan 30, 2025\n\n\nUnivariate numeric data\n\n\nUnivariate data, GSS\n\n\n\n\nFeb 5, 2025\n\n\nBivariate data\n\n\nbivariate data, mosaicplots, scatterplots, simple linear regression\n\n\n\n\nFeb 19, 2025\n\n\nLinear Regression I\n\n\nLinear regression, OLS, lm\n\n\n\n\nMar 12, 2025\n\n\nSVD and PCA\n\n\nSVD, PCA\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nTip\n\n\n\nBefore working out a lab, make sure the relevant packages are installed in your environment.",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Course team",
    "section": "",
    "text": "Teacher 2024-25\n\n\n\n\n Former contributors\n\nAurélie Fischer\nSothéa Has\nClément Levrard\nMaud Thomas",
    "crumbs": [
      "Information",
      "Team"
    ]
  },
  {
    "objectID": "core/projects/hmw_gapminder_oecd.html",
    "href": "core/projects/hmw_gapminder_oecd.html",
    "title": "Hmw I : Tables and visualization",
    "section": "",
    "text": "Important\n\n\n\n\nDue : February 7, 2025\nWork in pairs\nDeliver your work as a qmd file through a github  repository\nUse the quarto package for reproducible research\nThe report should be rendered at least in HTML format, and possibly also in PDF format"
  },
  {
    "objectID": "core/projects/hmw_gapminder_oecd.html#report-organization",
    "href": "core/projects/hmw_gapminder_oecd.html#report-organization",
    "title": "Hmw I : Tables and visualization",
    "section": " Report organization",
    "text": "Report organization\nThe first part (introduction) of the report shall be dedicated to the description of the data you have downloaded. You shall motivate your choice and non-trivial aspects of the data (for example if you were discussing GDP per capita against Life expectancy, you should remind the reader about the definition of Life expectancy and GDP). You shall also give a hint about why you intend to plot some variables against others.\nThe second part (results) shall be dedicated to plots and animations. Commenting a plot is not paraphrasing. It consists in adding informations and explanations that are not already in and around the plot (this includes the plot itself, title, subtitle, caption, and guides). It also consists in questions and issues that the plot raises. For example, in the Gapminder presentation, the apparent connection between life expectancy and GDP per capita deserves to be discussed (is it stationary? is it homogeneous throughout continents ? …). Refrain from overplaying your hand: yours plots are not likely to provide causal explanations. Comment the data, all the data, and nothing but the data.\nThe third part is the appendix. The first two parts should be text and plots only. The third part should be code only.\nThe appendix shall be dedicated to the description of the data wrangling pipeline. You shall give the code.\nYou shall also give the code of the graphical pipelines in the appendix.\nYou shall avoid copy-paste coding. Don’t Repeat Yourself. The tidyverse is your friend. knitr provide the tools to organize the Quarto file so that you can write your code once and use it many times, once for data wrangling and plotting (without echoing), then for listing and explanation.\n\n\n\n\n\n\nTip for organizing the report\n\n\n\nLook at fake report organized along this principles. Note that works with the knitr engine.\nHave a look at Rmarkdown the definitive guide to leanr about knitr tricks.\nThis trick is described in Section 4.19"
  },
  {
    "objectID": "core/projects/hmw_gapminder_oecd.html#grading-criteria",
    "href": "core/projects/hmw_gapminder_oecd.html#grading-criteria",
    "title": "Hmw I : Tables and visualization",
    "section": " Grading criteria",
    "text": "Grading criteria\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nNarrative, spelling and syntax\n25%\nEnglish/French \n\n\nPlots correction\n20%\nchoice of aesthetics, geom, scale … \n\n\nPlot style\n15%\nTitles, legends, labels, breaks … \n\n\nTable wrangling\n15%\nETL, SQL like manipulations \n\n\nComputing Statistics\n5%\nAggregations, LR, PCA, CA, … \n\n\nDRY compliance\n20%\nDRY principle at  Wikipedia"
  },
  {
    "objectID": "core/labs/lab-univariate-numeric.html#numerical-summary",
    "href": "core/labs/lab-univariate-numeric.html#numerical-summary",
    "title": "LAB: Univariate analysis",
    "section": "Numerical summary",
    "text": "Numerical summary\nUse skimr::skim()\n\n\n\n\n\n\nQuestion\n\n\n\nCompare mean and median, sd and IQR.\nAre mean and median systematically related?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre standard deviation and IQR systematically related ?"
  },
  {
    "objectID": "core/labs/lab-univariate-numeric.html#boxplots",
    "href": "core/labs/lab-univariate-numeric.html#boxplots",
    "title": "LAB: Univariate analysis",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n\n\n\n\nQuestion\n\n\n\nDraw a boxplot of the Age distribution\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you get rid of the useless ticks on the x-axis?"
  },
  {
    "objectID": "core/labs/lab-univariate-numeric.html#histograms",
    "href": "core/labs/lab-univariate-numeric.html#histograms",
    "title": "LAB: Univariate analysis",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nQuestion\n\n\n\nPlot a histogram of the empirical distribution of the AGE column\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nTry different values for the bins parameter of geom_histogram()"
  },
  {
    "objectID": "core/labs/lab-univariate-numeric.html#density-estimates",
    "href": "core/labs/lab-univariate-numeric.html#density-estimates",
    "title": "LAB: Univariate analysis",
    "section": "Density estimates",
    "text": "Density estimates\n\n\n\n\n\n\nQuestion\n\n\n\nPlot a density estimate of the AGE column (use stat_density.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPlay with parameters bw, kernel and adjust.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nOverlay the two plots (histogram and density)."
  },
  {
    "objectID": "core/labs/lab-univariate-numeric.html#ecdf",
    "href": "core/labs/lab-univariate-numeric.html#ecdf",
    "title": "LAB: Univariate analysis",
    "section": "ECDF",
    "text": "ECDF\n\n\n\n\n\n\nQuestion\n\n\n\nPlot the Empirical CDF of the AGE distribution\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you read the quartiles from the ECDF pplot?"
  },
  {
    "objectID": "core/labs/lab-univariate-numeric.html#quantile-function",
    "href": "core/labs/lab-univariate-numeric.html#quantile-function",
    "title": "LAB: Univariate analysis",
    "section": "Quantile function",
    "text": "Quantile function\n\n\n\n\n\n\nQuestion\n\n\n\nPlot the quantile function of the AGE distribution."
  },
  {
    "objectID": "core/labs/lab-tables.html#setup",
    "href": "core/labs/lab-tables.html#setup",
    "title": "Tables manipulation II",
    "section": "Setup",
    "text": "Setup\nWe will use the following packages. If needed, we install them.\n\nCodeold_theme &lt;- theme_set(theme_minimal())\n\n\nCheck nycflights13 for any explanation concerning the tables and their columns."
  },
  {
    "objectID": "core/labs/lab-tables.html#data-loading",
    "href": "core/labs/lab-tables.html#data-loading",
    "title": "Tables manipulation II",
    "section": "Data loading",
    "text": "Data loading\n\nCodeflights &lt;- nycflights13::flights\nweather &lt;- nycflights13::weather\nairports &lt;- nycflights13::airports\nairlines &lt;- nycflights13::airlines\nplanes &lt;- nycflights13::planes\n\n\n\nCodecon &lt;- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\nflights_lite &lt;- copy_to(con, nycflights13::flights)\nairports_lite &lt;- copy_to(con, nycflights13::airports)\nplanes_lite &lt;-  copy_to(con, nycflights13::planes)\nweather_lite &lt;- copy_to(con, nycflights13::weather)\nairlines_lite &lt;- copy_to(con, nycflights13::airlines)\n\n\n\nCodeflights_lite |&gt;\n  select(contains(\"delay\")) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT `dep_delay`, `arr_delay`\nFROM `nycflights13::flights`\n\n\nView data in spreadsheet style.\n\nCodeView(flights)\n\n\nAsk for help about table flights"
  },
  {
    "objectID": "core/labs/lab-tables.html#first-queries-the-dplyr-way",
    "href": "core/labs/lab-tables.html#first-queries-the-dplyr-way",
    "title": "Tables manipulation II",
    "section": "First Queries (the dplyr way)",
    "text": "First Queries (the dplyr way)\nFind all flights that\n\nHad an arrival delay of two or more hours\n\n\nFlew to Houston (IAH or HOU)\n\n\nWere operated by United, American, or Delta\n\n\n\n\n\n\n\nPackage stringr could be useful.\n\nCodeairlines |&gt; \n  filter(stringr::str_starts(name, \"United\") |\n        stringr::str_starts(name, \"American\") |\n        stringr::str_starts(name, \"Delta\"))\n\n# A tibble: 3 × 2\n  carrier name                  \n  &lt;chr&gt;   &lt;chr&gt;                 \n1 AA      American Airlines Inc.\n2 DL      Delta Air Lines Inc.  \n3 UA      United Air Lines Inc. \n\nCodeairlines |&gt; \n  filter(stringr::str_detect(name, (\"United|American|Delta\"))) |&gt; \n  pull(carrier)\n\n[1] \"AA\" \"DL\" \"UA\"\n\n\n#| eval: false\nairlines_lite |&gt; \n  filter(stringr::str_starts(name, \"United\") |\n        stringr::str_starts(name, \"American\") |\n        stringr::str_starts(name, \"Delta\")) |&gt; \n  show_query()\nSELECT *\nFROM `nycflights13::airlines`\nWHERE \"name\" LIKE 'United%' OR \n      \"name\" LIKE 'American%' OR \n      \"name\" LIKE 'Delta%' ;\nstringr is part of tidyverse\n\n\n\n\nDeparted in summer (July, August, and September)\n\n\n\n\n\n\n\nWhen manipulating temporal information (date, time, duration), keep an eye on what lubridate offers. The API closely parallels what RDMS and Python offer.\n\n\n\n\nArrived more than two hours late, but didn’t leave late\n\n\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\nDeparted between midnight and 6am (inclusive)\n\n\n\n\n\n\n\nRead filter() in R for Data Science 1st Ed\nRead Chapter Transform in R for Data Science 2nd Ed"
  },
  {
    "objectID": "core/labs/lab-tables.html#missing-data",
    "href": "core/labs/lab-tables.html#missing-data",
    "title": "Tables manipulation II",
    "section": "Missing data",
    "text": "Missing data\n\nHow many flights per origin have a missing dep_time?\n\n\nWhat other variables are missing?\n\n\n\n\n\n\n\nThe introduction to tidyselect is a must read.\n\n\n\n\nWhat might these rows with missing data represent?\n\n\nCodenot_cancelled &lt;-  flights |&gt; \n  filter(!is.na(dep_time))\n\n\n\n\nMore questions: for each column in flight report the number of missing values."
  },
  {
    "objectID": "core/labs/lab-tables.html#arrange",
    "href": "core/labs/lab-tables.html#arrange",
    "title": "Tables manipulation II",
    "section": "Arrange",
    "text": "Arrange\n\nHow could you use arrange() to sort all missing values to the start? (Hint: use is.na()).\n\n\nSort flights to find the most delayed flights.\n\n\nPick the ten most delayed flights (with finite dep_delay)\n\n\nFind the flights that left earliest.\n\n\nSort flights to find the fastest (highest speed) flights.\n\n\nWhich flights travelled the farthest?\n\n\n\n\n\n\n\nThe database provides all we need with columns distance and air_time. Otherwise, with the positions of airports from table airports, we should be able to compute distances using :\n\n‘Haversine’ formula.\n\nhttps://en.wikipedia.org/wiki/Haversine_formula\n\n\n\n\nWhich travelled the shortest?"
  },
  {
    "objectID": "core/labs/lab-tables.html#projection",
    "href": "core/labs/lab-tables.html#projection",
    "title": "Tables manipulation II",
    "section": "Projection",
    "text": "Projection\n\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\n\n\nWhat happens if you include the name of a variable multiple times in a select() call?\n\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\nvars &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\nDoes the result of running the following code surprise you?\n\n\nCodeselect(\n  flights, \n  contains(\"TIME\", ignore.case =TRUE))  |&gt; \n  head()\n\n# A tibble: 6 × 6\n  dep_time sched_dep_time arr_time sched_arr_time air_time time_hour          \n     &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dttm&gt;             \n1      517            515      830            819      227 2013-01-01 05:00:00\n2      533            529      850            830      227 2013-01-01 05:00:00\n3      542            540      923            850      160 2013-01-01 05:00:00\n4      544            545     1004           1022      183 2013-01-01 05:00:00\n5      554            600      812            837      116 2013-01-01 06:00:00\n6      554            558      740            728      150 2013-01-01 05:00:00\n\n\n\nHow do the select helpers deal with case by default?\n\n\nHow can you change that default?"
  },
  {
    "objectID": "core/labs/lab-tables.html#mutations",
    "href": "core/labs/lab-tables.html#mutations",
    "title": "Tables manipulation II",
    "section": "Mutations",
    "text": "Mutations\n\nCurrently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.\n\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\n\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\n\n\nFind the 10 most delayed flights using a ranking function. How do you want to handle ties?\n\n\nCarefully read the documentation for min_rank().\nWindowed rank functions."
  },
  {
    "objectID": "core/labs/lab-tables.html#aggregations",
    "href": "core/labs/lab-tables.html#aggregations",
    "title": "Tables manipulation II",
    "section": "Aggregations",
    "text": "Aggregations\n\nBrainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios:\n\nA flight is 15 minutes early 50% of the time, and 15 minutes late 10% of the time.\nA flight is always 10 minutes late.\nA flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time.\n99% of the time a flight is on time. 1% of the time it’s 2 hours late.\n\n\n\n\nCodeflights |&gt; \n  group_by(dest) |&gt; \n  summarise(n_cancelled = sum(is.na(dep_time)))\n\n# A tibble: 105 × 2\n   dest  n_cancelled\n   &lt;chr&gt;       &lt;int&gt;\n 1 ABQ             0\n 2 ACK             0\n 3 ALB            20\n 4 ANC             0\n 5 ATL           317\n 6 AUS            21\n 7 AVL            12\n 8 BDL            31\n 9 BGR            15\n10 BHM            25\n# ℹ 95 more rows\n\n\n\nCodeflights_lite |&gt; \n  group_by(dest) |&gt; \n  summarise(n_cancelled = sum(is.na(dep_time))) |&gt; \n  show_query()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n&lt;SQL&gt;\nSELECT `dest`, SUM((`dep_time` IS NULL)) AS `n_cancelled`\nFROM `nycflights13::flights`\nGROUP BY `dest`\n\n\n\nWhich is more important: arrival delay or departure delay?\n\n\nCome up with another approach that will give you the same output as not_cancelled |&gt; count(dest) and (without usingcount()`).\n\n\nOur definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay) ) is slightly suboptimal. Why? Which is the most important column?\n\n\nLook at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay?\n\n\nWhich carrier has the worst delays?\n\nChallenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarise(n()))\n\nWhat does the sort argument to count() do. When might you use it?"
  },
  {
    "objectID": "core/labs/lab-tables.html#miscellanea",
    "href": "core/labs/lab-tables.html#miscellanea",
    "title": "Tables manipulation II",
    "section": "Miscellanea",
    "text": "Miscellanea\n\nWhich carriers serve all destination airports (in the table) ?\n\n\nRefer back to the lists of useful mutate and filtering functions.\nDescribe how each operation changes when you combine it with grouping.\n\n\nWhich plane (tailnum) has the worst on-time record amongst planes with at least ten flights?\n\n\nWhat time of day should you fly if you want to avoid delays as much as possible?\n\n\nFor each destination, compute the total minutes of delay.\n\n\nFor each flight, compute the proportion of the total positive arrival delays for its destination.\n\nUsing dplyr, it is easy. See A second look at group_by\n\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the delay of a flight is related to the delay of the immediately preceding flight.\n\n\n\n\n\n\n\nlag() is an example of window function. If we were using SQL, we would define a WINDOW using an expression like\nWINDOW w As (PARTITION BY origin ORDER BY year, month, day, sched_dep_time)\nSomething still needs fixing here: some flights never took off (is.na(dep_time)). Should they be sided out? assigned an infinite departure delay?\n\n\n\n\nLook at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\n\nConsider all flights with average speed above \\(950\\text{km/h}\\) as suspicious.\nLet us visualize destinations and origins of the speedy flights.\n\nFind all destinations that are flown by at least two carriers. Use that information to rank the carriers.\n\n\nFor each plane, count the number of flights before the first delay greater than 1 hour.\n\n\n\n\n\n\n\nAssume a plane is characterized by tailnum. Some flights have no tailnum. We ignore them."
  },
  {
    "objectID": "core/labs/lab-tables.html#references",
    "href": "core/labs/lab-tables.html#references",
    "title": "Tables manipulation II",
    "section": "References",
    "text": "References\n\nData transformation cheatsheet\nR4Data Science Tidy\nBenchmarking\ndplyr and vctrs\nPosts on dplyr\nWindow functions on dplyr"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#grammar-of-graphics",
    "href": "core/labs/lab-gapminder.html#grammar-of-graphics",
    "title": "Data visualization",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nWe will use the Grammar of Graphics approach to visualization\nThe expression Grammar of Graphics was coined by Leiland Wilkinson to describe a principled approach to visualization in Data Analysis (EDA)\nA plot is organized around tabular data (a table with rows (observations) and columns (variables))\nA plot is a graphical object that can be built layer by layer\nBuilding a graphical object consists in chaining elementary operations\nThe acclaimed TED presentation by Hans Rosling illustrates the Grammar of Graphics approach\n\nWe will reproduce the animated demonstration using\n\n\nggplot2: an implementation of grammar of graphics in `R\n\nplotly: a bridge between R and the javascript library D3.js\n\nUsing plotly, opting for html ouput, brings the possibility of interactivity and animation"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#setup",
    "href": "core/labs/lab-gapminder.html#setup",
    "title": "Data visualization",
    "section": "Setup",
    "text": "Setup\nWe will use the following packages. If needed, we install them.\n\nCodestopifnot(\n  require(tidyverse), \n  require(patchwork), \n  require(glue), \n  require(ggforce), \n  require(plotly),\n  require(ggthemes),\n  require(gapminder),\n  require(ggrepel)\n)\n\n\nThe data we will use can be obtained by loading package gapminder\n\n\n\n\n\n\nTip\n\n\n\nIf the packages have not yet been installed on your hard drive, install them.\nYou can do that using base R install.packages() function:\ninstall.packages(\"tidyverse\")\nIt is often faster to use functions from package pak\ninstall.packages(\"pak\")\npak::pkg_install(\"tidyverse\")\n\n\nYou need to understand the difference between installing and loading a package\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow do we get the list of installed packages?\nHow do we get the list of loaded packages?\nWhich objects are made available by a package?"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#have-a-look-at-gapminder-dataset",
    "href": "core/labs/lab-gapminder.html#have-a-look-at-gapminder-dataset",
    "title": "Data visualization",
    "section": "Have a look at gapminder dataset",
    "text": "Have a look at gapminder dataset\nThe gapminder table can be found at gapminder::gapminder\n\nA table has a schema: a list of named columns, each with a given type\nA table has a content: rows. Each row is a collection of items, corresponding to the columns\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplore gapminder::gapminder, using glimpse() and head()\n\n\nglimpse() allows to see the schema and the first rows\n\nhead() allows to see the first rows\nUse the pipe |&gt; to chain operations"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#get-a-feeling-of-the-dataset",
    "href": "core/labs/lab-gapminder.html#get-a-feeling-of-the-dataset",
    "title": "Data visualization",
    "section": "Get a feeling of the dataset",
    "text": "Get a feeling of the dataset\n\n\n\n\n\n\nQuestion\n\n\n\nPick two random rows for each continent using slice_sample()\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat makes a table tidy?\n\n\n\n\n\n\n\n\nTip\n\n\n\nHave a look at Data tidying in R for Data Science (2nd ed.)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs the gapminder table redundant?"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#gapminder-tibble-extract",
    "href": "core/labs/lab-gapminder.html#gapminder-tibble-extract",
    "title": "Data visualization",
    "section": "Gapminder tibble (extract)",
    "text": "Gapminder tibble (extract)\n\n\n\n\n\n\nQuestion\n\n\n\nExtract/filter a subset of rows using dplyr::filter(...)\n\nAll rows concerning a given country\nAll rows concerning a year\nAll rows concerning a given continnent and a year"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#filtering-selection-σ-from-database-theory-picking-one-year-of-data",
    "href": "core/labs/lab-gapminder.html#filtering-selection-σ-from-database-theory-picking-one-year-of-data",
    "title": "Data visualization",
    "section": "Filtering (selection \\(σ\\) from database theory) : Picking one year of data",
    "text": "Filtering (selection \\(σ\\) from database theory) : Picking one year of data\nThere is simple way to filter rows satisfying some condition. It consists in mimicking indexation in a matrix, leaving the colum index empty, replacing the row index by a condition statement (a logical expression) also called a mask.\n\nCode# q: in gapminder table extract all raws concerning year 2002\n\ngapminder_2002 &lt;- gapminder |&gt;\n  filter(year==2002)  # \n\ngapminder_2002 &lt;- gapminder[gapminder$year==2002,]\n\n\nHave a look at\n\nCodegapminder$year==2002\n\n\nWhat is the type/class of this expression?\nThis is possible in base R and very often convenient.\nNevertheless, this way of performing row filtering does not emphasize the connection between the dataframe and the condition. Any logical vector with the right length could be used as a mask. Moreover, this way of performing filtering is not very functional.\n\n\n\n\n\n\nIn the parlance of Relational Algebra, filter performs a selection of rows. Relational expression\n\\[σ_{\\text{condition}}(\\text{Table})\\]\ntranslates to\n\nCodefilter(Table, condition)\n\n\nwhere \\(\\text{condition}\\) is a boolean expression that can be evaluated on each row of \\(\\text{Table}\\). In SQL, the relational expression would translate into\n\nCodeSELECT \n  *\nFROM \n  Table\nWHERE \n  condition\n\n\nCheck Package dplyr docs\nThe posit cheatsheet on dplyr is an unvaluable resource for table manipulation.\n\n\n\nUse dplyr::filter() to perform row filtering"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#static-plotting-first-attempt",
    "href": "core/labs/lab-gapminder.html#static-plotting-first-attempt",
    "title": "Data visualization",
    "section": "Static plotting: First attempt",
    "text": "Static plotting: First attempt\n\n\n\n\n\n\nQuestion\n\n\n\nDefine a plot with respect to gapminder_2002 along the lines suggested by Rosling’s presentation.\n\n\n\n\n\n\n\n\nYou should define a ggplot object with data layer gapminder_2022 and call this object p for further reuse.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMap variables gdpPercap and lifeExp to axes x and y. Define the axes. In ggplot2 parlance, this is called aesthetic mapping. Use aes().\n\n\n\n\n\n\n\n\nUse ggplot object p and add a global aesthetic mapping gdpPercap and lifeExp to axes x and y (using + from ggplot2) .\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nFor each row, draw a point at coordinates defined by the mapping. You need to add a geom_ layer to your ggplot object, in this case geom_point() will do.\n\n\n\n\n\n\n\n\nWhat’s up?\n\n\n\nWe are building a graphical object (a ggplot object) around a data frame (gapminder)\nWe supply aesthetic mappings (aes()) that can be either global or specifically bound to some geometries (geom_point()) or statistics\nThe global aesthetic mapping defines which columns (variables) are\n\nmapped to position (which columns are mapped to axes),\npossibly mapped to colours, linetypes, shapes, …\n\nGeometries and Statistics describe the building blocks of graphics\n\n\nWhat’s missing here?\nwhen comparing to the Gapminder demonstration, we can spot that\n\ncolors are missing\nbubble sizes are all the same. They should reflect the population size of the country\ntitles and legends are missing. This means the graphic object is useless.\n\nWe will add other layers to the graphical object to complete the plot"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#second-attempt-display-more-information",
    "href": "core/labs/lab-gapminder.html#second-attempt-display-more-information",
    "title": "Data visualization",
    "section": "Second attempt: display more information",
    "text": "Second attempt: display more information\n\n\n\n\n\n\nQuestion\n\n\n\n\nMap continent to color (use aes())\nMap pop to bubble size (use aes())\nMake point transparent by tuning alpha (inside geom_point() avoid overplotting)"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#scaling",
    "href": "core/labs/lab-gapminder.html#scaling",
    "title": "Data visualization",
    "section": "Scaling",
    "text": "Scaling\nTo pay tribute to Hans Rosling, we need to take care of two scaling issues:\n\nthe gdp per capita axis should be logarithmic scale_x_log10()\n\nthe area of the point should be proportional to the population scale_size_area()\n\n\n\n\n\n\n\n\nComplete the graphical object accordingly\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMotivate the proposed scalings.\n\nWhy is it important to use logarithmic scaling for gdp per capita?\nWhen is it important to use logarithmic scaling on some axis (in other contexts)?\nWhy is it important to specify scale_size_area() ?"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#in-perspective",
    "href": "core/labs/lab-gapminder.html#in-perspective",
    "title": "Data visualization",
    "section": "In perspective",
    "text": "In perspective\n\n\n\n\n\n\nQuestion\n\n\n\nUsing copilots completions, we can summarize the construcion of the graphical object in a series of questions.\n# q: Define a plot with respect to table gapminder_2002 along the lines suggested by Rosling's TED presentation\n# q: Map variables gdpPercap and lifeExp to axes x and y. Define the axes. \n# q: For each row, draw a point at coordinates defined by the mapping.\n# q: Map continent to color\n# q: Map pop to bubble size\n# q: Make point transparent by tuning alpha (inside geom_point() avoid overplotting)\n# q: Add a plot title\n# q: Make axes titles explicit and readable\n# q: Use labs(...)  \n# q: Use scale_x_log10() and scale_size_area()\n# q: Fine tune the guides: replace pop by Population and titlecase continent\n# q: Use theme_minimal()\n# q: Use scale_color_manual(...) to fine tune the color aesthetic mapping.\n# q: Use facet_zoom() from package ggforce\n# q: Add labels to points. This can be done by aesthetic mapping. Use aes(label=..)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat should be the respective purposes of Title, Subtitle, Caption, … ?"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#theming-using-ggthemes-or-not",
    "href": "core/labs/lab-gapminder.html#theming-using-ggthemes-or-not",
    "title": "Data visualization",
    "section": "Theming using ggthemes (or not)",
    "text": "Theming using ggthemes (or not)\n\nCodestopifnot(\n  require(\"ggthemes\")\n)\n\n\nA theme defines the look and feel of plots\nWithin a single document, we should use only one theme\nSee Getting the theme for a gallery of available themes\n\nCodep +\n  theme_economist()"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#tuning-scales",
    "href": "core/labs/lab-gapminder.html#tuning-scales",
    "title": "Data visualization",
    "section": "Tuning scales",
    "text": "Tuning scales\n\n\n\n\n\n\nQuestion\n\n\n\nUse scale_color_manual(...) to fine tune the color aesthetic mapping.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nChoosing a color scale is a difficult task\nviridis is often a good pick."
  },
  {
    "objectID": "core/labs/lab-gapminder.html#zooming-on-a-continent",
    "href": "core/labs/lab-gapminder.html#zooming-on-a-continent",
    "title": "Data visualization",
    "section": "Zooming on a continent",
    "text": "Zooming on a continent\n\nCodezoom_continent &lt;- 'Europe'  # choose another continent at your convenience \n\n\n\n\n\n\n\n\nUse facet_zoom() from package ggforce"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#adding-labels",
    "href": "core/labs/lab-gapminder.html#adding-labels",
    "title": "Data visualization",
    "section": "Adding labels",
    "text": "Adding labels\n\n\n\n\n\n\nQuestion\n\n\n\nAdd labels to points. This can be done by aesthetic mapping. Use aes(label=..)\nTo avoid text cluttering, package ggrepel offers interesting tools."
  },
  {
    "objectID": "core/labs/lab-gapminder.html#facetting",
    "href": "core/labs/lab-gapminder.html#facetting",
    "title": "Data visualization",
    "section": "Facetting",
    "text": "Facetting\nSo far we have only presented one year of data (2002)\nRosling used an animation to display the flow of time\nIf we have to deliver a printable report, we cannot rely on animation, but we can rely on facetting\nFacets are collections of small plots constructed in the same way on subsets of the data\n\n\n\n\n\n\nQuestion\n\n\n\nAdd a layer to the graphical object using facet_wrap()\n\n\n\nAs all rows in gapminder_2002 are all related to year 2002, we need to rebuild the graphical object along the same lines (using the same graphical pipeline) but starting from the whole gapminder dataset.\nShould we do this using cut and paste?\n No!!!\n\n\n\n\n\n\n\nDon’t Repeat Yoursel (DRY)\n\n\n\n\nAbide to the DRY principle using operator %+%: the ggplot2 object p can be fed with another dataframe and all you need is proper facetting."
  },
  {
    "objectID": "core/labs/lab-gapminder.html#animate-for-free-with-plotly",
    "href": "core/labs/lab-gapminder.html#animate-for-free-with-plotly",
    "title": "Data visualization",
    "section": "Animate for free with plotly\n",
    "text": "Animate for free with plotly\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse plotly::ggplotly() to create a Rosling like animation.\nUse frame aesthetics."
  },
  {
    "objectID": "core/labs/lab-gapminder.html#suggestions",
    "href": "core/labs/lab-gapminder.html#suggestions",
    "title": "Data visualization",
    "section": "Suggestions",
    "text": "Suggestions\nThink about ways to visualize specific aspects of the gapminder data.\n\nHow could you overlay the world in 1952 and 2007?\nHow could you visualize the evolution of life expectancy and population across the different countries?\nVisualize the evolution of former colonies and their colonizers.\nVisualize the evolution of countries from the former Soviet Union, Warsaw Pact, and Yugoslavia.\nVisualize the evolution of countries from the former British Empire."
  },
  {
    "objectID": "core/labs/lab-gapminder.html#more-material",
    "href": "core/labs/lab-gapminder.html#more-material",
    "title": "Data visualization",
    "section": "More material",
    "text": "More material\n\nRead Visualization in R for Data Science"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#packages",
    "href": "core/labs/lab-r-intro.html#packages",
    "title": "R language: a tour",
    "section": "Packages",
    "text": "Packages\nBase R can do a lot. But the full power of R comes from a fast growing collection of packages.\nPackages are first installed (that is downloaded from cran and copied somewhere on the hard drive), and if needed, loaded during a session.\n\nInstallation can usually be performed using command install.packages(). In some circumstances, ad hoc installation commands (often from packages devtools) are needed\nPackage pak offers an interesting alternative to base R install.packages()\n\nOnce a package has been installed/downloaded on your hard drive\n\nif you want all objects exported by the package to be available in your session, you should load the package, using library() or require() (what’s the difference?). Technically, this loads the NameSpace defined by the package.\nif you just want to pick some objects exported from the package, you can use qualified names like package_name::object_name to access the object (function, dataset, …).\n\n\n\nFor example, when we write\ngapminder &lt;- gapminder::gapminder\nwe assign dataframe/tibble gapminder from package gapminder to identifier \"gapminder\" in global environment .\nFunction p_load() from pacman (package manager) blends installation and loading: if the package named in the argument of p_load() is not installed (not among the installed.packages()), p_load() attempts to install the package. If installation is successful, the package is loaded.\n\nif (! require(pak)){\n  install.packages(\"pak\")\n}\n\n\nstopifnot(\n  require(\"tidyverse\"), \n  require(\"lobstr\"),\n  require(\"ggforce\"),\n  require(\"nycflights13\"),\n  require(\"patchwork\"), \n  require(\"viridis\"),\n  require(\"MASS\"),\n  require(\"gapminder\"),\n  require(\"pryr\"),\n  require(\"pak\")\n)\n\n\n\n\n\n\n\nOptional arguments\n\n\n\nA very nice feature of R is that functions from base R as well as from packages have optional arguments with sensible default values. Look for example at documentation of require() using expression ?require.\nOptional settings may concern individual functions or the collection of functions exported by some packages. In the next chunk, we reset the default color scales used by graphical functions from ggplot2.\n\nopts &lt;- options()  # save old options\n\noptions(ggplot2.discrete.colour=\"viridis\")\noptions(ggplot2.continuous.colour=\"viridis\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou shall not confuse installing (on your hard-drive) and loading (in session) a package.\n\n\n\n\n\n\n\n\nQuestion for Pythonistas\n\n\n\n\nIn  what is the analogue of install.packages()?\nIn  what is the analogue of require()/library()?"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#vector-creation-and-assignment",
    "href": "core/labs/lab-r-intro.html#vector-creation-and-assignment",
    "title": "R language: a tour",
    "section": "Vector creation and assignment",
    "text": "Vector creation and assignment\nThe next three lines create three numerical atomic vectors.\nIn IDE Rstudio, have a look at the environment pane on the right before running the chunk, and after.\nUse ls() to investigate the environment before and after the execution of the three assignments.\n\nls()\nx &lt;- c(1, 2, 12)\ny &lt;- 5:7\nz &lt;- 10:1\nx ; y ; z \nls()\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat are the identifiers known in the global environment before execution of lines 2-4?\nWhat are the identifiers known in the global environment after execution of lines 2-4?\nWhich objects are attached to identifiers x, y, and z?\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does the next chunk?\n\nls()\nw &lt;- y\nls()\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nIs the content of object denoted by y copied to a new object bound to w?\nInterpret the result of w == y.\nInterpret the result of identical(w,y) (use help(\"identical\") if needed).\n\n\nw == y \nidentical(w,y)"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#indexation-slicing-modification",
    "href": "core/labs/lab-r-intro.html#indexation-slicing-modification",
    "title": "R language: a tour",
    "section": "Indexation, slicing, modification",
    "text": "Indexation, slicing, modification\nSlicing a vector can be done in two ways:\n\nproviding a vector of indices to be selected. Indices need not be consecutive.\nproviding a Boolean mask, that is a logical vector to select a set of positions.\n\n\nx &lt;- c(1, 2, 12) ; y &lt;- 5:7 ; z &lt;- 10:1\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next lines\n\nz[1]   # slice of length 1\nz[0]   # What did you expect?\nz[x]   # slice of length ??? index error ?\nz[y]\nz[x %% 2]   # what happens with x[0] ?\nz[0 == (x %% 2)] # masking\nz[c(2, 1, 1)]\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIf the length of mask and and the length of the sliced vector do not coincide, what happens?\n\n\n\n\n\n\n\n\n\n\n\n\nA scalar is just a vector of length \\(1\\)!\n\nclass(z)\n\n[1] \"integer\"\n\nclass(z[1])\n\n[1] \"integer\"\n\nclass(z[c(2,1)])\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next lines\n\ny[2:3] &lt;- z[2:3]\ny == z[-10]\n\nz[-11]\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next line\n\nz[-(1:5)]\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you select the last element from a vector (say z)?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nReverse the entries of a vector. Find two ways to do that.\n\n\nIn statistics, machine learning, we are often faced with the task of building grids of regularly spaced elements (these elements can be numeric or not). R offers a collection of tools to perform this. The most basic tool is rep().\n\n\n\n\n\n\nQuestion\n\n\n\n\nRepeat a vector \\(2\\) times\nRepeat each element of a vector twice\n\n\n\nLet us remove objects from the global environment.\n\nrm(w, x, y ,z)"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#numbers",
    "href": "core/labs/lab-r-intro.html#numbers",
    "title": "R language: a tour",
    "section": "Numbers",
    "text": "Numbers\nSo far, we told about numeric vectors. Numeric vectors are vectors of floating point numbers. R distinguishes several kinds of numbers.\n\nIntegers\nFloating point numbers (double)\n\nTo check whether a vector is made of numeric or of integer, use is.numeric() or is.integer(). Use as.integer, as.numeric() to enforce type conversion.\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the outcome of the next chunks\n\nclass(113L) ; class(113) ; class(113L + 113) ; class(2 * 113L) ; class(pi) ; as.integer(pi)\n\n[1] \"integer\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] 3\n\n\n\nfloor(pi) ; class(floor(pi)) # mind the floor\n\n[1] 3\n\n\n[1] \"numeric\""
  },
  {
    "objectID": "core/labs/lab-r-intro.html#integer-arithmetic",
    "href": "core/labs/lab-r-intro.html#integer-arithmetic",
    "title": "R language: a tour",
    "section": "Integer arithmetic",
    "text": "Integer arithmetic\n\n29L * 31L ; 899L %/% 32L ; 899L %% 30L\n\n[1] 899\n\n\n[1] 28\n\n\n[1] 29\n\n\n\n\n\n\n\n\nR integers are not the natural numbers from Mathematics\nR numerics are not the real numbers from Mathematics\n\n.Machine$double.eps\n\n[1] 2.220446e-16\n\n.Machine$double.xmax\n\n[1] 1.797693e+308\n\n.Machine$sizeof.longlong\n\n[1] 8\n\nu &lt;- double(19L)\nv &lt;- numeric(5L)\nw &lt;- integer(7L)\nlapply(list(u, v, w), typeof)\n\n[[1]]\n[1] \"double\"\n\n[[2]]\n[1] \"double\"\n\n[[3]]\n[1] \"integer\"\n\nlength(c(u, v, w))\n\n[1] 31\n\ntypeof(c(u, v, w))\n\n[1] \"double\"\n\n\n\n\n\nR is (sometimes) able to make sensible use of Infinite.\n\nlog(0)\n\n[1] -Inf\n\nlog(Inf)\n\n[1] Inf\n\n1/0\n\n[1] Inf\n\n0/0\n\n[1] NaN\n\nmax(c( 0/0,1,10))\n\n[1] NaN\n\nmax(c(NA,1,10))\n\n[1] NA\n\nmax(c(-Inf,1,10))\n\n[1] 10\n\nis.finite(c(-Inf,1,10))\n\n[1] FALSE  TRUE  TRUE\n\nis.na(c(NA,1,10))\n\n[1]  TRUE FALSE FALSE\n\nis.nan(c(NaN,1,10))\n\n[1]  TRUE FALSE FALSE"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#computing-with-vectors",
    "href": "core/labs/lab-r-intro.html#computing-with-vectors",
    "title": "R language: a tour",
    "section": "Computing with vectors",
    "text": "Computing with vectors\nSumming, scalar multiplication\n\nx &lt;- 1:3\ny &lt;- 9:7\n\nsum(x) ; prod(x)\n\n[1] 6\n\n\n[1] 6\n\nz &lt;- cumsum(1:3)\nw &lt;- cumprod(3:5)\n\nx + y\n\n[1] 10 10 10\n\nx + z\n\n[1] 2 5 9\n\n2 * w\n\n[1]   6  24 120\n\n2 + w\n\n[1]  5 14 62\n\nw / 2\n\n[1]  1.5  6.0 30.0\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute a factorial?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nApproximate \\(\\sum_{n=1}^\\infty 1/n^2\\) within \\(10^{-3}\\)?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute the inner product between two (atomic numeric) vectors?\n\n\n\n\n\n\n\n\nWhat we have called vectors so far are indeed atomic vectors.\n\nRead Chapter on Vectors in R advanced Programming\n\nKeep an eye on package vctrs for getting insights into the R vectors."
  },
  {
    "objectID": "core/labs/lab-r-intro.html#creation-transposition-and-reshaping",
    "href": "core/labs/lab-r-intro.html#creation-transposition-and-reshaping",
    "title": "R language: a tour",
    "section": "Creation, transposition and reshaping",
    "text": "Creation, transposition and reshaping\nA vector can be turned into a column matrix.\n\nv &lt;- as.matrix(1:5)\nv\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[4,]    4\n[5,]    5\n\n\nA matrix can be transposed\n\nt(v)  # transpose \n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n\ncat(dim(v), ' ', dim(t(v)), '\\n')\n\n5 1   1 5 \n\n\n\nA &lt;- matrix(1, nrow=5, ncol=2) ; A\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n[3,]    1    1\n[4,]    1    1\n[5,]    1    1\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nlobstr::mem_used() allows us to keep track of the amount of memory used by our R session. lobstr::obj_size() tells us the amount of memory used by the representation of an object.\nComment the next chunk\n\nm1 &lt;-lobstr::mem_used()\nA &lt;- matrix(rnorm(100000L), nrow=1000L)\nm2 &lt;- lobstr::mem_used()\nlobstr::obj_size(A)\n\n800.22 kB\n\nB &lt;- t(A)\nlobstr::obj_size(B)\n\n800.22 kB\n\nm3 &lt;- lobstr::mem_used()\nm2-m1 ; m3-m2\n\n804.94 kB\n\n\n1.09 MB\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nIs there a difference between the next two assignments?\nHow would you assign value to all entries of a matrix?\n\n\nA &lt;- matrix(rnorm(16), nrow=4)\nA[] &lt;- 0 ; A\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n\nA   &lt;- 0 ; A\n\n[1] 0\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the final shape of A?\n\nA &lt;- matrix(1, nrow=5, ncol=2) \nA\nA[] &lt;- 1:15 \nA\n\n\n\nWe can easily generate diagonal matrices and constant matrices.\n\ndiag(1, 3)  # building identity matrix\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\nmatrix(0, 3, 3) # building null matrix\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs there any difference between the next two assignments?\n\nB &lt;- A[]\nB ; A\n\n[1] 0\n\n\n[1] 0\n\nlobstr::obj_addr(B) ; lobstr::obj_addr(A)\n\n[1] \"0x5f6a8d4cfb48\"\n\n\n[1] \"0x5f6a8e04d5c8\"\n\nB &lt;- A\nlobstr::obj_addr(B) ; lobstr::obj_addr(A)\n\n[1] \"0x5f6a8e04d5c8\"\n\n\n[1] \"0x5f6a8e04d5c8\""
  },
  {
    "objectID": "core/labs/lab-r-intro.html#indexation-slicing-modification-1",
    "href": "core/labs/lab-r-intro.html#indexation-slicing-modification-1",
    "title": "R language: a tour",
    "section": "Indexation, slicing, modification",
    "text": "Indexation, slicing, modification\nIndexation consists in getting one item from a vector/list/matrix/array/dataframe.\nSlicing and subsetting consists in picking a substructure:\n\nsubsetting a vector returns a vector\nsubsetting a list returns a list\nsubsetting a matrix/array returns a matrix/array (beware of implicit simplifications and dimension dropping)\nsubsetting a dataframe returns a dataframe or a vector (again, beware of implicit simplifications).\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next results\n\nA &lt;- matrix(1, nrow=5, ncol=2)\n\ndim(A[sample(5, 3), -1])\ndim(A[sample(5, 3), 1])\nlength(A[sample(5, 3), 1])\nis.vector(A[sample(5, 3), 1])\nA[10:15]\nA[60]\ndim(A[])\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you create a fresh copy of a matrix?"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#computing-with-matrices",
    "href": "core/labs/lab-r-intro.html#computing-with-matrices",
    "title": "R language: a tour",
    "section": "Computing with matrices",
    "text": "Computing with matrices\n\n\n* versus %*%\n\n\n%*% stands for matrix multiplication. In order to use it, the two matrices should have conformant dimensions.\n\n\n\nt(v) %*% A\n\n          [,1]    [,2]\n[1,] -17.64316 11.6116\n\n\nThere are a variety of reasonable products around. Some of them are available in R.\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute the Hilbert-Schmidt inner product between two matrices?\n\\[\\langle A, B\\rangle_{\\text{HS}} = \\text{Trace} \\big(A \\times B^\\top\\big)\\]\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow can you invert a square (invertible) matrix?"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#handling-three-valued-logic",
    "href": "core/labs/lab-r-intro.html#handling-three-valued-logic",
    "title": "R language: a tour",
    "section": "Handling three-valued logic",
    "text": "Handling three-valued logic\n\n\n\n\n\n\nQuestion\n\n\n\n\nTRUE &  (1&gt; (0/0))\n(1&gt; (0/0)) | TRUE\n(1&gt; (0/0)) | FALSE\nTRUE || (1&gt; (0/0))\nTRUE |  (1&gt; (0/0))\nTRUE || stopifnot(4&lt;3)\n# TRUE |  stopifnot(4&lt;3)  \nFALSE && stopifnot(4&lt;3)\n# FALSE & stopifnot(4&lt;3)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the difference between logical operators || and | ?\n\n\n\n\n\n\n\n\n\n\n\n\nRemark: favor &, | over &&, ||."
  },
  {
    "objectID": "core/labs/lab-r-intro.html#all-and-any",
    "href": "core/labs/lab-r-intro.html#all-and-any",
    "title": "R language: a tour",
    "section": "\nall and any\n",
    "text": "all and any\n\nLook at the definition of all and any.\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow would you check that a square matrix is symmetric?\nHow would you check that a matrix is diagonal?"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#if-then-else",
    "href": "core/labs/lab-r-intro.html#if-then-else",
    "title": "R language: a tour",
    "section": "If () then {} else {}",
    "text": "If () then {} else {}\nIf expressions yes_expr and no_expr are complicated it makes sense to use the if (...) {...} else {...} construct\nThere is also a conditional statement with an optional else {}\n#| label: if-else\n#| eval: false\n#| collapse: false\nif (condition) {\n  ...\n} else {\n  ...\n}\n\n\n\n\n\n\nQuestion\n\n\n\nIs there an elif construct in R?\n\n\n R also offers a switch\n#| label: switch\nswitch (object,\n  case1 = {action1}, \n  case2 = {action2}, \n  ...\n)\n\n\n\n\n\n\nThere exists a selection function ifelse(test, yes_expr, no_expr).\n\nifelse(test, yes, no)\n\nNote that ifelse(...) is vectorized.\n\nx &lt;-  1L:6L\ny &lt;-  rep(\"odd\", 6)\nz &lt;- rep(\"even\", 6)\n\nifelse(x %% 2L, y, z)\n\n[1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\n This is a vectorized function"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#iterations-for-it-in-iterable-...",
    "href": "core/labs/lab-r-intro.html#iterations-for-it-in-iterable-...",
    "title": "R language: a tour",
    "section": "Iterations for (it in iterable) {...}\n",
    "text": "Iterations for (it in iterable) {...}\n\nHave a look at Iteration section in R for Data Science\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a lower triangular matrix which represents the 5 first lines of the Pascal triangle.\n\n\nRecall\n\\[\\binom{n}{k} = \\binom{n-1}{k-1} + \\binom{n-1}{k}\\]\n\n\n\n\n\n\nQuestion\n\n\n\nLocate the smallest element in a numerical vector"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#while-condition",
    "href": "core/labs/lab-r-intro.html#while-condition",
    "title": "R language: a tour",
    "section": "While (condition) {…}",
    "text": "While (condition) {…}\n\n\n\n\n\n\nQuestion\n\n\n\nFind the location of the minimum in a vector v\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a loop that checks whether vector v is non-decreasing."
  },
  {
    "objectID": "core/labs/lab-r-intro.html#operators-purrrmap_",
    "href": "core/labs/lab-r-intro.html#operators-purrrmap_",
    "title": "R language: a tour",
    "section": "Operators purrr::map_???\n",
    "text": "Operators purrr::map_???\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite truth tables for &, |, &&, ||, ! and xor\nHint: use purrr::map, function outer()\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a function that takes as input a square matrix and returns TRUE if it is lower triangular.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse map , choose and proper use of pronouns to deliver the n first lines of the Pascal triangle using one line of code.\nAs far as the total number of operations is concerned, would you recommend this way of computing the Pascal triangle?\n\n\n\n\n\n\n\n\nRead Chapter on Functional Programming in Advanced R"
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing environment",
    "section": "",
    "text": "In this course, we will use the R programming language. Install the latest version of R from the R Project website on your hard drive. In any case, you shall have Version &gt; 4.0.\nCheatsheets\n\n\nIDEs\nThere exist several Integrated Development Environments (IDEs) for R. Here are some of them:\n\nrstudio\nvs code\nvs codium\nemacs\n\nrstudio has been designed to work around R. It is free and open-source. It is available for Windows, Mac, and Linux. It works very well git. Quarto has been designed and developed by the same team.\nvs code is a general-purpose IDE. It works very well with git. It is very versatile and can be used for many programming languages. It is endowed with a large number of extensions for R, Python, SQL, and Quarto. It integrates very well with git. It is promoted by github just as copilot. vs codium is a telemetry-free version of vs code.\nemacs is a classical and powerful general-purpose editor that can be turned into an IDE. It is available for Windows, Mac, and Linux. It also works very well with git. Extension ESS (Emacs Speaks Statistics) is available for R.\nIn the course, we will use rstudio and vs code.\n\n\nPositcloud\nPosit Cloud lets you access Posit’s powerful set of data science tools right in your browser – no installation or complex configuration required.\n\n\nQuarto\nDownload from Quarto Website. It is very convenient to work the Command Line Interface (CLI) of Quarto.\n\n\n\n\n\n\nFrom the Quarto website\n\n\n\n\nAn open-source scientific and technical publishing system\nAuthor using Jupyter notebooks or with plain text markdown in your favorite editor.\nCreate dynamic content with Python, R, Julia, and Observable.\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nShare knowledge and insights organization-wide by publishing to Posit Connect, Confluence, or other publishing systems.\nWrite using Pandoc markdown, including equations, citations, cross references, figure panels, callouts, advanced layout, and more.",
    "crumbs": [
      "Support",
      "Computing resources"
    ]
  },
  {
    "objectID": "core/labs/lab-bivariate.html#setup",
    "href": "core/labs/lab-bivariate.html#setup",
    "title": "Bivariate analysis",
    "section": "Setup",
    "text": "Setup\n\nCodestopifnot(\n  require(glue),\n  require(magrittr),\n  require(lobstr),\n  require(arrow),\n  require(ggforce),\n  require(vcd),\n  require(ggmosaic),\n  require(httr),\n  require(patchwork),\n  require(corrr),\n  require(gapminder),\n  require(slider),\n  require(tidyverse) \n) \n\n\nBivariate techniques depend on the types of columns we are facing.\nFor numerical/numerical samples\n\nScatter plots\nSmoothed lineplots (for example linear regression)\n2-dimensional density plots\n\nFor categorical/categorical samples : mosaicplots and variants\nFor numerical/categorical samples\n\nBoxplots per group\nHistograms per group\nDensity plots per group"
  },
  {
    "objectID": "core/labs/lab-bivariate.html#chi-square-independenceassociation-test",
    "href": "core/labs/lab-bivariate.html#chi-square-independenceassociation-test",
    "title": "Bivariate analysis",
    "section": "Chi-square independence/association test",
    "text": "Chi-square independence/association test\nhttps://statsfonda.github.io/site/content/ch4_2.html#test-dindépendance\n\n\n\n\n\n\nQuestion\n\n\n\n\nCompute the chi-square association statistic between CATEGORIE and SEXE.\nDisplay the output of chisq.test() as a table, using broom::tidy()"
  },
  {
    "objectID": "core/labs/lab-bivariate.html#grouped-boxplots",
    "href": "core/labs/lab-bivariate.html#grouped-boxplots",
    "title": "Bivariate analysis",
    "section": "Grouped boxplots",
    "text": "Grouped boxplots\n\n\n\n\n\n\nQuestion\n\n\n\nPlot boxplots of AGE according to NIV_ETUDES\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDraw density plots of AGE, facet by NIV_ETUDES and SEXE\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCollapse rare levels of NIV_ETUDES and replay."
  },
  {
    "objectID": "core/labs/lab-bivariate.html#scatterplots",
    "href": "core/labs/lab-bivariate.html#scatterplots",
    "title": "Bivariate analysis",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\n\n\n\n\nQuestion\n\n\n\nMake a scatterplot of SAL_HORwith respect to AGE"
  },
  {
    "objectID": "core/labs/lab-bivariate.html#linear-correlation-coefficient",
    "href": "core/labs/lab-bivariate.html#linear-correlation-coefficient",
    "title": "Bivariate analysis",
    "section": "Linear correlation coefficient",
    "text": "Linear correlation coefficient\n\n\n\n\n\n\nQuestion\n\n\n\nCompute the Pearson, Spearman and Kendall correlation coefficients between AGE and SAL_HOR using function cor() from base R"
  },
  {
    "objectID": "core/labs/lab-bivariate.html#rank-based-methods",
    "href": "core/labs/lab-bivariate.html#rank-based-methods",
    "title": "Bivariate analysis",
    "section": "Rank based methods",
    "text": "Rank based methods\nSpearman’s rho (𝜌) and Kendall’s tau (𝜏) are both non-parametric correlation coefficients used to measure the strength and direction of a monotonic relationship between two variables.\n\nSpearman’s rho (𝜌)\n\nBased on rank differences. Defined as the Pearson correlation coefficient between the ranked variables.\\[\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\\] where \\(d_i\\) is the difference between the ranks of each pair, and \\(n\\) is the number of observations.\n\nKendall’s tau (𝜏)\n\nBased on concordant and discordant pairs.  Measures the proportion of pairs that have the same order in both variables compared to the total number of pairs. \\[\\tau = \\frac{(C - D)}{\\frac{1}{2} n (n - 1)}\\] where \\(C\\) is the number of concordant pairs, and \\(D\\) is the number of discordant pairs.\n\n\nWhen to Use Which?\n\n\n\n\n\n\n\nFactor\nSpearman’s rho (𝜌)\nKendall’s tau (𝜏)\n\n\n\nLarge differences in ranks\nMore sensitive\nLess sensitive\n\n\nSmall sample sizes\nLess reliable\nMore reliable\n\n\nOutlier resistance\nModerate\nHigh\n\n\nComputational efficiency\nFaster\nSlower (due to pairwise comparisons)\n\n\nInterpretation\nSimilar to Pearson’s correlation\nMore intuitive (proportion of concordance)"
  },
  {
    "objectID": "core/labs/lab-bivariate.html#chatterjees-correlation-coefficient-chatterjees-ξ",
    "href": "core/labs/lab-bivariate.html#chatterjees-correlation-coefficient-chatterjees-ξ",
    "title": "Bivariate analysis",
    "section": "Chatterjee’s correlation coefficient (Chatterjee’s \\(ξ\\))",
    "text": "Chatterjee’s correlation coefficient (Chatterjee’s \\(ξ\\))\n\nThe three most popular classical measures of statistical association are Pearson’s correlation coefficient, Spearman’s ρ, and Kendall’s τ . These coefficients are very powerful for detecting linear or monotone associations, and they have well-developed asymptotic theories for calculating P-values. However, the big problem is that they are not effective for detecting associations that are not monotonic, even in the complete absence of noise.\n\n\nLet \\((X, Y)\\) be a pair of random variables, where \\(Y\\) is not a constant. Let \\((X_1 , Y_1 ), \\ldots, (X_n , Y_n )\\) be i.i.d. pairs with the same law as \\((X, Y)\\), where \\(n ≥ 2\\). The new coefficient has a simpler formula if the \\(X_i\\)’s and the \\(Y_i\\) ’s have no ties. This simpler formula is presented first, and then the general case is given. Suppose that the \\(X_i\\)’s and the \\(Y_i\\) ’s have no ties. Rearrange the data as \\((X_{(1)} , Y_{(1)} ), . . . , (X_{(n)} , Y_{(n)} )\\) such that \\(X_{(1)} ≤ · · · ≤ X_{(n)}\\) . Since the \\(X_i\\)’s have no ties, there is a unique way of doing this. Let ri be the rank of \\(Y_{(i)}\\), that is, the number of \\(j\\) such that \\(Y_{(j)} ≤ Y_{(i)}\\). The new correlation coefficient is defined as\n\n\\[ξ_n(X, Y ) := 1 −  3\\sum_{i=1}^{n-1} \\frac{|r_{i+1} − r_i|}{n^2-1}\\]\n\nIn the presence of ties, \\(ξ_n\\) is defined as follows. If there are ties among the \\(X_i\\)’s, then choose an increasing rearrangement as above by breaking ties uniformly at random. Let \\(r_i\\) be as before, and additionally define \\(l_i\\) to be the number of \\(j\\) such that \\(Y_{(j)} ≥ Y_{(i)}\\). Then define\n\n\\[ξ_n(X, Y ) := 1 −  3n\\sum_{i=1}^{n-1} \\frac{|r_{i+1} − r_i|}{2 \\sum_{i=1}^n l_i (n − l_i )}\\]\n\nWhen there are no ties among the \\(Y_i\\) ’s, \\(l_1 , \\ldots , l_n\\) is just a permutation of \\(1, \\ldots , n\\), and so the denominator in the above expression is just \\(n(n^2 − 1)/3\\), which reduces this definition to the earlier expression.\n\nFrom Sourav Chatterjee: A new correlation coefficient\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a dplyr pipeline from computing the \\(ξ\\) correlation coefficient between Y=lifeExp and X=gdpPercap in the gapminder dataset, per year and continent."
  },
  {
    "objectID": "core/labs/lab-linear-regression-whiteside.html#introduction",
    "href": "core/labs/lab-linear-regression-whiteside.html#introduction",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "Introduction",
    "text": "Introduction\nThe purpose of this lab is to introduce linear regression using base R and the tidyverse. We work on a dataset provided by the MASS package. This dataset is investigated in the book by Venables and Ripley. This discusssion is worth being read. Our aim is to relate regression as a tool for data exploration with regression as a method in statistical inference. To perform regression, we will rely on the base R function lm() and on the eponymous S3 class lm. We will spend time understanding how the formula argument can be used to construct a design matrix from a dataframe representing a dataset."
  },
  {
    "objectID": "core/labs/lab-linear-regression-whiteside.html#packages-installation-and-loading-again",
    "href": "core/labs/lab-linear-regression-whiteside.html#packages-installation-and-loading-again",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "Packages installation and loading (again)",
    "text": "Packages installation and loading (again)\n\n\nCode\n# We will use the following packages. \n# If needed, install them : pak::pkg_install(). \nstopifnot(\n  require(\"magrittr\"),\n  require(\"lobstr\"),\n  require(\"ggforce\"),\n  require(\"patchwork\"), \n  require(\"gt\"),\n  require(\"glue\"),\n  require(\"skimr\"),\n  require(\"corrr\"),\n  require(\"GGally\"),\n  require(\"broom\"),\n  require(\"tidyverse\"),\n  require(\"ggfortify\"),\n  require(\"autoplotly\")\n\n)\n\n\nBesides the tidyverse, we rely on skimr to perform univariate analysis, GGally::ggpairs to perform pairwise (bivariate) analysis. Package corrr provide graphical tools to explore correlation matrices. At some point, we will showcase the exposing pipe %$% and the classical pipe %&gt;% of magrittr. We use gt to display handy tables, patchwork to compose graphical objects. glue provides a kind of formatted strings. Package broom proves very useful when milking lienar models produced by lm() (and many other objects produced by estimators, tests, …)"
  },
  {
    "objectID": "core/labs/lab-linear-regression-whiteside.html#s3-classes-in-r",
    "href": "core/labs/lab-linear-regression-whiteside.html#s3-classes-in-r",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "S3 classes in R",
    "text": "S3 classes in R"
  },
  {
    "objectID": "core/labs/lab-linear-regression-whiteside.html#generic-functions-for-s3-classes",
    "href": "core/labs/lab-linear-regression-whiteside.html#generic-functions-for-s3-classes",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "Generic functions for S3 classes",
    "text": "Generic functions for S3 classes\nmethods(autoplot) lists the S3 classes for which an autoplot method is defined. Some methods are defined in ggplot2, others like autoplot.lm are defined in extension packages like ggfortify."
  },
  {
    "objectID": "core/labs/lab-linear-regression-whiteside.html#s4-classes-in-r",
    "href": "core/labs/lab-linear-regression-whiteside.html#s4-classes-in-r",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "S4 classes in R",
    "text": "S4 classes in R\nThe output of autoplot.lm is an instance of S4 class"
  },
  {
    "objectID": "core/labs/lab-linear-regression-whiteside.html#tibbles-with-list-columns",
    "href": "core/labs/lab-linear-regression-whiteside.html#tibbles-with-list-columns",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "tibbles with list columns",
    "text": "tibbles with list columns"
  },
  {
    "objectID": "core/labs/lab-univariate-categorical.html#counting",
    "href": "core/labs/lab-univariate-categorical.html#counting",
    "title": "Univariate Categorical Analysis",
    "section": "Counting",
    "text": "Counting\n\n\n\n\n\n\nQuestion\n\n\n\nUse table, prop.table from base R to compute the frequencies and proportions of the different levels. In statistics, the result of table() is a (one-way) contingency table.\n\n\nWhat is the class of the object generated by table? Is it a vector, a list, a matrix, an array ?\n\n\n\n\n\n\nas.data.frame() (or as_tibble) can transform a table object into a dataframe.\n\nCodeta &lt;-  rename(as.data.frame(ta), REV_FOYER=`.`)\n\nta\n\n        REV_FOYER Freq\n1        [0-5000)    9\n2     [5000-7500)    5\n3    [7500-10000)    5\n4   [10000-12500)    9\n5   [12500-15000)    7\n6   [15000-17500)   19\n7   [17500-20000)   26\n8   [20000-25000)   38\n9   [25000-30000)   30\n10  [30000-35000)   35\n11  [35000-40000)   61\n12  [40000-50000)   70\n13  [50000-60000)   71\n14  [60000-75000)   89\n15  [75000-1e+05)   77\n16 [1e+05-150000)   48\n\n\n\n\n\nYou may use knitr::kabble(), possibly knitr::kable(., format=\"markdown\") to tweak the output.\nIf you are more ambitious, use gt::....\nIn order to feed ggplot with a contingency table, it is useful to build contingency tables as dataframes. Use dplyr::count() to do this.\n\n\n\n\n\n\nskimr::skim() allows us to perform univariate categorical analysis all at once.\n\nCodedf %&gt;% \n  skimr::skim(where(is.factor)) %&gt;% \n  print(n=50)\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Piped data\nNumber of rows             599       \nNumber of columns          11        \n_______________________              \nColumn type frequency:               \n  factor                   9         \n________________________             \nGroup variables            None      \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 SEXE                  0             1 FALSE          2\n2 REGION                0             1 FALSE          4\n3 STAT_MARI             0             1 FALSE          5\n4 SYNDICAT              0             1 FALSE          2\n5 CATEGORIE             0             1 FALSE         10\n6 NIV_ETUDES            0             1 FALSE         15\n7 NB_PERS               0             1 FALSE          9\n8 NB_ENF                0             1 FALSE          7\n9 REV_FOYER             0             1 FALSE         16\n  top_counts                           \n1 M: 302, F: 297                       \n2 S: 200, W: 148, NE: 129, NW: 122     \n3 M: 325, C: 193, D: 61, S: 14         \n4 non: 496, oui: 103                   \n5 Lib: 133, Ser: 125, Adm: 94, Sel: 48 \n6 12 : 187, Col: 148, Bac: 114, Ass: 45\n7 2: 196, 4: 130, 3: 122, 1: 63        \n8 0: 413, 1: 86, 2: 76, 3: 18          \n9 [60: 89, [75: 77, [50: 71, [40: 70   \n\n\nThe output can be tailored to your specific objectives and fed to functions that are geared to displaying large tables (see packages knitr, DT, and gt)"
  },
  {
    "objectID": "core/labs/lab-univariate-categorical.html#using-a-for-loop",
    "href": "core/labs/lab-univariate-categorical.html#using-a-for-loop",
    "title": "Univariate Categorical Analysis",
    "section": "Using a for loop",
    "text": "Using a for loop\nWe have to build a barplot for each categorical variable. Here, we just have nine of them. We could do this using cut and paste, and some editing. In doing so, we would not comply with the DRY (Don’t Repeat Yourself) principle.\nIn order to remain DRY, we will attempt to abstract the recipe we used to build our first barplot.\nThis recipe is pretty simple:\n\nBuild a ggplot object with df as the data layer.\nAdd an aesthetic mapping a categorical column to axis x\n\nAdd a geometry using geom_bar\n\nAdd labels explaining the reader which column is under scrutiny\n\nWe first need to gather the names of the categorical columns. The following chunk does this in a simple way.\nIn the next chunk, we shall build a named list of ggplot objects consisting of barplots. The for loop body is almost obtained by cutting and pasting the recipe for the first barplot.\n\n\n\n\n\n\nNote an important difference: instead of something aes(x=col) where col denotes a column in the dataframe, we shall write aes(x=.data[[col]]) where col is a string that matches a column name. Writing aes(x=col) would not work.\nThe loop variable col iterates over the column names, not over the columns themselves.\nWhen using ggplot in interactive computations, we write aes(x=col), and, under the hood, the interpreter uses the tidy evaluation mechanism that underpins R to map df$col to the x axis.\nggplot functions like aes() use data masking to alleviate the burden of the working Statistician.\nWithin the context of ggplot programming, pronoun .data refers to the data layer of the graphical object.\n\n\n\nIf the labels on the x-axis are not readable, we need to tweak them. This amounts to modifying the theme layer in the ggplot object, and more specifically the axis.text.x attribute."
  },
  {
    "objectID": "core/labs/lab-univariate-categorical.html#using-functional-programming-lapply-purrr...",
    "href": "core/labs/lab-univariate-categorical.html#using-functional-programming-lapply-purrr...",
    "title": "Univariate Categorical Analysis",
    "section": "Using functional programming (lapply, purrr::...)",
    "text": "Using functional programming (lapply, purrr::...)\nAnother way to compute the list of graphical objects replaces the for loop by calling a functional programming tool. This mechanism relies on the fact that in R, functions are first-class objects.\n\n\n\n\n\n\nPackage purrr offers a large range of tools with a clean API. Base R offers lapply().\n\n\n\nWe shall first define a function that takes as arguments a datafame, a column name, and a title. We do not perform any defensive programming. Call your function foo.\nFunctional programmming makes code easier to understand.\nUse foo, lapply or purrr::map() to build the list of graphical objects.\nWith purrr::map(), you may use either a formula or an anonymous function. With lapply use an anonymous function.\nPackage patchwork offers functions for displaying collections of related plots."
  },
  {
    "objectID": "core/projects/fake-report.html",
    "href": "core/projects/fake-report.html",
    "title": "LAB: Fake report",
    "section": "",
    "text": "Motivation\n\n\nAmet commodo faucibus risus neque rutrum sodales ante eleifend inceptos est pharetra?\n\n\n\n\n200 countries, 50 years, 20 lines of code\n\n\nSit varius nibh ultricies convallis fames velit ut inceptos dapibus. Mauris sed nisi magnis odio duis cum vel quis sagittis nisi – posuere, nisi parturient mus lectus. Curae hac nascetur eu netus vel eleifend, et duis facilisis augue nibh vulputate. Cursus vitae fringilla magnis, fames potenti velit nunc fermentum risus. Enim ante egestas rutrum nisl posuere quisque quisque.\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.9811\n\n\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.1134\n\n\n\n\n\n\n\nAdipiscing nascetur, per sem blandit montes venenatis cubilia nam parturient ac. Curae ridiculus leo, morbi at donec penatibus parturient. Phasellus netus consequat cursus litora fames, augue congue nunc. Vehicula egestas consequat laoreet aenean – consequat pellentesque, ut dignissim dictum dis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatic with plotly\n\n\nSit risus sapien feugiat porttitor curae – ante metus volutpat sagittis. Posuere aliquam enim tempor ac venenatis feugiat, sagittis accumsan sociis facilisis? Laoreet suspendisse arcu tellus habitasse nullam eget nam ridiculus tristique! Viverra rhoncus aliquet!\n\n\n\n\n\n\n\n\n\n\nWizardry\n\n\nSit ultrices nisi proin dictumst ac primis augue egestas a. Odio ultricies sagittis sapien sollicitudin sem convallis tempor montes quisque! Hendrerit urna nunc pulvinar nostra ullamcorper convallis sem. Blandit habitasse cubilia nisi nisi neque habitant, mollis taciti vivamus fermentum ultricies.\n\n\n\n\n\n\n\n\n\n\nAppendix\n\n\nDolor nulla facilisis duis at varius sociis! Lectus magna aptent hac suspendisse tempus torquent tempor primis magnis ornare netus hac lacinia, fames turpis vulputate nisl, bibendum gravida commodo.\n\n\n\nneat_color_scale &lt;-\n      c(\"Africa\" = \"#01d4e5\",\n        \"Americas\" = \"#7dea01\" ,\n        \"Asia\" = \"#fc5173\",\n        \"Europe\" = \"#fde803\",\n        \"Oceania\" = \"#536227\")\na_year &lt;- sample(gapminder$year, 1)\n\np &lt;- gapminder |&gt;\n    filter(year==a_year) |&gt;\n    ggplot() +\n    aes(x = gdpPercap) +\n    aes(y = lifeExp) +\n    aes(size = pop) +\n    aes(text = country) +                   #\n    aes(fill = continent) +\n    aes(color= continent) +\n    aes(frame = year) +                     #\n    geom_point(alpha=.5) +\n    scale_x_log10() +\n    scale_size_area(max_size = 15,\n                    labels= scales::label_number(scale=1/1e6,\n                                                suffix=\" M\")) +\n    scale_fill_manual(values = neat_color_scale) +\n    labs(title= glue(\"Gapminder  {a_year}\"),\n        x = \"Yearly Income per Capita\",\n        y = \"Life Expectancy\",\n        caption=\"From sick  and poor (bottom left) to healthy and rich (top right)\")\n\np \n\n(p + theme(legend.position = \"none\")) |&gt; \n    plotly::ggplotly(height = 500, width=750)\n\n(p %+% \n    gapminder + \n    theme(legend.position = \"none\") +\n    ggtitle(\"Gapminder\")) |&gt; \n    plotly::ggplotly(height = 500, width=750)"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "MA7BY020 Syllabus",
    "section": "",
    "text": "Schedule\n\n\n\n\n\n\n\nDay\nTime\nLocation\nStart\n\n\n\n\nLab session 1\nWednesday\n13:13-15:00\nSomewhere\n2025-01-15\n\n\nLab session 2\nFriday\n9:00-10:30\nSophie Germain 2012\n2024-01-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrganization \n\n\n\nWe will have two weeky sessions of 1+1/2 hours each. Each session is organized around a Lab. We will switch from blackboard to laptop and back. You are invited to bring your laptop to the lab sessions.\n We will not attempt to complete the labs during the sessions. You are expected to complete the labs on your own time. Solutions (at least partial solutions) are available on the course website.\nYou can fork the course repository and post issues, comments, and corrections.\n\n\n\n\n\n\n\n\nObjectives\n\n\n\nThe objectives of course Introduction to Exploratory Data Analysis are to develop the ability to:\n\nperform data collection, tidying, and management,\ncompute statistics for summarizing data,\nvisualize the output of data analysis in the Grammar of Graphics framework,\nuse the R programming language for data analysis.\nbuild an R package,\nwrite reports, design presentations, and possibly build dashboards in the quarto document format.\n\n\n\n\n\n\n\n\n\nCommunication \n\n\n\nMaterial is available from s-v-b.github.io/MA7BY020\nMoodle\nSubscribe to Moodle portal\n\n\n\n\n\n\n\n\nSoftware \n\n\n\n\nR\nPosit\nrstudio\nquarto\nvs code\ngit\ndocker\npostgresql\n\n\n\n\n\n\n\n\n\nReferences \n\n\n\n\nBin Yu and Rebecca Barter, Veridical Data Science\nHadley Wickham, ggplot2: Elegant Graphics for Data Analysis\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund, R for Data Science\nHadley Wickham, Advanced R\nHadley Wickham and Jennifer Bryan., R packages\nHadley Wickham, Mastering Shiny\nkaggle\n\n\n\n\n\n\n\n\n\nCourse material \n\n\n\nSlides\nLabs are available (html and pdf)\nLabs with corrections are available\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 projects:\n\n\\(\\textsf{P}_1\\) Visualization\n\\(\\textsf{P}_2\\) Regression\n\\(\\textsf{P}_3\\) Packaging\n\nGrading\n\n\\[.2 \\textsf{P}_1 + .3 \\textsf{P}_2 + .5 \\textsf{P}_3\\]\n\n\n\n\n\n\n\n\nCode of conduct\n\n\n\nTL;DR: No cheating!\n\n\n\n\n\n\n\n\nSave the dates ! \n\n\n\nClick here for U Paris Cite Calendar.\nClick here for M1 MIDS Calendar\n\n\n\n\n\n\n\n\nUniversité Paris Cité\n\n\n\nUseful links:\n\nCharte Université Paris Cité\nDémarches et accessibilité\nCentre de contact\nRelais handicap",
    "crumbs": [
      "Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA7BY020: Exporatory Data Analysis",
    "section": "",
    "text": "Github\n  \n\n  \n  \n\n\nCourse MA7BY020: Exploratory Data Analysis is an introduction to the principles and practice of data exploration and visualization using the R programming language. Keeping an eye on Machine Learning and Data Science, the course will cover the following topics:\n\nTabular data manipulation\nData visualization\nUnivariate and bivariate analysis for qualitative and quantitative data\nMultivariate analysis starting with multiple linear\nMatrix methods based on Singular Value Decomposition: PCA, CA, CCA, …\nClustering methods\n\n\n\n\nAfter that course, you will be able to:\n\nHandle tabular data using R version of the relational algebra (dplyr)\nVisualize data using ggplot2 and plotly\nPerform univariate and bivariate analysis, compute and assess statistical summaries\nPerform multivariate analysis using matrix methods\nDiagnose and validate the results of multivariate analysis\nPerform, discuss and communicate the results of SVD factorization methods\nPerform, discuss and communicate the results of clustering methods\nCommunicate the results of the analysis in a clear and concise manner using Quarto reports, presentations, and dashboards\n\n\n\n\nThe course is based on the R programming language, the RStudio IDE, and the VS Code Ide. We will rely on the tidyverse package and attempt to take advantage of R tidy evaluation mechanisms to write expressive and efficient code.\nWe will use the quarto package for reproducible research.\n\n\n\n\n\nSyllabus\nTeaching team\nComputing environment\nSlides\nLabs\nLabs solutions\nProjects\nClass notes",
    "crumbs": [
      "Information",
      "Glimpse"
    ]
  },
  {
    "objectID": "labs-solutions-listings.html",
    "href": "labs-solutions-listings.html",
    "title": "Labs Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 15, 2025\n\n\nIntroduction and Visualization\n\n\nVisualization, Public Statistics\n\n\n\n\nJan 15, 2025\n\n\nBrush up your R\n\n\nR language, Tidyverse, IDE\n\n\n\n\nJan 22, 2025\n\n\nTable wranglig\n\n\nR language, dplyr, tabula data\n\n\n\n\nJan 29, 2025\n\n\nUnivariate categorical data\n\n\nUnivariate data, GSS\n\n\n\n\nJan 30, 2025\n\n\nUnivariate numeric data\n\n\nUnivariate data, GSS\n\n\n\n\nFeb 5, 2025\n\n\nBivariate data\n\n\nbivariate data, mosaicplots, scatterplots, simple linear regression\n\n\n\n\nFeb 19, 2025\n\n\nLinear regression I\n\n\nLinear regression, OLS, lm\n\n\n\n\nMar 12, 2025\n\n\nSVD and PCA\n\n\nSVD, PCA\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Solutions"
    ]
  },
  {
    "objectID": "projects-listings.html",
    "href": "projects-listings.html",
    "title": "Projects",
    "section": "",
    "text": "Note\n\n\n\nCourse evaluation is based on Projects \n\n Find a friend : all work done by pairs of students\n Create a single private GitHub repository for each project and each pair of students.\n Grant me access to these repositories\n All work is transmitted through your private repository and nowhere else\n No emails for project submission\n All projects deliverables consist of Quarto notebooks\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Description\n        \n         \n          Tags\n        \n         \n          Due date - Oldest\n        \n         \n          Due date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDue date\n\n\nTitle\n\n\nDescription\n\n\nTags\n\n\n\n\n\n\nFeb 7, 2025\n\n\nData wrangling and Visualization\n\n\nData extraction, wrangling, visualization, reproducible data science\n\n\nOECD, Visualization, Quarto, tidyverse, plotly\n\n\n\n\nMar 28, 2025\n\n\nTBA\n\n\nStatistical summaries, Regression, Diagnostics, Model comparison\n\n\nRegression, Diagnostics, Selection, Quarto, tidyverse, tidymodels\n\n\n\n\nMay 16, 2025\n\n\nTBA\n\n\nSVD, Clustering, Regression, Package management\n\n\nSpatial data, Package, Tidy evaluation, Quarto\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nEvaluation guidelines\n\n\n\nGrading criteria are given per project. They involve the following aspects:\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nNarrative, spelling and syntax\n\nEnglish/French \n\n\nPlots correction\n\nchoice of aesthetics, geom, scale … \n\n\nPlots style\n\nTitles, legends, labels, breaks … \n\n\nTable wrangling\n\nETL, SQL like manipulations \n\n\nComputing Statistics\n\nAggregations, LR, PCA, CA, … \n\n\nDRY compliance\n\nDRY principle at  Wikipedia\n\n\nReport organization\n\n\n\n\nCode organization",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "rstudio-client.html#project-options",
    "href": "rstudio-client.html#project-options",
    "title": "Rstudio Desktop",
    "section": "Project options",
    "text": "Project options",
    "crumbs": [
      "Support",
      "rstudio"
    ]
  },
  {
    "objectID": "weeks-listings.html",
    "href": "weeks-listings.html",
    "title": "Journal",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 15, 2025\n\n\nWeek 1\n\n\nVisualization, Public Statistics, R language, Tidyverse\n\n\n\n\nJan 19, 2025\n\n\nWeek 6\n\n\nLinear regression, OLS, lm, Model diagnostics\n\n\n\n\nJan 22, 2025\n\n\nWeek 2\n\n\nR language, Tidyverse, Table manipulations\n\n\n\n\nJan 29, 2025\n\n\nWeek 3\n\n\nUnivariate data\n\n\n\n\nFeb 5, 2025\n\n\nweek 4\n\n\nBivariate data\n\n\n\n\nMar 5, 2025\n\n\nWeek 7\n\n\nLinear regression, OLS, lm, Model diagnostics\n\n\n\n\nMar 12, 2025\n\n\nWeek 8\n\n\nPCA, SVD\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Journal"
    ]
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (163) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-2.html#labs",
    "href": "weeks/week-2.html#labs",
    "title": "Week 2",
    "section": " Labs",
    "text": "Labs\nWe revisit the first two labs (solutions )\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\n\nWe use Rstudio, and our dedicated R project (without git and renv). We install packages gt, nycflights and slider.\nWe review our first slide deck:\n\nIntroduction to dplyr\n\nand dig into\n\nLab 3 - Working with dplyr"
  },
  {
    "objectID": "weeks/week-2.html#further-work",
    "href": "weeks/week-2.html#further-work",
    "title": "Week 2",
    "section": " Further work",
    "text": "Further work\n Review the content of the three labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-2.html#further-reading",
    "href": "weeks/week-2.html#further-reading",
    "title": "Week 2",
    "section": " Further reading",
    "text": "Further reading\n\nR for data science\nAdvanced R"
  },
  {
    "objectID": "weeks/week-2.html#logistics",
    "href": "weeks/week-2.html#logistics",
    "title": "Week 2",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (RH10A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-4.html#blackboard",
    "href": "weeks/week-4.html#blackboard",
    "title": "Week 4",
    "section": " Blackboard",
    "text": "Blackboard\n\nCategorical samples\n\n2-ways Contingency tables\nChi-square statistics\n\nNumeric samples\n\nCovariance\nCorrelation(s)"
  },
  {
    "objectID": "weeks/week-4.html#labs",
    "href": "weeks/week-4.html#labs",
    "title": "Week 4",
    "section": " Labs",
    "text": "Labs\n\nLab 6 - Introduction to bivariate analysis\n\nWe relied on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables"
  },
  {
    "objectID": "weeks/week-4.html#further-work",
    "href": "weeks/week-4.html#further-work",
    "title": "Week 4",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-4.html#further-reading",
    "href": "weeks/week-4.html#further-reading",
    "title": "Week 4",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\nCours de Statistique Fondamentale\n\nChap Tests du chi-deux\n\nSourav Chatterjee: A new correlation coefficient"
  },
  {
    "objectID": "weeks/week-4.html#logistics",
    "href": "weeks/week-4.html#logistics",
    "title": "Week 4",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (RH10A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-6.html#blackboard",
    "href": "weeks/week-6.html#blackboard",
    "title": "Week 6",
    "section": " Blackboard",
    "text": "Blackboard\n\nOrdinary Least Squares\nQR factorizatio\nRidge regression (Regularized Least Squares)\nPseudo-Inversion"
  },
  {
    "objectID": "weeks/week-6.html#labs",
    "href": "weeks/week-6.html#labs",
    "title": "Week 6",
    "section": " Labs",
    "text": "Labs\n\nLab 6 - Linear Regression I\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables"
  },
  {
    "objectID": "weeks/week-6.html#further-work",
    "href": "weeks/week-6.html#further-work",
    "title": "Week 6",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-6.html#further-reading",
    "href": "weeks/week-6.html#further-reading",
    "title": "Week 6",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\nCours de Statistique Fondamentale\n\nChap Tests du chi-deux\nModèles linéaires\n\nSourav Chatterjee: A new correlation coefficient"
  },
  {
    "objectID": "weeks/week-6.html#logistics",
    "href": "weeks/week-6.html#logistics",
    "title": "Week 6",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (103A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar"
  },
  {
    "objectID": "weeks/week-8.html#blackboard",
    "href": "weeks/week-8.html#blackboard",
    "title": "Week 8",
    "section": " Blackboard",
    "text": "Blackboard\n\nSVD\n\nDefinition\nExistence\n\nMatrix norms\nSVD and low rank approximations\n\nEckart-Young-Mirsky theorem for operator norm\nEckart-Young-Mirsky theorem for Frobenius-Hilbert-Schmidt norm"
  },
  {
    "objectID": "weeks/week-8.html#labs",
    "href": "weeks/week-8.html#labs",
    "title": "Week 8",
    "section": " Labs",
    "text": "Labs\n\nLab 8 Principal Component Analysis as an application of SVD\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\nLab 6 - Linear Regression I\nLab 6 - Linear Regression II"
  },
  {
    "objectID": "weeks/week-8.html#further-work",
    "href": "weeks/week-8.html#further-work",
    "title": "Week 8",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints."
  },
  {
    "objectID": "weeks/week-8.html#further-reading",
    "href": "weeks/week-8.html#further-reading",
    "title": "Week 8",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes"
  },
  {
    "objectID": "weeks/week-8.html#logistics",
    "href": "weeks/week-8.html#logistics",
    "title": "Week 8",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "core/labs/lab-pca.html",
    "href": "core/labs/lab-pca.html",
    "title": "LAB: Principal Component Analysis",
    "section": "",
    "text": "Code\n# We will use the following packages. \n# If needed, install them : pak::pkg_install(). \nstopifnot(\n  require(\"magrittr\"),\n  require(\"lobstr\"),\n  require(\"ggforce\"),\n  require(\"gt\"),\n  require(\"glue\"),\n  require(\"skimr\"),\n  require(\"corrr\"),\n  require(\"patchwork\"), \n  require(\"tidyverse\")\n  # require(\"autoplotly\")\n)\nCode\nold_theme &lt;- theme_set(theme_minimal())"
  },
  {
    "objectID": "core/labs/lab-pca.html#perform-pca-on-covariates",
    "href": "core/labs/lab-pca.html#perform-pca-on-covariates",
    "title": "LAB: Principal Component Analysis",
    "section": "Perform PCA on covariates",
    "text": "Perform PCA on covariates\n\n\n\n\n\n\nQuestion\n\n\n\nPairwise analysis did not provide us with a clear and simple picture of the French-speaking districts.\nPCA (Principal Component Analysis) aims at exploring the variations of multivariate datasets around their mean (center of inertia). In the sequel, we will perform PCA on the matrix of centered covariates, with and without standardizing the centered columns.\nBase R offers prcomp(). Call prcomp() on the centered covariates\nNote that R also offers princomp\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCheck that prcomp() is indeed a wrapper for svd().\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCheck that rows and columns of component rotation of the result of prcomp() have unit norm.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCheck Orthogonality of \\(V\\) (component rotation of the prcomp object)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMake a scatterplot from the first two columns of the \\(x\\) component of the prcomp object.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDefine a graphical pipeline for the screeplot.\nHint: use function tidy() from broom, to get the data in the right form from an instance of prcomp.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDefine a function that replicates autoplot.prcomp()\nProject the dataset on the first two principal components (perform dimension reduction) and build a scatterplot. Colour the points according to the value of original covariates.\nHint: use generic function augment from broom.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nApply broom::tidy() with optional argument matrix=\"v\" or matrix=\"loadings\" to the prcomp object.\nComment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBuild the third SVD plot, the so called correlation circle.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCompute PCA after standardizing the columns, draw the correlation circle."
  },
  {
    "objectID": "core/labs/lab-pca.html#compare-standardized-and-non-standardized-pca",
    "href": "core/labs/lab-pca.html#compare-standardized-and-non-standardized-pca",
    "title": "LAB: Principal Component Analysis",
    "section": "Compare standardized and non-standardized PCA",
    "text": "Compare standardized and non-standardized PCA\n\n\n\n\n\n\nQuestion\n\n\n\nPay attention to the correlation circles.\n\nHow well are variables represented?\nWhich variables contribute to the first axis?\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the contrast between the two correlation circles.\n\n\nIn the sequel we focus on standardized PCA."
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html",
    "href": "core/labs-solutions/lab-pca.html",
    "title": "LAB: Principal Component Analysis",
    "section": "",
    "text": "Code\n# We will use the following packages. \n# If needed, install them : pak::pkg_install(). \nstopifnot(\n  require(\"magrittr\"),\n  require(\"lobstr\"),\n  require(\"ggforce\"),\n  require(\"gt\"),\n  require(\"glue\"),\n  require(\"skimr\"),\n  require(\"corrr\"),\n  require(\"patchwork\"), \n  require(\"tidyverse\")\n  # require(\"autoplotly\")\n)\nCode\nold_theme &lt;- theme_set(theme_minimal())"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#investigate-correlations",
    "href": "core/labs-solutions/lab-pca.html#investigate-correlations",
    "title": "LAB: Principal Component Analysis",
    "section": "Investigate correlations",
    "text": "Investigate correlations\n\n\n\n\n\n\nQuestion\n\n\n\n\nCompute, display and comment the sample correlation matrix\nDisplay jointplots for each pair of variables\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nPackage corrr, functions correlate and rplot provide a conveniemt tool.\nNote that rplot() creates a graphical object of class ggplot. We can endow it with more layers.\n\n\nCode\ncorrr::correlate(swiss) %&gt;% \n  corrr::rplot() %&gt;% +\n  ggtitle(\"Correlation plot for Swiss Fertility data\")\n\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\n\n\n\nThe high positive linear correlation between Education and Examination is moderately surprising. The negative correlation between the proportion of people involved in Agriculture and Education and Examinationis also not too surprising. Secondary schooling required pupils from rural areas to move to cities.\nA more intriguing observation concerns the pairs Catholic and Examination (negative correlation) and Catholic and Education (little correlation).\nThe response variable Fertility looks negatively correlated with Examination an Education. These correlations are worth being further explored. In Demography, the decline of Fertility is often associated with the the rise of women education. Note that Examination is about males, and that Education does not give details about the way women complete primary education."
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#perform-pca-on-covariates",
    "href": "core/labs-solutions/lab-pca.html#perform-pca-on-covariates",
    "title": "LAB: Principal Component Analysis",
    "section": "Perform PCA on covariates",
    "text": "Perform PCA on covariates\n\n\n\n\n\n\nQuestion\n\n\n\nPairwise analysis did not provide us with a clear and simple picture of the French-speaking districts.\nPCA (Principal Component Analysis) aims at exploring the variations of multivariate datasets around their mean (center of inertia). In the sequel, we will perform PCA on the matrix of centered covariates, with and without standardizing the centered columns.\nBase R offers prcomp(). Call prcomp() on the centered covariates\nNote that R also offers princomp\n\n\nWe first call prcomp() with the default arguments for centering and scaling, that is, we center columns and do not attempt to standardize columns. Name the output pco.\nWhat is the result made of?\n\n\n\n\n\n\nsolution\n\n\n\n\n\nCode\npco &lt;- swiss |&gt; \n  select(-Fertility) |&gt; \n  scale(scale = F) |&gt; \n  prcomp(scale. = F)\n\n\npco is a list with 5 members. It as a class attribute pco. It is an object of class prcomp (function prcomp() acts as a constructor for class pco just as lm() acts as a constructor for class lm). Class pco is an S3 class\n\n\nCode\nrlang::is_list(pco)\n\n\n[1] TRUE\n\n\nCode\nattributes(pco)\n\n\n$names\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n$class\n[1] \"prcomp\"\n\n\nCode\nsloop::s3_class(pco)\n\n\n[1] \"prcomp\"\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCheck that prcomp() is indeed a wrapper for svd().\n\n\n\n\n\n\n\n\nsolution\n\n\n\nWe first check that the matrix can be recovered from the product of the components of the prcomp object.\n\n\nCode\n(Y - pco$x %*% t(pco$rotation )) %&gt;% \n  round(digits = 2)  %&gt;% \n  head()\n\n\n             Agriculture Examination Education Catholic Infant.Mortality\nCourtelary             0           0         0        0                0\nDelemont               0           0         0        0                0\nFranches-Mnt           0           0         0        0                0\nMoutier                0           0         0        0                0\nNeuveville             0           0         0        0                0\nPorrentruy             0           0         0        0                0\n\n\nWe now check that the rotation component is indeed made of the right singular vectors (the \\(V\\) factor)\n\n\nCode\n(svd_Y$v %*% t(pco$rotation )) %&gt;% \n  round(2) \n\n\n     Agriculture Examination Education Catholic Infant.Mortality\n[1,]           1           0         0        0                0\n[2,]           0           1         0        0                0\n[3,]           0           0         1        0                0\n[4,]           0           0         0        1                0\n[5,]           0           0         0        0                1\n\n\nThe column vectors of component \\(x\\) are pairwise orthogonal.\n\n\nCode\n(t(pco$x) %*% pco$x) %&gt;% \n  round(2)  \n\n\n         PC1      PC2     PC3    PC4    PC5\nPC1 86484.49     0.00    0.00   0.00   0.00\nPC2     0.00 21127.44    0.00   0.00   0.00\nPC3     0.00     0.00 2706.14   0.00   0.00\nPC4     0.00     0.00    0.00 639.22   0.00\nPC5     0.00     0.00    0.00   0.00 348.01\n\n\nThe x component of the prcomp object is the product of the \\(U\\) and \\(D\\) factors.\n\n\nCode\nnorm(pco$x - svd_Y$u %*% diag(svd_Y$d),type=\"F\")\n\n\n[1] 1.962527e-13\n\n\nCode\nnorm(as.matrix(pco$rotation) -svd_Y$v, type=\"F\")\n\n\n[1] 2.04934e-15\n\n\nThe connection between \\(pco\\)sdev$ and \\(svd_Y\\)d$ is somewhat less transparent.\n\n\nCode\nsum(abs(apply(pco$x, 2, sd) - pco$sdev))\n\n\n[1] 1.865175e-14\n\n\nThe components of pco$sdev are the standard deviations of the columns of \\(U \\times D\\). \\[\\texttt{pco\\$sdev[i]}^2 = \\frac{\\texttt{svd_Y\\$d[i]}^2}{n-1}\\]\n\n\nCode\npco$sdev - svd_Y$d/sqrt(nrow(Y)-1)\n\n\n[1] -7.105427e-15  0.000000e+00  8.881784e-16 -1.332268e-15 -1.332268e-15\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCheck that rows and columns of component rotation of the result of prcomp() have unit norm.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nCode\napply(pco$rotation, 2, \\(x) norm(x, \"2\"))\n\n\nPC1 PC2 PC3 PC4 PC5 \n  1   1   1   1   1 \n\n\nCode\napply(pco$rotation, 1, \\(x) norm(x, \"2\"))\n\n\n     Agriculture      Examination        Education         Catholic \n               1                1                1                1 \nInfant.Mortality \n               1 \n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nChecking Orthogonality of \\(V\\) (component rotation of the prcomp object)\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nCode\n# checking that pco$rotation is an orthogonal matrix \nt(pco$rotation) %*% pco$rotation\n\n\n              PC1           PC2           PC3           PC4           PC5\nPC1  1.000000e+00 -1.003429e-16  8.239937e-18 -1.097213e-16  6.938894e-18\nPC2 -1.003429e-16  1.000000e+00  1.181780e-16  5.074066e-17  4.857226e-17\nPC3  8.239937e-18  1.181780e-16  1.000000e+00  1.717376e-16 -6.938894e-17\nPC4 -1.097213e-16  5.074066e-17  1.717376e-16  1.000000e+00 -1.804112e-16\nPC5  6.938894e-18  4.857226e-17 -6.938894e-17 -1.804112e-16  1.000000e+00\n\n\nCode\npco$rotation %*% t(pco$rotation)\n\n\n                   Agriculture   Examination     Education      Catholic\nAgriculture       1.000000e+00  3.642919e-17 -1.153591e-16  1.689187e-16\nExamination       3.642919e-17  1.000000e+00 -8.630249e-17  2.244298e-17\nEducation        -1.153591e-16 -8.630249e-17  1.000000e+00 -1.127570e-16\nCatholic          1.689187e-16  2.244298e-17 -1.127570e-16  1.000000e+00\nInfant.Mortality  2.081668e-17 -1.734723e-16  8.326673e-17 -2.081668e-17\n                 Infant.Mortality\nAgriculture          2.081668e-17\nExamination         -1.734723e-16\nEducation            8.326673e-17\nCatholic            -2.081668e-17\nInfant.Mortality     1.000000e+00\n\n\n\n\n:::\n\n\n\n\n\n\nQuestion\n\n\n\nMake a scatterplot from the first two columns of the \\(x\\) component of the prcomp object.\n\n\n\n\n\n\n\n\nsolution\n\n\n\nObjects of class prcomp can be handled by generic functions like plot() or better autoplot(). Namely, method prcomp for generic S3 function autoplot() from ggplot2 delivers one of classical SVD plots.\n\n\n\n\nCode\nres &lt;- autoplot(pco) +\n  coord_fixed()\n\nres\n\n\n\n\n\n\n\n\n\n\nautoplot(pco) is a scatterplot for the dataframe defined by matrix \\(U \\times D\\) projected on its first two principal components (first two columns).\n\n\nAs autoplot(pco) is an instance of class ggplot, it can be annotated, decorated as any other ggplot object.\n\n\nCode\n(\n  res + aes(color=Catholic) # +\n#    labs(subtitle=\"color='share of catholics'\")\n) +\n( \n  res + aes(color=Education) # +\n#    labs(subtitle=\"color='% education beyond primary school for draftees'\")\n) +\n  patchwork::plot_annotation(\n    subtitle = \"Scatterplot on the first two principal components (no column scaling)\",\n    title= \"Share of catholics can almost be determined from the sign of the first PC\",\n    caption = \"Swiss  Fertility data from R datasets\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDefine a graphical pipeline for the screeplot.\nHint: use function tidy() from broom, to get the data in the right form from an instance of prcomp.\n\n\n\n\n\n\n\n\nsolution\n\n\n\nThe screeplot is a bar plot where each bar corresponds to a singular value. The bar height is proportional to the square of the corresponding singular value.\n\n\nCode\n1p_screeplot &lt;- . %&gt;%\n  broom::tidy(matrix=\"pcs\") %&gt;% { \n  ggplot(.) +\n3  aes(x=PC, y=percent, label=pct_format(1.-cumulative)) +\n  geom_text(angle=45, vjust=-1, hjust=-.1) + \n2  geom_col(fill=NA, colour=\"black\")\n  }\n\n\n\n1\n\nDefine a pipeline for building a screeplot\n\n2\n\nMind the braces on the right side of the first pipe\n\n3\n\n1- percent tell the reader about the relative Frobenious error achieved by keeping the first components of the SVD expansion.\n\n\n\n\n\n\nCode\npco %&gt;% \n  p_screeplot() +\n  ylab('Relative squared Frobenius error/Relative squared error') +\n  labs(\n    title=\"Screeplot for swiss fertility data\",\n    subtitle=\"Keeping the first two components is enough to achieve relative Froebenius relative error 3.3%\")\n\n\n\n\n\n\n\n\n\nThe screeplot is a visualization of the Eckart-Young-Mirsky Theorem. It tells us about the relative errors incurent when approximating the data matrix (with centered columns) by the low rank approximations defined by the truncated SVDs.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDefine a function that replicates autoplot.prcomp()\nProject the dataset on the first two principal components (perform dimension reduction) and build a scatterplot. Colour the points according to the value of original covariates.\nHint: use generic function augment from broom.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nCode\np &lt;-  pco %&gt;%\n  broom::augment(swiss) %&gt;% \n  ggplot() +\n  aes(x=.fittedPC1, y=.fittedPC2, label=.rownames) +\n  geom_point() +\n  coord_fixed() +\n  ggrepel::geom_text_repel() \n\n(p + \n  aes(color=Infant.Mortality)) +\n(p + \n   aes(color=Education)) +\n(p + \n   aes(color=Examination)) +\n(p + \n   aes(color=Catholic)) +\n(p + \n   aes(color=Agriculture)) +\n(p + \n   aes(color=Fertility)) +  \nplot_layout(ncol = 2) +\nplot_annotation(title=\"Swiss data on first two PCs\" , \n                subtitle = \"centered, unscaled\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nApply broom::tidy() with optional argument matrix=\"v\" or matrix=\"loadings\" to the prcomp object.\nComment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nWe can extract factor \\(V\\) from the SVD factorization using generic function tidy from package broom\n\n\nCode\npco %&gt;% \n  broom::tidy(matrix=\"v\") %&gt;% \n  sample_n(5) |&gt;\n  gt::gt()\n\n\n\n\n\n\n\n\ncolumn\nPC\nvalue\n\n\n\n\nEducation\n2\n0.31078077\n\n\nInfant.Mortality\n4\n-0.08667759\n\n\nInfant.Mortality\n1\n0.01047382\n\n\nCatholic\n3\n0.00165556\n\n\nInfant.Mortality\n5\n-0.99110977\n\n\n\n\n\n\n\nThe result is a tibble in long form. It is worth pivoting the dataframe into wide form. This gives back the rotation matrix.\n\n\nCode\nom &lt;- pco %&gt;% \n  broom::tidy(matrix=\"v\") %&gt;% \n  tidyr::pivot_wider(id_cols =column, \n              names_from = PC, \n              values_from = value) |&gt; \n  select(-1) |&gt;\n  as.matrix()\n\nnorm((om %*% t(om))-diag(1,5), \"F\")        \n\n\n[1] 8.196585e-16\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBuild the third SVD plot, the so called correlation circle.\n\n\n\n\n\n\n\n\nsolution\n\n\n\nThe correlation circle is built from the loadings, that is, from the rotation component of the prcomp object.\nWe define a preprocessing function to transform the rotation object into a proper tibble form.\n\n\nCode\nprep_co_circle &lt;- function(pco) {\n  r &lt;- pco$rotation\n  as_tibble(r) |&gt; \n    rename_with(.fn = \\(x) gsub('PC', '', x), .cols=everything()) |&gt;\n    mutate(row_id=rownames(r))\n}\n\n\nThe The next virtual graphical object will be our key tool to build the correlation circle.\n\n\nCode\nco_circle_ppl &lt;-  (\n    pco %&gt;% \n    prep_co_circle() %&gt;% \n    filter(F)\n    ) %&gt;% \n  ggplot() +\n  aes(x=`1`, y=`2`, label=row_id) +\n  geom_segment(aes(xend=0, yend=0), arrow = grid::arrow(ends = \"first\")) +\n  ggrepel::geom_text_repel() +\n1  coord_fixed() +\n  xlim(c(-1.1, 1.1)) + ylim(c(-1.1, 1.1))  +\n  ggforce::geom_circle(aes(x0=0, y0=0, r=1), linetype=\"dashed\") \n\n\n\n1\n\nimportant\n\n\n\n\n\n\n\n\nCode\nco_circle_ppl %+% (\n  pco %&gt;% \n  prep_co_circle()\n  )  +\n  labs(title=\"Correlation circle\", \n          subtitle = \"centered, unscaled\",\n          caption= \"Swiss Fertility dataset\")\n\n\n\n\n\n\n\n\n\n\nThe length of each arrow is the length of the projection of the corresponding column of the data matrix over the plane generated by the first two rescaled left singular vectors (rescaling by the reciprocal of the singular values).\nThe first two principal componants (left singular vectors) are highly correlated with columns Agriculture and Catholic.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCompute PCA after standardizing the columns, draw the correlation circle.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\n\nCode\npco2 &lt;- select(swiss, -Fertility) |&gt; \n prcomp(scale. = T)\n\nco_circle_ppl %+% (\n  pco2 %&gt;% \n  prep_co_circle()\n  )  +\n  labs(\n    title=\"Correlation circle\", \n    subtitle = \"centered, scaled\",\n    caption=\"Swiss fertility dataset\")\n\n\n\n\n\n\n\n\n\n\nScaling columns seriously modify the correlation circle."
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#sanity-checks",
    "href": "core/labs-solutions/lab-pca.html#sanity-checks",
    "title": "LAB: Principal Component Analysis",
    "section": "Sanity checks",
    "text": "Sanity checks\nIn the sequel, \\(X\\) denotes the data matrix after column centering (use scale(., center=T, scale-F))\n\n\n\n\n\n\nsolution\n\n\n\n\n\nCode\nX &lt;-  as.matrix(select(swiss, -Fertility)) |&gt; \n  scale(center = T, scale=F)\n\n# check centering, spot the difference in variances \nX |&gt;  \n  as_tibble() |&gt; \n  summarise(across(everything(), c(var, mean)))\n\n\n# A tibble: 1 × 10\n  Agriculture_1 Agriculture_2 Examination_1 Examination_2 Education_1\n          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1          516.      2.64e-15          63.6     -1.51e-16        92.5\n# ℹ 5 more variables: Education_2 &lt;dbl&gt;, Catholic_1 &lt;dbl&gt;, Catholic_2 &lt;dbl&gt;,\n#   Infant.Mortality_1 &lt;dbl&gt;, Infant.Mortality_2 &lt;dbl&gt;\n\n\nCode\n# should be 0\nnorm(X  %*% pco$rotation - pco$x)\n\n\n[1] 9.692247e-14\n\n\nCode\n# check the left singular vectors\npco$x %*% diag((pco$sdev)^(-1)) |&gt; \n  as_tibble() |&gt; \n  summarise(across(everything(), c(mean,var)))\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n\n# A tibble: 1 × 10\n      V1_1  V1_2     V2_1  V2_2     V3_1  V3_2      V4_1  V4_2      V5_1  V5_2\n     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 3.72e-17  1.00 4.84e-19     1 4.65e-17  1.00 -7.47e-17     1 -1.73e-17     1\n\n\nCode\n# \n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCheck that rows and columns of component rotation of the result of prcomp() have unit norm.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nCode\napply(pco$rotation, 2, \\(x) norm(x, \"2\"))\n\n\nPC1 PC2 PC3 PC4 PC5 \n  1   1   1   1   1 \n\n\nCode\napply(pco$rotation, 1, \\(x) norm(x, \"2\"))\n\n\n     Agriculture      Examination        Education         Catholic \n               1                1                1                1 \nInfant.Mortality \n               1 \n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nChecking Orthogonality of \\(V\\) (component rotation of the prcomp object)\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nCode\n# checking that pco$rotation is an orthogonal matrix \nt(pco$rotation) %*% pco$rotation\n\n\n              PC1           PC2           PC3           PC4           PC5\nPC1  1.000000e+00 -1.003429e-16  8.239937e-18 -1.097213e-16  6.938894e-18\nPC2 -1.003429e-16  1.000000e+00  1.181780e-16  5.074066e-17  4.857226e-17\nPC3  8.239937e-18  1.181780e-16  1.000000e+00  1.717376e-16 -6.938894e-17\nPC4 -1.097213e-16  5.074066e-17  1.717376e-16  1.000000e+00 -1.804112e-16\nPC5  6.938894e-18  4.857226e-17 -6.938894e-17 -1.804112e-16  1.000000e+00\n\n\nCode\npco$rotation %*% t(pco$rotation)\n\n\n                   Agriculture   Examination     Education      Catholic\nAgriculture       1.000000e+00  3.642919e-17 -1.153591e-16  1.689187e-16\nExamination       3.642919e-17  1.000000e+00 -8.630249e-17  2.244298e-17\nEducation        -1.153591e-16 -8.630249e-17  1.000000e+00 -1.127570e-16\nCatholic          1.689187e-16  2.244298e-17 -1.127570e-16  1.000000e+00\nInfant.Mortality  2.081668e-17 -1.734723e-16  8.326673e-17 -2.081668e-17\n                 Infant.Mortality\nAgriculture          2.081668e-17\nExamination         -1.734723e-16\nEducation            8.326673e-17\nCatholic            -2.081668e-17\nInfant.Mortality     1.000000e+00"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#compare-standardized-and-non-standardized-pca",
    "href": "core/labs-solutions/lab-pca.html#compare-standardized-and-non-standardized-pca",
    "title": "LAB: Principal Component Analysis",
    "section": "Compare standardized and non-standardized PCA",
    "text": "Compare standardized and non-standardized PCA\n\n\n\n\n\n\nQuestion\n\n\n\nPay attention to the correlation circles.\n\nHow well are variables represented?\nWhich variables contribute to the first axis?\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nCode\npco_c &lt;- swiss %&gt;% \n  select(-Fertility) %&gt;% \n  prcomp()\n\npco_cs &lt;- swiss %&gt;% \n  select(-Fertility) %&gt;% \n  prcomp(scale.=T, center=T)\n\n\n\n\nCode\n(\n  co_circle_ppl %+% \n  prep_co_circle(pco_c)  +\n  labs(\n    subtitle = \"centered, unscaled\"\n  )\n) +\n(\n  co_circle_ppl %+%  \n  prep_co_circle(pco_cs) +\n  labs(\n    subtitle = \"centered, scaled\")\n) +\n  patchwork::plot_annotation(\n        title=\"Swiss, correlation circle\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the contrast between the two correlation circles.\n\n\nIn the sequel we focus on standardized PCA.\n\n\n\n\n\n\nsolution\n\n\n\n\n\nCode\nq &lt;-  autoplot(pco_cs, data=swiss) \n\n(q + \n  aes(color=Infant.Mortality)) +\n(q + \n   aes(color=Education)) +\n(q + \n   aes(color=Examination)) +\n(q + \n   aes(color=Catholic)) +\n(q + \n   aes(color=Agriculture)) +\n(q + \n   aes(color=Fertility)) +  \npatchwork::plot_layout(ncol = 2) +\npatchwork::plot_annotation(\n    title=\"Scatterplot on first two PCs\", \n    subtitle = \"centered, scaled PCA\",\n    caption = \"Swiss Fertility dataset\")"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#provide-an-interpretation-of-the-first-two-principal-axes",
    "href": "core/labs-solutions/lab-pca.html#provide-an-interpretation-of-the-first-two-principal-axes",
    "title": "LAB: Principal Component Analysis",
    "section": "Provide an interpretation of the first two principal axes",
    "text": "Provide an interpretation of the first two principal axes\n\n\n\n\n\n\nQuestion\n\n\n\nWhich variables contribute to the two first principal axes?\n\n\n\n\n\n\n\n\nsolution\n\n\n\nThis comes from the correlation circle. We rely on function prep_co_circle and on the graphical pipeline co_circle_ppl.\n\n\nCode\n(\n  co_circle_ppl %+% \n    prep_co_circle(pco_cs) +\n    ggtitle(\"Swiss, correlation circle\", \n            subtitle = \"centered, scaled\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAnalyze the signs of correlations between variables and axes?\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nCode\nswiss |&gt;   # ggrepel::geom_text_repel(data=df_cocirc, \n  #                          aes(x= 4* `1`,\n  #                              y= 4 * `2`, \n  #                              label=column), \n  #                          color=\"red\")\n  select(-Fertility) |&gt; \n  corrr::correlate() |&gt; \n  corrr::shave() |&gt; \n  corrr::rplot(print_cor = T) +\n  theme_minimal()\n\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#add-the-fertility-variable",
    "href": "core/labs-solutions/lab-pca.html#add-the-fertility-variable",
    "title": "LAB: Principal Component Analysis",
    "section": "Add the Fertility variable",
    "text": "Add the Fertility variable\n\n\n\n\n\n\nQuestion\n\n\n\nPlot again the correlation circle using the same principal axes as before, but add the Fertility variable.\nHow does Fertility relate with covariates? with principal axes?\n\n\n\n\n\n\n\n\nsolution\n\n\n\nWe use \\[D^{-1} \\times U^\\top \\times X = V^\\top\\]\nIt is enough to multipliy the data matrix by \\(D^{-1} \\times U^\\top\\) and to pipe the result into the coorelation circle graphical pipeline.\n\n\nCode\nfoo &lt;- t(diag(svd_Y$d^(-1)) %*% t(svd_Y$u) %*% as.matrix(scale(swiss, scale=F)))  \n \nco_circle_ppl  %+% (\n  as_tibble(foo) |&gt;\n  rename_with(.fn = \\(x) gsub('V', '', x), .cols=everything()) |&gt;\n  mutate(row_id=rownames(foo))\n)\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`."
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#display-individuals-districts",
    "href": "core/labs-solutions/lab-pca.html#display-individuals-districts",
    "title": "LAB: Principal Component Analysis",
    "section": "Display individuals (districts)",
    "text": "Display individuals (districts)\n\n\n\n\n\n\nQuestion"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#comment",
    "href": "core/labs-solutions/lab-pca.html#comment",
    "title": "LAB: Principal Component Analysis",
    "section": "Comment",
    "text": "Comment"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#biplot",
    "href": "core/labs-solutions/lab-pca.html#biplot",
    "title": "LAB: Principal Component Analysis",
    "section": "Biplot",
    "text": "Biplot\n\n\n\n\n\n\nQuestion\n\n\n\nThe last svd plot (biplot) consists of overlaying the scatter plot of component x of the prcomp object and the correlation circle.\nSo the biplot is a graphical object built on two dataframes derived on components x and rotation of the prcomp objects.\nDesign a graphical pipeline.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nCode\npco &lt;- swiss %&gt;% \n  select(-Fertility) %&gt;% \n  prcomp(scale.=T)\n\ndf_cocirc &lt;- pco %&gt;% \n  broom::tidy(matrix=\"v\") %&gt;% \n  tidyr::pivot_wider(id_cols =column, \n              names_from = PC, \n              values_from = value) \n\nbroom::augment(pco, data=swiss) %&gt;% \n  ggplot() + \n  geom_point(aes(x=.fittedPC1, \n                 y=.fittedPC2, \n                 color=Fertility, label=.rownames)) +\n  coord_fixed() + \n  ggrepel::geom_text_repel(aes(x=.fittedPC1, \n                               y=.fittedPC2,\n                               color=Infant.Mortality,\n                               label=.rownames)) + \n  geom_segment(data=df_cocirc,  \n               mapping=aes(x= 4* `1`, \n                           y= 4 * `2`, \n                           linetype=factor(column),\n                           label=column,\n                           xend=0, \n                           yend=0), \n               arrow = grid::arrow(ends = \"first\",\n                                    unit(.1, \"inches\")\n                                  )) + \n  scale_color_viridis_c() +\n  xlim(c(-5,5)) + \n  ylim(c(-5,5)) \n\n\nWarning in geom_point(aes(x = .fittedPC1, y = .fittedPC2, color = Fertility, :\nIgnoring unknown aesthetics: label\n\n\nWarning in geom_segment(data = df_cocirc, mapping = aes(x = 4 * `1`, y = 4 * :\nIgnoring unknown aesthetics: label\n\n\nWarning: ggrepel: 21 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nautoplot.prcomp() has optional arguments. If set to True, logical argument loadings overlays the scatterplot defined by the principal components with the correlation circle.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nCode\nbip &lt;- autoplot(pco_cs, \n         data=swiss, \n         color=\"Fertility\", \n         loadings = TRUE, \n         loadings.colour = 'blue',\n         loadings.label = TRUE) +\n  coord_fixed() +\n  labs(\n    title = \"Biplot\",\n    subtitle = \"PCA after centering and scaling\",\n    caption = \"Swiss Fertility dataset\"\n  )\n\nbip\n\n\n\n\n\n\n\n\n\n\n\nCode\nbip |&gt; plotly::ggplotly()"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#swiss-fertility-data",
    "href": "core/labs-solutions/lab-pca.html#swiss-fertility-data",
    "title": "LAB: Principal Component Analysis",
    "section": "Swiss fertility data",
    "text": "Swiss fertility data\nDataset swiss from datasets::swiss connect fertility and social, economic data within 47 French-speaking districts in Switzerland.\n\nFertility : fertility index\nAgriculture : jobs in agricultural sector\nExamination : literacy index (military examination)\nEducation : proportion of people with successful secondary education\nCatholic : proportion of Catholics\nInfant.Mortality : mortality quotient at age 0\n\nFertility index (Fertility) is considered as the response variable\nThe social and economic variables are covariates (explanatory variables).\nSee European Fertility Project for more on this dataset.\n\nPCA (Principal Component Analysis) is concerned with covariates.\n\n\nCode\ndata(\"swiss\")\n\nswiss %&gt;% \n  glimpse(50)\n\n\nRows: 47\nColumns: 6\n$ Fertility        &lt;dbl&gt; 80.2, 83.1, 92.5, 85.8,…\n$ Agriculture      &lt;dbl&gt; 17.0, 45.1, 39.7, 36.5,…\n$ Examination      &lt;int&gt; 15, 6, 5, 12, 17, 9, 16…\n$ Education        &lt;int&gt; 12, 9, 5, 7, 15, 7, 7, …\n$ Catholic         &lt;dbl&gt; 9.96, 84.84, 93.40, 33.…\n$ Infant.Mortality &lt;dbl&gt; 22.2, 22.2, 20.2, 20.3,…\n\n\nHave a look at the documentation of the dataset"
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#describe-the-dataset",
    "href": "core/labs-solutions/lab-pca.html#describe-the-dataset",
    "title": "LAB: Principal Component Analysis",
    "section": "Describe the dataset",
    "text": "Describe the dataset\n\n\n\n\n\n\nQuestion\n\n\n\nCompute summary for each variable\n\n\n\n\n\n\n\n\nsolution\n\n\n\nIt is enough to call summary() on each column of swiss. This can be done in a functional programming style using package purrr. The collections of summaries can be rearranged so as to build a dataframe that is fit for reporting.\n\n\nCode\ntt &lt;- map_dfr(swiss, summary, .id = \"var\")  \n\n\n\n\nCode\ntt |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=1)\n\n\n\n\n\n\n\n\nvar\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n\n\n\n\nFertility\n35.00\n64.700\n70.40\n70.14255\n78.450\n92.5\n\n\nAgriculture\n1.20\n35.900\n54.10\n50.65957\n67.650\n89.7\n\n\nExamination\n3.00\n12.000\n16.00\n16.48936\n22.000\n37.0\n\n\nEducation\n1.00\n6.000\n8.00\n10.97872\n12.000\n53.0\n\n\nCatholic\n2.15\n5.195\n15.14\n41.14383\n93.125\n100.0\n\n\nInfant.Mortality\n10.80\n18.150\n20.00\n19.94255\n21.700\n26.6\n\n\n\n\n\n\n\nFunction skim from skimr delivers all univariate summaries in suitable form.\n\n\nCode\nfoo &lt;- swiss %&gt;% \n  select(-Fertility) %&gt;% \n  skim()  \n\n\n\n\nCode\nfoobar &lt;- foo %&gt;%  \n  filter(skim_type==\"numeric\") %&gt;% \n  rename(variable=skim_variable)  %&gt;% \n    mutate(across(where(is.numeric), ~ round(.x, digits=1))) \n\n\n\n\nCode\nfoobar %&gt;% \n  gt::gt() \n\n\n\n\n\n\n\n\nskim_type\nvariable\nn_missing\ncomplete_rate\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nnumeric\nAgriculture\n0\n1\n50.7\n22.7\n1.2\n35.9\n54.1\n67.7\n89.7\n▃▃▆▇▅\n\n\nnumeric\nExamination\n0\n1\n16.5\n8.0\n3.0\n12.0\n16.0\n22.0\n37.0\n▅▇▆▂▂\n\n\nnumeric\nEducation\n0\n1\n11.0\n9.6\n1.0\n6.0\n8.0\n12.0\n53.0\n▇▃▁▁▁\n\n\nnumeric\nCatholic\n0\n1\n41.1\n41.7\n2.1\n5.2\n15.1\n93.1\n100.0\n▇▁▁▁▅\n\n\nnumeric\nInfant.Mortality\n0\n1\n19.9\n2.9\n10.8\n18.1\n20.0\n21.7\n26.6\n▁▂▇▆▂\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDisplay graphic summary for each variable.\n\n\n\n\n\n\n\n\nsolution\n\n\n\nWe have to pick some graphical summary of the data. Boxplots and violine plots could be used if we look for concision.\nWe use histograms to get more details about each column.\nNot that covariates have different meanings: Agriculture, Catholic, Examination, and Education are percentages with values between \\(0\\) and \\(100\\).\nWe have no details about the standardized fertility index Fertility\nInfant.Mortality is also a rate:\n\nInfant mortality is the death of an infant before his or her first birthday. The infant mortality rate is the number of infant deaths for every 1,000 live births. In addition to giving us key information about maternal and infant health, the infant mortality rate is an important marker of the overall health of a society.\n\nsee Center for Desease Control\nWe reuse the function we have already developped during previous sessions.\n\n\nCode\nmake_biotifoul(swiss, .f = is.numeric)\n\n\n\n\n\n\n\n\n\nHistograms reveal that our covariates have very different distributions.\nReligious affiliation (Catholic) tells us that there two types of districts, which is reminiscent of the old principle Cujus regio, ejus religio , see Old Swiss Confederacy.\nAgriculture shows that in most districts, agriculture was still a very important activity.\nEducation reveals that in all but a few districts, most children did not receive secondary education. Examination shows that some districts lag behind the bulk of districts. Even less exhibit a superior performance.\nThe two demographic variables Fertility and Infant.Mortality look roughly unimodal with a few extreme districts."
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#investigate-pairwise-correlations",
    "href": "core/labs-solutions/lab-pca.html#investigate-pairwise-correlations",
    "title": "LAB: Principal Component Analysis",
    "section": "Investigate pairwise correlations",
    "text": "Investigate pairwise correlations\n\n\n\n\n\n\nQuestion\n\n\n\n\nCompute, display and comment the sample correlation matrix\nDisplay jointplots for each pair of variables\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nPackage corrr, functions correlate and rplot provide a convenient tool.\nNote that corrr::rplot() creates a graphical object of class ggplot. We can endow it with more layers.\n\n\nCode\ncorrr::correlate(swiss) |&gt; \n  corrr::rplot() + \n  labs(title=\"Correlation plot for Swiss Fertility data\")\n\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\n\n\n\nThe high positive linear correlation between Education and Examination is moderately surprising. The negative correlation between the proportion of people involved in Agriculture and Education and Examinationis also not too surprising. Secondary schooling required pupils from rural areas to move to cities.\nA more intriguing observation concerns the pairs Catholic and Examination (negative correlation) and Catholic and Education (little correlation).\nThe response variable Fertility looks negatively correlated with Examination an Education. These correlations are worth being further explored. In Demography, the decline of Fertility is often associated with the the rise of women education. Note that Examination is about males, and that Education does not give details about the way women complete primary education."
  },
  {
    "objectID": "core/labs-solutions/lab-pca.html#singular-value-decomposition-svd",
    "href": "core/labs-solutions/lab-pca.html#singular-value-decomposition-svd",
    "title": "LAB: Principal Component Analysis",
    "section": "Singular Value Decomposition (SVD)",
    "text": "Singular Value Decomposition (SVD)\n\n\n\n\n\n\nQuestion\n\n\n\n\nProject the swiss dataset on the covariates (all columns but Fertility)\nCenter the projected data using matrix manipulation\nCenter the projected data using dplyr verbs\nCompare the results with the output of scale() with various optional arguments\nCall the centered matrix Y\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nHand-made centering of the dataframe emphasises the fact that centering is a linear operation. As a matter of fact, it consists in projecting the data frame on the linear space orthogonal to the constant vector.\n\n\nCode\nX &lt;- select(swiss, -Fertility) |&gt; \n    as.matrix()\n\nn &lt;- nrow(X)\nones &lt;-  matrix(1, nrow = n, ncol=1) \n\nY &lt;-  X - (1/n)* (ones  %*%  t(ones) %*% X) \n\n\nWe can also perform centering using dplyr verbs. This can be viewed as computing a window function over a trivial partition.\n\n\nCode\nswiss |&gt; \n  select(-Fertility) |&gt;\n  mutate(across(everything(), \\(x) x-mean(x)))  \n\n\n\n\n\n\n\n\n\n\nAnyway, function scale(X, scale=F) from base R does the job.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCheck that the ouput of svd(Y) actually defines a Singular Value Decomposition.\n\n\n\n\n\n\n\n\nsolution\n\n\n\nsvd(Y) is a list with \\(3\\) elements (u,d,v).\n\\[\nY = U \\times D \\times V^\\top\n\\]\n\n\nCode\nsvd_Y &lt;-  svd(Y)\n\n1svd_Y %$%\n  (Y - u %*% diag(d) %*% t(v)) %&gt;% \n2  norm(type = \"F\")\n\nnorm( \n  diag(1, ncol(Y)) - \n  (svd_Y %$% (t(v) %*% v)), \n  'F'\n)  # &lt;3&gt;. \n\n\n\n1\n\nExposing pipe from magrittr\n\n2\n\nChecking the factorization\n\n\n\n\n[1] 1.04354e-13\n[1] 1.137847e-15\n\n\n\n\n\n\n\n\n\n\nNote that we used the exposing pipe %$% from magrittr to unpack svd_Y which is a list with class svd and members named u, d and v.\nWe could have used with(,) from base R.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nRelate the SVD of \\(Y\\) and the eigen decomposition of \\(Y^\\top \\times Y\\)\n\n\n\n\n\n\n\n\nsolution\n\n\n\nThe matrix \\(1/n Y^\\top \\times Y\\) is the covariance matrix of the covariates.\nThe spectral decomposition of the symmetric Semi Definite Positive (SDP) matrix \\(1/n Y^\\top \\times Y\\) is related with the SVD factorization of \\(Y\\).\nThe spectral/eigen decomposition of \\(Y^\\top \\times Y\\) can be obtained using eigen().\nThe eigenspaces of \\(Y^\\top \\times Y\\) are the right eigenspaces of \\(Y\\).\n\n\nCode\n(t(eigen(t(Y) %*% Y )$vectors) %*% svd_Y$v ) %&gt;% \n  round(digits=2)\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    0    0    0\n[2,]    0   -1    0    0    0\n[3,]    0    0    1    0    0\n[4,]    0    0    0    1    0\n[5,]    0    0    0    0    1\n\n\nThe eigenvalues of \\(Y^\\top \\times Y\\) are the squared singular values of \\(Y\\)\n\n\nCode\neigen(t(Y) %*% Y )$values - (svd_Y$d)^2\n\n\n[1] -1.309672e-10 -1.455192e-11  8.640200e-12  8.526513e-12 -3.410605e-13\n\n\n\n\n\n\n\n\n\n\nHere, the eigenvectors of \\(Y^\\top \\times Y\\) coincide with the right singular vectors of \\(Y\\) corresponding to non-zero singular values. Up to sign changes, it is always true when the non-zero singular values are pairwise distinct."
  },
  {
    "objectID": "core/labs/lab-pca.html#swiss-fertility-data",
    "href": "core/labs/lab-pca.html#swiss-fertility-data",
    "title": "LAB: Principal Component Analysis",
    "section": "Swiss fertility data",
    "text": "Swiss fertility data\nDataset swiss from datasets::swiss connect fertility and social, economic data within 47 French-speaking districts in Switzerland.\n\nFertility : fertility index\nAgriculture : jobs in agricultural sector\nExamination : literacy index (military examination)\nEducation : proportion of people with successful secondary education\nCatholic : proportion of Catholics\nInfant.Mortality : mortality quotient at age 0\n\nFertility index (Fertility) is considered as the response variable\nThe social and economic variables are covariates (explanatory variables).\nSee European Fertility Project for more on this dataset.\n\nPCA (Principal Component Analysis) is concerned with covariates.\n\n\nCode\ndata(\"swiss\")\n\nswiss %&gt;% \n  glimpse(50)\n\n\nRows: 47\nColumns: 6\n$ Fertility        &lt;dbl&gt; 80.2, 83.1, 92.5, 85.8,…\n$ Agriculture      &lt;dbl&gt; 17.0, 45.1, 39.7, 36.5,…\n$ Examination      &lt;int&gt; 15, 6, 5, 12, 17, 9, 16…\n$ Education        &lt;int&gt; 12, 9, 5, 7, 15, 7, 7, …\n$ Catholic         &lt;dbl&gt; 9.96, 84.84, 93.40, 33.…\n$ Infant.Mortality &lt;dbl&gt; 22.2, 22.2, 20.2, 20.3,…\n\n\nHave a look at the documentation of the dataset"
  },
  {
    "objectID": "core/labs/lab-pca.html#describe-the-dataset",
    "href": "core/labs/lab-pca.html#describe-the-dataset",
    "title": "LAB: Principal Component Analysis",
    "section": "Describe the dataset",
    "text": "Describe the dataset\n\n\n\n\n\n\nQuestion\n\n\n\nCompute summary for each variable\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDisplay graphic summary for each variable."
  },
  {
    "objectID": "core/labs/lab-pca.html#investigate-pairwise-correlations",
    "href": "core/labs/lab-pca.html#investigate-pairwise-correlations",
    "title": "LAB: Principal Component Analysis",
    "section": "Investigate pairwise correlations",
    "text": "Investigate pairwise correlations\n\n\n\n\n\n\nQuestion\n\n\n\n\nCompute, display and comment the sample correlation matrix\nDisplay jointplots for each pair of variables"
  },
  {
    "objectID": "core/labs/lab-pca.html#singular-value-decomposition-svd",
    "href": "core/labs/lab-pca.html#singular-value-decomposition-svd",
    "title": "LAB: Principal Component Analysis",
    "section": "Singular Value Decomposition (SVD)",
    "text": "Singular Value Decomposition (SVD)\n\n\n\n\n\n\nQuestion\n\n\n\n\nProject the swiss dataset on the covariates (all columns but Fertility)\nCenter the projected data using matrix manipulation\nCenter the projected data using dplyr verbs\nCompare the results with the output of scale() with various optional arguments\nCall the centered matrix Y\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCheck that the ouput of svd(Y) actually defines a Singular Value Decomposition.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nRelate the SVD of \\(Y\\) and the eigen decomposition of \\(Y^\\top \\times Y\\)"
  },
  {
    "objectID": "core/labs/lab-pca.html#provide-an-interpretation-of-the-first-two-principal-axes",
    "href": "core/labs/lab-pca.html#provide-an-interpretation-of-the-first-two-principal-axes",
    "title": "LAB: Principal Component Analysis",
    "section": "Provide an interpretation of the first two principal axes",
    "text": "Provide an interpretation of the first two principal axes\n\n\n\n\n\n\nQuestion\n\n\n\nWhich variables contribute to the two first principal axes?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAnalyze the signs of correlations between variables and axes?"
  },
  {
    "objectID": "core/labs/lab-pca.html#add-the-fertility-variable",
    "href": "core/labs/lab-pca.html#add-the-fertility-variable",
    "title": "LAB: Principal Component Analysis",
    "section": "Add the Fertility variable",
    "text": "Add the Fertility variable\n\n\n\n\n\n\nQuestion\n\n\n\nPlot again the correlation circle using the same principal axes as before, but add the Fertility variable.\nHow does Fertility relate with covariates? with principal axes?"
  },
  {
    "objectID": "core/labs/lab-pca.html#biplot",
    "href": "core/labs/lab-pca.html#biplot",
    "title": "LAB: Principal Component Analysis",
    "section": "Biplot",
    "text": "Biplot\n\n\n\n\n\n\nQuestion\n\n\n\nThe last svd plot (biplot) consists of overlaying the scatter plot of component x of the prcomp object and the correlation circle.\nSo the biplot is a graphical object built on two dataframes derived on components x and rotation of the prcomp objects.\nDesign a graphical pipeline.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nautoplot.prcomp() has optional arguments. If set to True, logical argument loadings overlays the scatterplot defined by the principal components with the correlation circle."
  }
]