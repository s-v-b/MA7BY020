[
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (255) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar",
    "crumbs": [
      "Journal",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#blackboard",
    "href": "weeks/week-7.html#blackboard",
    "title": "Week 7",
    "section": " Blackboard",
    "text": "Blackboard\n\nOrdinary Least Squares\nQR factorizatio\nRidge regression (Regularized Least Squares)\nPseudo-Inversion\nLinear regrssion with R",
    "crumbs": [
      "Journal",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#labs",
    "href": "weeks/week-7.html#labs",
    "title": "Week 7",
    "section": " Labs",
    "text": "Labs\n\nLab 6 - Linear Regression I  –&gt;\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables",
    "crumbs": [
      "Journal",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#further-work",
    "href": "weeks/week-7.html#further-work",
    "title": "Week 7",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints.",
    "crumbs": [
      "Journal",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#further-reading",
    "href": "weeks/week-7.html#further-reading",
    "title": "Week 7",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\n\nS3 classes\nS4 classes\n\nCours de Statistique Fondamentale\n\nModèles linéaires\nSherman–Morrison–Woodbury formula\n\nSourav Chatterjee: A new correlation coefficient",
    "crumbs": [
      "Journal",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#logistics",
    "href": "weeks/week-7.html#logistics",
    "title": "Week 7",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎",
    "crumbs": [
      "Journal",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (RH04A) Wednesday 13h30-15h\nSession Ibis Sophie Germain (014) Thursday 10h45-12h45\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar",
    "crumbs": [
      "Journal",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#blackboard",
    "href": "weeks/week-3.html#blackboard",
    "title": "Week 3",
    "section": " Blackboard",
    "text": "Blackboard\nWe reviewed the toolkit of univariate analysis\n\nLexicon\nCategorical samples\n\nCounts, Contingency tables, barplots, colplots\n\nNumeric samples\n\nNumerical summaries for location and scale\nCumulative distribution functions and Quantiles functions\nQuantile-quantile plots\nHistograms",
    "crumbs": [
      "Journal",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#labs",
    "href": "weeks/week-3.html#labs",
    "title": "Week 3",
    "section": " Labs",
    "text": "Labs\nWe went (briefly) through the two labs\n\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables\n\nWe relied on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr",
    "crumbs": [
      "Journal",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#further-work",
    "href": "weeks/week-3.html#further-work",
    "title": "Week 3",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints.",
    "crumbs": [
      "Journal",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#further-reading",
    "href": "weeks/week-3.html#further-reading",
    "title": "Week 3",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\nCours de Statistique Fondamentale\n\nChap Estimation de densité\nChap Test de Kolmogorov-Smirnov",
    "crumbs": [
      "Journal",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#logistics",
    "href": "weeks/week-3.html#logistics",
    "title": "Week 3",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎",
    "crumbs": [
      "Journal",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (163) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar",
    "crumbs": [
      "Journal",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#labs",
    "href": "weeks/week-1.html#labs",
    "title": "Week 1",
    "section": " Labs",
    "text": "Labs\nWe worked on the next two labs (solutions )\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\n\nWe used Rstudio, created and initialized a dedicated R project (without git and renv). We installed packages tidyverse, pak and gapminder.\nWe reconstructed the gapminder demo using ggplot2 and plotly.\nWe went through several aspects of R following Intro to R. This is an incentive to install packages lobstr and rlang. They can be very hepful when visualizing the data structures that underpin vectors, matrices, lists, and data frames.",
    "crumbs": [
      "Journal",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#further-work",
    "href": "weeks/week-1.html#further-work",
    "title": "Week 1",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints.",
    "crumbs": [
      "Journal",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#further-reading",
    "href": "weeks/week-1.html#further-reading",
    "title": "Week 1",
    "section": " Further reading",
    "text": "Further reading\n\nThe ggplot book\nR for data science\n\nData visualization\nTransform\n\nLogical vectors\nNumbers\nFactors\nMissing values\nFunctions\nIteration\nA field guide to base R\n\n\nAdvanced R\n\nNames and values\nVectors\nSubsetting",
    "crumbs": [
      "Journal",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#logistics",
    "href": "weeks/week-1.html#logistics",
    "title": "Week 1",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎",
    "crumbs": [
      "Journal",
      "Week 1"
    ]
  },
  {
    "objectID": "rstudio-client.html#project-options",
    "href": "rstudio-client.html#project-options",
    "title": "Rstudio Desktop",
    "section": "Project options",
    "text": "Project options",
    "crumbs": [
      "Support",
      "rstudio"
    ]
  },
  {
    "objectID": "projects-listings.html",
    "href": "projects-listings.html",
    "title": "Projects",
    "section": "",
    "text": "Note\n\n\n\nCourse evaluation is based on Projects \n\n Find a friend : all work done by pairs of students\n Create a single private GitHub repository for each project and each pair of students.\n Grant me access to these repositories\n All work is transmitted through your private repository and nowhere else\n No emails for project submission\n All projects deliverables consist of Quarto notebooks\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Description\n        \n         \n          Tags\n        \n         \n          Due date - Oldest\n        \n         \n          Due date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDue date\n\n\nTitle\n\n\nDescription\n\n\nTags\n\n\n\n\n\n\nFeb 7, 2025\n\n\nData wrangling and Visualization\n\n\nData extraction, wrangling, visualization, reproducible data science\n\n\nOECD, Visualization, Quarto, tidyverse, plotly\n\n\n\n\nMar 28, 2025\n\n\nTBA\n\n\nStatistical summaries, Regression, Diagnostics, Model comparison\n\n\nRegression, Diagnostics, Selection, Quarto, tidyverse, tidymodels\n\n\n\n\nMay 16, 2025\n\n\nTBA\n\n\nSVD, Clustering, Regression, Package management\n\n\nSpatial data, Package, Tidy evaluation, Quarto\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nEvaluation guidelines\n\n\n\nGrading criteria are given per project. They involve the following aspects:\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nNarrative, spelling and syntax\n\nEnglish/French \n\n\nPlots correction\n\nchoice of aesthetics, geom, scale … \n\n\nPlots style\n\nTitles, legends, labels, breaks … \n\n\nTable wrangling\n\nETL, SQL like manipulations \n\n\nComputing Statistics\n\nAggregations, LR, PCA, CA, … \n\n\nDRY compliance\n\nDRY principle at  Wikipedia\n\n\nReport organization\n\n\n\n\nCode organization",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "labs-solutions-listings.html",
    "href": "labs-solutions-listings.html",
    "title": "Labs Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 15, 2025\n\n\nIntroduction and Visualization\n\n\nVisualization, Public Statistics\n\n\n\n\nJan 15, 2025\n\n\nBrush up your R\n\n\nR language, Tidyverse, IDE\n\n\n\n\nJan 22, 2025\n\n\nTable wranglig\n\n\nR language, dplyr, tabula data\n\n\n\n\nJan 29, 2025\n\n\nUnivariate categorical data\n\n\nUnivariate data, GSS\n\n\n\n\nJan 30, 2025\n\n\nUnivariate numeric data\n\n\nUnivariate data, GSS\n\n\n\n\nFeb 5, 2025\n\n\nBivariate data\n\n\nbivariate data, mosaicplots, scatterplots, simple linear regression\n\n\n\n\nFeb 19, 2025\n\n\nLinear regression I\n\n\nLinear regression, OLS, lm\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Solutions"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA7BY020: Exporatory Data Analysis",
    "section": "",
    "text": "Github\n  \n\n  \n  \n\n\nCourse MA7BY020: Exploratory Data Analysis is an introduction to the principles and practice of data exploration and visualization using the R programming language. Keeping an eye on Machine Learning and Data Science, the course will cover the following topics:\n\nTabular data manipulation\nData visualization\nUnivariate and bivariate analysis for qualitative and quantitative data\nMultivariate analysis starting with multiple linear\nMatrix methods based on Singular Value Decomposition: PCA, CA, CCA, …\nClustering methods\n\n\n\n\nAfter that course, you will be able to:\n\nHandle tabular data using R version of the relational algebra (dplyr)\nVisualize data using ggplot2 and plotly\nPerform univariate and bivariate analysis, compute and assess statistical summaries\nPerform multivariate analysis using matrix methods\nDiagnose and validate the results of multivariate analysis\nPerform, discuss and communicate the results of SVD factorization methods\nPerform, discuss and communicate the results of clustering methods\nCommunicate the results of the analysis in a clear and concise manner using Quarto reports, presentations, and dashboards\n\n\n\n\nThe course is based on the R programming language, the RStudio IDE, and the VS Code Ide. We will rely on the tidyverse package and attempt to take advantage of R tidy evaluation mechanisms to write expressive and efficient code.\nWe will use the quarto package for reproducible research.\n\n\n\n\n\nSyllabus\nTeaching team\nComputing environment\nSlides\nLabs\nLabs solutions\nProjects\nClass notes",
    "crumbs": [
      "Information",
      "Glimpse"
    ]
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "MA7BY020 Syllabus",
    "section": "",
    "text": "Schedule\n\n\n\n\n\n\n\nDay\nTime\nLocation\nStart\n\n\n\n\nLab session 1\nWednesday\n13:13-15:00\nSomewhere\n2025-01-15\n\n\nLab session 2\nFriday\n9:00-10:30\nSophie Germain 2012\n2024-01-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrganization \n\n\n\nWe will have two weeky sessions of 1+1/2 hours each. Each session is organized around a Lab. We will switch from blackboard to laptop and back. You are invited to bring your laptop to the lab sessions.\n We will not attempt to complete the labs during the sessions. You are expected to complete the labs on your own time. Solutions (at least partial solutions) are available on the course website.\nYou can fork the course repository and post issues, comments, and corrections.\n\n\n\n\n\n\n\n\nObjectives\n\n\n\nThe objectives of course Introduction to Exploratory Data Analysis are to develop the ability to:\n\nperform data collection, tidying, and management,\ncompute statistics for summarizing data,\nvisualize the output of data analysis in the Grammar of Graphics framework,\nuse the R programming language for data analysis.\nbuild an R package,\nwrite reports, design presentations, and possibly build dashboards in the quarto document format.\n\n\n\n\n\n\n\n\n\nCommunication \n\n\n\nMaterial is available from s-v-b.github.io/MA7BY020\nMoodle\nSubscribe to Moodle portal\n\n\n\n\n\n\n\n\nSoftware \n\n\n\n\nR\nPosit\nrstudio\nquarto\nvs code\ngit\ndocker\npostgresql\n\n\n\n\n\n\n\n\n\nReferences \n\n\n\n\nBin Yu and Rebecca Barter, Veridical Data Science\nHadley Wickham, ggplot2: Elegant Graphics for Data Analysis\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund, R for Data Science\nHadley Wickham, Advanced R\nHadley Wickham and Jennifer Bryan., R packages\nHadley Wickham, Mastering Shiny\nkaggle\n\n\n\n\n\n\n\n\n\nCourse material \n\n\n\nSlides\nLabs are available (html and pdf)\nLabs with corrections are available\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 projects:\n\n\\(\\textsf{P}_1\\) Visualization\n\\(\\textsf{P}_2\\) Regression\n\\(\\textsf{P}_3\\) Packaging\n\nGrading\n\n\\[.2 \\textsf{P}_1 + .3 \\textsf{P}_2 + .5 \\textsf{P}_3\\]\n\n\n\n\n\n\n\n\nCode of conduct\n\n\n\nTL;DR: No cheating!\n\n\n\n\n\n\n\n\nSave the dates ! \n\n\n\nClick here for U Paris Cite Calendar.\nClick here for M1 MIDS Calendar\n\n\n\n\n\n\n\n\nUniversité Paris Cité\n\n\n\nUseful links:\n\nCharte Université Paris Cité\nDémarches et accessibilité\nCentre de contact\nRelais handicap",
    "crumbs": [
      "Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "core/projects/fake-report.html",
    "href": "core/projects/fake-report.html",
    "title": "LAB: Fake report",
    "section": "",
    "text": "Motivation\n\n\nAmet commodo faucibus risus neque rutrum sodales ante eleifend inceptos est pharetra?\n\n\n\n\n200 countries, 50 years, 20 lines of code\n\n\nSit varius nibh ultricies convallis fames velit ut inceptos dapibus. Mauris sed nisi magnis odio duis cum vel quis sagittis nisi – posuere, nisi parturient mus lectus. Curae hac nascetur eu netus vel eleifend, et duis facilisis augue nibh vulputate. Cursus vitae fringilla magnis, fames potenti velit nunc fermentum risus. Enim ante egestas rutrum nisl posuere quisque quisque.\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.9811\n\n\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.1134\n\n\n\n\n\n\n\nAdipiscing nascetur, per sem blandit montes venenatis cubilia nam parturient ac. Curae ridiculus leo, morbi at donec penatibus parturient. Phasellus netus consequat cursus litora fames, augue congue nunc. Vehicula egestas consequat laoreet aenean – consequat pellentesque, ut dignissim dictum dis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatic with plotly\n\n\nSit risus sapien feugiat porttitor curae – ante metus volutpat sagittis. Posuere aliquam enim tempor ac venenatis feugiat, sagittis accumsan sociis facilisis? Laoreet suspendisse arcu tellus habitasse nullam eget nam ridiculus tristique! Viverra rhoncus aliquet!\n\n\n\n\n\n\n\n\n\n\nWizardry\n\n\nSit ultrices nisi proin dictumst ac primis augue egestas a. Odio ultricies sagittis sapien sollicitudin sem convallis tempor montes quisque! Hendrerit urna nunc pulvinar nostra ullamcorper convallis sem. Blandit habitasse cubilia nisi nisi neque habitant, mollis taciti vivamus fermentum ultricies.\n\n\n\n\n\n\n\n\n\n\nAppendix\n\n\nDolor nulla facilisis duis at varius sociis! Lectus magna aptent hac suspendisse tempus torquent tempor primis magnis ornare netus hac lacinia, fames turpis vulputate nisl, bibendum gravida commodo.\n\n\n\nneat_color_scale &lt;-\n      c(\"Africa\" = \"#01d4e5\",\n        \"Americas\" = \"#7dea01\" ,\n        \"Asia\" = \"#fc5173\",\n        \"Europe\" = \"#fde803\",\n        \"Oceania\" = \"#536227\")\na_year &lt;- sample(gapminder$year, 1)\n\np &lt;- gapminder |&gt;\n    filter(year==a_year) |&gt;\n    ggplot() +\n    aes(x = gdpPercap) +\n    aes(y = lifeExp) +\n    aes(size = pop) +\n    aes(text = country) +                   #\n    aes(fill = continent) +\n    aes(color= continent) +\n    aes(frame = year) +                     #\n    geom_point(alpha=.5) +\n    scale_x_log10() +\n    scale_size_area(max_size = 15,\n                    labels= scales::label_number(scale=1/1e6,\n                                                suffix=\" M\")) +\n    scale_fill_manual(values = neat_color_scale) +\n    labs(title= glue(\"Gapminder  {a_year}\"),\n        x = \"Yearly Income per Capita\",\n        y = \"Life Expectancy\",\n        caption=\"From sick  and poor (bottom left) to healthy and rich (top right)\")\n\np \n\n(p + theme(legend.position = \"none\")) |&gt; \n    plotly::ggplotly(height = 500, width=750)\n\n(p %+% \n    gapminder + \n    theme(legend.position = \"none\") +\n    ggtitle(\"Gapminder\")) |&gt; \n    plotly::ggplotly(height = 500, width=750)"
  },
  {
    "objectID": "core/labs/lab-univariate-categorical.html#counting",
    "href": "core/labs/lab-univariate-categorical.html#counting",
    "title": "Univariate Categorical Analysis",
    "section": "Counting",
    "text": "Counting\n\n\n\n\n\n\nQuestion\n\n\n\nUse table, prop.table from base R to compute the frequencies and proportions of the different levels. In statistics, the result of table() is a (one-way) contingency table.\n\n\nWhat is the class of the object generated by table? Is it a vector, a list, a matrix, an array ?\n\n\n\n\n\n\nas.data.frame() (or as_tibble) can transform a table object into a dataframe.\n\nCodeta &lt;-  rename(as.data.frame(ta), REV_FOYER=`.`)\n\nta\n\n        REV_FOYER Freq\n1        [0-5000)    9\n2     [5000-7500)    5\n3    [7500-10000)    5\n4   [10000-12500)    9\n5   [12500-15000)    7\n6   [15000-17500)   19\n7   [17500-20000)   26\n8   [20000-25000)   38\n9   [25000-30000)   30\n10  [30000-35000)   35\n11  [35000-40000)   61\n12  [40000-50000)   70\n13  [50000-60000)   71\n14  [60000-75000)   89\n15  [75000-1e+05)   77\n16 [1e+05-150000)   48\n\n\n\n\n\nYou may use knitr::kabble(), possibly knitr::kable(., format=\"markdown\") to tweak the output.\nIf you are more ambitious, use gt::....\nIn order to feed ggplot with a contingency table, it is useful to build contingency tables as dataframes. Use dplyr::count() to do this.\n\n\n\n\n\n\nskimr::skim() allows us to perform univariate categorical analysis all at once.\n\nCodedf %&gt;% \n  skimr::skim(where(is.factor)) %&gt;% \n  print(n=50)\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Piped data\nNumber of rows             599       \nNumber of columns          11        \n_______________________              \nColumn type frequency:               \n  factor                   9         \n________________________             \nGroup variables            None      \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 SEXE                  0             1 FALSE          2\n2 REGION                0             1 FALSE          4\n3 STAT_MARI             0             1 FALSE          5\n4 SYNDICAT              0             1 FALSE          2\n5 CATEGORIE             0             1 FALSE         10\n6 NIV_ETUDES            0             1 FALSE         15\n7 NB_PERS               0             1 FALSE          9\n8 NB_ENF                0             1 FALSE          7\n9 REV_FOYER             0             1 FALSE         16\n  top_counts                           \n1 M: 302, F: 297                       \n2 S: 200, W: 148, NE: 129, NW: 122     \n3 M: 325, C: 193, D: 61, S: 14         \n4 non: 496, oui: 103                   \n5 Lib: 133, Ser: 125, Adm: 94, Sel: 48 \n6 12 : 187, Col: 148, Bac: 114, Ass: 45\n7 2: 196, 4: 130, 3: 122, 1: 63        \n8 0: 413, 1: 86, 2: 76, 3: 18          \n9 [60: 89, [75: 77, [50: 71, [40: 70   \n\n\nThe output can be tailored to your specific objectives and fed to functions that are geared to displaying large tables (see packages knitr, DT, and gt)"
  },
  {
    "objectID": "core/labs/lab-univariate-categorical.html#using-a-for-loop",
    "href": "core/labs/lab-univariate-categorical.html#using-a-for-loop",
    "title": "Univariate Categorical Analysis",
    "section": "Using a for loop",
    "text": "Using a for loop\nWe have to build a barplot for each categorical variable. Here, we just have nine of them. We could do this using cut and paste, and some editing. In doing so, we would not comply with the DRY (Don’t Repeat Yourself) principle.\nIn order to remain DRY, we will attempt to abstract the recipe we used to build our first barplot.\nThis recipe is pretty simple:\n\nBuild a ggplot object with df as the data layer.\nAdd an aesthetic mapping a categorical column to axis x\n\nAdd a geometry using geom_bar\n\nAdd labels explaining the reader which column is under scrutiny\n\nWe first need to gather the names of the categorical columns. The following chunk does this in a simple way.\nIn the next chunk, we shall build a named list of ggplot objects consisting of barplots. The for loop body is almost obtained by cutting and pasting the recipe for the first barplot.\n\n\n\n\n\n\nNote an important difference: instead of something aes(x=col) where col denotes a column in the dataframe, we shall write aes(x=.data[[col]]) where col is a string that matches a column name. Writing aes(x=col) would not work.\nThe loop variable col iterates over the column names, not over the columns themselves.\nWhen using ggplot in interactive computations, we write aes(x=col), and, under the hood, the interpreter uses the tidy evaluation mechanism that underpins R to map df$col to the x axis.\nggplot functions like aes() use data masking to alleviate the burden of the working Statistician.\nWithin the context of ggplot programming, pronoun .data refers to the data layer of the graphical object.\n\n\n\nIf the labels on the x-axis are not readable, we need to tweak them. This amounts to modifying the theme layer in the ggplot object, and more specifically the axis.text.x attribute."
  },
  {
    "objectID": "core/labs/lab-univariate-categorical.html#using-functional-programming-lapply-purrr...",
    "href": "core/labs/lab-univariate-categorical.html#using-functional-programming-lapply-purrr...",
    "title": "Univariate Categorical Analysis",
    "section": "Using functional programming (lapply, purrr::...)",
    "text": "Using functional programming (lapply, purrr::...)\nAnother way to compute the list of graphical objects replaces the for loop by calling a functional programming tool. This mechanism relies on the fact that in R, functions are first-class objects.\n\n\n\n\n\n\nPackage purrr offers a large range of tools with a clean API. Base R offers lapply().\n\n\n\nWe shall first define a function that takes as arguments a datafame, a column name, and a title. We do not perform any defensive programming. Call your function foo.\nFunctional programmming makes code easier to understand.\nUse foo, lapply or purrr::map() to build the list of graphical objects.\nWith purrr::map(), you may use either a formula or an anonymous function. With lapply use an anonymous function.\nPackage patchwork offers functions for displaying collections of related plots."
  },
  {
    "objectID": "core/labs/lab-linear-regression-whiteside.html#introduction",
    "href": "core/labs/lab-linear-regression-whiteside.html#introduction",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "Introduction",
    "text": "Introduction\nThe purpose of this lab is to introduce linear regression using base R and the tidyverse. We work on a dataset provided by the MASS package. This dataset is investigated in the book by Venables and Ripley. This discusssion is worth being read. Our aim is to relate regression as a tool for data exploration with regression as a method in statistical inference. To perform regression, we will rely on the base R function lm() and on the eponymous S3 class lm. We will spend time understanding how the formula argument can be used to construct a design matrix from a dataframe representing a dataset."
  },
  {
    "objectID": "core/labs/lab-linear-regression-whiteside.html#packages-installation-and-loading-again",
    "href": "core/labs/lab-linear-regression-whiteside.html#packages-installation-and-loading-again",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "Packages installation and loading (again)",
    "text": "Packages installation and loading (again)\n\n\nCode\n# We will use the following packages. \n# If needed, install them : pak::pkg_install(). \nstopifnot(\n  require(\"magrittr\"),\n  require(\"lobstr\"),\n  require(\"ggforce\"),\n  require(\"patchwork\"), \n  require(\"gt\"),\n  require(\"glue\"),\n  require(\"skimr\"),\n  require(\"corrr\"),\n  require(\"GGally\"),\n  require(\"broom\"),\n  require(\"tidyverse\"),\n  require(\"ggfortify\"),\n  require(\"autoplotly\")\n\n)\n\n\nBesides the tidyverse, we rely on skimr to perform univariate analysis, GGally::ggpairs to perform pairwise (bivariate) analysis. Package corrr provide graphical tools to explore correlation matrices. At some point, we will showcase the exposing pipe %$% and the classical pipe %&gt;% of magrittr. We use gt to display handy tables, patchwork to compose graphical objects. glue provides a kind of formatted strings. Package broom proves very useful when milking lienar models produced by lm() (and many other objects produced by estimators, tests, …)"
  },
  {
    "objectID": "core/labs/lab-linear-regression-whiteside.html#s3-classes-in-r",
    "href": "core/labs/lab-linear-regression-whiteside.html#s3-classes-in-r",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "S3 classes in R",
    "text": "S3 classes in R"
  },
  {
    "objectID": "core/labs/lab-linear-regression-whiteside.html#generic-functions-for-s3-classes",
    "href": "core/labs/lab-linear-regression-whiteside.html#generic-functions-for-s3-classes",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "Generic functions for S3 classes",
    "text": "Generic functions for S3 classes\nmethods(autoplot) lists the S3 classes for which an autoplot method is defined. Some methods are defined in ggplot2, others like autoplot.lm are defined in extension packages like ggfortify."
  },
  {
    "objectID": "core/labs/lab-linear-regression-whiteside.html#s4-classes-in-r",
    "href": "core/labs/lab-linear-regression-whiteside.html#s4-classes-in-r",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "S4 classes in R",
    "text": "S4 classes in R\nThe output of autoplot.lm is an instance of S4 class"
  },
  {
    "objectID": "core/labs/lab-linear-regression-whiteside.html#tibbles-with-list-columns",
    "href": "core/labs/lab-linear-regression-whiteside.html#tibbles-with-list-columns",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "tibbles with list columns",
    "text": "tibbles with list columns"
  },
  {
    "objectID": "core/labs/lab-bivariate.html#setup",
    "href": "core/labs/lab-bivariate.html#setup",
    "title": "Bivariate analysis",
    "section": "Setup",
    "text": "Setup\n\nCodestopifnot(\n  require(glue),\n  require(magrittr),\n  require(lobstr),\n  require(arrow),\n  require(ggforce),\n  require(vcd),\n  require(ggmosaic),\n  require(httr),\n  require(patchwork),\n  require(corrr),\n  require(gapminder),\n  require(slider),\n  require(tidyverse) \n) \n\n\nBivariate techniques depend on the types of columns we are facing.\nFor numerical/numerical samples\n\nScatter plots\nSmoothed lineplots (for example linear regression)\n2-dimensional density plots\n\nFor categorical/categorical samples : mosaicplots and variants\nFor numerical/categorical samples\n\nBoxplots per group\nHistograms per group\nDensity plots per group"
  },
  {
    "objectID": "core/labs/lab-bivariate.html#chi-square-independenceassociation-test",
    "href": "core/labs/lab-bivariate.html#chi-square-independenceassociation-test",
    "title": "Bivariate analysis",
    "section": "Chi-square independence/association test",
    "text": "Chi-square independence/association test\nhttps://statsfonda.github.io/site/content/ch4_2.html#test-dindépendance\n\n\n\n\n\n\nQuestion\n\n\n\n\nCompute the chi-square association statistic between CATEGORIE and SEXE.\nDisplay the output of chisq.test() as a table, using broom::tidy()"
  },
  {
    "objectID": "core/labs/lab-bivariate.html#grouped-boxplots",
    "href": "core/labs/lab-bivariate.html#grouped-boxplots",
    "title": "Bivariate analysis",
    "section": "Grouped boxplots",
    "text": "Grouped boxplots\n\n\n\n\n\n\nQuestion\n\n\n\nPlot boxplots of AGE according to NIV_ETUDES\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDraw density plots of AGE, facet by NIV_ETUDES and SEXE\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCollapse rare levels of NIV_ETUDES and replay."
  },
  {
    "objectID": "core/labs/lab-bivariate.html#scatterplots",
    "href": "core/labs/lab-bivariate.html#scatterplots",
    "title": "Bivariate analysis",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\n\n\n\n\nQuestion\n\n\n\nMake a scatterplot of SAL_HORwith respect to AGE"
  },
  {
    "objectID": "core/labs/lab-bivariate.html#linear-correlation-coefficient",
    "href": "core/labs/lab-bivariate.html#linear-correlation-coefficient",
    "title": "Bivariate analysis",
    "section": "Linear correlation coefficient",
    "text": "Linear correlation coefficient\n\n\n\n\n\n\nQuestion\n\n\n\nCompute the Pearson, Spearman and Kendall correlation coefficients between AGE and SAL_HOR using function cor() from base R"
  },
  {
    "objectID": "core/labs/lab-bivariate.html#rank-based-methods",
    "href": "core/labs/lab-bivariate.html#rank-based-methods",
    "title": "Bivariate analysis",
    "section": "Rank based methods",
    "text": "Rank based methods\nSpearman’s rho (𝜌) and Kendall’s tau (𝜏) are both non-parametric correlation coefficients used to measure the strength and direction of a monotonic relationship between two variables.\n\nSpearman’s rho (𝜌)\n\nBased on rank differences. Defined as the Pearson correlation coefficient between the ranked variables.\\[\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\\] where \\(d_i\\) is the difference between the ranks of each pair, and \\(n\\) is the number of observations.\n\nKendall’s tau (𝜏)\n\nBased on concordant and discordant pairs.  Measures the proportion of pairs that have the same order in both variables compared to the total number of pairs. \\[\\tau = \\frac{(C - D)}{\\frac{1}{2} n (n - 1)}\\] where \\(C\\) is the number of concordant pairs, and \\(D\\) is the number of discordant pairs.\n\n\nWhen to Use Which?\n\n\n\n\n\n\n\nFactor\nSpearman’s rho (𝜌)\nKendall’s tau (𝜏)\n\n\n\nLarge differences in ranks\nMore sensitive\nLess sensitive\n\n\nSmall sample sizes\nLess reliable\nMore reliable\n\n\nOutlier resistance\nModerate\nHigh\n\n\nComputational efficiency\nFaster\nSlower (due to pairwise comparisons)\n\n\nInterpretation\nSimilar to Pearson’s correlation\nMore intuitive (proportion of concordance)"
  },
  {
    "objectID": "core/labs/lab-bivariate.html#chatterjees-correlation-coefficient-chatterjees-ξ",
    "href": "core/labs/lab-bivariate.html#chatterjees-correlation-coefficient-chatterjees-ξ",
    "title": "Bivariate analysis",
    "section": "Chatterjee’s correlation coefficient (Chatterjee’s \\(ξ\\))",
    "text": "Chatterjee’s correlation coefficient (Chatterjee’s \\(ξ\\))\n\nThe three most popular classical measures of statistical association are Pearson’s correlation coefficient, Spearman’s ρ, and Kendall’s τ . These coefficients are very powerful for detecting linear or monotone associations, and they have well-developed asymptotic theories for calculating P-values. However, the big problem is that they are not effective for detecting associations that are not monotonic, even in the complete absence of noise.\n\n\nLet \\((X, Y)\\) be a pair of random variables, where \\(Y\\) is not a constant. Let \\((X_1 , Y_1 ), \\ldots, (X_n , Y_n )\\) be i.i.d. pairs with the same law as \\((X, Y)\\), where \\(n ≥ 2\\). The new coefficient has a simpler formula if the \\(X_i\\)’s and the \\(Y_i\\) ’s have no ties. This simpler formula is presented first, and then the general case is given. Suppose that the \\(X_i\\)’s and the \\(Y_i\\) ’s have no ties. Rearrange the data as \\((X_{(1)} , Y_{(1)} ), . . . , (X_{(n)} , Y_{(n)} )\\) such that \\(X_{(1)} ≤ · · · ≤ X_{(n)}\\) . Since the \\(X_i\\)’s have no ties, there is a unique way of doing this. Let ri be the rank of \\(Y_{(i)}\\), that is, the number of \\(j\\) such that \\(Y_{(j)} ≤ Y_{(i)}\\). The new correlation coefficient is defined as\n\n\\[ξ_n(X, Y ) := 1 −  3\\sum_{i=1}^{n-1} \\frac{|r_{i+1} − r_i|}{n^2-1}\\]\n\nIn the presence of ties, \\(ξ_n\\) is defined as follows. If there are ties among the \\(X_i\\)’s, then choose an increasing rearrangement as above by breaking ties uniformly at random. Let \\(r_i\\) be as before, and additionally define \\(l_i\\) to be the number of \\(j\\) such that \\(Y_{(j)} ≥ Y_{(i)}\\). Then define\n\n\\[ξ_n(X, Y ) := 1 −  3n\\sum_{i=1}^{n-1} \\frac{|r_{i+1} − r_i|}{2 \\sum_{i=1}^n l_i (n − l_i )}\\]\n\nWhen there are no ties among the \\(Y_i\\) ’s, \\(l_1 , \\ldots , l_n\\) is just a permutation of \\(1, \\ldots , n\\), and so the denominator in the above expression is just \\(n(n^2 − 1)/3\\), which reduces this definition to the earlier expression.\n\nFrom Sourav Chatterjee: A new correlation coefficient\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a dplyr pipeline from computing the \\(ξ\\) correlation coefficient between Y=lifeExp and X=gdpPercap in the gapminder dataset, per year and continent."
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing environment",
    "section": "",
    "text": "In this course, we will use the R programming language. Install the latest version of R from the R Project website on your hard drive. In any case, you shall have Version &gt; 4.0.\nCheatsheets\n\n\nIDEs\nThere exist several Integrated Development Environments (IDEs) for R. Here are some of them:\n\nrstudio\nvs code\nvs codium\nemacs\n\nrstudio has been designed to work around R. It is free and open-source. It is available for Windows, Mac, and Linux. It works very well git. Quarto has been designed and developed by the same team.\nvs code is a general-purpose IDE. It works very well with git. It is very versatile and can be used for many programming languages. It is endowed with a large number of extensions for R, Python, SQL, and Quarto. It integrates very well with git. It is promoted by github just as copilot. vs codium is a telemetry-free version of vs code.\nemacs is a classical and powerful general-purpose editor that can be turned into an IDE. It is available for Windows, Mac, and Linux. It also works very well with git. Extension ESS (Emacs Speaks Statistics) is available for R.\nIn the course, we will use rstudio and vs code.\n\n\nPositcloud\nPosit Cloud lets you access Posit’s powerful set of data science tools right in your browser – no installation or complex configuration required.\n\n\nQuarto\nDownload from Quarto Website. It is very convenient to work the Command Line Interface (CLI) of Quarto.\n\n\n\n\n\n\nFrom the Quarto website\n\n\n\n\nAn open-source scientific and technical publishing system\nAuthor using Jupyter notebooks or with plain text markdown in your favorite editor.\nCreate dynamic content with Python, R, Julia, and Observable.\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nShare knowledge and insights organization-wide by publishing to Posit Connect, Confluence, or other publishing systems.\nWrite using Pandoc markdown, including equations, citations, cross references, figure panels, callouts, advanced layout, and more.",
    "crumbs": [
      "Support",
      "Computing resources"
    ]
  },
  {
    "objectID": "core/labs/lab-r-intro.html#packages",
    "href": "core/labs/lab-r-intro.html#packages",
    "title": "R language: a tour",
    "section": "Packages",
    "text": "Packages\nBase R can do a lot. But the full power of R comes from a fast growing collection of packages.\nPackages are first installed (that is downloaded from cran and copied somewhere on the hard drive), and if needed, loaded during a session.\n\nInstallation can usually be performed using command install.packages(). In some circumstances, ad hoc installation commands (often from packages devtools) are needed\nPackage pak offers an interesting alternative to base R install.packages()\n\nOnce a package has been installed/downloaded on your hard drive\n\nif you want all objects exported by the package to be available in your session, you should load the package, using library() or require() (what’s the difference?). Technically, this loads the NameSpace defined by the package.\nif you just want to pick some objects exported from the package, you can use qualified names like package_name::object_name to access the object (function, dataset, …).\n\n\n\nFor example, when we write\ngapminder &lt;- gapminder::gapminder\nwe assign dataframe/tibble gapminder from package gapminder to identifier \"gapminder\" in global environment .\nFunction p_load() from pacman (package manager) blends installation and loading: if the package named in the argument of p_load() is not installed (not among the installed.packages()), p_load() attempts to install the package. If installation is successful, the package is loaded.\n\nif (! require(pak)){\n  install.packages(\"pak\")\n}\n\n\nstopifnot(\n  require(\"tidyverse\"), \n  require(\"lobstr\"),\n  require(\"ggforce\"),\n  require(\"nycflights13\"),\n  require(\"patchwork\"), \n  require(\"viridis\"),\n  require(\"MASS\"),\n  require(\"gapminder\"),\n  require(\"pryr\"),\n  require(\"pak\")\n)\n\n\n\n\n\n\n\nOptional arguments\n\n\n\nA very nice feature of R is that functions from base R as well as from packages have optional arguments with sensible default values. Look for example at documentation of require() using expression ?require.\nOptional settings may concern individual functions or the collection of functions exported by some packages. In the next chunk, we reset the default color scales used by graphical functions from ggplot2.\n\nopts &lt;- options()  # save old options\n\noptions(ggplot2.discrete.colour=\"viridis\")\noptions(ggplot2.continuous.colour=\"viridis\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou shall not confuse installing (on your hard-drive) and loading (in session) a package.\n\n\n\n\n\n\n\n\nQuestion for Pythonistas\n\n\n\n\nIn  what is the analogue of install.packages()?\nIn  what is the analogue of require()/library()?"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#vector-creation-and-assignment",
    "href": "core/labs/lab-r-intro.html#vector-creation-and-assignment",
    "title": "R language: a tour",
    "section": "Vector creation and assignment",
    "text": "Vector creation and assignment\nThe next three lines create three numerical atomic vectors.\nIn IDE Rstudio, have a look at the environment pane on the right before running the chunk, and after.\nUse ls() to investigate the environment before and after the execution of the three assignments.\n\nls()\nx &lt;- c(1, 2, 12)\ny &lt;- 5:7\nz &lt;- 10:1\nx ; y ; z \nls()\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat are the identifiers known in the global environment before execution of lines 2-4?\nWhat are the identifiers known in the global environment after execution of lines 2-4?\nWhich objects are attached to identifiers x, y, and z?\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does the next chunk?\n\nls()\nw &lt;- y\nls()\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nIs the content of object denoted by y copied to a new object bound to w?\nInterpret the result of w == y.\nInterpret the result of identical(w,y) (use help(\"identical\") if needed).\n\n\nw == y \nidentical(w,y)"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#indexation-slicing-modification",
    "href": "core/labs/lab-r-intro.html#indexation-slicing-modification",
    "title": "R language: a tour",
    "section": "Indexation, slicing, modification",
    "text": "Indexation, slicing, modification\nSlicing a vector can be done in two ways:\n\nproviding a vector of indices to be selected. Indices need not be consecutive.\nproviding a Boolean mask, that is a logical vector to select a set of positions.\n\n\nx &lt;- c(1, 2, 12) ; y &lt;- 5:7 ; z &lt;- 10:1\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next lines\n\nz[1]   # slice of length 1\nz[0]   # What did you expect?\nz[x]   # slice of length ??? index error ?\nz[y]\nz[x %% 2]   # what happens with x[0] ?\nz[0 == (x %% 2)] # masking\nz[c(2, 1, 1)]\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIf the length of mask and and the length of the sliced vector do not coincide, what happens?\n\n\n\n\n\n\n\n\n\n\n\n\nA scalar is just a vector of length \\(1\\)!\n\nclass(z)\n\n[1] \"integer\"\n\nclass(z[1])\n\n[1] \"integer\"\n\nclass(z[c(2,1)])\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next lines\n\ny[2:3] &lt;- z[2:3]\ny == z[-10]\n\nz[-11]\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next line\n\nz[-(1:5)]\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you select the last element from a vector (say z)?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nReverse the entries of a vector. Find two ways to do that.\n\n\nIn statistics, machine learning, we are often faced with the task of building grids of regularly spaced elements (these elements can be numeric or not). R offers a collection of tools to perform this. The most basic tool is rep().\n\n\n\n\n\n\nQuestion\n\n\n\n\nRepeat a vector \\(2\\) times\nRepeat each element of a vector twice\n\n\n\nLet us remove objects from the global environment.\n\nrm(w, x, y ,z)"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#numbers",
    "href": "core/labs/lab-r-intro.html#numbers",
    "title": "R language: a tour",
    "section": "Numbers",
    "text": "Numbers\nSo far, we told about numeric vectors. Numeric vectors are vectors of floating point numbers. R distinguishes several kinds of numbers.\n\nIntegers\nFloating point numbers (double)\n\nTo check whether a vector is made of numeric or of integer, use is.numeric() or is.integer(). Use as.integer, as.numeric() to enforce type conversion.\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the outcome of the next chunks\n\nclass(113L) ; class(113) ; class(113L + 113) ; class(2 * 113L) ; class(pi) ; as.integer(pi)\n\n[1] \"integer\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] 3\n\n\n\nfloor(pi) ; class(floor(pi)) # mind the floor\n\n[1] 3\n\n\n[1] \"numeric\""
  },
  {
    "objectID": "core/labs/lab-r-intro.html#integer-arithmetic",
    "href": "core/labs/lab-r-intro.html#integer-arithmetic",
    "title": "R language: a tour",
    "section": "Integer arithmetic",
    "text": "Integer arithmetic\n\n29L * 31L ; 899L %/% 32L ; 899L %% 30L\n\n[1] 899\n\n\n[1] 28\n\n\n[1] 29\n\n\n\n\n\n\n\n\nR integers are not the natural numbers from Mathematics\nR numerics are not the real numbers from Mathematics\n\n.Machine$double.eps\n\n[1] 2.220446e-16\n\n.Machine$double.xmax\n\n[1] 1.797693e+308\n\n.Machine$sizeof.longlong\n\n[1] 8\n\nu &lt;- double(19L)\nv &lt;- numeric(5L)\nw &lt;- integer(7L)\nlapply(list(u, v, w), typeof)\n\n[[1]]\n[1] \"double\"\n\n[[2]]\n[1] \"double\"\n\n[[3]]\n[1] \"integer\"\n\nlength(c(u, v, w))\n\n[1] 31\n\ntypeof(c(u, v, w))\n\n[1] \"double\"\n\n\n\n\n\nR is (sometimes) able to make sensible use of Infinite.\n\nlog(0)\n\n[1] -Inf\n\nlog(Inf)\n\n[1] Inf\n\n1/0\n\n[1] Inf\n\n0/0\n\n[1] NaN\n\nmax(c( 0/0,1,10))\n\n[1] NaN\n\nmax(c(NA,1,10))\n\n[1] NA\n\nmax(c(-Inf,1,10))\n\n[1] 10\n\nis.finite(c(-Inf,1,10))\n\n[1] FALSE  TRUE  TRUE\n\nis.na(c(NA,1,10))\n\n[1]  TRUE FALSE FALSE\n\nis.nan(c(NaN,1,10))\n\n[1]  TRUE FALSE FALSE"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#computing-with-vectors",
    "href": "core/labs/lab-r-intro.html#computing-with-vectors",
    "title": "R language: a tour",
    "section": "Computing with vectors",
    "text": "Computing with vectors\nSumming, scalar multiplication\n\nx &lt;- 1:3\ny &lt;- 9:7\n\nsum(x) ; prod(x)\n\n[1] 6\n\n\n[1] 6\n\nz &lt;- cumsum(1:3)\nw &lt;- cumprod(3:5)\n\nx + y\n\n[1] 10 10 10\n\nx + z\n\n[1] 2 5 9\n\n2 * w\n\n[1]   6  24 120\n\n2 + w\n\n[1]  5 14 62\n\nw / 2\n\n[1]  1.5  6.0 30.0\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute a factorial?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nApproximate \\(\\sum_{n=1}^\\infty 1/n^2\\) within \\(10^{-3}\\)?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute the inner product between two (atomic numeric) vectors?\n\n\n\n\n\n\n\n\nWhat we have called vectors so far are indeed atomic vectors.\n\nRead Chapter on Vectors in R advanced Programming\n\nKeep an eye on package vctrs for getting insights into the R vectors."
  },
  {
    "objectID": "core/labs/lab-r-intro.html#creation-transposition-and-reshaping",
    "href": "core/labs/lab-r-intro.html#creation-transposition-and-reshaping",
    "title": "R language: a tour",
    "section": "Creation, transposition and reshaping",
    "text": "Creation, transposition and reshaping\nA vector can be turned into a column matrix.\n\nv &lt;- as.matrix(1:5)\nv\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[4,]    4\n[5,]    5\n\n\nA matrix can be transposed\n\nt(v)  # transpose \n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n\ncat(dim(v), ' ', dim(t(v)), '\\n')\n\n5 1   1 5 \n\n\n\nA &lt;- matrix(1, nrow=5, ncol=2) ; A\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n[3,]    1    1\n[4,]    1    1\n[5,]    1    1\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nlobstr::mem_used() allows us to keep track of the amount of memory used by our R session. lobstr::obj_size() tells us the amount of memory used by the representation of an object.\nComment the next chunk\n\nm1 &lt;-lobstr::mem_used()\nA &lt;- matrix(rnorm(100000L), nrow=1000L)\nm2 &lt;- lobstr::mem_used()\nlobstr::obj_size(A)\n\n800.22 kB\n\nB &lt;- t(A)\nlobstr::obj_size(B)\n\n800.22 kB\n\nm3 &lt;- lobstr::mem_used()\nm2-m1 ; m3-m2\n\n804.94 kB\n\n\n1.09 MB\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nIs there a difference between the next two assignments?\nHow would you assign value to all entries of a matrix?\n\n\nA &lt;- matrix(rnorm(16), nrow=4)\nA[] &lt;- 0 ; A\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n\nA   &lt;- 0 ; A\n\n[1] 0\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the final shape of A?\n\nA &lt;- matrix(1, nrow=5, ncol=2) \nA\nA[] &lt;- 1:15 \nA\n\n\n\nWe can easily generate diagonal matrices and constant matrices.\n\ndiag(1, 3)  # building identity matrix\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\nmatrix(0, 3, 3) # building null matrix\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs there any difference between the next two assignments?\n\nB &lt;- A[]\nB ; A\n\n[1] 0\n\n\n[1] 0\n\nlobstr::obj_addr(B) ; lobstr::obj_addr(A)\n\n[1] \"0x5f6a8d4cfb48\"\n\n\n[1] \"0x5f6a8e04d5c8\"\n\nB &lt;- A\nlobstr::obj_addr(B) ; lobstr::obj_addr(A)\n\n[1] \"0x5f6a8e04d5c8\"\n\n\n[1] \"0x5f6a8e04d5c8\""
  },
  {
    "objectID": "core/labs/lab-r-intro.html#indexation-slicing-modification-1",
    "href": "core/labs/lab-r-intro.html#indexation-slicing-modification-1",
    "title": "R language: a tour",
    "section": "Indexation, slicing, modification",
    "text": "Indexation, slicing, modification\nIndexation consists in getting one item from a vector/list/matrix/array/dataframe.\nSlicing and subsetting consists in picking a substructure:\n\nsubsetting a vector returns a vector\nsubsetting a list returns a list\nsubsetting a matrix/array returns a matrix/array (beware of implicit simplifications and dimension dropping)\nsubsetting a dataframe returns a dataframe or a vector (again, beware of implicit simplifications).\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next results\n\nA &lt;- matrix(1, nrow=5, ncol=2)\n\ndim(A[sample(5, 3), -1])\ndim(A[sample(5, 3), 1])\nlength(A[sample(5, 3), 1])\nis.vector(A[sample(5, 3), 1])\nA[10:15]\nA[60]\ndim(A[])\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you create a fresh copy of a matrix?"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#computing-with-matrices",
    "href": "core/labs/lab-r-intro.html#computing-with-matrices",
    "title": "R language: a tour",
    "section": "Computing with matrices",
    "text": "Computing with matrices\n\n\n* versus %*%\n\n\n%*% stands for matrix multiplication. In order to use it, the two matrices should have conformant dimensions.\n\n\n\nt(v) %*% A\n\n          [,1]    [,2]\n[1,] -17.64316 11.6116\n\n\nThere are a variety of reasonable products around. Some of them are available in R.\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute the Hilbert-Schmidt inner product between two matrices?\n\\[\\langle A, B\\rangle_{\\text{HS}} = \\text{Trace} \\big(A \\times B^\\top\\big)\\]\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow can you invert a square (invertible) matrix?"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#handling-three-valued-logic",
    "href": "core/labs/lab-r-intro.html#handling-three-valued-logic",
    "title": "R language: a tour",
    "section": "Handling three-valued logic",
    "text": "Handling three-valued logic\n\n\n\n\n\n\nQuestion\n\n\n\n\nTRUE &  (1&gt; (0/0))\n(1&gt; (0/0)) | TRUE\n(1&gt; (0/0)) | FALSE\nTRUE || (1&gt; (0/0))\nTRUE |  (1&gt; (0/0))\nTRUE || stopifnot(4&lt;3)\n# TRUE |  stopifnot(4&lt;3)  \nFALSE && stopifnot(4&lt;3)\n# FALSE & stopifnot(4&lt;3)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the difference between logical operators || and | ?\n\n\n\n\n\n\n\n\n\n\n\n\nRemark: favor &, | over &&, ||."
  },
  {
    "objectID": "core/labs/lab-r-intro.html#all-and-any",
    "href": "core/labs/lab-r-intro.html#all-and-any",
    "title": "R language: a tour",
    "section": "\nall and any\n",
    "text": "all and any\n\nLook at the definition of all and any.\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow would you check that a square matrix is symmetric?\nHow would you check that a matrix is diagonal?"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#if-then-else",
    "href": "core/labs/lab-r-intro.html#if-then-else",
    "title": "R language: a tour",
    "section": "If () then {} else {}",
    "text": "If () then {} else {}\nIf expressions yes_expr and no_expr are complicated it makes sense to use the if (...) {...} else {...} construct\nThere is also a conditional statement with an optional else {}\n#| label: if-else\n#| eval: false\n#| collapse: false\nif (condition) {\n  ...\n} else {\n  ...\n}\n\n\n\n\n\n\nQuestion\n\n\n\nIs there an elif construct in R?\n\n\n R also offers a switch\n#| label: switch\nswitch (object,\n  case1 = {action1}, \n  case2 = {action2}, \n  ...\n)\n\n\n\n\n\n\nThere exists a selection function ifelse(test, yes_expr, no_expr).\n\nifelse(test, yes, no)\n\nNote that ifelse(...) is vectorized.\n\nx &lt;-  1L:6L\ny &lt;-  rep(\"odd\", 6)\nz &lt;- rep(\"even\", 6)\n\nifelse(x %% 2L, y, z)\n\n[1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\n This is a vectorized function"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#iterations-for-it-in-iterable-...",
    "href": "core/labs/lab-r-intro.html#iterations-for-it-in-iterable-...",
    "title": "R language: a tour",
    "section": "Iterations for (it in iterable) {...}\n",
    "text": "Iterations for (it in iterable) {...}\n\nHave a look at Iteration section in R for Data Science\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a lower triangular matrix which represents the 5 first lines of the Pascal triangle.\n\n\nRecall\n\\[\\binom{n}{k} = \\binom{n-1}{k-1} + \\binom{n-1}{k}\\]\n\n\n\n\n\n\nQuestion\n\n\n\nLocate the smallest element in a numerical vector"
  },
  {
    "objectID": "core/labs/lab-r-intro.html#while-condition",
    "href": "core/labs/lab-r-intro.html#while-condition",
    "title": "R language: a tour",
    "section": "While (condition) {…}",
    "text": "While (condition) {…}\n\n\n\n\n\n\nQuestion\n\n\n\nFind the location of the minimum in a vector v\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a loop that checks whether vector v is non-decreasing."
  },
  {
    "objectID": "core/labs/lab-r-intro.html#operators-purrrmap_",
    "href": "core/labs/lab-r-intro.html#operators-purrrmap_",
    "title": "R language: a tour",
    "section": "Operators purrr::map_???\n",
    "text": "Operators purrr::map_???\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite truth tables for &, |, &&, ||, ! and xor\nHint: use purrr::map, function outer()\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a function that takes as input a square matrix and returns TRUE if it is lower triangular.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse map , choose and proper use of pronouns to deliver the n first lines of the Pascal triangle using one line of code.\nAs far as the total number of operations is concerned, would you recommend this way of computing the Pascal triangle?\n\n\n\n\n\n\n\n\nRead Chapter on Functional Programming in Advanced R"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#grammar-of-graphics",
    "href": "core/labs/lab-gapminder.html#grammar-of-graphics",
    "title": "Data visualization",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nWe will use the Grammar of Graphics approach to visualization\nThe expression Grammar of Graphics was coined by Leiland Wilkinson to describe a principled approach to visualization in Data Analysis (EDA)\nA plot is organized around tabular data (a table with rows (observations) and columns (variables))\nA plot is a graphical object that can be built layer by layer\nBuilding a graphical object consists in chaining elementary operations\nThe acclaimed TED presentation by Hans Rosling illustrates the Grammar of Graphics approach\n\nWe will reproduce the animated demonstration using\n\n\nggplot2: an implementation of grammar of graphics in `R\n\nplotly: a bridge between R and the javascript library D3.js\n\nUsing plotly, opting for html ouput, brings the possibility of interactivity and animation"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#setup",
    "href": "core/labs/lab-gapminder.html#setup",
    "title": "Data visualization",
    "section": "Setup",
    "text": "Setup\nWe will use the following packages. If needed, we install them.\n\nCodestopifnot(\n  require(tidyverse), \n  require(patchwork), \n  require(glue), \n  require(ggforce), \n  require(plotly),\n  require(ggthemes),\n  require(gapminder),\n  require(ggrepel)\n)\n\n\nThe data we will use can be obtained by loading package gapminder\n\n\n\n\n\n\nTip\n\n\n\nIf the packages have not yet been installed on your hard drive, install them.\nYou can do that using base R install.packages() function:\ninstall.packages(\"tidyverse\")\nIt is often faster to use functions from package pak\ninstall.packages(\"pak\")\npak::pkg_install(\"tidyverse\")\n\n\nYou need to understand the difference between installing and loading a package\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow do we get the list of installed packages?\nHow do we get the list of loaded packages?\nWhich objects are made available by a package?"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#have-a-look-at-gapminder-dataset",
    "href": "core/labs/lab-gapminder.html#have-a-look-at-gapminder-dataset",
    "title": "Data visualization",
    "section": "Have a look at gapminder dataset",
    "text": "Have a look at gapminder dataset\nThe gapminder table can be found at gapminder::gapminder\n\nA table has a schema: a list of named columns, each with a given type\nA table has a content: rows. Each row is a collection of items, corresponding to the columns\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplore gapminder::gapminder, using glimpse() and head()\n\n\nglimpse() allows to see the schema and the first rows\n\nhead() allows to see the first rows\nUse the pipe |&gt; to chain operations"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#get-a-feeling-of-the-dataset",
    "href": "core/labs/lab-gapminder.html#get-a-feeling-of-the-dataset",
    "title": "Data visualization",
    "section": "Get a feeling of the dataset",
    "text": "Get a feeling of the dataset\n\n\n\n\n\n\nQuestion\n\n\n\nPick two random rows for each continent using slice_sample()\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat makes a table tidy?\n\n\n\n\n\n\n\n\nTip\n\n\n\nHave a look at Data tidying in R for Data Science (2nd ed.)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs the gapminder table redundant?"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#gapminder-tibble-extract",
    "href": "core/labs/lab-gapminder.html#gapminder-tibble-extract",
    "title": "Data visualization",
    "section": "Gapminder tibble (extract)",
    "text": "Gapminder tibble (extract)\n\n\n\n\n\n\nQuestion\n\n\n\nExtract/filter a subset of rows using dplyr::filter(...)\n\nAll rows concerning a given country\nAll rows concerning a year\nAll rows concerning a given continnent and a year"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#filtering-selection-σ-from-database-theory-picking-one-year-of-data",
    "href": "core/labs/lab-gapminder.html#filtering-selection-σ-from-database-theory-picking-one-year-of-data",
    "title": "Data visualization",
    "section": "Filtering (selection \\(σ\\) from database theory) : Picking one year of data",
    "text": "Filtering (selection \\(σ\\) from database theory) : Picking one year of data\nThere is simple way to filter rows satisfying some condition. It consists in mimicking indexation in a matrix, leaving the colum index empty, replacing the row index by a condition statement (a logical expression) also called a mask.\n\nCode# q: in gapminder table extract all raws concerning year 2002\n\ngapminder_2002 &lt;- gapminder |&gt;\n  filter(year==2002)  # \n\ngapminder_2002 &lt;- gapminder[gapminder$year==2002,]\n\n\nHave a look at\n\nCodegapminder$year==2002\n\n\nWhat is the type/class of this expression?\nThis is possible in base R and very often convenient.\nNevertheless, this way of performing row filtering does not emphasize the connection between the dataframe and the condition. Any logical vector with the right length could be used as a mask. Moreover, this way of performing filtering is not very functional.\n\n\n\n\n\n\nIn the parlance of Relational Algebra, filter performs a selection of rows. Relational expression\n\\[σ_{\\text{condition}}(\\text{Table})\\]\ntranslates to\n\nCodefilter(Table, condition)\n\n\nwhere \\(\\text{condition}\\) is a boolean expression that can be evaluated on each row of \\(\\text{Table}\\). In SQL, the relational expression would translate into\n\nCodeSELECT \n  *\nFROM \n  Table\nWHERE \n  condition\n\n\nCheck Package dplyr docs\nThe posit cheatsheet on dplyr is an unvaluable resource for table manipulation.\n\n\n\nUse dplyr::filter() to perform row filtering"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#static-plotting-first-attempt",
    "href": "core/labs/lab-gapminder.html#static-plotting-first-attempt",
    "title": "Data visualization",
    "section": "Static plotting: First attempt",
    "text": "Static plotting: First attempt\n\n\n\n\n\n\nQuestion\n\n\n\nDefine a plot with respect to gapminder_2002 along the lines suggested by Rosling’s presentation.\n\n\n\n\n\n\n\n\nYou should define a ggplot object with data layer gapminder_2022 and call this object p for further reuse.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMap variables gdpPercap and lifeExp to axes x and y. Define the axes. In ggplot2 parlance, this is called aesthetic mapping. Use aes().\n\n\n\n\n\n\n\n\nUse ggplot object p and add a global aesthetic mapping gdpPercap and lifeExp to axes x and y (using + from ggplot2) .\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nFor each row, draw a point at coordinates defined by the mapping. You need to add a geom_ layer to your ggplot object, in this case geom_point() will do.\n\n\n\n\n\n\n\n\nWhat’s up?\n\n\n\nWe are building a graphical object (a ggplot object) around a data frame (gapminder)\nWe supply aesthetic mappings (aes()) that can be either global or specifically bound to some geometries (geom_point()) or statistics\nThe global aesthetic mapping defines which columns (variables) are\n\nmapped to position (which columns are mapped to axes),\npossibly mapped to colours, linetypes, shapes, …\n\nGeometries and Statistics describe the building blocks of graphics\n\n\nWhat’s missing here?\nwhen comparing to the Gapminder demonstration, we can spot that\n\ncolors are missing\nbubble sizes are all the same. They should reflect the population size of the country\ntitles and legends are missing. This means the graphic object is useless.\n\nWe will add other layers to the graphical object to complete the plot"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#second-attempt-display-more-information",
    "href": "core/labs/lab-gapminder.html#second-attempt-display-more-information",
    "title": "Data visualization",
    "section": "Second attempt: display more information",
    "text": "Second attempt: display more information\n\n\n\n\n\n\nQuestion\n\n\n\n\nMap continent to color (use aes())\nMap pop to bubble size (use aes())\nMake point transparent by tuning alpha (inside geom_point() avoid overplotting)"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#scaling",
    "href": "core/labs/lab-gapminder.html#scaling",
    "title": "Data visualization",
    "section": "Scaling",
    "text": "Scaling\nTo pay tribute to Hans Rosling, we need to take care of two scaling issues:\n\nthe gdp per capita axis should be logarithmic scale_x_log10()\n\nthe area of the point should be proportional to the population scale_size_area()\n\n\n\n\n\n\n\n\nComplete the graphical object accordingly\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMotivate the proposed scalings.\n\nWhy is it important to use logarithmic scaling for gdp per capita?\nWhen is it important to use logarithmic scaling on some axis (in other contexts)?\nWhy is it important to specify scale_size_area() ?"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#in-perspective",
    "href": "core/labs/lab-gapminder.html#in-perspective",
    "title": "Data visualization",
    "section": "In perspective",
    "text": "In perspective\n\n\n\n\n\n\nQuestion\n\n\n\nUsing copilots completions, we can summarize the construcion of the graphical object in a series of questions.\n# q: Define a plot with respect to table gapminder_2002 along the lines suggested by Rosling's TED presentation\n# q: Map variables gdpPercap and lifeExp to axes x and y. Define the axes. \n# q: For each row, draw a point at coordinates defined by the mapping.\n# q: Map continent to color\n# q: Map pop to bubble size\n# q: Make point transparent by tuning alpha (inside geom_point() avoid overplotting)\n# q: Add a plot title\n# q: Make axes titles explicit and readable\n# q: Use labs(...)  \n# q: Use scale_x_log10() and scale_size_area()\n# q: Fine tune the guides: replace pop by Population and titlecase continent\n# q: Use theme_minimal()\n# q: Use scale_color_manual(...) to fine tune the color aesthetic mapping.\n# q: Use facet_zoom() from package ggforce\n# q: Add labels to points. This can be done by aesthetic mapping. Use aes(label=..)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat should be the respective purposes of Title, Subtitle, Caption, … ?"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#theming-using-ggthemes-or-not",
    "href": "core/labs/lab-gapminder.html#theming-using-ggthemes-or-not",
    "title": "Data visualization",
    "section": "Theming using ggthemes (or not)",
    "text": "Theming using ggthemes (or not)\n\nCodestopifnot(\n  require(\"ggthemes\")\n)\n\n\nA theme defines the look and feel of plots\nWithin a single document, we should use only one theme\nSee Getting the theme for a gallery of available themes\n\nCodep +\n  theme_economist()"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#tuning-scales",
    "href": "core/labs/lab-gapminder.html#tuning-scales",
    "title": "Data visualization",
    "section": "Tuning scales",
    "text": "Tuning scales\n\n\n\n\n\n\nQuestion\n\n\n\nUse scale_color_manual(...) to fine tune the color aesthetic mapping.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nChoosing a color scale is a difficult task\nviridis is often a good pick."
  },
  {
    "objectID": "core/labs/lab-gapminder.html#zooming-on-a-continent",
    "href": "core/labs/lab-gapminder.html#zooming-on-a-continent",
    "title": "Data visualization",
    "section": "Zooming on a continent",
    "text": "Zooming on a continent\n\nCodezoom_continent &lt;- 'Europe'  # choose another continent at your convenience \n\n\n\n\n\n\n\n\nUse facet_zoom() from package ggforce"
  },
  {
    "objectID": "core/labs/lab-gapminder.html#adding-labels",
    "href": "core/labs/lab-gapminder.html#adding-labels",
    "title": "Data visualization",
    "section": "Adding labels",
    "text": "Adding labels\n\n\n\n\n\n\nQuestion\n\n\n\nAdd labels to points. This can be done by aesthetic mapping. Use aes(label=..)\nTo avoid text cluttering, package ggrepel offers interesting tools."
  },
  {
    "objectID": "core/labs/lab-gapminder.html#facetting",
    "href": "core/labs/lab-gapminder.html#facetting",
    "title": "Data visualization",
    "section": "Facetting",
    "text": "Facetting\nSo far we have only presented one year of data (2002)\nRosling used an animation to display the flow of time\nIf we have to deliver a printable report, we cannot rely on animation, but we can rely on facetting\nFacets are collections of small plots constructed in the same way on subsets of the data\n\n\n\n\n\n\nQuestion\n\n\n\nAdd a layer to the graphical object using facet_wrap()\n\n\n\nAs all rows in gapminder_2002 are all related to year 2002, we need to rebuild the graphical object along the same lines (using the same graphical pipeline) but starting from the whole gapminder dataset.\nShould we do this using cut and paste?\n No!!!\n\n\n\n\n\n\n\nDon’t Repeat Yoursel (DRY)\n\n\n\n\nAbide to the DRY principle using operator %+%: the ggplot2 object p can be fed with another dataframe and all you need is proper facetting."
  },
  {
    "objectID": "core/labs/lab-gapminder.html#animate-for-free-with-plotly",
    "href": "core/labs/lab-gapminder.html#animate-for-free-with-plotly",
    "title": "Data visualization",
    "section": "Animate for free with plotly\n",
    "text": "Animate for free with plotly\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse plotly::ggplotly() to create a Rosling like animation.\nUse frame aesthetics."
  },
  {
    "objectID": "core/labs/lab-gapminder.html#suggestions",
    "href": "core/labs/lab-gapminder.html#suggestions",
    "title": "Data visualization",
    "section": "Suggestions",
    "text": "Suggestions\nThink about ways to visualize specific aspects of the gapminder data.\n\nHow could you overlay the world in 1952 and 2007?\nHow could you visualize the evolution of life expectancy and population across the different countries?\nVisualize the evolution of former colonies and their colonizers.\nVisualize the evolution of countries from the former Soviet Union, Warsaw Pact, and Yugoslavia.\nVisualize the evolution of countries from the former British Empire."
  },
  {
    "objectID": "core/labs/lab-gapminder.html#more-material",
    "href": "core/labs/lab-gapminder.html#more-material",
    "title": "Data visualization",
    "section": "More material",
    "text": "More material\n\nRead Visualization in R for Data Science"
  },
  {
    "objectID": "core/labs/lab-tables.html#setup",
    "href": "core/labs/lab-tables.html#setup",
    "title": "Tables manipulation II",
    "section": "Setup",
    "text": "Setup\nWe will use the following packages. If needed, we install them.\n\nCodeold_theme &lt;- theme_set(theme_minimal())\n\n\nCheck nycflights13 for any explanation concerning the tables and their columns."
  },
  {
    "objectID": "core/labs/lab-tables.html#data-loading",
    "href": "core/labs/lab-tables.html#data-loading",
    "title": "Tables manipulation II",
    "section": "Data loading",
    "text": "Data loading\n\nCodeflights &lt;- nycflights13::flights\nweather &lt;- nycflights13::weather\nairports &lt;- nycflights13::airports\nairlines &lt;- nycflights13::airlines\nplanes &lt;- nycflights13::planes\n\n\n\nCodecon &lt;- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\nflights_lite &lt;- copy_to(con, nycflights13::flights)\nairports_lite &lt;- copy_to(con, nycflights13::airports)\nplanes_lite &lt;-  copy_to(con, nycflights13::planes)\nweather_lite &lt;- copy_to(con, nycflights13::weather)\nairlines_lite &lt;- copy_to(con, nycflights13::airlines)\n\n\n\nCodeflights_lite |&gt;\n  select(contains(\"delay\")) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT `dep_delay`, `arr_delay`\nFROM `nycflights13::flights`\n\n\nView data in spreadsheet style.\n\nCodeView(flights)\n\n\nAsk for help about table flights"
  },
  {
    "objectID": "core/labs/lab-tables.html#first-queries-the-dplyr-way",
    "href": "core/labs/lab-tables.html#first-queries-the-dplyr-way",
    "title": "Tables manipulation II",
    "section": "First Queries (the dplyr way)",
    "text": "First Queries (the dplyr way)\nFind all flights that\n\nHad an arrival delay of two or more hours\n\n\nFlew to Houston (IAH or HOU)\n\n\nWere operated by United, American, or Delta\n\n\n\n\n\n\n\nPackage stringr could be useful.\n\nCodeairlines |&gt; \n  filter(stringr::str_starts(name, \"United\") |\n        stringr::str_starts(name, \"American\") |\n        stringr::str_starts(name, \"Delta\"))\n\n# A tibble: 3 × 2\n  carrier name                  \n  &lt;chr&gt;   &lt;chr&gt;                 \n1 AA      American Airlines Inc.\n2 DL      Delta Air Lines Inc.  \n3 UA      United Air Lines Inc. \n\nCodeairlines |&gt; \n  filter(stringr::str_detect(name, (\"United|American|Delta\"))) |&gt; \n  pull(carrier)\n\n[1] \"AA\" \"DL\" \"UA\"\n\n\n#| eval: false\nairlines_lite |&gt; \n  filter(stringr::str_starts(name, \"United\") |\n        stringr::str_starts(name, \"American\") |\n        stringr::str_starts(name, \"Delta\")) |&gt; \n  show_query()\nSELECT *\nFROM `nycflights13::airlines`\nWHERE \"name\" LIKE 'United%' OR \n      \"name\" LIKE 'American%' OR \n      \"name\" LIKE 'Delta%' ;\nstringr is part of tidyverse\n\n\n\n\nDeparted in summer (July, August, and September)\n\n\n\n\n\n\n\nWhen manipulating temporal information (date, time, duration), keep an eye on what lubridate offers. The API closely parallels what RDMS and Python offer.\n\n\n\n\nArrived more than two hours late, but didn’t leave late\n\n\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\nDeparted between midnight and 6am (inclusive)\n\n\n\n\n\n\n\nRead filter() in R for Data Science 1st Ed\nRead Chapter Transform in R for Data Science 2nd Ed"
  },
  {
    "objectID": "core/labs/lab-tables.html#missing-data",
    "href": "core/labs/lab-tables.html#missing-data",
    "title": "Tables manipulation II",
    "section": "Missing data",
    "text": "Missing data\n\nHow many flights per origin have a missing dep_time?\n\n\nWhat other variables are missing?\n\n\n\n\n\n\n\nThe introduction to tidyselect is a must read.\n\n\n\n\nWhat might these rows with missing data represent?\n\n\nCodenot_cancelled &lt;-  flights |&gt; \n  filter(!is.na(dep_time))\n\n\n\n\nMore questions: for each column in flight report the number of missing values."
  },
  {
    "objectID": "core/labs/lab-tables.html#arrange",
    "href": "core/labs/lab-tables.html#arrange",
    "title": "Tables manipulation II",
    "section": "Arrange",
    "text": "Arrange\n\nHow could you use arrange() to sort all missing values to the start? (Hint: use is.na()).\n\n\nSort flights to find the most delayed flights.\n\n\nPick the ten most delayed flights (with finite dep_delay)\n\n\nFind the flights that left earliest.\n\n\nSort flights to find the fastest (highest speed) flights.\n\n\nWhich flights travelled the farthest?\n\n\n\n\n\n\n\nThe database provides all we need with columns distance and air_time. Otherwise, with the positions of airports from table airports, we should be able to compute distances using :\n\n‘Haversine’ formula.\n\nhttps://en.wikipedia.org/wiki/Haversine_formula\n\n\n\n\nWhich travelled the shortest?"
  },
  {
    "objectID": "core/labs/lab-tables.html#projection",
    "href": "core/labs/lab-tables.html#projection",
    "title": "Tables manipulation II",
    "section": "Projection",
    "text": "Projection\n\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\n\n\nWhat happens if you include the name of a variable multiple times in a select() call?\n\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\nvars &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\nDoes the result of running the following code surprise you?\n\n\nCodeselect(\n  flights, \n  contains(\"TIME\", ignore.case =TRUE))  |&gt; \n  head()\n\n# A tibble: 6 × 6\n  dep_time sched_dep_time arr_time sched_arr_time air_time time_hour          \n     &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dttm&gt;             \n1      517            515      830            819      227 2013-01-01 05:00:00\n2      533            529      850            830      227 2013-01-01 05:00:00\n3      542            540      923            850      160 2013-01-01 05:00:00\n4      544            545     1004           1022      183 2013-01-01 05:00:00\n5      554            600      812            837      116 2013-01-01 06:00:00\n6      554            558      740            728      150 2013-01-01 05:00:00\n\n\n\nHow do the select helpers deal with case by default?\n\n\nHow can you change that default?"
  },
  {
    "objectID": "core/labs/lab-tables.html#mutations",
    "href": "core/labs/lab-tables.html#mutations",
    "title": "Tables manipulation II",
    "section": "Mutations",
    "text": "Mutations\n\nCurrently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.\n\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\n\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\n\n\nFind the 10 most delayed flights using a ranking function. How do you want to handle ties?\n\n\nCarefully read the documentation for min_rank().\nWindowed rank functions."
  },
  {
    "objectID": "core/labs/lab-tables.html#aggregations",
    "href": "core/labs/lab-tables.html#aggregations",
    "title": "Tables manipulation II",
    "section": "Aggregations",
    "text": "Aggregations\n\nBrainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios:\n\nA flight is 15 minutes early 50% of the time, and 15 minutes late 10% of the time.\nA flight is always 10 minutes late.\nA flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time.\n99% of the time a flight is on time. 1% of the time it’s 2 hours late.\n\n\n\n\nCodeflights |&gt; \n  group_by(dest) |&gt; \n  summarise(n_cancelled = sum(is.na(dep_time)))\n\n# A tibble: 105 × 2\n   dest  n_cancelled\n   &lt;chr&gt;       &lt;int&gt;\n 1 ABQ             0\n 2 ACK             0\n 3 ALB            20\n 4 ANC             0\n 5 ATL           317\n 6 AUS            21\n 7 AVL            12\n 8 BDL            31\n 9 BGR            15\n10 BHM            25\n# ℹ 95 more rows\n\n\n\nCodeflights_lite |&gt; \n  group_by(dest) |&gt; \n  summarise(n_cancelled = sum(is.na(dep_time))) |&gt; \n  show_query()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n&lt;SQL&gt;\nSELECT `dest`, SUM((`dep_time` IS NULL)) AS `n_cancelled`\nFROM `nycflights13::flights`\nGROUP BY `dest`\n\n\n\nWhich is more important: arrival delay or departure delay?\n\n\nCome up with another approach that will give you the same output as not_cancelled |&gt; count(dest) and (without usingcount()`).\n\n\nOur definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay) ) is slightly suboptimal. Why? Which is the most important column?\n\n\nLook at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay?\n\n\nWhich carrier has the worst delays?\n\nChallenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarise(n()))\n\nWhat does the sort argument to count() do. When might you use it?"
  },
  {
    "objectID": "core/labs/lab-tables.html#miscellanea",
    "href": "core/labs/lab-tables.html#miscellanea",
    "title": "Tables manipulation II",
    "section": "Miscellanea",
    "text": "Miscellanea\n\nWhich carriers serve all destination airports (in the table) ?\n\n\nRefer back to the lists of useful mutate and filtering functions.\nDescribe how each operation changes when you combine it with grouping.\n\n\nWhich plane (tailnum) has the worst on-time record amongst planes with at least ten flights?\n\n\nWhat time of day should you fly if you want to avoid delays as much as possible?\n\n\nFor each destination, compute the total minutes of delay.\n\n\nFor each flight, compute the proportion of the total positive arrival delays for its destination.\n\nUsing dplyr, it is easy. See A second look at group_by\n\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the delay of a flight is related to the delay of the immediately preceding flight.\n\n\n\n\n\n\n\nlag() is an example of window function. If we were using SQL, we would define a WINDOW using an expression like\nWINDOW w As (PARTITION BY origin ORDER BY year, month, day, sched_dep_time)\nSomething still needs fixing here: some flights never took off (is.na(dep_time)). Should they be sided out? assigned an infinite departure delay?\n\n\n\n\nLook at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\n\nConsider all flights with average speed above \\(950\\text{km/h}\\) as suspicious.\nLet us visualize destinations and origins of the speedy flights.\n\nFind all destinations that are flown by at least two carriers. Use that information to rank the carriers.\n\n\nFor each plane, count the number of flights before the first delay greater than 1 hour.\n\n\n\n\n\n\n\nAssume a plane is characterized by tailnum. Some flights have no tailnum. We ignore them."
  },
  {
    "objectID": "core/labs/lab-tables.html#references",
    "href": "core/labs/lab-tables.html#references",
    "title": "Tables manipulation II",
    "section": "References",
    "text": "References\n\nData transformation cheatsheet\nR4Data Science Tidy\nBenchmarking\ndplyr and vctrs\nPosts on dplyr\nWindow functions on dplyr"
  },
  {
    "objectID": "core/labs/lab-univariate-numeric.html#numerical-summary",
    "href": "core/labs/lab-univariate-numeric.html#numerical-summary",
    "title": "LAB: Univariate analysis",
    "section": "Numerical summary",
    "text": "Numerical summary\nUse skimr::skim()\n\n\n\n\n\n\nQuestion\n\n\n\nCompare mean and median, sd and IQR.\nAre mean and median systematically related?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre standard deviation and IQR systematically related ?"
  },
  {
    "objectID": "core/labs/lab-univariate-numeric.html#boxplots",
    "href": "core/labs/lab-univariate-numeric.html#boxplots",
    "title": "LAB: Univariate analysis",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n\n\n\n\nQuestion\n\n\n\nDraw a boxplot of the Age distribution\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you get rid of the useless ticks on the x-axis?"
  },
  {
    "objectID": "core/labs/lab-univariate-numeric.html#histograms",
    "href": "core/labs/lab-univariate-numeric.html#histograms",
    "title": "LAB: Univariate analysis",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nQuestion\n\n\n\nPlot a histogram of the empirical distribution of the AGE column\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nTry different values for the bins parameter of geom_histogram()"
  },
  {
    "objectID": "core/labs/lab-univariate-numeric.html#density-estimates",
    "href": "core/labs/lab-univariate-numeric.html#density-estimates",
    "title": "LAB: Univariate analysis",
    "section": "Density estimates",
    "text": "Density estimates\n\n\n\n\n\n\nQuestion\n\n\n\nPlot a density estimate of the AGE column (use stat_density.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPlay with parameters bw, kernel and adjust.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nOverlay the two plots (histogram and density)."
  },
  {
    "objectID": "core/labs/lab-univariate-numeric.html#ecdf",
    "href": "core/labs/lab-univariate-numeric.html#ecdf",
    "title": "LAB: Univariate analysis",
    "section": "ECDF",
    "text": "ECDF\n\n\n\n\n\n\nQuestion\n\n\n\nPlot the Empirical CDF of the AGE distribution\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you read the quartiles from the ECDF pplot?"
  },
  {
    "objectID": "core/labs/lab-univariate-numeric.html#quantile-function",
    "href": "core/labs/lab-univariate-numeric.html#quantile-function",
    "title": "LAB: Univariate analysis",
    "section": "Quantile function",
    "text": "Quantile function\n\n\n\n\n\n\nQuestion\n\n\n\nPlot the quantile function of the AGE distribution."
  },
  {
    "objectID": "core/projects/hmw_gapminder_oecd.html",
    "href": "core/projects/hmw_gapminder_oecd.html",
    "title": "Hmw I : Tables and visualization",
    "section": "",
    "text": "Important\n\n\n\n\nDue : February 7, 2025\nWork in pairs\nDeliver your work as a qmd file through a github  repository\nUse the quarto package for reproducible research\nThe report should be rendered at least in HTML format, and possibly also in PDF format"
  },
  {
    "objectID": "core/projects/hmw_gapminder_oecd.html#report-organization",
    "href": "core/projects/hmw_gapminder_oecd.html#report-organization",
    "title": "Hmw I : Tables and visualization",
    "section": " Report organization",
    "text": "Report organization\nThe first part (introduction) of the report shall be dedicated to the description of the data you have downloaded. You shall motivate your choice and non-trivial aspects of the data (for example if you were discussing GDP per capita against Life expectancy, you should remind the reader about the definition of Life expectancy and GDP). You shall also give a hint about why you intend to plot some variables against others.\nThe second part (results) shall be dedicated to plots and animations. Commenting a plot is not paraphrasing. It consists in adding informations and explanations that are not already in and around the plot (this includes the plot itself, title, subtitle, caption, and guides). It also consists in questions and issues that the plot raises. For example, in the Gapminder presentation, the apparent connection between life expectancy and GDP per capita deserves to be discussed (is it stationary? is it homogeneous throughout continents ? …). Refrain from overplaying your hand: yours plots are not likely to provide causal explanations. Comment the data, all the data, and nothing but the data.\nThe third part is the appendix. The first two parts should be text and plots only. The third part should be code only.\nThe appendix shall be dedicated to the description of the data wrangling pipeline. You shall give the code.\nYou shall also give the code of the graphical pipelines in the appendix.\nYou shall avoid copy-paste coding. Don’t Repeat Yourself. The tidyverse is your friend. knitr provide the tools to organize the Quarto file so that you can write your code once and use it many times, once for data wrangling and plotting (without echoing), then for listing and explanation.\n\n\n\n\n\n\nTip for organizing the report\n\n\n\nLook at fake report organized along this principles. Note that works with the knitr engine.\nHave a look at Rmarkdown the definitive guide to leanr about knitr tricks.\nThis trick is described in Section 4.19"
  },
  {
    "objectID": "core/projects/hmw_gapminder_oecd.html#grading-criteria",
    "href": "core/projects/hmw_gapminder_oecd.html#grading-criteria",
    "title": "Hmw I : Tables and visualization",
    "section": " Grading criteria",
    "text": "Grading criteria\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nNarrative, spelling and syntax\n25%\nEnglish/French \n\n\nPlots correction\n20%\nchoice of aesthetics, geom, scale … \n\n\nPlot style\n15%\nTitles, legends, labels, breaks … \n\n\nTable wrangling\n15%\nETL, SQL like manipulations \n\n\nComputing Statistics\n5%\nAggregations, LR, PCA, CA, … \n\n\nDRY compliance\n20%\nDRY principle at  Wikipedia"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Course team",
    "section": "",
    "text": "Teacher 2024-25\n\n\n\n\n Former contributors\n\nAurélie Fischer\nSothéa Has\nClément Levrard\nMaud Thomas",
    "crumbs": [
      "Information",
      "Team"
    ]
  },
  {
    "objectID": "labs-listings.html",
    "href": "labs-listings.html",
    "title": "Labs",
    "section": "",
    "text": "Note\n\n\n\nSessions are organized around labs. Feel free to look at the lab before sessions. Do not rush to solutions proposed here\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 15, 2025\n\n\nIntroduction and Visualization\n\n\nVisualization, Public Statistics\n\n\n\n\nJan 15, 2025\n\n\nBrush up your R\n\n\nR language, Tidyverse, IDE\n\n\n\n\nJan 22, 2025\n\n\nTable wranglig\n\n\nR language, dplyr, tabula data\n\n\n\n\nJan 29, 2025\n\n\nUnivariate categorical data\n\n\nUnivariate data, GSS\n\n\n\n\nJan 30, 2025\n\n\nUnivariate numeric data\n\n\nUnivariate data, GSS\n\n\n\n\nFeb 5, 2025\n\n\nBivariate data\n\n\nbivariate data, mosaicplots, scatterplots, simple linear regression\n\n\n\n\nFeb 19, 2025\n\n\nLinear Regression I\n\n\nLinear regression, OLS, lm\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nTip\n\n\n\nBefore working out a lab, make sure the relevant packages are installed in your environment.",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "posit-cloud.html",
    "href": "posit-cloud.html",
    "title": "Positcloud",
    "section": "",
    "text": "Website",
    "crumbs": [
      "Support",
      "Posit cloud"
    ]
  },
  {
    "objectID": "quarto-format.html#a-translator",
    "href": "quarto-format.html#a-translator",
    "title": "Quarto",
    "section": "A translator",
    "text": "A translator",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "quarto-format.html#quarto-and-rstudio",
    "href": "quarto-format.html#quarto-and-rstudio",
    "title": "Quarto",
    "section": "Quarto and rstudio",
    "text": "Quarto and rstudio",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "quarto-format.html#quarto-and-vs-code",
    "href": "quarto-format.html#quarto-and-vs-code",
    "title": "Quarto",
    "section": "Quarto and vs code",
    "text": "Quarto and vs code",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "quarto-format.html#command-line-tool",
    "href": "quarto-format.html#command-line-tool",
    "title": "Quarto",
    "section": "Command Line Tool",
    "text": "Command Line Tool",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "slides-listings.html",
    "href": "slides-listings.html",
    "title": "Slides",
    "section": "",
    "text": "Slides summarize the lectures. Feel free to watch them before and after the lectures.\n point to material to be developped on blackboard.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nDec 21, 2021\n\n\nTables manipulation with dplyr\n\n\ndplyr, SQL, tables\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nMode d’emploi\n\n\n\nSlides use libraries revealjs or remark from . They are displayed in your browser.\nTo get help, press",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Olympe de Gouges (163) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar",
    "crumbs": [
      "Journal",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#labs",
    "href": "weeks/week-2.html#labs",
    "title": "Week 2",
    "section": " Labs",
    "text": "Labs\nWe revisit the first two labs (solutions )\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\n\nWe use Rstudio, and our dedicated R project (without git and renv). We install packages gt, nycflights and slider.\nWe review our first slide deck:\n\nIntroduction to dplyr\n\nand dig into\n\nLab 3 - Working with dplyr",
    "crumbs": [
      "Journal",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#further-work",
    "href": "weeks/week-2.html#further-work",
    "title": "Week 2",
    "section": " Further work",
    "text": "Further work\n Review the content of the three labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints.",
    "crumbs": [
      "Journal",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#further-reading",
    "href": "weeks/week-2.html#further-reading",
    "title": "Week 2",
    "section": " Further reading",
    "text": "Further reading\n\nR for data science\nAdvanced R",
    "crumbs": [
      "Journal",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#logistics",
    "href": "weeks/week-2.html#logistics",
    "title": "Week 2",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎",
    "crumbs": [
      "Journal",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (RH10A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar",
    "crumbs": [
      "Journal",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#blackboard",
    "href": "weeks/week-4.html#blackboard",
    "title": "Week 4",
    "section": " Blackboard",
    "text": "Blackboard\n\nCategorical samples\n\n2-ways Contingency tables\nChi-square statistics\n\nNumeric samples\n\nCovariance\nCorrelation(s)",
    "crumbs": [
      "Journal",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#labs",
    "href": "weeks/week-4.html#labs",
    "title": "Week 4",
    "section": " Labs",
    "text": "Labs\n\nLab 6 - Introduction to bivariate analysis\n\nWe relied on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables",
    "crumbs": [
      "Journal",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#further-work",
    "href": "weeks/week-4.html#further-work",
    "title": "Week 4",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints.",
    "crumbs": [
      "Journal",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#further-reading",
    "href": "weeks/week-4.html#further-reading",
    "title": "Week 4",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\nCours de Statistique Fondamentale\n\nChap Tests du chi-deux\n\nSourav Chatterjee: A new correlation coefficient",
    "crumbs": [
      "Journal",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#logistics",
    "href": "weeks/week-4.html#logistics",
    "title": "Week 4",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎",
    "crumbs": [
      "Journal",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important\n\n\n\n\nSession I Buffon (RH10A) Wednesday 13h30-15h\nSession II Sophie Germain (2012) Friday 9h00-10h30\n Calendar",
    "crumbs": [
      "Journal",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#blackboard",
    "href": "weeks/week-6.html#blackboard",
    "title": "Week 6",
    "section": " Blackboard",
    "text": "Blackboard\n\nOrdinary Least Squares\nQR factorizatio\nRidge regression (Regularized Least Squares)\nPseudo-Inversion",
    "crumbs": [
      "Journal",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#labs",
    "href": "weeks/week-6.html#labs",
    "title": "Week 6",
    "section": " Labs",
    "text": "Labs\n\nLab 6 - Linear Regression I\n\nWe still rely on the previous labs\n\nLab 1 - Introduction to R and RStudio\nLab 2 - Introduction to Data Visualization using Gapminder dataset\nLab 3 - Working with dplyr\nLab 4 Univariate categorical variables\nLab 5 Univariate numeric variables",
    "crumbs": [
      "Journal",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#further-work",
    "href": "weeks/week-6.html#further-work",
    "title": "Week 6",
    "section": " Further work",
    "text": "Further work\n Review the content of the two labs. Work out every part you do not already know. Report an issue if you are unhappy with the proposed solutions/hints.",
    "crumbs": [
      "Journal",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#further-reading",
    "href": "weeks/week-6.html#further-reading",
    "title": "Week 6",
    "section": " Further reading",
    "text": "Further reading\n\nBin Yu and Rebecca Barter, Veridical Data Science\nR for data science\nAdvanced R\nCours de Statistique Fondamentale\n\nChap Tests du chi-deux\nModèles linéaires\n\nSourav Chatterjee: A new correlation coefficient",
    "crumbs": [
      "Journal",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#logistics",
    "href": "weeks/week-6.html#logistics",
    "title": "Week 6",
    "section": " Logistics",
    "text": "Logistics\n : you will work with the R programming language in this course.\nYou need either to install R, RStudio and VS Code on your computer, or to use your posit-cloud account.\n Install Quarto on your computer to render the .qmd files.\nPlease follow the instructions here to install R, RStudio, VS Code, and Quarto or to access posit-cloud.\n Please activate your ENT account (follow the instructions on Moodle). You will be able to access the PostGres server.\n\n\nBack to Agenda ⏎",
    "crumbs": [
      "Journal",
      "Week 6"
    ]
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tables-examples",
    "href": "core/slides/slides-dplyr.html#tables-examples",
    "title": "Tables manipulation with dplyr",
    "section": "Tables (examples)",
    "text": "Tables (examples)\n\nSpeadsheets (Excel)\n Relational tables\n\nDataframes in datascience frameworks\n\n\n: data.frame, tibble, …\n\n: pandas.dataframe\n\n\nspark: dataframe\n\n\nDask: dataframe\n\nand many others"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tables-why",
    "href": "core/slides/slides-dplyr.html#tables-why",
    "title": "Tables manipulation with dplyr",
    "section": "Tables (Why ?)",
    "text": "Tables (Why ?)\nIn Data Science, each framework comes with its own flavor(s) of table(s)\n Tables from relational databases serve as inspiration\nIn  legacy dataframes shape the life of statisticians and data scientists\nThe purpose of this session is\n\ndescribe dataframes from an end-user viewpoint (we leave aside implementations)\n\npresenting tools for\n\naccessing information within dataframes (querying)\nsummarizing information (aggregation queries)\ncleaning/cleaning dataframes (tidying)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#loading-tables-and-packages",
    "href": "core/slides/slides-dplyr.html#loading-tables-and-packages",
    "title": "Tables manipulation with dplyr",
    "section": "Loading tables and packages",
    "text": "Loading tables and packages\n\nrequire(\"tidyverse\")      # All we need is there\nrequire(\"nycflights13\")    # for flight data\nrequire(\"gt\")\nrequire(\"kableExtra\")\n# \ndata(flights)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#about-loaded-packages",
    "href": "core/slides/slides-dplyr.html#about-loaded-packages",
    "title": "Tables manipulation with dplyr",
    "section": "About loaded packages",
    "text": "About loaded packages\n\nMetapackage tidyverse provides tools to create, query, tidy dataframes as well as tools to load data from various sources and save them in persistent storage\nnycflights13 provides the dataframes we play with\ngt for tayloring table displays"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#the-flights-table",
    "href": "core/slides/slides-dplyr.html#the-flights-table",
    "title": "Tables manipulation with dplyr",
    "section": "The flights table",
    "text": "The flights table\n\n\n\nhead(flights) |&gt;\n  glimpse(width = 30) \n\nRows: 6\nColumns: 19\n$ year           &lt;int&gt; 2013,…\n$ month          &lt;int&gt; 1, 1,…\n$ day            &lt;int&gt; 1, 1,…\n$ dep_time       &lt;int&gt; 517, …\n$ sched_dep_time &lt;int&gt; 515, …\n$ dep_delay      &lt;dbl&gt; 2, 4,…\n$ arr_time       &lt;int&gt; 830, …\n$ sched_arr_time &lt;int&gt; 819, …\n$ arr_delay      &lt;dbl&gt; 11, 2…\n$ carrier        &lt;chr&gt; \"UA\",…\n$ flight         &lt;int&gt; 1545,…\n$ tailnum        &lt;chr&gt; \"N142…\n$ origin         &lt;chr&gt; \"EWR\"…\n$ dest           &lt;chr&gt; \"IAH\"…\n$ air_time       &lt;dbl&gt; 227, …\n$ distance       &lt;dbl&gt; 1400,…\n$ hour           &lt;dbl&gt; 5, 5,…\n$ minute         &lt;dbl&gt; 15, 2…\n$ time_hour      &lt;dttm&gt; 2013…\n\n\n\n\nA dataframe is a two-ways (two-dimensional) table\nhead(df) displays the first 6 rows of its first argument\nThe vectors making a dataframe may have different types/classes (a dataframe is not a matrix)\nCompare str(), glimpse(), head()"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#table-schema",
    "href": "core/slides/slides-dplyr.html#table-schema",
    "title": "Tables manipulation with dplyr",
    "section": "Table schema",
    "text": "Table schema\nA table is a list of columns\nEach column has\n\n\nname and\n\ntype (class in \n\n\n\n\nglimpse(flights,   #&lt;&lt;\n        width=50)\n\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12,…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 46…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, …\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 7…\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0,…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 201…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section",
    "href": "core/slides/slides-dplyr.html#section",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "flights has 19 columns\nEach column is a sequence (vector) of items with the same type/class\nAll columns have the same length\n\nflights has 336776 rows\nIn  parlance, a row is (often) called a tuple\n\nIn  parlance, a column is (often) called a variable"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#column-types",
    "href": "core/slides/slides-dplyr.html#column-types",
    "title": "Tables manipulation with dplyr",
    "section": "Column types",
    "text": "Column types\n\n\n\n\n\n\n\n\nclass\ncolumns\n\n\n\ninteger\n‘year’ ‘month’ ‘day’ ‘dep_time’ ‘sched_dep_time’ ‘arr_time’ ‘sched_arr_time’ ‘flight’\n\n\nnumeric\n‘dep_delay’ ‘arr_delay’ ‘air_time’ ‘distance’ ‘hour’ ‘minute’\n\n\ncharacter\n‘carrier’ ‘tailnum’ ‘origin’ ‘dest’\n\n\nPOSIXct\n‘time_hour’\n\n\nPOSIXt\n‘time_hour’\n\n\n\n\nA column, as a vector, may be belong to different classes\nOther classes: factor for categorical variables\nColumns dest, origin carrier could be coerced as factors\nShould columns dest and origin be coerced to the same factor?"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#nycflights13",
    "href": "core/slides/slides-dplyr.html#nycflights13",
    "title": "Tables manipulation with dplyr",
    "section": "nycflights13",
    "text": "nycflights13"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#columns-specification",
    "href": "core/slides/slides-dplyr.html#columns-specification",
    "title": "Tables manipulation with dplyr",
    "section": "Columns specification",
    "text": "Columns specification\n\n\n\nas.col_spec(flights)\n\n\n\ncols(\n  year = col_integer(),\n  month = col_integer(),\n  day = col_integer(),\n  dep_time = col_integer(),\n  sched_dep_time = col_integer(),\n  dep_delay = col_double(),\n  arr_time = col_integer(),\n  sched_arr_time = col_integer(),\n  arr_delay = col_double(),\n  carrier = col_character(),\n  flight = col_integer(),\n  tailnum = col_character(),\n  origin = col_character(),\n  dest = col_character(),\n  air_time = col_double(),\n  distance = col_double(),\n  hour = col_double(),\n  minute = col_double(),\n  time_hour = col_datetime(format = \"\")\n)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-1",
    "href": "core/slides/slides-dplyr.html#section-1",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "\\(\\approx\\) table schema in relational databases\nColumn specifications are useful when loading dataframes from structured text files like .csv files\n.csv files do not contain typing information\nFile loaders from package readr can be tipped about column classes using column specifications"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-2",
    "href": "core/slides/slides-dplyr.html#section-2",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "SQL stands for structured/simple Query Language\nA query language elaborated during the 1970’s at IBM by E. Codd\nGeared towards exploitation of collections of relational tables\nLess powerful but simpler to use than a programming language\ndplyr is a principled -friendly implementation of SQL ideas (and other things)\n\nAt the core of SQL lies the idea of a table calculus called relational algebra"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#relational-algebra-basics",
    "href": "core/slides/slides-dplyr.html#relational-algebra-basics",
    "title": "Tables manipulation with dplyr",
    "section": "Relational algebra (basics)",
    "text": "Relational algebra (basics)\nConvention: \\(R\\) is a table with columns \\(A_1, \\ldots, A_k\\)\n\n\n\n\n\n\nProjection (picking columns)\n\n\n\\(\\pi(R, A_1, A_3)\\)\n\n\n\n\n\n\n\n\n\nSelection/Filtering (picking rows)\n\n\n\\(\\sigma(R, {\\text{condition}})\\)\n\n\n\n\n\n\n\n\n\nJoin (mulitple tables operation)\n\n\n\\(\\bowtie(R,S, {\\text{condition}})\\)\n\n\n\n Any operation produces a table\n The schema of the derived table depends on the operation (but does not depend on the content/value of the operands)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-3",
    "href": "core/slides/slides-dplyr.html#section-3",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Table calculus relies on a small set of basic operations \\(\\pi, \\sigma, \\bowtie\\)\nEach operation has one or two table operands and produce a table\n There is more to SQL than relational algebra"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#projection-pi",
    "href": "core/slides/slides-dplyr.html#projection-pi",
    "title": "Tables manipulation with dplyr",
    "section": "Projection \\(\\pi\\)\n",
    "text": "Projection \\(\\pi\\)\n\n\\(\\pi(R, {A_1, A_3})\\)\nA projection \\(\\pi(\\cdot, {A_1, A_3})\\) is defined by a set of column names, say \\(A_1, A_3\\)\nIf \\(R\\) has columns with given names, the result is a table with names \\(A_1, A_3\\) and one row per row of \\(R\\)\nA projection is parametrized by a list of column names"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#package-dplyr",
    "href": "core/slides/slides-dplyr.html#package-dplyr",
    "title": "Tables manipulation with dplyr",
    "section": "\n Package dplyr\n",
    "text": "Package dplyr\n\n\n\n\nTranformation chapter in R4DS\nCheat sheet I\nCheat sheet II\n\n\n\n\n\n\nhttps://dplyr.tidyverse.org"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-4",
    "href": "core/slides/slides-dplyr.html#section-4",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Base  provides tools to perform relational algebra operations\nBut:\n\nBase  does not provide a consistent API\nThe lack of a consistent API makes operation chaining tricky"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#dplyr-verbs",
    "href": "core/slides/slides-dplyr.html#dplyr-verbs",
    "title": "Tables manipulation with dplyr",
    "section": "\ndplyr verbs",
    "text": "dplyr verbs\nFive basic verbs:\n\nPick observations/rows by their values (filter()) σ(…)\nPick variables by their names (select()) π(…)\nReorder the rows (arrange())\nCreate new variables with functions of existing variables (mutate())\nCollapse many values down to a single summary (summarise())\n\n\nAnd\n\n\ngroup_by() changes the scope of each function from operating on the entire dataset to operating on it group-by-group"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tidyverse",
    "href": "core/slides/slides-dplyr.html#tidyverse",
    "title": "Tables manipulation with dplyr",
    "section": "\n tidyverse",
    "text": "tidyverse\n\n\n\nAll verbs work similarly:\n\n\nThe first argument is a data frame (table).\n\n\nThe subsequent arguments describe what to do with the data frame, using the variable/column names (without quotes)\n\n\nThe result is a new data frame (table)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#dplyrselect-as-a-projection-operator-π",
    "href": "core/slides/slides-dplyr.html#dplyrselect-as-a-projection-operator-π",
    "title": "Tables manipulation with dplyr",
    "section": "\ndplyr::select() as a projection operator (π)",
    "text": "dplyr::select() as a projection operator (π)\n\\(\\pi(R, \\underbrace{A_1, \\ldots, A_3}_{\\text{column names}})\\)\nselect(R, A1, A3) #&lt;&lt;\nor, equivalently\nR |&gt; select(A1, A3) #&lt;&lt;\n |&gt; is the pipe operator\n x |&gt; f(y, z) is translated to f(x, y, z) and then evaluated"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#dplyrselect",
    "href": "core/slides/slides-dplyr.html#dplyrselect",
    "title": "Tables manipulation with dplyr",
    "section": "dplyr::select()",
    "text": "dplyr::select()\n\nFunction select has a variable number of arguments\nFunction select has a variable number of arguments\nFunction select allows to pick column by names (and much more)\nNote that in the current environment, there are no objects called A1, A3\nThe consistent API allows to use the pipe operator\n\n\n\n\n\n\n\nCaution\n\n\nThere is also a select() function in base R"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-tables",
    "href": "core/slides/slides-dplyr.html#toy-tables",
    "title": "Tables manipulation with dplyr",
    "section": "Toy tables",
    "text": "Toy tables\n\n\n\nspam &lt;- set.seed(42)\n\nR &lt;-  tibble(A1=seq(2, 10, 2),\n             A2=sample(letters, 5),\n             A3=seq(from=date(\"2021-10-21\"),\n                    to=date(\"2021-11-20\"),\n                    by=7),\n             D=sample(letters, 5))\n\nS &lt;- tibble(E=c(3,4,6,9, 10),\n            F=sample(letters, 5),\n            G=seq(from=date(\"2021-10-21\"),\n                   to=date(\"2021-10-21\")+4, by=1),\n            D=sample(letters,5)\n          )\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#projecting-toy-tables",
    "href": "core/slides/slides-dplyr.html#projecting-toy-tables",
    "title": "Tables manipulation with dplyr",
    "section": "Projecting toy tables",
    "text": "Projecting toy tables\n\n\n\nR |&gt; \n  dplyr::select(A2,D) |&gt; \n  knitr::kable(caption=\"Projecting R\")\n\n\nProjecting R\n\nA2\nD\n\n\n\nq\nr\n\n\ne\nq\n\n\na\no\n\n\nj\ng\n\n\nd\nd\n\n\n\n\n\n\n\nR |&gt; \n  dplyr::select(- where(is.character)) |&gt; \n  knitr::kable(caption=\"Projecting R, all but character columns\")\n\n\nProjecting R, all but character columns\n\nA1\nA3\n\n\n\n2\n2021-10-21\n\n\n4\n2021-10-28\n\n\n6\n2021-11-04\n\n\n8\n2021-11-11\n\n\n10\n2021-11-18"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#projecting-flights-on-origin-and-dest",
    "href": "core/slides/slides-dplyr.html#projecting-flights-on-origin-and-dest",
    "title": "Tables manipulation with dplyr",
    "section": "Projecting flights on origin and dest\n",
    "text": "Projecting flights on origin and dest\n\n\n\nflights |&gt;\n  select(origin, dest) |&gt;  #&lt;&lt;\n  head()\n\n\n# A tibble: 6 × 2\n  origin dest \n  &lt;chr&gt;  &lt;chr&gt;\n1 EWR    IAH  \n2 LGA    IAH  \n3 JFK    MIA  \n4 JFK    BQN  \n5 LGA    ATL  \n6 EWR    ORD  \n\n\nA more readable equivalent of\nhead(select(flights, origin, dest), 10)\nor\nSELECT \n  origin, dest\nFROM \n  flights\nLIMIT 6;"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#sigmar-textcondition",
    "href": "core/slides/slides-dplyr.html#sigmar-textcondition",
    "title": "Tables manipulation with dplyr",
    "section": "\\(\\sigma(R, \\text{condition})\\)",
    "text": "\\(\\sigma(R, \\text{condition})\\)\n\nA selection/filtering operation is defined by a condition that can be checked on the rows of tables with convenient schema\n\\(\\sigma(R, \\text{condition})\\) returns a table with the same schema as \\(R\\)\nThe resulting table contains the rows/tuples of \\(R\\) that satisfy \\(\\text{condition}\\)\n\\(\\sigma(R, \\text{FALSE})\\) returns an empty table with the same schema as \\(R\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#chaining-filtering-and-projecting",
    "href": "core/slides/slides-dplyr.html#chaining-filtering-and-projecting",
    "title": "Tables manipulation with dplyr",
    "section": "Chaining filtering and projecting",
    "text": "Chaining filtering and projecting\n\n\nstart &lt;- date(\"2021-10-27\")\nend &lt;- start + 21\n\nR |&gt;\n#  filter(A2 &gt; \"n\") |&gt;  #&lt;&lt;\n  filter(between(A3, start, end)) |&gt;\n  select(A1, A3) #&lt;&lt;\n\n\n# A tibble: 3 × 2\n     A1 A3        \n  &lt;dbl&gt; &lt;date&gt;    \n1     4 2021-10-28\n2     6 2021-11-04\n3     8 2021-11-11\n\n\n\n\nFiltering dropped one row\nProjecting dropped two columns"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#selecting-flights-based-on-origin-and-dest",
    "href": "core/slides/slides-dplyr.html#selecting-flights-based-on-origin-and-dest",
    "title": "Tables manipulation with dplyr",
    "section": "Selecting flights based on origin and dest\n",
    "text": "Selecting flights based on origin and dest\n\nand then projecting on dest, time_hour, carrier\n\n\nflights |&gt;\n  filter(dest %in% c('ATL', 'LAX'), #&lt;&lt;\n         origin == 'JFK') |&gt;\n  select(dest, time_hour, carrier) |&gt; #&lt;&lt;\n  head()\n\n\n# A tibble: 6 × 3\n  dest  time_hour           carrier\n  &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt;  \n1 LAX   2013-01-01 06:00:00 UA     \n2 ATL   2013-01-01 06:00:00 DL     \n3 LAX   2013-01-01 07:00:00 VX     \n4 LAX   2013-01-01 07:00:00 B6     \n5 LAX   2013-01-01 07:00:00 AA     \n6 ATL   2013-01-01 08:00:00 DL     \n\n\nIn SQL ( parlance:\nSELECT \n  dest, time_hour, carrier\nFROM \n  flights\nWHERE \n  dest IN ('ATL', 'LAX') AND\n  origin = 'JFK'\nLIMIT 6"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#logical-operations",
    "href": "core/slides/slides-dplyr.html#logical-operations",
    "title": "Tables manipulation with dplyr",
    "section": "Logical operations",
    "text": "Logical operations\n\nfilter(R, condition_1, condition_2) is meant to return the rows of R that satisfy condition_1 and condition_2\nfilter(R, condition_1 & condition_2) is an equivalent formulation\nfilter(R, condition_1 | condition_2) is meant to return the rows of R that satisfy condition_1 or condition_2 (possibly both)\nfilter(R, xor(condition_1,condition_2)) is meant to return the rows of R that satisfy either condition_1 or condition_2 (just one of them)\nfilter(R, ! condition_1) is meant to return the rows of R that do not satisfy condition_1"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#missing-values",
    "href": "core/slides/slides-dplyr.html#missing-values",
    "title": "Tables manipulation with dplyr",
    "section": "\n Missing values!",
    "text": "Missing values!\nNumerical column dep_time contains many NA's (missing values)\n\n# flights |&gt; pull(dep_time) |&gt; summary()\nsummary(flights$dep_time)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      1     907    1401    1349    1744    2400    8255 \n\n\n Missing values (NA and variants) should be handled with care\nNA & TRUE\n[1] NA\nNA | TRUE\n[1] TRUE"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#truth-tables-for-three-valued-logic",
    "href": "core/slides/slides-dplyr.html#truth-tables-for-three-valued-logic",
    "title": "Tables manipulation with dplyr",
    "section": "Truth tables for three-valued logic",
    "text": "Truth tables for three-valued logic\n\n\n  uses three-valued logic\n Generate complete truth tables for and, or, xor\n\nv &lt;- c(TRUE, FALSE, NA) # truth values\n\nlist_tt &lt;- map(c(`&`, `|`, xor),  #&lt;&lt;\n               ~ outer(v, v, .x)) #&lt;&lt;\n\nfor (i in seq_along(list_tt)){\n  colnames(list_tt[[i]]) &lt;- v\n  rownames(list_tt[[i]]) &lt;- v\n}\n\nnames(list_tt) &lt;- c('& AND',\n                    'OR',\n                    'XOR')\n\n\n\n\n\n\n& AND\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\n\nTRUE\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\nFALSE\n\n\nFALSE\n\n\nFALSE\n\n\nFALSE\n\n\n\n\nNA\n\n\nNA\n\n\nFALSE\n\n\nNA\n\n\n\n\n\n\n\n\nOR\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\n\nTRUE\n\n\nTRUE\n\n\nTRUE\n\n\nTRUE\n\n\n\n\nFALSE\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\nNA\n\n\nTRUE\n\n\nNA\n\n\nNA\n\n\n\n\n\n\n\n\nXOR\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\n\nTRUE\n\n\nFALSE\n\n\nTRUE\n\n\nNA\n\n\n\n\nFALSE\n\n\nTRUE\n\n\nFALSE\n\n\nNA\n\n\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#slice-choosing-rows-based-on-location",
    "href": "core/slides/slides-dplyr.html#slice-choosing-rows-based-on-location",
    "title": "Tables manipulation with dplyr",
    "section": "\nslice(): choosing rows based on location",
    "text": "slice(): choosing rows based on location\n\n\nIn base  dataframe cells can be addressed by indices\nflights[5000:5010,seq(1, 19, by=5)] returns rows 5000:5010 and columns 1, 6, 11 from dataframe flights\nThis can be done in a (verbose) dplyr way using slice() and select()\n\n\nflights |&gt;\n  slice(5001:5005) |&gt;  #&lt;&lt;\n  select(seq(1, 19, by=5))\n\n# A tibble: 5 × 4\n   year dep_delay flight distance\n  &lt;int&gt;     &lt;dbl&gt;  &lt;int&gt;    &lt;dbl&gt;\n1  2013         3   4437      602\n2  2013        43   1016      187\n3  2013        -2   2190     1089\n4  2013        -1     91     2576\n5  2013         5   2131      502\n\n\n\n combined with aggregation (group_by()) variants of slice_ may be used to perform windowing operations\n Useful variant slice_sample()"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-5",
    "href": "core/slides/slides-dplyr.html#section-5",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Note\n\n\n\\(\\bowtie(R,S, {\\text{condition}})\\)\nstands for\n\njoin rows/tuples of \\(R\\) and rows/tuples of \\(S\\) that satisfy \\(\\text{condition}\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#nycflights-tables",
    "href": "core/slides/slides-dplyr.html#nycflights-tables",
    "title": "Tables manipulation with dplyr",
    "section": "\nnycflights tables",
    "text": "nycflights tables\n\n\nThe nycflights13 package offers five related tables:\n\n\nFact tables:\n\nflights\n\nweather (hourly weather conditions at different locations)\n\n\n\nDimension tables:\n\n\nairports (airports full names, location, …)\n\nplanes (model, manufacturer, year, …)\n\nairlines (full names)\n\n\n\nThis is an instance of a Star Schema"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#about-star-schemas",
    "href": "core/slides/slides-dplyr.html#about-star-schemas",
    "title": "Tables manipulation with dplyr",
    "section": "About Star schemas\n",
    "text": "About Star schemas\n\n\nFact tables record measurements for a specific event\nFact tables generally consist of numeric values, and foreign keys to dimensional data where descriptive information is kept\nDimension tables record informations about entities involved in events recorded in Fact tables\n\n\n\nFrom Wikipedia]"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#weather-conditions",
    "href": "core/slides/slides-dplyr.html#weather-conditions",
    "title": "Tables manipulation with dplyr",
    "section": "\n weather conditions",
    "text": "weather conditions\n\n\nweather |&gt;\n  glimpse(width = 50)\n\n\nRows: 26,115\nColumns: 15\n$ origin     &lt;chr&gt; \"EWR\", \"EWR\", \"EWR\", \"EWR\", \"…\n$ year       &lt;int&gt; 2013, 2013, 2013, 2013, 2013,…\n$ month      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ day        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ hour       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10…\n$ temp       &lt;dbl&gt; 39.02, 39.02, 39.02, 39.92, 3…\n$ dewp       &lt;dbl&gt; 26.06, 26.96, 28.04, 28.04, 2…\n$ humid      &lt;dbl&gt; 59.37, 61.63, 64.43, 62.21, 6…\n$ wind_dir   &lt;dbl&gt; 270, 250, 240, 250, 260, 240,…\n$ wind_speed &lt;dbl&gt; 10.35702, 8.05546, 11.50780, …\n$ wind_gust  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, N…\n$ precip     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pressure   &lt;dbl&gt; 1012.0, 1012.3, 1012.5, 1012.…\n$ visib      &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 1…\n$ time_hour  &lt;dttm&gt; 2013-01-01 01:00:00, 2013-01…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#connecting-flights-and-weather",
    "href": "core/slides/slides-dplyr.html#connecting-flights-and-weather",
    "title": "Tables manipulation with dplyr",
    "section": "Connecting flights and weather\n",
    "text": "Connecting flights and weather\n\nWe want to complement information in flights using data weather\nMotivation: we would like to relate delays (arr_delay) and weather conditions\n\ncan we explain (justify) delays using weather data?\ncan we predict delays using weather data?"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-6",
    "href": "core/slides/slides-dplyr.html#section-6",
    "title": "Tables manipulation with dplyr",
    "section": "\n ⋈ \n",
    "text": "⋈ \n\nFor each flight (row in flights)\n\nyear, month, day, hour (computed from time_hour) indicate the approaximate time of departure\norigin indicates the airport where the plane takes off\n\nEach row of weather contains corresponding information\n for each row of flights we look for rows of weather with matching values in year, month, day, hour and origin\n NATURAL INNER JOIN between the tables"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#inner_join-natural-join",
    "href": "core/slides/slides-dplyr.html#inner_join-natural-join",
    "title": "Tables manipulation with dplyr",
    "section": "\ninner_join: natural join",
    "text": "inner_join: natural join\n\n\nf_w &lt;- flights |&gt;\n  inner_join(weather) #&lt;&lt;\n\nf_w |&gt; \n  select(seq(1, \n             ncol(f_w),\n             by=2)) |&gt; \n  glimpse(width=50)\n\n\nRows: 335,220\nColumns: 14\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12,…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 46…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 201…\n$ dewp           &lt;dbl&gt; 28.04, 24.98, 26.96, 26.9…\n$ wind_dir       &lt;dbl&gt; 260, 250, 260, 260, 260, …\n$ wind_gust      &lt;dbl&gt; NA, 21.86482, NA, NA, 23.…\n$ pressure       &lt;dbl&gt; 1011.9, 1011.4, 1012.1, 1…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#join-schema",
    "href": "core/slides/slides-dplyr.html#join-schema",
    "title": "Tables manipulation with dplyr",
    "section": "Join schema",
    "text": "Join schema\n\n\nRows: 335,220\nColumns: 28\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12,…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 46…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, …\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 7…\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0,…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 201…\n$ temp           &lt;dbl&gt; 39.02, 39.92, 39.02, 39.0…\n$ dewp           &lt;dbl&gt; 28.04, 24.98, 26.96, 26.9…\n$ humid          &lt;dbl&gt; 64.43, 54.81, 61.63, 61.6…\n$ wind_dir       &lt;dbl&gt; 260, 250, 260, 260, 260, …\n$ wind_speed     &lt;dbl&gt; 12.65858, 14.96014, 14.96…\n$ wind_gust      &lt;dbl&gt; NA, 21.86482, NA, NA, 23.…\n$ precip         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pressure       &lt;dbl&gt; 1011.9, 1011.4, 1012.1, 1…\n$ visib          &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 1…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-7",
    "href": "core/slides/slides-dplyr.html#section-7",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "The schema of the result is the union of the schemas of the operands\nA tuple from flights matches a tuple from weather if the tuple have the same values in the common columns:\n\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\"      \"temp\"          \n[21] \"dewp\"           \"humid\"          \"wind_dir\"       \"wind_speed\"    \n[25] \"wind_gust\"      \"precip\"         \"pressure\"       \"visib\""
  },
  {
    "objectID": "core/slides/slides-dplyr.html#which-columns-are-used-when-joining-tables-r-and-s",
    "href": "core/slides/slides-dplyr.html#which-columns-are-used-when-joining-tables-r-and-s",
    "title": "Tables manipulation with dplyr",
    "section": "Which columns are used when joining tables \\(R\\) and \\(S\\)?",
    "text": "Which columns are used when joining tables \\(R\\) and \\(S\\)?\n\ndefault behavior of inner_join: all columns shared by \\(R\\) and \\(S\\). Common columns have the same name in both schema. They are expected to have the same class\nmanual definition: in many settings, we want to overrule the default behavior. We specify manually which column from \\(R\\) should match which column from \\(S\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#natural-join-of-flights-and-weather",
    "href": "core/slides/slides-dplyr.html#natural-join-of-flights-and-weather",
    "title": "Tables manipulation with dplyr",
    "section": "Natural join of flights and weather:",
    "text": "Natural join of flights and weather:\n\ncommon_names &lt;- base::intersect(names(weather),\n                                names(flights))\n\nsetequal(\n  inner_join(flights, weather),\n  inner_join(flights,\n             weather,\n             by=common_names)\n)\n\n[1] TRUE"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#are-you-surprised-by-the-next-chunk",
    "href": "core/slides/slides-dplyr.html#are-you-surprised-by-the-next-chunk",
    "title": "Tables manipulation with dplyr",
    "section": "\n Are you surprised by the next chunk?",
    "text": "Are you surprised by the next chunk?\n\ndtu  &lt;- inner_join(flights,\n           weather,\n           by=c(\"year\", \"month\", \"day\", \"origin\", \"hour\"))\n\ndtv &lt;- inner_join(flights,\n           weather,\n           by=c(\"origin\", \"time_hour\"))\n\n# setequal(dtu, dtv)\n\nRecall that columns year, month day hour can be computed from time_hour\n\nflights |&gt;\n  filter(year!=year(time_hour) |\n         month!=month(time_hour) |\n         day!=day(time_hour) |\n         hour!=hour(time_hour)) |&gt;\n  nrow()\n\n[1] 0"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#the-two-results-do-not-have-the-same-schema",
    "href": "core/slides/slides-dplyr.html#the-two-results-do-not-have-the-same-schema",
    "title": "Tables manipulation with dplyr",
    "section": "The two results do not have the same schema!",
    "text": "The two results do not have the same schema!\n\nsetdiff(colnames(dtv), colnames(dtu))\n\n[1] \"year.x\"    \"month.x\"   \"day.x\"     \"hour.x\"    \"time_hour\" \"year.y\"   \n[7] \"month.y\"   \"day.y\"     \"hour.y\"   \n\nsetdiff(colnames(dtu), colnames(dtv))\n\n[1] \"year\"        \"month\"       \"day\"         \"hour\"        \"time_hour.x\"\n[6] \"time_hour.y\""
  },
  {
    "objectID": "core/slides/slides-dplyr.html#fixing",
    "href": "core/slides/slides-dplyr.html#fixing",
    "title": "Tables manipulation with dplyr",
    "section": "Fixing",
    "text": "Fixing\n\ndtu  &lt;- inner_join(flights,\n           weather,\n           by=c(\"year\", \"month\", \"day\", \"origin\", \"hour\"),\n           suffix= c(\"\", \".y\")) |&gt;  #&lt;&lt;\n           select(-ends_with(\".y\"))  #&lt;&lt;\n\ndtv &lt;- inner_join(flights,\n           weather,\n           by=c(\"origin\", \"time_hour\"),\n           suffix= c(\"\", \".y\")) |&gt;  #&lt;&lt;\n           select(-ends_with(\".y\"))  #&lt;&lt;\n\nsetequal(dtu, dtv)\n\n[1] TRUE"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#about-inner_join",
    "href": "core/slides/slides-dplyr.html#about-inner_join",
    "title": "Tables manipulation with dplyr",
    "section": "About inner_join\n",
    "text": "About inner_join\n\n\n\ninner_join(\n  x, y,\n  by = NULL,      #&lt;&lt;\n  copy = FALSE,\n  suffix = c(\".x\", \".y\"), #&lt;&lt;\n  ...,\n  keep = FALSE,  #&lt;&lt;\n  na_matches = \"na\")  #&lt;&lt;\n\n\n\nby:\n\n\nby=c(\"A1\", \"A3\", \"A7\") row r from R and s from S match if r.A1 == s.A1, r.A3 == s.A3, r.A7 == s.A7\n\n\nby=c(\"A1\"=\"B\", \"A3\"=\"C\", \"A7\"=\"D\") row r from R and s from S match if r.A1 == s.B, r.A3 == s.C, r.A7 == s.D\n\n\n\nsuffix: If there are non-joined duplicate variables in x and y, these suffixes will be added to the output to disambiguate them.\nkeep: Should the join keys from both x and y be preserved in the output?\nna_matches: Should NA and NaN values match one another?\n\n\n\n\nFrom online documentation"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#join-flavors",
    "href": "core/slides/slides-dplyr.html#join-flavors",
    "title": "Tables manipulation with dplyr",
    "section": "Join flavors",
    "text": "Join flavors\nDifferent flavors of join can be used to join one table to columns from another, matching values with the rows that they correspond to\nEach join retains a different combination of values from the tables\n\n\nleft_join(x, y, by = NULL, suffix = c(\".x\", \".y\"), ...) Join matching values from y to x. Retain all rows of x padding missing values from y by NA\nsemi_join …\nanti_join …"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-examples-inner_join",
    "href": "core/slides/slides-dplyr.html#toy-examples-inner_join",
    "title": "Tables manipulation with dplyr",
    "section": "Toy examples : inner_join\n",
    "text": "Toy examples : inner_join\n\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\ninner_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD.x\nA2\nA3\nD.y\n\n\n\n4\ne\n2021-10-22\nc\ne\n2021-10-28\nq\n\n\n6\nn\n2021-10-23\ni\na\n2021-11-04\no\n\n\n10\nr\n2021-10-25\ne\nd\n2021-11-18\nd"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-examples-left_join",
    "href": "core/slides/slides-dplyr.html#toy-examples-left_join",
    "title": "Tables manipulation with dplyr",
    "section": "Toy examples : left_join\n",
    "text": "Toy examples : left_join\n\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\nleft_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD.x\nA2\nA3\nD.y\n\n\n\n3\ny\n2021-10-21\no\nNA\nNA\nNA\n\n\n4\ne\n2021-10-22\nc\ne\n2021-10-28\nq\n\n\n6\nn\n2021-10-23\ni\na\n2021-11-04\no\n\n\n9\nt\n2021-10-24\nd\nNA\nNA\nNA\n\n\n10\nr\n2021-10-25\ne\nd\n2021-11-18\nd"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#toy-examples-semi_join-anti_join",
    "href": "core/slides/slides-dplyr.html#toy-examples-semi_join-anti_join",
    "title": "Tables manipulation with dplyr",
    "section": "Toy examples : semi_join anti_join\n",
    "text": "Toy examples : semi_join anti_join\n\n\n\n\n\n\nR\n\nA1\nA2\nA3\nD\n\n\n\n2\nq\n2021-10-21\nr\n\n\n4\ne\n2021-10-28\nq\n\n\n6\na\n2021-11-04\no\n\n\n8\nj\n2021-11-11\ng\n\n\n10\nd\n2021-11-18\nd\n\n\n\n\n\n\nS\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n9\nt\n2021-10-24\nd\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\nsemi_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD\n\n\n\n4\ne\n2021-10-22\nc\n\n\n6\nn\n2021-10-23\ni\n\n\n10\nr\n2021-10-25\ne\n\n\n\n\n\n\n\n\n\nanti_join(S, R, by=c(“E”=“A1”))\n\nE\nF\nG\nD\n\n\n\n3\ny\n2021-10-21\no\n\n\n9\nt\n2021-10-24\nd"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#conditional-theta--join",
    "href": "core/slides/slides-dplyr.html#conditional-theta--join",
    "title": "Tables manipulation with dplyr",
    "section": "Conditional/ \\(\\theta\\) -join",
    "text": "Conditional/ \\(\\theta\\) -join\nIn relational databases, joins are not restricted to natural joins\n\n\\[U \\leftarrow R \\bowtie_{\\theta} S\\]\nreads as\n\\[\\begin{array}{rl} T & \\leftarrow R \\times S\\\\ U & \\leftarrow \\sigma(T, \\theta)\\end{array}\\]\nwhere\n\n\\(R \\times S\\) is the cartesian product of \\(R\\) and \\(S\\)\n\\(\\theta\\) is a boolean expression that can be evaluated on any tuple of \\(R \\times S\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins",
    "href": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins",
    "title": "Tables manipulation with dplyr",
    "section": "Do we need conditional/ \\(\\theta\\) -joins?",
    "text": "Do we need conditional/ \\(\\theta\\) -joins?\n\n\n\n\n\n\nNote\n\n\n: We can implement \\(\\theta\\)/conditional-joins by pipelining a cross product and a filtering\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n: Cross products are costly:\n\n\\(\\#\\text{rows}(R \\times S) = \\#\\text{rows}(R) \\times \\#\\text{rows}(S)\\)\n\\(\\#\\text{cols}(R \\times S) = \\#\\text{cols}(R) + \\#\\text{cols}(S)\\)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins-1",
    "href": "core/slides/slides-dplyr.html#do-we-need-conditional-theta--joins-1",
    "title": "Tables manipulation with dplyr",
    "section": "Do we need conditional/ \\(\\theta\\) -joins?",
    "text": "Do we need conditional/ \\(\\theta\\) -joins?\n\n\n\n\n\n\nNote\n\n\n: RDBMS use query planning and optimization, indexing to circumvent the cross product bottleneck (when possible)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n: if we need to perform a \\(\\theta\\)-join\n\noutsource it to a RDBMS, or\ndesign an ad hoc pipeline\n\n\n\n\n\n\n\nAbout conditional join"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#a-conditional-join-between-flights-and-weather",
    "href": "core/slides/slides-dplyr.html#a-conditional-join-between-flights-and-weather",
    "title": "Tables manipulation with dplyr",
    "section": "A conditional join between flights and weather\n",
    "text": "A conditional join between flights and weather\n\n\nThe natural join between flights and weather we implemented can be regarded as an ad hoc conditional join between normalized versions of weather and flights \nTable flights and weather are redundant: year, month, day, hour can be computed from time_hour\nAssume flights and weather are trimmed so as to become irredundant\nThe conditional join is then based on truncations of variables time_hour\n\nSELECT \n  *\nFROM \n  flights AS f, weather AS w\nWHERE \n  date_trunc('hour', f.time_hour) = date_trunc('hour', w.time_hour)\n\nAdding redundant columns to flights and weather allows us to transform a tricky conditional join into a simple natural join \n\n\n\n\nPostgreSQL documentation"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-8",
    "href": "core/slides/slides-dplyr.html#section-8",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "Creation of new columns may happen\n\non the fly\nwhen altering (enriching) the schema of a table\n\nIn databases, creation of new columns may be the result of a query or be the result of altering a table schema with ALTER TABLE ADD COLUMN ...\nIn tidyverse() we use verbs mutate or add_column to add columns to the input table"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#mutate",
    "href": "core/slides/slides-dplyr.html#mutate",
    "title": "Tables manipulation with dplyr",
    "section": "mutate",
    "text": "mutate\n\n\nmutate(   #&lt;&lt;\n  .data,\n  new_col= expression, #&lt;&lt;\n  ...,   #&lt;&lt;\n  .keep = c(\"all\", \"used\", \"unused\", \"none\"),\n  .before = NULL,\n  .after = NULL\n)\n\n.data: the input data frame\nnew_col= expression:\n\nnew_col is the name of a new column\nexpression is evaluated on each row of .data or it is a vector of length 1\nall is the default behavior, retains all columns from .data"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#creating-a-categorical-column-to-spot-large-delays",
    "href": "core/slides/slides-dplyr.html#creating-a-categorical-column-to-spot-large-delays",
    "title": "Tables manipulation with dplyr",
    "section": "Creating a categorical column to spot large delays",
    "text": "Creating a categorical column to spot large delays\n\n\nbreaks_delay &lt;- with(flights,\n  c(min(arr_delay, na.rm=TRUE),\n    0, 30,\n    max(arr_delay, na.rm=TRUE))\n)\n\nlevel_delay &lt;- c(\"None\",\n                 \"Moderate\",\n                 \"Large\")\n\nflights |&gt;\n  mutate(large_delay = cut(\n    arr_delay,  #&lt;&lt;\n    breaks=breaks_delay, #&lt;&lt;\n    labels=level_delay,  #&lt;&lt;\n    ordered_result=TRUE)) |&gt;   #&lt;&lt;\n  select(large_delay, arr_delay) |&gt;\n  sample_n(5)\n\n\n# A tibble: 5 × 2\n  large_delay arr_delay\n  &lt;ord&gt;           &lt;dbl&gt;\n1 Large             219\n2 Moderate           18\n3 None              -19\n4 None              -16\n5 None               -1"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-9",
    "href": "core/slides/slides-dplyr.html#section-9",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "flights |&gt;\n  mutate(foo = if_else(arr_time &gt; sched_arr_time,        #&lt;&lt;\n                              arr_time - sched_arr_time,\n                              0L,\n                              missing = NA_integer_)) |&gt;\n  group_by( (foo &gt;0) & abs(foo - arr_delay)  &gt; 100) |&gt;\n  summarise(N=n())\n\n\n# A tibble: 3 × 2\n  `(foo &gt; 0) & abs(foo - arr_delay) &gt; 100`      N\n  &lt;lgl&gt;                                     &lt;int&gt;\n1 FALSE                                    322281\n2 TRUE                                       5157\n3 NA                                         9338"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#changing-the-class-of-a-column",
    "href": "core/slides/slides-dplyr.html#changing-the-class-of-a-column",
    "title": "Tables manipulation with dplyr",
    "section": "Changing the class of a column",
    "text": "Changing the class of a column\n\n\nflights |&gt;\n  mutate(large_delay = cut(arr_delay,  #&lt;&lt;\n    breaks=breaks_delay,\n    labels=level_delay,\n    ordered_result=TRUE),\n    origin = as.factor(origin), #&lt;&lt;\n    dest = as.factor(dest)    #&lt;&lt;\n  ) |&gt;\n  select(\n    large_delay,\n    arr_delay,\n    origin,\n    dest) |&gt;\n  sample_n(5)\n\n\n# A tibble: 5 × 4\n  large_delay arr_delay origin dest \n  &lt;ord&gt;           &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;\n1 None              -44 LGA    CVG  \n2 None              -15 EWR    DAY  \n3 Large             136 EWR    DEN  \n4 None               -9 EWR    TPA  \n5 Moderate           14 LGA    TPA"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#tidying-tables-is-part-of-data-cleaning",
    "href": "core/slides/slides-dplyr.html#tidying-tables-is-part-of-data-cleaning",
    "title": "Tables manipulation with dplyr",
    "section": "Tidying tables is part of data cleaning",
    "text": "Tidying tables is part of data cleaning\n\nA (tidy) dataset is a collection of values, usually either numbers (if quantitative) or strings (if qualitative)\n\n\nValues are organised in two ways\n\n\nEvery value belongs to a variable and an observation\n\n\nA variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units\n\n\nAn observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes\n\n\nThe principles of tidy data are tied to those of relational databases and Codd’s relational algebra\n\n\n\n The tidy data paper"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#codds-principles",
    "href": "core/slides/slides-dplyr.html#codds-principles",
    "title": "Tables manipulation with dplyr",
    "section": "Codd’s principles",
    "text": "Codd’s principles\n\nInformation is represented logically in tables\n\nData must be logically accessible by table, primary key, and column.\n\nNull values must be uniformly treated as “missing information,” not as empty strings, blanks, or zeros.\nMetadata (data about the database) must be stored in the database just as regular data is\nA single language must be able to define data, views, integrity constraints, authorization, transactions, and data manipulation\n\nViews must show the updates of their base tables and vice versa\nA single operation must be available to do each of the following operations: retrieve data, insert data, update data, or delete data\nBatch and end-user operations are logically separate from physical storage and access methods\nBatch and end-user operations can change the database schema without having to recreate it or the applications built upon it\n\nIntegrity constraints must be available and stored in the metadata, not in an application program\nThe data manipulation language of the relational system should not care where or how the physical data is distributed and should not require alteration if the physical data is centralized or distributed\nAny row processing done in the system must obey the same integrity rules and constraints that set-processing operations do"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-10",
    "href": "core/slides/slides-dplyr.html#section-10",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "dplyr functions expect and return tidy tables\nIn a tidy table\n\nEach variable is a column\nEach observation is a row\nEvery cell is a single value\n\n\n\n The tidy data paper]"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#section-11",
    "href": "core/slides/slides-dplyr.html#section-11",
    "title": "Tables manipulation with dplyr",
    "section": "",
    "text": "In order to tell whether a table is tidy, we need to know what is the population under investigation, what are the observations/individuals, which measures are performed on each individual, …"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#untidy-data",
    "href": "core/slides/slides-dplyr.html#untidy-data",
    "title": "Tables manipulation with dplyr",
    "section": "Untidy data",
    "text": "Untidy data\n\nColumn headers are values, not variable names.\n\n\nMultiple variables are stored in one column.\n\n\nVariables are stored in both rows and columns.\n\n\nMultiple types of observational units are stored in the same table.\n\n\nA single observational unit is stored in multiple tables.\n\n\n…"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#functions-from-tidyr...",
    "href": "core/slides/slides-dplyr.html#functions-from-tidyr...",
    "title": "Tables manipulation with dplyr",
    "section": "Functions from tidyr::...\n",
    "text": "Functions from tidyr::...\n\n\npivot_wider and pivot_longer\nseparate and unite\nHandling missing values with complete, fill, …\n…\n\ntidyr website"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#pivot-longer",
    "href": "core/slides/slides-dplyr.html#pivot-longer",
    "title": "Tables manipulation with dplyr",
    "section": "Pivot longer",
    "text": "Pivot longer\n\n\npivot_longer() is commonly needed to tidy wild-caught datasets as they often optimise for ease of data entry or ease of comparison rather than ease of analysis.\n\n\n\nmessy &lt;- tibble::tribble(\n  ~row, ~a, ~b, ~c,\n  \"A\", 1, 4, 7,\n  \"B\", 2, 5, 8,\n  \"C\", 3, 6, 9,\n)\nmessy |&gt; kable()\n\n\n\n\nrow\na\nb\nc\n\n\n\nA\n1\n4\n7\n\n\nB\n2\n5\n8\n\n\nC\n3\n6\n9\n\n\n\n\n\n\n\nmessy_long &lt;- messy |&gt; \n  pivot_longer(\n    cols=c(-row),  #&lt;&lt;\n    names_to = \"name\",\n    values_to = \"value\")\n  \nmessy_long  |&gt; \n  kable()\n\n\n\n\nrow\nname\nvalue\n\n\n\nA\na\n1\n\n\nA\nb\n4\n\n\nA\nc\n7\n\n\nB\na\n2\n\n\nB\nb\n5\n\n\nB\nc\n8\n\n\nC\na\n3\n\n\nC\nb\n6\n\n\nC\nc\n9\n\n\n\n\n\n\n\n\n\npivot_longer() makes datasets longer by increasing the number of rows and decreasing the number of columns. I don’t believe it makes sense to describe a dataset as being in “long form”. Length is a relative term, and you can only say (e.g.) that dataset A is longer than dataset B."
  },
  {
    "objectID": "core/slides/slides-dplyr.html#pivot-wider",
    "href": "core/slides/slides-dplyr.html#pivot-wider",
    "title": "Tables manipulation with dplyr",
    "section": "Pivot wider",
    "text": "Pivot wider\n\n\npivot_wider(  #&lt;&lt;\n  data,\n  id_cols = NULL, #&lt;&lt;\n  names_from = name, #&lt;&lt;\n  names_prefix = \"\",\n  values_from = value, #&lt;&lt;\n  ...\n)\n some optional arguments are missing\n\nWhen reporting, we often use pivot_wider (explicitely or implicitely) to make results more readable, possibly to conform to a tradition\n\nLife tables in demography and actuarial science\nLongitudinal data\nSee slide How many flights per day of week per departure airport?"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#pivot_wider-in-action",
    "href": "core/slides/slides-dplyr.html#pivot_wider-in-action",
    "title": "Tables manipulation with dplyr",
    "section": "\npivot_wider() in action",
    "text": "pivot_wider() in action\n\n\nmessy_long |&gt;\n  pivot_wider( \n  id_cols = c(\"row\"), #&lt;&lt;\n  names_from = name, #&lt;&lt;\n  names_prefix = \"\",\n  values_from = value\n)\n\n\n# A tibble: 3 × 4\n  row       a     b     c\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         1     4     7\n2 B         2     5     8\n3 C         3     6     9"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#how-many-flights-per-carrier",
    "href": "core/slides/slides-dplyr.html#how-many-flights-per-carrier",
    "title": "Tables manipulation with dplyr",
    "section": "How many flights per carrier?",
    "text": "How many flights per carrier?\n\n\nflights |&gt;\n  group_by(carrier) |&gt;  #&lt;&lt;\n  summarise(count=n()) |&gt;  #&lt;&lt;\n  arrange(desc(count))\n\n\n# A tibble: 16 × 2\n   carrier count\n   &lt;chr&gt;   &lt;int&gt;\n 1 UA      58665\n 2 B6      54635\n 3 EV      54173\n 4 DL      48110\n 5 AA      32729\n 6 MQ      26397\n 7 US      20536\n 8 9E      18460\n 9 WN      12275\n10 VX       5162\n11 FL       3260\n12 AS        714\n13 F9        685\n14 YV        601\n15 HA        342\n16 OO         32\n\n\nSELECT \n  carrier, COUNT(*) AS n\nFROM \n  flights\nGROUP BY \n  carrier\nORDER BY \n  n DESCENDING"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#how-many-flights-per-day-of-week-per-departure-airport",
    "href": "core/slides/slides-dplyr.html#how-many-flights-per-day-of-week-per-departure-airport",
    "title": "Tables manipulation with dplyr",
    "section": "How many flights per day of week per departure airport?",
    "text": "How many flights per day of week per departure airport?\n\n\nflights |&gt;\n  group_by(origin,  wday(time_hour, abbr=T, label=T)) |&gt;  #&lt;&lt;\n  summarise(count=n(), .groups=\"drop\") |&gt;       #&lt;&lt;\n  rename(day_of_week=`wday(time_hour, abbr = T, label = T)`) |&gt;\n  pivot_wider(  #&lt;&lt;\n    id_cols=\"origin\",   #&lt;&lt;\n    names_from=\"day_of_week\", #&lt;&lt;\n    values_from=\"count\") |&gt;  #&lt;&lt;\n  kable(caption=\"Departures per day\")\n\n\n\nDepartures per day\n\norigin\ndim.\nlun.\nmar.\nmer.\njeu.\nven.\nsam.\n\n\n\nEWR\n16425\n18329\n18243\n18180\n18169\n18142\n13347\n\n\nJFK\n15966\n16104\n16017\n15841\n16087\n16176\n15088\n\n\nLGA\n13966\n16257\n16162\n16039\n15963\n15990\n10285"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#window-queries-1",
    "href": "core/slides/slides-dplyr.html#window-queries-1",
    "title": "Tables manipulation with dplyr",
    "section": "Window queries",
    "text": "Window queries\nAssume we want to answer the question: for each day of week (Monday, Tuesday, …), what are the five carriers that experience the largest average delay?\n\n\nflights |&gt;\n  group_by(weekdays(time_hour), carrier) |&gt;\n  summarise(avg_dep_delay=mean(dep_delay, na.rm=T)) |&gt;\n  slice_max(n=2, order_by=avg_dep_delay)\n\n\n# A tibble: 14 × 3\n# Groups:   weekdays(time_hour) [7]\n   `weekdays(time_hour)` carrier avg_dep_delay\n   &lt;chr&gt;                 &lt;chr&gt;           &lt;dbl&gt;\n 1 dimanche              F9               23.7\n 2 dimanche              VX               17.4\n 3 jeudi                 YV               29.7\n 4 jeudi                 F9               26.5\n 5 lundi                 FL               24.8\n 6 lundi                 EV               23.4\n 7 mardi                 YV               19.1\n 8 mardi                 FL               17.7\n 9 mercredi              OO               52  \n10 mercredi              HA               24.5\n11 samedi                OO               41  \n12 samedi                F9               15.8\n13 vendredi              OO               29  \n14 vendredi              F9               25.6"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#the-sql-way",
    "href": "core/slides/slides-dplyr.html#the-sql-way",
    "title": "Tables manipulation with dplyr",
    "section": "The SQL way",
    "text": "The SQL way\nWITH R AS (\n  SELECT \n    EXTRACT(dow FROM time_hour) AS day_of_week,\n    carrier,\n    AVG(dep_delay) AS avg_dep_delay\n  FROM \n    flights\n  GROUP BY \n    EXTRACT(dow FROM time_hour), carrier\n), S AS (\n  SELECT \n    day_of_week,\n    carrier,\n    rank() OVER (PARTITION by day_of_week ORDER BY avg_dep_delay DESC) AS rnk\n  FROM \n    R\n)\n\nSELECT \n  day_of_week, \n  carrier, \n  rnk\nFROM \n  S\nWHERE \n  rnk &lt;= 10 ;"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#sliding-windows-and-package-slider",
    "href": "core/slides/slides-dplyr.html#sliding-windows-and-package-slider",
    "title": "Tables manipulation with dplyr",
    "section": "Sliding windows and package slider\n",
    "text": "Sliding windows and package slider\n\nTODO"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#and-other-pipes",
    "href": "core/slides/slides-dplyr.html#and-other-pipes",
    "title": "Tables manipulation with dplyr",
    "section": "\n|>, %>% and other pipes",
    "text": "|&gt;, %&gt;% and other pipes\n\nAll dplyr functions take a table as the first argument\nRather than forcing the user to either save intermediate objects or nest functions, dplyr provides the |&gt; operator from magrittr\nx |&gt; f(y) turns into f(x, y)\nThe result from one step is piped into the next step\nUse |&gt; to rewrite multiple operations that you can read left-to-right/top-to-bottom\n\n\ng(f(x, y), z)\n\nx |&gt;\n  f(y) |&gt;\n  g(z)\n\n\n\nFrom dplyr vignette"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#magrittr",
    "href": "core/slides/slides-dplyr.html#magrittr",
    "title": "Tables manipulation with dplyr",
    "section": "Magrittr %>%\n",
    "text": "Magrittr %&gt;%\n\n\n\n\n\n%&gt;% is not tied to dplyr\n\n\n%&gt;% can be used with packages from tidyverse\n\n\n%&gt;% can be used outside tidyverse that is with functions which take a table (or something else) as a second, third or keyword argument\n\n Use pronoun . to denote the LHS of the pipe expression\n\nSecond argument of g has the same type as the result of f\n\ng(z, f(x, y))\n\nx %&gt;%\n  f(y) %&gt;%\n  g(z, .)   #&lt;&lt;\n\nx %&gt;% f(y) is a shorthand for x %&gt;% f(., y)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#standard-pipe-version-4.",
    "href": "core/slides/slides-dplyr.html#standard-pipe-version-4.",
    "title": "Tables manipulation with dplyr",
    "section": "Standard pipe |> (version > 4.)",
    "text": "Standard pipe |&gt; (version &gt; 4.)\nAs of version 4.1 (2021), base  offers a pipe operator denoted by |&gt;\n\n\nx |&gt; f(y) turns into f(x, y)\n\ng(f(x, y), z)\n\nx |&gt;\n  f(y) |&gt;\n  g(z)\n\n\n the standard pipe |&gt; has no pronoun/placeholder to denote the LHS of the pipe expression\nThe roundabout consists in using another new construct \\(x)\ng(z, w)\n\nx |&gt;\n  (\\(x) g(z, w=x))()\n\n\"une\" |&gt;\n  (\\(x) str_c(\"ceci n'est pas\", x, sep=\" \"))() |&gt;\n  str_c(\"pipe\", sep=\" \") |&gt;\n  cat()\n\nceci n'est pas une pipe\n\n\n\n\n\nSee Blog on the new standard pipe]"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#other-pipes",
    "href": "core/slides/slides-dplyr.html#other-pipes",
    "title": "Tables manipulation with dplyr",
    "section": "Other pipes",
    "text": "Other pipes\nMagrittr offers several variants of |&gt;\n\nTee operator %T&gt;%\n\nAssignement pipe %&lt;&gt;%\n\nExposition operator %$%\n\n…\n\nSee pipes for beginners\n Base  has a pipe() function to manipulate connections (Files, URLs, …)"
  },
  {
    "objectID": "core/slides/slides-dplyr.html#references",
    "href": "core/slides/slides-dplyr.html#references",
    "title": "Tables manipulation with dplyr",
    "section": "\n References",
    "text": "References\n\n\nR for Data Science\n\nData transformation\n\n\nRstudio cheat sheets\n\ndplyr\ntidyr\ndatatable\nreadr"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#setup",
    "href": "core/labs-solutions/lab-bivariate.html#setup",
    "title": "Bivariate analysis",
    "section": "Setup",
    "text": "Setup\n\nCodestopifnot(\n  require(glue),\n  require(magrittr),\n  require(lobstr),\n  require(arrow),\n  require(ggforce),\n  require(vcd),\n  require(ggmosaic),\n  require(httr),\n  require(patchwork),\n  require(corrr),\n  require(gapminder),\n  require(slider),\n  require(tidyverse) \n) \n\n\nBivariate techniques depend on the types of columns we are facing.\nFor numerical/numerical samples\n\nScatter plots\nSmoothed lineplots (for example linear regression)\n2-dimensional density plots\n\nFor categorical/categorical samples : mosaicplots and variants\nFor numerical/categorical samples\n\nBoxplots per group\nHistograms per group\nDensity plots per group"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#chi-square-independenceassociation-test",
    "href": "core/labs-solutions/lab-bivariate.html#chi-square-independenceassociation-test",
    "title": "Bivariate analysis",
    "section": "Chi-square independence/association test",
    "text": "Chi-square independence/association test\nhttps://statsfonda.github.io/site/content/ch4_2.html#test-dindépendance\n\n\n\n\n\n\nQuestion\n\n\n\n\nCompute the chi-square association statistic between CATEGORIE and SEXE.\nDisplay the output of chisq.test() as a table, using broom::tidy()\n\n\n\n\n\n\n\n\n\n\n\nCodetest_1 &lt;- df |&gt;\n  select(CATEGORIE, SEXE) |&gt;\n  table() |&gt;\n  chisq.test()\n\n# test_1 \n\ntest_1 |&gt;\n  broom::tidy() |&gt;\n  knitr::kable()\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n140.6717\n0\n5\nPearson’s Chi-squared test\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCompute the Chi-square statistics from the contingeny table\n\n\n\n\n\n\n\n\n\nCoderowcounts &lt;- apply(tb, MARGIN = 1, FUN = sum)\ncolcounts &lt;- apply(tb, MARGIN = 2, FUN = sum)\n\nexpected &lt;- (rowcounts %*% t(colcounts))/sum(colcounts)\n\n# norm((tb - expected) / sqrt(expected), type = \"F\")^2\n\nexpected |&gt;\n  as_tibble() |&gt;\n  knitr::kable()\n\n\n\nF\nM\n\n\n\n22.80801\n23.19199\n\n\n65.94491\n67.05509\n\n\n61.97830\n63.02170\n\n\n23.79967\n24.20033\n\n\n46.60768\n47.39232\n\n\n75.86144\n77.13856"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#grouped-boxplots",
    "href": "core/labs-solutions/lab-bivariate.html#grouped-boxplots",
    "title": "Bivariate analysis",
    "section": "Grouped boxplots",
    "text": "Grouped boxplots\n\n\n\n\n\n\nQuestion\n\n\n\nPlot boxplots of AGE according to NIV_ETUDES\n\n\n\n\n\n\n\n\n\nCodedf |&gt;\n  ggplot() +\n  aes(x=NIV_ETUDES, y=AGE) +\n  geom_boxplot() +\n  rot_x_text\n\n\n\n\n\n\n\n\nCodedf |&gt;\n  ggplot() +\n  aes(x=fct_infreq(NIV_ETUDES), y=AGE) +\n  geom_boxplot(varwidth = T) +\n  rot_x_text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDraw density plots of AGE, facet by NIV_ETUDES and SEXE\n\n\n\n\n\n\n\n\n\nCodep &lt;- df |&gt; \n  ggplot() +\n  aes(x=AGE) +\n  stat_density(fill=\"white\", color=\"black\") +\n  facet_grid(rows=vars(NIV_ETUDES), \n             cols=vars(SEXE))\n\np\n\nWarning: Groups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCollapse rare levels of NIV_ETUDES and replay.\n\n\n\n\n\n\n\n\n\nCodep %+% (\n  df |&gt; \n    mutate(NIV_ETUDES = fct_lump_min(NIV_ETUDES, min=30)) \n)"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#scatterplots",
    "href": "core/labs-solutions/lab-bivariate.html#scatterplots",
    "title": "Bivariate analysis",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\n\n\n\n\nQuestion\n\n\n\nMake a scatterplot of SAL_HORwith respect to AGE\n\n\n\n\n\n\n\n\n\nCodedf |&gt; \n  ggplot() +\n  aes(x=AGE, y=SAL_HOR, color=SEXE) +\n  geom_point(alpha=.7)"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#linear-correlation-coefficient",
    "href": "core/labs-solutions/lab-bivariate.html#linear-correlation-coefficient",
    "title": "Bivariate analysis",
    "section": "Linear correlation coefficient",
    "text": "Linear correlation coefficient\n\n\n\n\n\n\nQuestion\n\n\n\nCompute the Pearson, Spearman and Kendall correlation coefficients between AGE and SAL_HOR using function cor() from base R\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodedf |&gt;\n  summarise(\n    pearson=cor(AGE, SAL_HOR), \n    spearman=cor(AGE, SAL_HOR, method = \"spearman\"),\n    kendall=cor(AGE, SAL_HOR, method=\"kendall\")) |&gt; \n  gt::gt() |&gt;\n  gt::fmt_number(decimals=2) |&gt;\n  gt::tab_caption(\n    \"Correlation coefficients between SAL_HOR and AGE\\nRecensement dataset\"\n    )\n\n\n\n\nCorrelation coefficients between SAL_HOR and AGE Recensement dataset\n\npearson\nspearman\nkendall\n\n\n0.25\n0.32\n0.22"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#rank-based-methods",
    "href": "core/labs-solutions/lab-bivariate.html#rank-based-methods",
    "title": "Bivariate analysis",
    "section": "Rank based methods",
    "text": "Rank based methods\nSpearman’s rho (𝜌) and Kendall’s tau (𝜏) are both non-parametric correlation coefficients used to measure the strength and direction of a monotonic relationship between two variables.\n\nSpearman’s rho (𝜌)\n\nBased on rank differences. Defined as the Pearson correlation coefficient between the ranked variables.\\[\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\\] where \\(d_i\\) is the difference between the ranks of each pair, and \\(n\\) is the number of observations.\n\nKendall’s tau (𝜏)\n\nBased on concordant and discordant pairs.  Measures the proportion of pairs that have the same order in both variables compared to the total number of pairs. \\[\\tau = \\frac{(C - D)}{\\frac{1}{2} n (n - 1)}\\] where \\(C\\) is the number of concordant pairs, and \\(D\\) is the number of discordant pairs.\n\n\nWhen to Use Which?\n\n\n\n\n\n\n\nFactor\nSpearman’s rho (𝜌)\nKendall’s tau (𝜏)\n\n\n\nLarge differences in ranks\nMore sensitive\nLess sensitive\n\n\nSmall sample sizes\nLess reliable\nMore reliable\n\n\nOutlier resistance\nModerate\nHigh\n\n\nComputational efficiency\nFaster\nSlower (due to pairwise comparisons)\n\n\nInterpretation\nSimilar to Pearson’s correlation\nMore intuitive (proportion of concordance)"
  },
  {
    "objectID": "core/labs-solutions/lab-bivariate.html#chatterjees-correlation-coefficient-chatterjees-ξ",
    "href": "core/labs-solutions/lab-bivariate.html#chatterjees-correlation-coefficient-chatterjees-ξ",
    "title": "Bivariate analysis",
    "section": "Chatterjee’s correlation coefficient (Chatterjee’s \\(ξ\\))",
    "text": "Chatterjee’s correlation coefficient (Chatterjee’s \\(ξ\\))\n\nThe three most popular classical measures of statistical association are Pearson’s correlation coefficient, Spearman’s ρ, and Kendall’s τ . These coefficients are very powerful for detecting linear or monotone associations, and they have well-developed asymptotic theories for calculating P-values. However, the big problem is that they are not effective for detecting associations that are not monotonic, even in the complete absence of noise.\n\n\nLet \\((X, Y)\\) be a pair of random variables, where \\(Y\\) is not a constant. Let \\((X_1 , Y_1 ), \\ldots, (X_n , Y_n )\\) be i.i.d. pairs with the same law as \\((X, Y)\\), where \\(n ≥ 2\\). The new coefficient has a simpler formula if the \\(X_i\\)’s and the \\(Y_i\\) ’s have no ties. This simpler formula is presented first, and then the general case is given. Suppose that the \\(X_i\\)’s and the \\(Y_i\\) ’s have no ties. Rearrange the data as \\((X_{(1)} , Y_{(1)} ), . . . , (X_{(n)} , Y_{(n)} )\\) such that \\(X_{(1)} ≤ · · · ≤ X_{(n)}\\) . Since the \\(X_i\\)’s have no ties, there is a unique way of doing this. Let ri be the rank of \\(Y_{(i)}\\), that is, the number of \\(j\\) such that \\(Y_{(j)} ≤ Y_{(i)}\\). The new correlation coefficient is defined as\n\n\\[ξ_n(X, Y ) := 1 −  3\\sum_{i=1}^{n-1} \\frac{|r_{i+1} − r_i|}{n^2-1}\\]\n\nIn the presence of ties, \\(ξ_n\\) is defined as follows. If there are ties among the \\(X_i\\)’s, then choose an increasing rearrangement as above by breaking ties uniformly at random. Let \\(r_i\\) be as before, and additionally define \\(l_i\\) to be the number of \\(j\\) such that \\(Y_{(j)} ≥ Y_{(i)}\\). Then define\n\n\\[ξ_n(X, Y ) := 1 −  3n\\sum_{i=1}^{n-1} \\frac{|r_{i+1} − r_i|}{2 \\sum_{i=1}^n l_i (n − l_i )}\\]\n\nWhen there are no ties among the \\(Y_i\\) ’s, \\(l_1 , \\ldots , l_n\\) is just a permutation of \\(1, \\ldots , n\\), and so the denominator in the above expression is just \\(n(n^2 − 1)/3\\), which reduces this definition to the earlier expression.\n\nFrom Sourav Chatterjee: A new correlation coefficient\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a dplyr pipeline from computing the \\(ξ\\) correlation coefficient between Y=lifeExp and X=gdpPercap in the gapminder dataset, per year and continent.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodetab_xi &lt;- gapminder::gapminder |&gt;  \n  group_by(year, continent) |&gt; \n  arrange(gdpPercap) |&gt; \n  mutate(rnk= row_number(lifeExp), \n         lnk=rank(desc(lifeExp), ties.method = \"max\"), \n         N=n()) |&gt; \n  mutate(fol=lead(rnk), dd=abs(fol-rnk)) |&gt; \n  summarise(Xi=1-n()*sum(dd, na.rm = T)/(2*sum(lnk*(N-lnk), na.rm = T))) \n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nCodetab_xi  |&gt; \n  print(n=10)  \n\n# A tibble: 60 × 3\n# Groups:   year [12]\n    year continent     Xi\n   &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;\n 1  1952 Africa    0.0377\n 2  1952 Americas  0.135 \n 3  1952 Asia      0.244 \n 4  1952 Europe    0.610 \n 5  1952 Oceania   0     \n 6  1957 Africa    0.0466\n 7  1957 Americas  0.240 \n 8  1957 Asia      0.330 \n 9  1957 Europe    0.516 \n10  1957 Oceania   0     \n# ℹ 50 more rows\n\n\n\nCodetab_xi |&gt;\n  pivot_wider(\n    id_cols=continent, \n    names_from=year,\n    values_from=Xi\n    ) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(decimals=2)\n\n\n\n\n\ncontinent\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\n\nAfrica\n0.04\n0.05\n−0.01\n0.05\n0.24\n0.27\n0.38\n0.27\n0.36\n0.31\n0.13\n0.17\n\n\nAmericas\n0.13\n0.24\n0.35\n0.17\n0.19\n0.13\n0.30\n0.43\n0.52\n0.33\n0.29\n0.23\n\n\nAsia\n0.24\n0.33\n0.38\n0.27\n0.32\n0.26\n0.29\n0.30\n0.43\n0.50\n0.50\n0.46\n\n\nEurope\n0.61\n0.52\n0.40\n0.38\n0.32\n0.38\n0.06\n0.29\n0.39\n0.51\n0.48\n0.51\n\n\nOceania\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#setup",
    "href": "core/labs-solutions/lab-tables.html#setup",
    "title": "Tables manipulation II",
    "section": "Setup",
    "text": "Setup\nWe will use the following packages. If needed, we install them.\n\nCodeold_theme &lt;- theme_set(theme_minimal())\n\n\nCheck nycflights13 for any explanation concerning the tables and their columns."
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#data-loading",
    "href": "core/labs-solutions/lab-tables.html#data-loading",
    "title": "Tables manipulation II",
    "section": "Data loading",
    "text": "Data loading\n\nCodeflights &lt;- nycflights13::flights\nweather &lt;- nycflights13::weather\nairports &lt;- nycflights13::airports\nairlines &lt;- nycflights13::airlines\nplanes &lt;- nycflights13::planes\n\n\n\nCodecon &lt;- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\nflights_lite &lt;- copy_to(con, nycflights13::flights)\nairports_lite &lt;- copy_to(con, nycflights13::airports)\nplanes_lite &lt;-  copy_to(con, nycflights13::planes)\nweather_lite &lt;- copy_to(con, nycflights13::weather)\nairlines_lite &lt;- copy_to(con, nycflights13::airlines)\n\n\n\nCodeflights_lite |&gt;\n  select(contains(\"delay\")) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT `dep_delay`, `arr_delay`\nFROM `nycflights13::flights`\n\n\nView data in spreadsheet style.\n\nCodeView(flights)\n\n\nAsk for help about table flights\n\n\n\n\n\n\nSolution\n\n\n\n\nCode?flights\n\n\n\nCodeflights |&gt; \n  glimpse()\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\n\nCodeairports |&gt; \n  glimpse()\n\nRows: 1,458\nColumns: 8\n$ faa   &lt;chr&gt; \"04G\", \"06A\", \"06C\", \"06N\", \"09J\", \"0A9\", \"0G6\", \"0G7\", \"0P2\", \"…\n$ name  &lt;chr&gt; \"Lansdowne Airport\", \"Moton Field Municipal Airport\", \"Schaumbur…\n$ lat   &lt;dbl&gt; 41.13047, 32.46057, 41.98934, 41.43191, 31.07447, 36.37122, 41.4…\n$ lon   &lt;dbl&gt; -80.61958, -85.68003, -88.10124, -74.39156, -81.42778, -82.17342…\n$ alt   &lt;dbl&gt; 1044, 264, 801, 523, 11, 1593, 730, 492, 1000, 108, 409, 875, 10…\n$ tz    &lt;dbl&gt; -5, -6, -6, -5, -5, -5, -5, -5, -5, -8, -5, -6, -5, -5, -5, -5, …\n$ dst   &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"U\", \"A\", \"A\", \"U\", \"A\",…\n$ tzone &lt;chr&gt; \"America/New_York\", \"America/Chicago\", \"America/Chicago\", \"Ameri…"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#first-queries-the-dplyr-way",
    "href": "core/labs-solutions/lab-tables.html#first-queries-the-dplyr-way",
    "title": "Tables manipulation II",
    "section": "First Queries (the dplyr way)",
    "text": "First Queries (the dplyr way)\nFind all flights that\n\nHad an arrival delay of two or more hours\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(arr_delay &gt;= 120) |&gt; \n  nrow()\n\n[1] 10200\n\n\n\nCodeflights_lite |&gt; \n  filter(arr_delay &gt;= 120) |&gt; \n  count()  |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT COUNT(*) AS `n`\nFROM (\n  SELECT `nycflights13::flights`.*\n  FROM `nycflights13::flights`\n  WHERE (`arr_delay` &gt;= 120.0)\n) AS `q01`\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe can translate the dplyr pipeline into an SQL query.\n\nCodeflights_lite |&gt; \n  filter(arr_delay &gt;= 120) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `nycflights13::flights`.*\nFROM `nycflights13::flights`\nWHERE (`arr_delay` &gt;= 120.0)\n\n\nWe can even get some explanations\nflights_lite |&gt; \n  filter(arr_delay &gt;= 120) |&gt; \n  explain()\n\n\n\nFlew to Houston (IAH or HOU)\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(dest %in% c(\"HOU\", \"IAH\")) |&gt;  \n  nrow()\n\n[1] 9313\n\nCodeflights |&gt; \n  filter(dest == \"HOU\" | dest == \"IAH\") |&gt; \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  9313\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights_lite |&gt; \n  filter(dest %in% c(\"HOU\", \"IAH\")) |&gt;  \n  show_query()\n\n&lt;SQL&gt;\nSELECT `nycflights13::flights`.*\nFROM `nycflights13::flights`\nWHERE (`dest` IN ('HOU', 'IAH'))\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights_lite |&gt; \n  filter(dest == \"HOU\" | dest == \"IAH\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `nycflights13::flights`.*\nFROM `nycflights13::flights`\nWHERE (`dest` = 'HOU' OR `dest` = 'IAH')\n\n\n\n\n\nWere operated by United, American, or Delta\n\n\n\n\n\n\n\nPackage stringr could be useful.\n\nCodeairlines |&gt; \n  filter(stringr::str_starts(name, \"United\") |\n        stringr::str_starts(name, \"American\") |\n        stringr::str_starts(name, \"Delta\"))\n\n# A tibble: 3 × 2\n  carrier name                  \n  &lt;chr&gt;   &lt;chr&gt;                 \n1 AA      American Airlines Inc.\n2 DL      Delta Air Lines Inc.  \n3 UA      United Air Lines Inc. \n\nCodeairlines |&gt; \n  filter(stringr::str_detect(name, (\"United|American|Delta\"))) |&gt; \n  pull(carrier)\n\n[1] \"AA\" \"DL\" \"UA\"\n\n\n#| eval: false\nairlines_lite |&gt; \n  filter(stringr::str_starts(name, \"United\") |\n        stringr::str_starts(name, \"American\") |\n        stringr::str_starts(name, \"Delta\")) |&gt; \n  show_query()\nSELECT *\nFROM `nycflights13::airlines`\nWHERE \"name\" LIKE 'United%' OR \n      \"name\" LIKE 'American%' OR \n      \"name\" LIKE 'Delta%' ;\nstringr is part of tidyverse\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe build on the tip above to extract the matching airlines codes.\nWe may proceed using a subquery\n\nCodeflights |&gt; \n  filter(carrier %in% (\n        airlines |&gt; \n        filter(stringr::str_detect(name, (\"United|American|Delta\"))) |&gt; \n        pluck(\"carrier\")\n      )\n) |&gt; \nhead(6)\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1      517            515         2      830            819\n2  2013     1     1      533            529         4      850            830\n3  2013     1     1      542            540         2      923            850\n4  2013     1     1      554            600        -6      812            837\n5  2013     1     1      554            558        -4      740            728\n6  2013     1     1      558            600        -2      753            745\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nCodeflights_lite |&gt; \n  filter(carrier %in% c(\"AA\", \"DL\", \"UA\")) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `nycflights13::flights`.*\nFROM `nycflights13::flights`\nWHERE (`carrier` IN ('AA', 'DL', 'UA'))\n\n\nSELECT *\nFROM `nycflights13::flights`\nWHERE carrier IN (SELECT carrier \n  FROM `nycflights13::airlines`\n  WHERE \"name\" LIKE 'United%' OR \n        \"name\" LIKE 'American%' OR \n        \"name\" LIKE 'Delta%')\n) ; \nWe may also rely on a NATURAL JOIN.\n\nCodeairlines |&gt; \n  filter(str_detect(name, \"United|Delta|American\")) |&gt; \n  inner_join(flights) |&gt; \n  head(6)\n\nJoining with `by = join_by(carrier)`\n\n\n# A tibble: 6 × 20\n  carrier name       year month   day dep_time sched_dep_time dep_delay arr_time\n  &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n1 AA      American…  2013     1     1      542            540         2      923\n2 AA      American…  2013     1     1      558            600        -2      753\n3 AA      American…  2013     1     1      559            600        -1      941\n4 AA      American…  2013     1     1      606            610        -4      858\n5 AA      American…  2013     1     1      623            610        13      920\n6 AA      American…  2013     1     1      628            630        -2     1137\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\nCode#  sample_n(size=6)    # random sampling \n\n\n#| eval: false\nairlines_lite |&gt; \n  filter(stringr::str_starts(name, \"United\") |\n        stringr::str_starts(name, \"American\") |\n        stringr::str_starts(name, \"Delta\")) |&gt; \n  inner_join(flights_lite) |&gt; \n  show_query()\n\n\n\nDeparted in summer (July, August, and September)\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(month %in% c(7,8,9)) |&gt; \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1 86326\n\n\nA more ambitious (and sustainable) approach relies on the date/time manipulation function from lubridate\n\nCodeflights |&gt; \n  filter(lubridate::month(time_hour) %in% 7:9) |&gt; \n  head()\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     7     1        1           2029       212      236           2359\n2  2013     7     1        2           2359         3      344            344\n3  2013     7     1       29           2245       104      151              1\n4  2013     7     1       43           2130       193      322             14\n5  2013     7     1       44           2150       174      300            100\n6  2013     7     1       46           2051       235      304           2358\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nor even\n\nCodemy_locale &lt;- 'en_US.UTF-8'\n\nflights |&gt; \n  filter(lubridate::month(time_hour, label=T, abbr=F, \n                          locale=my_locale) %in% \n                    c('juillet', 'August', 'September')) |&gt; \n  head()\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     8     1       12           2130       162      257             14\n2  2013     8     1       12           2359        13      349            350\n3  2013     8     1       22           2146       156      255             30\n4  2013     8     1       26           2051       215      333           2358\n5  2013     8     1       32           2359        33      420            344\n6  2013     8     1       33           2231       122      412            226\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\n\n\n\n\nWhen manipulating temporal information (date, time, duration), keep an eye on what lubridate offers. The API closely parallels what RDMS and Python offer.\n\n\n\n\nArrived more than two hours late, but didn’t leave late\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(arr_delay &gt;= 120, dep_delay &lt;= 0) |&gt; \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    29\n\n\n\n\n\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(dep_delay &gt; 60, \n         arr_delay &lt; dep_delay -30) |&gt; \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  1819\n\n\n\n\n\nDeparted between midnight and 6am (inclusive)\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(dep_time&lt;=600, dep_time&gt;0) |&gt; \n  head()\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1      517            515         2      830            819\n2  2013     1     1      533            529         4      850            830\n3  2013     1     1      542            540         2      923            850\n4  2013     1     1      544            545        -1     1004           1022\n5  2013     1     1      554            600        -6      812            837\n6  2013     1     1      554            558        -4      740            728\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nCodeflights |&gt; \n  filter(lubridate::hour(time_hour)&lt;=5  | (\n         lubridate::hour(time_hour)==6 & minute(time_hour)==0)) |&gt; \n  head()\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1      517            515         2      830            819\n2  2013     1     1      533            529         4      850            830\n3  2013     1     1      542            540         2      923            850\n4  2013     1     1      544            545        -1     1004           1022\n5  2013     1     1      554            600        -6      812            837\n6  2013     1     1      554            558        -4      740            728\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\n\n\n\n\nRead filter() in R for Data Science 1st Ed\nRead Chapter Transform in R for Data Science 2nd Ed"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#missing-data",
    "href": "core/labs-solutions/lab-tables.html#missing-data",
    "title": "Tables manipulation II",
    "section": "Missing data",
    "text": "Missing data\n\nHow many flights per origin have a missing dep_time?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(is.na(dep_time)) |&gt; \n  count(by=origin)\n\n# A tibble: 3 × 2\n  by        n\n  &lt;chr&gt; &lt;int&gt;\n1 EWR    3239\n2 JFK    1863\n3 LGA    3153\n\n\n\nCodeflights_lite |&gt; \n  filter(is.na(dep_time)) |&gt; \n  count(by=origin) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `by`, COUNT(*) AS `n`\nFROM (\n  SELECT `nycflights13::flights`.*, `origin` AS `by`\n  FROM `nycflights13::flights`\n  WHERE ((`dep_time` IS NULL))\n) AS `q01`\nGROUP BY `by`\n\n\nNot far from a spontaneous answer! We could obtain the latter anyway.\n\nCodeflights_lite |&gt; \n  filter(is.na(dep_time)) |&gt; \n  group_by(origin) |&gt; \n  summarise(n=n()) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `origin`, COUNT(*) AS `n`\nFROM (\n  SELECT `nycflights13::flights`.*\n  FROM `nycflights13::flights`\n  WHERE ((`dep_time` IS NULL))\n) AS `q01`\nGROUP BY `origin`\n\n\nNote that combining dplyr verbs and pipes (|&gt; or |&gt;) provides a much more readable and modular approach than vanilla SQL.\n\nUsing dplyr and pipe, the order in which operation are executed is clear and obvious. In SQL, it is counterintuitive.\n\n\n\n\nIn table flights, 8255 (nrow(filter(flights, is.na(dep_time)))) rows have a missing dep_time field.\n\nWhat other variables are missing?\n\n\n\n\n\n\n\nThe introduction to tidyselect is a must read.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(is.na(dep_time)) |&gt; \n  summarize(across(everything(), ~ all(is.na(.)))) |&gt; \n  pivot_longer(cols = everything()) |&gt; \n  filter(value) |&gt; \n  select(name)\n\n# A tibble: 5 × 1\n  name     \n  &lt;chr&gt;    \n1 dep_time \n2 dep_delay\n3 arr_time \n4 arr_delay\n5 air_time \n\n\n\nCodeflights_lite |&gt; \n  filter(is.na(dep_time)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `nycflights13::flights`.*\nFROM `nycflights13::flights`\nWHERE ((`dep_time` IS NULL))\n\n\n\n\n\nWhat might these rows with missing data represent?\n\nAll the information you can only get if the flight did take off.\n\nCodenot_cancelled &lt;-  flights |&gt; \n  filter(!is.na(dep_time))\n\n\n\n\nMore questions: for each column in flight report the number of missing values.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  summarise(across(everything(), ~  sum(is.na(.)))) |&gt; \n  pivot_longer(cols = everything()) |&gt; \n  filter(value &gt; 0) |&gt; \n  arrange(desc(value))\n\n# A tibble: 6 × 2\n  name      value\n  &lt;chr&gt;     &lt;int&gt;\n1 arr_delay  9430\n2 air_time   9430\n3 arr_time   8713\n4 dep_time   8255\n5 dep_delay  8255\n6 tailnum    2512\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  skimr::skim()\n\n\nData summary\n\n\nName\nflights\n\n\nNumber of rows\n336776\n\n\nNumber of columns\n19\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n14\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\ncarrier\n0\n1.00\n2\n2\n0\n16\n0\n\n\ntailnum\n2512\n0.99\n5\n6\n0\n4043\n0\n\n\norigin\n0\n1.00\n3\n3\n0\n3\n0\n\n\ndest\n0\n1.00\n3\n3\n0\n105\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nyear\n0\n1.00\n2013.00\n0.00\n2013\n2013\n2013\n2013\n2013\n▁▁▇▁▁\n\n\nmonth\n0\n1.00\n6.55\n3.41\n1\n4\n7\n10\n12\n▇▆▆▆▇\n\n\nday\n0\n1.00\n15.71\n8.77\n1\n8\n16\n23\n31\n▇▇▇▇▆\n\n\ndep_time\n8255\n0.98\n1349.11\n488.28\n1\n907\n1401\n1744\n2400\n▁▇▆▇▃\n\n\nsched_dep_time\n0\n1.00\n1344.25\n467.34\n106\n906\n1359\n1729\n2359\n▁▇▇▇▃\n\n\ndep_delay\n8255\n0.98\n12.64\n40.21\n-43\n-5\n-2\n11\n1301\n▇▁▁▁▁\n\n\narr_time\n8713\n0.97\n1502.05\n533.26\n1\n1104\n1535\n1940\n2400\n▁▃▇▇▇\n\n\nsched_arr_time\n0\n1.00\n1536.38\n497.46\n1\n1124\n1556\n1945\n2359\n▁▃▇▇▇\n\n\narr_delay\n9430\n0.97\n6.90\n44.63\n-86\n-17\n-5\n14\n1272\n▇▁▁▁▁\n\n\nflight\n0\n1.00\n1971.92\n1632.47\n1\n553\n1496\n3465\n8500\n▇▃▃▁▁\n\n\nair_time\n9430\n0.97\n150.69\n93.69\n20\n82\n129\n192\n695\n▇▂▂▁▁\n\n\ndistance\n0\n1.00\n1039.91\n733.23\n17\n502\n872\n1389\n4983\n▇▃▂▁▁\n\n\nhour\n0\n1.00\n13.18\n4.66\n1\n9\n13\n17\n23\n▁▇▇▇▅\n\n\nminute\n0\n1.00\n26.23\n19.30\n0\n8\n29\n44\n59\n▇▃▆▃▅\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\ntime_hour\n0\n1\n2013-01-01 05:00:00\n2013-12-31 23:00:00\n2013-07-03 10:00:00\n6936"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#arrange",
    "href": "core/labs-solutions/lab-tables.html#arrange",
    "title": "Tables manipulation II",
    "section": "Arrange",
    "text": "Arrange\n\nHow could you use arrange() to sort all missing values to the start? (Hint: use is.na()).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nSort flights to find the most delayed flights.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  arrange(desc(dep_delay)) |&gt; \n  select(dep_delay, arr_delay, everything())\n\n# A tibble: 336,776 × 19\n   dep_delay arr_delay  year month   day dep_time sched_dep_time arr_time\n       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;int&gt;\n 1      1301      1272  2013     1     9      641            900     1242\n 2      1137      1127  2013     6    15     1432           1935     1607\n 3      1126      1109  2013     1    10     1121           1635     1239\n 4      1014      1007  2013     9    20     1139           1845     1457\n 5      1005       989  2013     7    22      845           1600     1044\n 6       960       931  2013     4    10     1100           1900     1342\n 7       911       915  2013     3    17     2321            810      135\n 8       899       850  2013     6    27      959           1900     1236\n 9       898       895  2013     7    22     2257            759      121\n10       896       878  2013    12     5      756           1700     1058\n# ℹ 336,766 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nPick the ten most delayed flights (with finite dep_delay)\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  slice_max(order_by=dep_delay, n = 10, na_rm = T)\n\n# A tibble: 10 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nFind the flights that left earliest.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  arrange(dep_time) |&gt; \n  select(dest, origin, dep_time)\n\n# A tibble: 336,776 × 3\n   dest  origin dep_time\n   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;\n 1 SYR   JFK           1\n 2 MDW   LGA           1\n 3 SJU   JFK           1\n 4 BQN   JFK           1\n 5 SJU   JFK           1\n 6 SJU   JFK           1\n 7 BQN   JFK           1\n 8 MDW   LGA           1\n 9 PWM   JFK           1\n10 PSE   JFK           1\n# ℹ 336,766 more rows\n\n\n\n\n\nSort flights to find the fastest (highest speed) flights.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(!is.na(air_time)) |&gt; \n  mutate(speed = distance/air_time) |&gt; \n  arrange(desc(speed)) |&gt; \n  select(speed, distance, air_time, origin, dest, everything())\n\n# A tibble: 327,346 × 20\n   speed distance air_time origin dest   year month   day dep_time\n   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n 1 11.7       762       65 LGA    ATL    2013     5    25     1709\n 2 10.8      1008       93 EWR    MSP    2013     7     2     1558\n 3 10.8       594       55 EWR    GSP    2013     5    13     2040\n 4 10.7       748       70 EWR    BNA    2013     3    23     1914\n 5  9.86     1035      105 LGA    PBI    2013     1    12     1559\n 6  9.4      1598      170 JFK    SJU    2013    11    17      650\n 7  9.29     1598      172 JFK    SJU    2013     2    21     2355\n 8  9.27     1623      175 JFK    STT    2013    11    17      759\n 9  9.24     1598      173 JFK    SJU    2013    11    16     2003\n10  9.24     1598      173 JFK    SJU    2013    11    16     2349\n# ℹ 327,336 more rows\n# ℹ 11 more variables: sched_dep_time &lt;int&gt;, dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;,\n#   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nThis is an overkill. We are sorting in order to perform maximum selection. Recall that sorting requires more comparisons than selection. The comparison between sorting and selecting with respect to data shuffling is even less favourable.\n\nCodeflights |&gt; \n  filter(!is.na(air_time)) |&gt; \n  mutate(speed = distance/air_time) |&gt;\n  slice_max(n=10, speed) |&gt;\n  select(speed, distance, air_time, origin, dest, everything())\n\n# A tibble: 15 × 20\n   speed distance air_time origin dest   year month   day dep_time\n   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n 1 11.7       762       65 LGA    ATL    2013     5    25     1709\n 2 10.8      1008       93 EWR    MSP    2013     7     2     1558\n 3 10.8       594       55 EWR    GSP    2013     5    13     2040\n 4 10.7       748       70 EWR    BNA    2013     3    23     1914\n 5  9.86     1035      105 LGA    PBI    2013     1    12     1559\n 6  9.4      1598      170 JFK    SJU    2013    11    17      650\n 7  9.29     1598      172 JFK    SJU    2013     2    21     2355\n 8  9.27     1623      175 JFK    STT    2013    11    17      759\n 9  9.24     1598      173 JFK    SJU    2013    11    16     2003\n10  9.24     1598      173 JFK    SJU    2013    11    16     2349\n11  9.24     1598      173 JFK    SJU    2013    11    17      851\n12  9.24     1598      173 JFK    SJU    2013    11    17     1926\n13  9.24     1598      173 JFK    SJU    2013    12     5     1858\n14  9.24     1598      173 JFK    SJU    2013     2    10     1658\n15  9.24     1598      173 JFK    SJU    2013     2    10     1958\n# ℹ 11 more variables: sched_dep_time &lt;int&gt;, dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;,\n#   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nWhich flights travelled the farthest?\n\n\n\n\n\n\n\nThe database provides all we need with columns distance and air_time. Otherwise, with the positions of airports from table airports, we should be able to compute distances using :\n\n‘Haversine’ formula.\n\nhttps://en.wikipedia.org/wiki/Haversine_formula\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  arrange(desc(distance)) |&gt; \n  distinct(distance, dest, origin)\n\n# A tibble: 226 × 3\n   distance dest  origin\n      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1     4983 HNL   JFK   \n 2     4963 HNL   EWR   \n 3     3370 ANC   EWR   \n 4     2586 SFO   JFK   \n 5     2576 OAK   JFK   \n 6     2569 SJC   JFK   \n 7     2565 SFO   EWR   \n 8     2521 SMF   JFK   \n 9     2475 LAX   JFK   \n10     2465 LGB   JFK   \n# ℹ 216 more rows\n\n\n\n\n\nWhich travelled the shortest?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  arrange(distance) |&gt; \n  select(distance, dest, origin, everything())\n\n# A tibble: 336,776 × 19\n   distance dest  origin  year month   day dep_time sched_dep_time dep_delay\n      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1       17 LGA   EWR     2013     7    27       NA            106        NA\n 2       80 PHL   EWR     2013     1     3     2127           2129        -2\n 3       80 PHL   EWR     2013     1     4     1240           1200        40\n 4       80 PHL   EWR     2013     1     4     1829           1615       134\n 5       80 PHL   EWR     2013     1     4     2128           2129        -1\n 6       80 PHL   EWR     2013     1     5     1155           1200        -5\n 7       80 PHL   EWR     2013     1     6     2125           2129        -4\n 8       80 PHL   EWR     2013     1     7     2124           2129        -5\n 9       80 PHL   EWR     2013     1     8     2127           2130        -3\n10       80 PHL   EWR     2013     1     9     2126           2129        -3\n# ℹ 336,766 more rows\n# ℹ 10 more variables: arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;,\n#   carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, air_time &lt;dbl&gt;, hour &lt;dbl&gt;,\n#   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nAvoid an overkill …\n\nCodeflights |&gt;\n  distinct(distance, origin, dest) |&gt; \n  slice_min(distance, n=10)\n\n# A tibble: 11 × 3\n   distance origin dest \n      &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1       17 EWR    LGA  \n 2       80 EWR    PHL  \n 3       94 JFK    PHL  \n 4       96 LGA    PHL  \n 5      116 EWR    BDL  \n 6      143 EWR    ALB  \n 7      160 EWR    PVD  \n 8      169 EWR    BWI  \n 9      173 JFK    MVY  \n10      184 JFK    BWI  \n11      184 LGA    BOS"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#projection",
    "href": "core/labs-solutions/lab-tables.html#projection",
    "title": "Tables manipulation II",
    "section": "Projection",
    "text": "Projection\n\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  select(dep_time, dep_delay, arr_time, arr_delay) |&gt; \n  head()\n\n# A tibble: 6 × 4\n  dep_time dep_delay arr_time arr_delay\n     &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1      517         2      830        11\n2      533         4      850        20\n3      542         2      923        33\n4      544        -1     1004       -18\n5      554        -6      812       -25\n6      554        -4      740        12\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  select(starts_with(\"dep\"), starts_with(\"arr\")) |&gt;  \n  head()\n\n# A tibble: 6 × 4\n  dep_time dep_delay arr_time arr_delay\n     &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1      517         2      830        11\n2      533         4      850        20\n3      542         2      923        33\n4      544        -1     1004       -18\n5      554        -6      812       -25\n6      554        -4      740        12\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  select(starts_with(\"dep_\") | starts_with(\"arr\")) |&gt; \n  head()\n\n# A tibble: 6 × 4\n  dep_time dep_delay arr_time arr_delay\n     &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1      517         2      830        11\n2      533         4      850        20\n3      542         2      923        33\n4      544        -1     1004       -18\n5      554        -6      812       -25\n6      554        -4      740        12\n\n\n\n\n\nWhat happens if you include the name of a variable multiple times in a select() call?\n\n\n\n\n\n\n\nSolution\n\n\n\nIt is included once in the result.\n\nCodeflights |&gt; \n  select(arr_time, starts_with(\"dep_\") | starts_with(\"arr\"), arr_time) |&gt; \n  head()\n\n# A tibble: 6 × 4\n  arr_time dep_time dep_delay arr_delay\n     &lt;int&gt;    &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1      830      517         2        11\n2      850      533         4        20\n3      923      542         2        33\n4     1004      544        -1       -18\n5      812      554        -6       -25\n6      740      554        -4        12\n\n\n\n\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\nvars &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\n\n\n\n\n\nSolution\n\n\n\n\nCodevars &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\nflights |&gt; \n  filter(across(any_of(vars), is.numeric))\n\nWarning: Using `across()` in `filter()` was deprecated in dplyr 1.0.8.\nℹ Please use `if_any()` or `if_all()` instead.\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nCodeflights |&gt; \n  filter(if_any(vars, \\(x) x&lt;0)) |&gt; \n  relocate(ends_with('delay')) |&gt; \n  head()\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(vars)\n\n  # Now:\n  data %&gt;% select(all_of(vars))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\n# A tibble: 6 × 19\n  dep_delay arr_delay  year month   day dep_time sched_dep_time arr_time\n      &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;int&gt;\n1        -1       -18  2013     1     1      544            545     1004\n2        -6       -25  2013     1     1      554            600      812\n3        -4        12  2013     1     1      554            558      740\n4        -5        19  2013     1     1      555            600      913\n5        -3       -14  2013     1     1      557            600      709\n6        -3        -8  2013     1     1      557            600      838\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nDoes the result of running the following code surprise you?\n\n\nCodeselect(\n  flights, \n  contains(\"TIME\", ignore.case =TRUE))  |&gt; \n  head()\n\n# A tibble: 6 × 6\n  dep_time sched_dep_time arr_time sched_arr_time air_time time_hour          \n     &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dttm&gt;             \n1      517            515      830            819      227 2013-01-01 05:00:00\n2      533            529      850            830      227 2013-01-01 05:00:00\n3      542            540      923            850      160 2013-01-01 05:00:00\n4      544            545     1004           1022      183 2013-01-01 05:00:00\n5      554            600      812            837      116 2013-01-01 06:00:00\n6      554            558      740            728      150 2013-01-01 05:00:00\n\n\n\nHow do the select helpers deal with case by default?\n\n\n\n\n\n\n\nSolution\n\n\n\nBy default, select helpers ignore case. This complies with a similar behavior in RDBMS\n\n\n\nHow can you change that default?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeselect(flights, \n  contains(\"TIME\", ignore.case=FALSE))  |&gt; \n  head()\n\n# A tibble: 6 × 0"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#mutations",
    "href": "core/labs-solutions/lab-tables.html#mutations",
    "title": "Tables manipulation II",
    "section": "Mutations",
    "text": "Mutations\n\nCurrently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  mutate(across(ends_with('dep_time') | ends_with('arr_time'), \n                (\\(x) 60L * (x %/% 100L) + (x %% 100L)),\n                .names=\"cor_{.col}\")) |&gt;\n  select(ends_with('dep_time'), ends_with('arr_time'), everything())\n\n# A tibble: 336,776 × 23\n   dep_time sched_dep_time cor_dep_time cor_sched_dep_time arr_time\n      &lt;int&gt;          &lt;int&gt;        &lt;int&gt;              &lt;int&gt;    &lt;int&gt;\n 1      517            515          317                315      830\n 2      533            529          333                329      850\n 3      542            540          342                340      923\n 4      544            545          344                345     1004\n 5      554            600          354                360      812\n 6      554            558          354                358      740\n 7      555            600          355                360      913\n 8      557            600          357                360      709\n 9      557            600          357                360      838\n10      558            600          358                360      753\n# ℹ 336,766 more rows\n# ℹ 18 more variables: sched_arr_time &lt;int&gt;, cor_arr_time &lt;int&gt;,\n#   cor_sched_arr_time &lt;int&gt;, year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;,\n#   dep_delay &lt;dbl&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\nCodedf &lt;- flights |&gt; \n  mutate(across(ends_with('dep_time') | ends_with('arr_time'), \n                (\\(x) 60L * (x %/% 100L) + (x %% 100L)),\n                .names=\"{.col}\"))\n\n\n\n\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodedf |&gt; \n  mutate(est_arr_time = (dep_time + air_time) %% 1440) |&gt; \n  filter(abs(est_arr_time - arr_time)&lt;1000) |&gt; \n  arrange(desc(abs(est_arr_time - arr_time))) |&gt; \n  select(dest, arr_time, est_arr_time, ends_with(\"time\"), everything())\n\n# A tibble: 314,414 × 20\n   dest  arr_time est_arr_time dep_time sched_dep_time sched_arr_time air_time\n   &lt;chr&gt;    &lt;int&gt;        &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt;\n 1 HNL        881         1226      594            600            890      632\n 2 HNL        858         1203      594            600            910      609\n 3 HNL        865         1210      594            600            900      616\n 4 HNL       1084         1429      810            815           1097      619\n 5 HNL        855         1199      598            600            890      601\n 6 HNL       1090         1434      815            809           1093      619\n 7 HNL        842         1186      585            600            885      601\n 8 HNL       1062         1405      809            809           1093      596\n 9 HNL        889         1232      597            600            890      635\n10 HNL        900         1243      597            600            930      646\n# ℹ 314,404 more rows\n# ℹ 13 more variables: year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, dep_delay &lt;dbl&gt;,\n#   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodedf |&gt; \n  mutate(est_arr_time = (dep_time + air_time) %% 1440) |&gt; \n  filter(!is.na(arr_time), !is.na(est_arr_time)) |&gt; \n  ggplot() +\n  aes(x = arr_time, y = est_arr_time) +\n  geom_point(alpha=.1, size=.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodedf |&gt; \n  mutate(est_arr_time = (dep_time + air_time) %% 1440) |&gt; \n  filter(!is.na(arr_time), !is.na(est_arr_time)) |&gt; \n  filter(abs(arr_time -est_arr_time) &lt;600) |&gt; \n  ggplot() +\n  aes(x = (arr_time -est_arr_time)%%60) +\n  stat_density() +\n  scale_x_continuous(breaks=seq(0, 60, 10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\n\n\nWe expect dep_time == sched_dep_time + dep_delay %% 1440\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodedf |&gt; \n  filter(!is.na(dep_time)) |&gt; \n  mutate(est_dep_time = (sched_dep_time + dep_delay) %% 1440L) |&gt; \n  ggplot() +\n  aes(x=dep_time, y=est_dep_time) +\n  geom_point(alpha=.1, size=.1) + \n  coord_fixed() \n\n\n\n\n\n\n\n\n\nFor most of the flights, the discrepancy between dep_time and the computed value is 0.\nFor 29 flights it is positive. The discrepancy is then exactly equal to 24 hours.\n\n\n\n\n\n\nSolution\n\n\n\n\nCodedf |&gt; \n  filter(!is.na(dep_time)) |&gt; \n  mutate(discr = dep_time - (sched_dep_time + dep_delay) %% 1440L) |&gt; \n  filter(discr &gt; 0) |&gt; \n  select(dest, origin, carrier, distance, discr) |&gt; \n  left_join(airports, by=c(\"dest\"=\"faa\")) |&gt; \n  arrange(carrier, desc(distance), dest) |&gt; \n  select(dest, origin, carrier, name, discr)\n\n# A tibble: 29 × 5\n   dest  origin carrier name               discr\n   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n 1 MIA   LGA    AA      Miami Intl          1440\n 2 ORD   LGA    AA      Chicago Ohare Intl  1440\n 3 PSE   JFK    B6      &lt;NA&gt;                1440\n 4 PSE   JFK    B6      &lt;NA&gt;                1440\n 5 PSE   JFK    B6      &lt;NA&gt;                1440\n 6 PSE   JFK    B6      &lt;NA&gt;                1440\n 7 PSE   JFK    B6      &lt;NA&gt;                1440\n 8 PSE   JFK    B6      &lt;NA&gt;                1440\n 9 SJU   JFK    B6      &lt;NA&gt;                1440\n10 SJU   JFK    B6      &lt;NA&gt;                1440\n# ℹ 19 more rows\n\n\n\n\n\nFind the 10 most delayed flights using a ranking function. How do you want to handle ties?\n\n\nCarefully read the documentation for min_rank().\nWindowed rank functions.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(dense_rank(-arr_delay) &lt;=10) |&gt; \n  arrange(desc(arr_delay)) |&gt; \n  sample_n(10)\n\n# A tibble: 10 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7    22     2257            759       898      121           1026\n 2  2013     1     9      641            900      1301     1242           1530\n 3  2013     5     3     1133           2055       878     1250           2215\n 4  2013     1    10     1121           1635      1126     1239           1810\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013    12     5      756           1700       896     1058           2020\n 8  2013     9    20     1139           1845      1014     1457           2210\n 9  2013     6    15     1432           1935      1137     1607           2120\n10  2013     3    17     2321            810       911      135           1020\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#aggregations",
    "href": "core/labs-solutions/lab-tables.html#aggregations",
    "title": "Tables manipulation II",
    "section": "Aggregations",
    "text": "Aggregations\n\nBrainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios:\n\nA flight is 15 minutes early 50% of the time, and 15 minutes late 10% of the time.\nA flight is always 10 minutes late.\nA flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time.\n99% of the time a flight is on time. 1% of the time it’s 2 hours late.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights_gp &lt;- flights |&gt; \n  group_by(flight, carrier, dest, origin) \n\n\n\nCodeflights_gp |&gt; \n  summarize(q50 = quantile(arr_delay,.5, , na.rm=T), \n            q90 = quantile(arr_delay,.9, , na.rm=T),\n            .groups=\"drop\") |&gt; \n  filter(q50&lt;=-15, q90&gt;=15) |&gt; \n  head()\n\n# A tibble: 6 × 6\n  flight carrier dest  origin   q50   q90\n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1      5 AS      SEA   EWR      -21  26.4\n2      6 DL      SLC   JFK      -18  20  \n3     10 UA      IAH   LGA      -19  28.5\n4     11 AS      SEA   EWR      -15  26.4\n5     31 DL      SFO   JFK      -22  17.2\n6     59 AA      SFO   JFK      -16  20.2\n\n\nAnother approach builds on experimental reframe()\n\nCodequantile_df &lt;- function(x, probs = c(0.5, 0.9)) {\n  tibble(\n    val = quantile(x, probs, na.rm = TRUE),\n    quant = probs\n  )\n}\n\n\n\nCodetmp &lt;- flights_gp |&gt; \n  reframe(q= quantile_df(arr_delay)) |&gt; \n  unnest(cols=q)\n\n\nreframe() generates a list-column (a column is tibble-valued). The result has to be unnested.\n\nCodetmp |&gt; \n  pivot_wider(\n    id_cols=c(flight, carrier, dest, origin),\n    names_from= quant,\n    values_from= val,\n    names_prefix=\"q\"\n  ) |&gt; \n  head()\n\n# A tibble: 6 × 6\n  flight carrier dest  origin  q0.5   q0.9\n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1      1 AA      LAX   JFK    -13    19   \n2      1 B6      FLL   JFK      2    47.8 \n3      1 DL      SJU   JFK    -21    -6.40\n4      1 UA      ORD   EWR    -26   -26   \n5      1 UA      PBI   EWR    -19   -14.2 \n6      1 WN      MDW   LGA      2.5  33.6 \n\n\n\n\n\nCodeflights |&gt; \n  group_by(dest) |&gt; \n  summarise(n_cancelled = sum(is.na(dep_time)))\n\n# A tibble: 105 × 2\n   dest  n_cancelled\n   &lt;chr&gt;       &lt;int&gt;\n 1 ABQ             0\n 2 ACK             0\n 3 ALB            20\n 4 ANC             0\n 5 ATL           317\n 6 AUS            21\n 7 AVL            12\n 8 BDL            31\n 9 BGR            15\n10 BHM            25\n# ℹ 95 more rows\n\n\n\nCodeflights_lite |&gt; \n  group_by(dest) |&gt; \n  summarise(n_cancelled = sum(is.na(dep_time))) |&gt; \n  show_query()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n&lt;SQL&gt;\nSELECT `dest`, SUM((`dep_time` IS NULL)) AS `n_cancelled`\nFROM `nycflights13::flights`\nGROUP BY `dest`\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  group_by(flight, carrier, dest, origin) |&gt; \n  summarize(q = quantile(arr_delay,.5, na.rm=T), m=mean(arr_delay, na.rm=TRUE)) |&gt; \n  filter((q &lt; -15) | (q &gt; 15)) |&gt; \n  arrange(desc(m))\n\n`summarise()` has grouped output by 'flight', 'carrier', 'dest'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 4,161 × 6\n# Groups:   flight, carrier, dest [4,106]\n   flight carrier dest  origin     q     m\n    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1    372 UA      CLE   LGA      455   455\n 2    488 UA      DEN   LGA      359   359\n 3    521 WN      BNA   EWR      335   335\n 4   5017 EV      DTW   LGA      334   334\n 5    468 UA      MCO   EWR      323   323\n 6    390 DL      DEN   LGA      318   318\n 7    809 DL      SLC   EWR      299   299\n 8   3261 EV      CLE   EWR      292   292\n 9   3760 9E      RIC   JFK      288   288\n10   1510 UA      IAH   EWR      283   283\n# ℹ 4,151 more rows\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  group_by(jour=lubridate::wday(time_hour, label=T, abbr = F)) |&gt; \n  summarise(n_cancelled=sum(is.na(dep_time)), n_tot=n())  |&gt; \n  mutate(prop_cancelled = n_cancelled/n_tot)\n\n# A tibble: 7 × 4\n  jour     n_cancelled n_tot prop_cancelled\n  &lt;ord&gt;          &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;\n1 dimanche         714 46357         0.0154\n2 lundi           1220 50690         0.0241\n3 mardi           1152 50422         0.0228\n4 mercredi        1202 50060         0.0240\n5 jeudi           1562 50219         0.0311\n6 vendredi        1608 50308         0.0320\n7 samedi           797 38720         0.0206\n\nCode?lubridate::wday\n\n\n\n\n\nWhich is more important: arrival delay or departure delay?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nCome up with another approach that will give you the same output as not_cancelled |&gt; count(dest) and (without usingcount()`).\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodenot_cancelled |&gt; \n  summarise(n_distinct(dest), n_distinct(tailnum))\n\n# A tibble: 1 × 2\n  `n_distinct(dest)` `n_distinct(tailnum)`\n               &lt;int&gt;                 &lt;int&gt;\n1                104                  4037\n\n\n\n\n\nOur definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay) ) is slightly suboptimal. Why? Which is the most important column?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(xor(is.na(arr_delay), is.na(dep_delay))) |&gt; \n  group_by(is.na(arr_delay), is.na(dep_delay)) |&gt; \n  summarise(n())\n\n`summarise()` has grouped output by 'is.na(arr_delay)'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 1 × 3\n# Groups:   is.na(arr_delay) [1]\n  `is.na(arr_delay)` `is.na(dep_delay)` `n()`\n  &lt;lgl&gt;              &lt;lgl&gt;              &lt;int&gt;\n1 TRUE               FALSE               1175\n\n\n\n\n\nLook at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  mutate(day_of_week=wday(time_hour, label=TRUE)) |&gt; \n  group_by(day_of_week) |&gt; \n  summarise(n_cancelled= sum(is.na(arr_delay)), n(), prop= n_cancelled/n()) |&gt; \n  ggplot() +\n  aes(x=day_of_week, y=prop) +\n  geom_col(width=.5) +\n  labs(title=\"Proportion of cancelled flights with respect to day of week\") +\n  xlab(\"Day of Week\") +\n  ylab(\"Proportion\")\n\n\n\n\n\n\n\n\n\n\nWhich carrier has the worst delays?\n\nChallenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarise(n()))\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(!is.na(arr_delay)) |&gt; \n  group_by(carrier, dest) |&gt; \n  summarise(m=mean(arr_delay), q9=quantile(arr_delay, (c(9)/10)), nn=n()) |&gt; \n  ungroup() |&gt; \n  filter(min_rank(-nn) &lt;50) |&gt; \n  ggplot() +\n  aes(x=carrier, y=dest) +\n  geom_point(aes(color=q9, size=nn)) +\n  scale_color_viridis_c() +\n  scale_size_area()\n\n`summarise()` has grouped output by 'carrier'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\nWhat does the sort argument to count() do. When might you use it?\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#miscellanea",
    "href": "core/labs-solutions/lab-tables.html#miscellanea",
    "title": "Tables manipulation II",
    "section": "Miscellanea",
    "text": "Miscellanea\n\nWhich carriers serve all destination airports (in the table) ?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  distinct(origin, dest) |&gt; \n  mutate(tot_dest = n_distinct(dest)) |&gt; \n  group_by(tot_dest, origin) |&gt; \n  summarise(n_dest=n(), .groups=\"drop\") |&gt; \n  filter(n_dest==tot_dest) \n\n# A tibble: 0 × 3\n# ℹ 3 variables: tot_dest &lt;int&gt;, origin &lt;chr&gt;, n_dest &lt;int&gt;\n\n\n Verb mutate() handles function n_distinct() as a window function with trivial window.\n\n\n\n\n\n\n\n\nThere is no need to perform grouping and aggregation to answer this question. Basic relational algebra is enough. This operation is called division in SQL language\n\nCodeR &lt;- flights |&gt;\n    distinct(carrier, dest)\n\nC &lt;- R |&gt; distinct(carrier) \nD &lt;- R |&gt; distinct(dest)\n\nsetdiff(C, crossing(C, D) |&gt; \n           setdiff(R) |&gt; \n           distinct(carrier))\n\n# A tibble: 0 × 1\n# ℹ 1 variable: carrier &lt;chr&gt;\n\n\nLet \\(\\mathcal{R}(A, B\\) be the schema of some table \\(R\\), then\\[\nR \\div \\pi_B(R) = \\pi_A(R) \\setminus \\left(\\pi_A \\left(\\pi_A(R) \\times \\pi_B(R) \\setminus R \\right)\\right)\n\\]\nMore generally if \\(S\\) has schema \\(\\mathcal{S}(B)\\) \\[\nR \\div S = \\pi_A(R) \\setminus \\left(\\pi_A \\left(\\pi_A(R) \\times \\pi_B(S) \\setminus R \\right)\\right)\n\\]\n\n\n\n\nRefer back to the lists of useful mutate and filtering functions.\nDescribe how each operation changes when you combine it with grouping.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nWhich plane (tailnum) has the worst on-time record amongst planes with at least ten flights?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(!is.na(arr_delay)) |&gt; \n  group_by(tailnum) |&gt; \n  summarise(m = mean(arr_delay), n_flights=n() ) |&gt; \n  filter(n_flights &gt; 10 ) |&gt; \n  filter(min_rank(-m) == 1) |&gt; \n  inner_join(distinct(flights, tailnum, carrier))\n\nJoining with `by = join_by(tailnum)`\n\n\n# A tibble: 1 × 4\n  tailnum     m n_flights carrier\n  &lt;chr&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;  \n1 N337AT   66.5        13 FL     \n\n\n\n\n\nWhat time of day should you fly if you want to avoid delays as much as possible?\n\n\n\n\n\n\n\nSolution\n\n\n\nDefinitely before 9a.m.\n\nCodeflights |&gt; \n  filter(!is.na(arr_delay)) |&gt; \n  group_by(hour) |&gt; \n  summarise(m = mean(arr_delay, na.rm=T)) |&gt; \n  arrange(m) |&gt; \n  ggplot() + \n  aes(x=hour, y=m) +\n  geom_col(color=\"black\", fill=\"white\", alpha=.2) +\n  labs(title=\"Mean arrival delay as a function of departure hour\",\n       subtitle= \"nycflights13 data\")\n\n\n\n\n\n\n\n\n\n\nFor each destination, compute the total minutes of delay.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt;\n    filter(!is.na(arr_delay)) |&gt;\n    group_by(dest) |&gt;\n    summarise(m = sum(arr_delay, na.rm = T)) |&gt; \n    arrange(desc(m)) |&gt; \n    rename(faa=dest) |&gt;\n    dplyr::inner_join(airports)\n\nJoining with `by = join_by(faa)`\n\n\n# A tibble: 100 × 9\n   faa        m name                          lat    lon   alt    tz dst   tzone\n   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 ATL   190260 Hartsfield Jackson Atlanta…  33.6  -84.4  1026    -5 A     Amer…\n 2 CLT   100645 Charlotte Douglas Intl       35.2  -80.9   748    -5 A     Amer…\n 3 ORD    97352 Chicago Ohare Intl           42.0  -87.9   668    -6 A     Amer…\n 4 FLL    96153 Fort Lauderdale Hollywood …  26.1  -80.2     9    -5 A     Amer…\n 5 DCA    82609 Ronald Reagan Washington N…  38.9  -77.0    15    -5 A     Amer…\n 6 RDU    78107 Raleigh Durham Intl          35.9  -78.8   435    -5 A     Amer…\n 7 MCO    76185 Orlando Intl                 28.4  -81.3    96    -5 A     Amer…\n 8 IAD    74631 Washington Dulles Intl       38.9  -77.5   313    -5 A     Amer…\n 9 BNA    71867 Nashville Intl               36.1  -86.7   599    -6 A     Amer…\n10 DEN    61700 Denver Intl                  39.9 -105.   5431    -7 A     Amer…\n# ℹ 90 more rows\n\n\n\n\n\nFor each flight, compute the proportion of the total positive arrival delays for its destination.\n\n\n\n\n\n\n\nSolution\n\n\n\nWould be easy with ROLLUP (dest, flight)\nWITH R AS (\n  SELECT dest, flight, sum(arr_delay)\n  FROM flights\n  WHERE arr_delay &gt;0\n  GROUP BY ROLLUP(dest, flight)\n)\nSELECT r1.dest, r2.flight, r2.sum/r1.sum AS rap\nFROM (SELECT dest, sum \n      FROM R WHERE flight IS NULL) r1 \n      JOIN R r2 \n      ON (r1.dest=r2.dest)\nWHERE r2.flight IS NOT NULL AND r2.sum/r1.sum &gt;.1\nORDER BY r1.dest, r2.flight ;\nWith WINDOW it is even easier\n\n\nUsing dplyr, it is easy. See A second look at group_by\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n   dplyr::filter(arr_delay &gt;0 ) |&gt; \n   group_by(dest, flight) |&gt; \n   summarise(total_delay_flight=sum(arr_delay)) |&gt; # result is a grouped tibble\n   mutate(total_delay_dest=sum(total_delay_flight), \n          delay_ratio= total_delay_flight/total_delay_dest)  |&gt; \n  filter(dest %in% c('LAX', 'ATL'), delay_ratio &gt; .02)   \n\n`summarise()` has grouped output by 'dest'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 20 × 5\n# Groups:   dest [2]\n   dest  flight total_delay_flight total_delay_dest delay_ratio\n   &lt;chr&gt;  &lt;int&gt;              &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt;\n 1 ATL      348               7000           300299      0.0233\n 2 ATL      926               9908           300299      0.0330\n 3 ATL      947               8670           300299      0.0289\n 4 ATL     1147               6968           300299      0.0232\n 5 ATL     1499               6155           300299      0.0205\n 6 ATL     1942               6224           300299      0.0207\n 7 ATL     2042              11583           300299      0.0386\n 8 ATL     4705               6458           300299      0.0215\n 9 LAX       21               8378           203226      0.0412\n10 LAX      119               4685           203226      0.0231\n11 LAX      133               5644           203226      0.0278\n12 LAX      169               6560           203226      0.0323\n13 LAX      181               8228           203226      0.0405\n14 LAX      185               5925           203226      0.0292\n15 LAX      413               6271           203226      0.0309\n16 LAX      415              10102           203226      0.0497\n17 LAX      523               5055           203226      0.0249\n18 LAX      535               6030           203226      0.0297\n19 LAX      771               6963           203226      0.0343\n20 LAX     2363               4273           203226      0.0210\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe have used the fact that the output of summarise(., total_delay_flight=sum(arr_delay)) is still a grouped dataframe/tibble (with grouping variable dest). At the following line, mutate performs aggregation with sum() in a groupwise manner, that is per group, and the result of the aggregation is appended to each row of the group, because we use mutate instead of summarize.\nWITH R AS (\n  SELECT dest, \n         flight, \n         SUM(arr_delay)  AS  total_delay_per_flight\n  FROM flights\nWHERE arr_delay &gt; 0 \nGROUP BY dest, flight\n) \nSELECT dest, flight, \n       total_delay_per_flight, \n       SUM(total_delay_per_flight) OVER w_per_dest AS     total_delay_per_dest \nFROM R\nWINDOW w_per_dest AS (PARTITION BY dest) ;\n\n\n\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the delay of a flight is related to the delay of the immediately preceding flight.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n    group_by(origin)  |&gt; \n    arrange(year,month, day, sched_dep_time, .by_group=T) |&gt;   # .by_group -&gt; see documentation of arrange\n    mutate(prev_delay=lag(dep_delay, n=1L), prev_origin=lag(origin, n=1L)) |&gt; \n    slice_head(n=6L) |&gt;    # inspect the result \n    select(origin, prev_origin, dep_delay, prev_delay, everything())\n\n# A tibble: 18 × 21\n# Groups:   origin [3]\n   origin prev_origin dep_delay prev_delay  year month   day dep_time\n   &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n 1 EWR    &lt;NA&gt;                2         NA  2013     1     1      517\n 2 EWR    EWR                -4          2  2013     1     1      554\n 3 EWR    EWR                -5         -4  2013     1     1      555\n 4 EWR    EWR                -2         -5  2013     1     1      558\n 5 EWR    EWR                -1         -2  2013     1     1      559\n 6 EWR    EWR                 1         -1  2013     1     1      601\n 7 JFK    &lt;NA&gt;                2         NA  2013     1     1      542\n 8 JFK    JFK                -1          2  2013     1     1      544\n 9 JFK    JFK                 0         -1  2013     1     1      559\n10 JFK    JFK                -3          0  2013     1     1      557\n11 JFK    JFK                -2         -3  2013     1     1      558\n12 JFK    JFK                -2         -2  2013     1     1      558\n13 LGA    &lt;NA&gt;                4         NA  2013     1     1      533\n14 LGA    LGA                -6          4  2013     1     1      554\n15 LGA    LGA                -3         -6  2013     1     1      557\n16 LGA    LGA                -2         -3  2013     1     1      558\n17 LGA    LGA                -1         -2  2013     1     1      559\n18 LGA    LGA                 0         -1  2013     1     1      600\n# ℹ 13 more variables: sched_dep_time &lt;int&gt;, arr_time &lt;int&gt;,\n#   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n#   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\n\n\n\n\nlag() is an example of window function. If we were using SQL, we would define a WINDOW using an expression like\nWINDOW w As (PARTITION BY origin ORDER BY year, month, day, sched_dep_time)\nSomething still needs fixing here: some flights never took off (is.na(dep_time)). Should they be sided out? assigned an infinite departure delay?\n\n\n\n\nLook at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodedf &lt;- flights |&gt; \n    filter(!is.na(air_time), air_time &gt; 0) |&gt; \n    mutate(speed= (distance * 1.852)/ (air_time/60)) \n      \np1 &lt;- df |&gt; \n    ggplot() +\n        aes(x=origin, y=speed) +\n        geom_boxplot()\n\np2 &lt;- df |&gt; \n    ggplot() +\n        aes(x=speed) +\n        geom_histogram(color=\"black\", fill=\"white\", alpha=.2)\n\n\npatchwork::wrap_plots(p1 + p2 )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodep2 + geom_rug(alpha=.5) +\n    facet_zoom(xlim=c(900,1500))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nConsider all flights with average speed above \\(950\\text{km/h}\\) as suspicious.\nLet us visualize destinations and origins of the speedy flights.\n\n\n\n\n\n\nSolution\n\n\n\n\nCodedf |&gt; \n    filter(speed&gt;950) |&gt; \n    mutate(dest=as_factor(dest)) |&gt; \n    ggplot() +\n        aes(x=fct_infreq(dest)) +\n        geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\nSJU, BQN are located in Puerto Rico. LAX is Los Angeles airport. STT is located in Virgin Islands.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodedf |&gt; \n    filter(speed&gt;950, ! dest %in% c('SJU', 'BQN')) |&gt; \n    mutate(dest=as_factor(dest)) |&gt; \n    ggplot() +\n        aes(x=fct_infreq(dest), fill=origin) +\n        geom_bar() +\n        labs(x=\"Destination\", \n        title=\"Distribution of speedy flights\",\n        subtitle=\"Excluding SJU and BQN\")\n\n\n\n\n\n\n\n\n\n\nFind all destinations that are flown by at least two carriers. Use that information to rank the carriers.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nFor each plane, count the number of flights before the first delay greater than 1 hour.\n\n\n\n\n\n\n\nAssume a plane is characterized by tailnum. Some flights have no tailnum. We ignore them.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCodeflights |&gt; \n  filter(!is.na(tailnum)) |&gt; \n  group_by(tailnum) |&gt; \n  arrange(year, month, day, sched_dep_time, .bygroup=T) |&gt; \n  mutate(rnk=row_number(), tot=n()) |&gt; \n  filter(is.na(arr_time) | arr_delay &gt;=60) |&gt; \n  slice_head(n=1)  |&gt; \n  mutate(rel_rnk = rnk/tot) |&gt; \n  select(tailnum, rnk, tot,  rel_rnk, carrier) |&gt; \n  arrange(desc(tot), desc(rel_rnk)) |&gt; \n  ungroup() \n\n# A tibble: 3,454 × 5\n   tailnum   rnk   tot rel_rnk carrier\n   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1 N725MQ     17   575 0.0296  MQ     \n 2 N722MQ     31   513 0.0604  MQ     \n 3 N723MQ     24   507 0.0473  MQ     \n 4 N711MQ     24   486 0.0494  MQ     \n 5 N713MQ      4   483 0.00828 MQ     \n 6 N258JB      8   427 0.0187  B6     \n 7 N298JB     17   407 0.0418  B6     \n 8 N353JB     34   404 0.0842  B6     \n 9 N351JB     10   402 0.0249  B6     \n10 N735MQ      3   396 0.00758 MQ     \n# ℹ 3,444 more rows"
  },
  {
    "objectID": "core/labs-solutions/lab-tables.html#references",
    "href": "core/labs-solutions/lab-tables.html#references",
    "title": "Tables manipulation II",
    "section": "References",
    "text": "References\n\nData transformation cheatsheet\nR4Data Science Tidy\nBenchmarking\ndplyr and vctrs\nPosts on dplyr\nWindow functions on dplyr"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#grammar-of-graphics",
    "href": "core/labs-solutions/lab-gapminder.html#grammar-of-graphics",
    "title": "Data visualization",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nWe will use the Grammar of Graphics approach to visualization\nThe expression Grammar of Graphics was coined by Leiland Wilkinson to describe a principled approach to visualization in Data Analysis (EDA)\nA plot is organized around tabular data (a table with rows (observations) and columns (variables))\nA plot is a graphical object that can be built layer by layer\nBuilding a graphical object consists in chaining elementary operations\nThe acclaimed TED presentation by Hans Rosling illustrates the Grammar of Graphics approach\n\nWe will reproduce the animated demonstration using\n\n\nggplot2: an implementation of grammar of graphics in `R\n\nplotly: a bridge between R and the javascript library D3.js\n\nUsing plotly, opting for html ouput, brings the possibility of interactivity and animation"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#setup",
    "href": "core/labs-solutions/lab-gapminder.html#setup",
    "title": "Data visualization",
    "section": "Setup",
    "text": "Setup\nWe will use the following packages. If needed, we install them.\n\nCodestopifnot(\n  require(tidyverse), \n  require(patchwork), \n  require(glue), \n  require(ggforce), \n  require(plotly),\n  require(ggthemes),\n  require(gapminder),\n  require(ggrepel)\n)\n\n\nThe data we will use can be obtained by loading package gapminder\n\n\n\n\n\n\nTip\n\n\n\nIf the packages have not yet been installed on your hard drive, install them.\nYou can do that using base R install.packages() function:\ninstall.packages(\"tidyverse\")\nIt is often faster to use functions from package pak\ninstall.packages(\"pak\")\npak::pkg_install(\"tidyverse\")\n\n\nYou need to understand the difference between installing and loading a package\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow do we get the list of installed packages?\nHow do we get the list of loaded packages?\nWhich objects are made available by a package?\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nThe (usually very long) list of installed packages can be obtained by a simple function call.\n\nCodedf &lt;- installed.packages()\nhead(df)\n##             Package       LibPath                                            \n## abind       \"abind\"       \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## arkhe       \"arkhe\"       \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## arrow       \"arrow\"       \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## ash         \"ash\"         \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## AsioHeaders \"AsioHeaders\" \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## askpass     \"askpass\"     \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n##             Version    Priority Depends       \n## abind       \"1.4-5\"    NA       \"R (&gt;= 1.5.0)\"\n## arkhe       \"1.6.0\"    NA       \"R (&gt;= 3.5)\"  \n## arrow       \"16.1.0\"   NA       \"R (&gt;= 4.0)\"  \n## ash         \"1.0-15\"   NA       NA            \n## AsioHeaders \"1.22.1-2\" NA       NA            \n## askpass     \"1.2.0\"    NA       NA            \n##             Imports                                                                                                                \n## abind       \"methods, utils\"                                                                                                       \n## arkhe       \"graphics, methods, stats, utils\"                                                                                      \n## arrow       \"assertthat, bit64 (&gt;= 0.9-7), glue, methods, purrr, R6, rlang\\n(&gt;= 1.0.0), stats, tidyselect (&gt;= 1.0.0), utils, vctrs\"\n## ash         NA                                                                                                                     \n## AsioHeaders NA                                                                                                                     \n## askpass     \"sys (&gt;= 2.1)\"                                                                                                         \n##             LinkingTo         \n## abind       NA                \n## arkhe       NA                \n## arrow       \"cpp11 (&gt;= 0.4.2)\"\n## ash         NA                \n## AsioHeaders NA                \n## askpass     NA                \n##             Suggests                                                                                                                                                                                                            \n## abind       NA                                                                                                                                                                                                                  \n## arkhe       \"tinytest\"                                                                                                                                                                                                          \n## arrow       \"blob, curl, cli, DBI, dbplyr, decor, distro, dplyr, duckdb\\n(&gt;= 0.2.8), hms, jsonlite, knitr, lubridate, pillar, pkgload,\\nreticulate, rmarkdown, stringi, stringr, sys, testthat (&gt;=\\n3.1.0), tibble, tzdb, withr\"\n## ash         NA                                                                                                                                                                                                                  \n## AsioHeaders NA                                                                                                                                                                                                                  \n## askpass     \"testthat\"                                                                                                                                                                                                          \n##             Enhances License                   License_is_FOSS\n## abind       NA       \"LGPL (&gt;= 2)\"             NA             \n## arkhe       NA       \"GPL (&gt;= 3)\"              NA             \n## arrow       NA       \"Apache License (&gt;= 2.0)\" NA             \n## ash         NA       \"GPL (&gt;= 2)\"              NA             \n## AsioHeaders NA       \"BSL-1.0\"                 NA             \n## askpass     NA       \"MIT + file LICENSE\"      NA             \n##             License_restricts_use OS_type MD5sum NeedsCompilation Built  \n## abind       NA                    NA      NA     \"no\"             \"4.4.0\"\n## arkhe       NA                    NA      NA     \"no\"             \"4.4.0\"\n## arrow       NA                    NA      NA     \"yes\"            \"4.4.0\"\n## ash         NA                    NA      NA     \"yes\"            \"4.4.0\"\n## AsioHeaders NA                    NA      NA     \"no\"             \"4.4.0\"\n## askpass     NA                    NA      NA     \"yes\"            \"4.4.0\"\n\n\nNote that the output is tabular (it is a matrix and an array) that contains much more than the names of installed packages. If we just want the names of the installed packages, we can extract the column named Package.\n\nCodedf[1:5, c(\"Package\", \"Version\") ]\n##             Package       Version   \n## abind       \"abind\"       \"1.4-5\"   \n## arkhe       \"arkhe\"       \"1.6.0\"   \n## arrow       \"arrow\"       \"16.1.0\"  \n## ash         \"ash\"         \"1.0-15\"  \n## AsioHeaders \"AsioHeaders\" \"1.22.1-2\"\n\n\nMatrices and arrays represent mathematical object and are fit for computations. They are not so convenient as far as querying is concerned. Dataframes which are also tabular objects can be queried like tables in a relational database.\nLoading a package amounts to make a number of objects available in the current session. The objects are made available though Namespaces.\n\nCodeloadedNamespaces()\n##  [1] \"methods\"     \"graphics\"    \"plotly\"      \"utf8\"        \"generics\"   \n##  [6] \"tidyr\"       \"stringi\"     \"hms\"         \"digest\"      \"magrittr\"   \n## [11] \"evaluate\"    \"grid\"        \"timechange\"  \"grDevices\"   \"fastmap\"    \n## [16] \"jsonlite\"    \"ggrepel\"     \"tidyverse\"   \"ggthemes\"    \"httr\"       \n## [21] \"purrr\"       \"fansi\"       \"viridisLite\" \"scales\"      \"tweenr\"     \n## [26] \"codetools\"   \"lazyeval\"    \"cli\"         \"rlang\"       \"polyclip\"   \n## [31] \"munsell\"     \"withr\"       \"utils\"       \"yaml\"        \"stats\"      \n## [36] \"tools\"       \"base\"        \"tzdb\"        \"dplyr\"       \"colorspace\" \n## [41] \"ggplot2\"     \"forcats\"     \"vctrs\"       \"R6\"          \"lifecycle\"  \n## [46] \"lubridate\"   \"stringr\"     \"htmlwidgets\" \"MASS\"        \"pkgconfig\"  \n## [51] \"pillar\"      \"gtable\"      \"glue\"        \"data.table\"  \"Rcpp\"       \n## [56] \"ggforce\"     \"xfun\"        \"tibble\"      \"tidyselect\"  \"knitr\"      \n## [61] \"farver\"      \"datasets\"    \"gapminder\"   \"htmltools\"   \"patchwork\"  \n## [66] \"rmarkdown\"   \"readr\"       \"compiler\"\n\n\nNote that we did not load explicitly some of the loadedNamespaces. Many of the loaded packages were loaded while loading other packages, for example metapackages like tidyverse."
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#have-a-look-at-gapminder-dataset",
    "href": "core/labs-solutions/lab-gapminder.html#have-a-look-at-gapminder-dataset",
    "title": "Data visualization",
    "section": "Have a look at gapminder dataset",
    "text": "Have a look at gapminder dataset\nThe gapminder table can be found at gapminder::gapminder\n\nA table has a schema: a list of named columns, each with a given type\nA table has a content: rows. Each row is a collection of items, corresponding to the columns\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplore gapminder::gapminder, using glimpse() and head()\n\n\nglimpse() allows to see the schema and the first rows\n\nhead() allows to see the first rows\nUse the pipe |&gt; to chain operations\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nDataframes\n\nCodegapminder &lt;- gapminder::gapminder\n\nglimpse(gapminder)\n## Rows: 1,704\n## Columns: 6\n## $ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n## $ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n## $ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n## $ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\ngapminder |&gt;  \n  glimpse()\n## Rows: 1,704\n## Columns: 6\n## $ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n## $ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n## $ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n## $ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\ngapminder |&gt; \n  head()\n## # A tibble: 6 × 6\n##   country     continent  year lifeExp      pop gdpPercap\n##   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n## 1 Afghanistan Asia       1952    28.8  8425333      779.\n## 2 Afghanistan Asia       1957    30.3  9240934      821.\n## 3 Afghanistan Asia       1962    32.0 10267083      853.\n## 4 Afghanistan Asia       1967    34.0 11537966      836.\n## 5 Afghanistan Asia       1972    36.1 13079460      740.\n## 6 Afghanistan Asia       1977    38.4 14880372      786.\n\n\nEven an empty dataframe has a scheme:\n\nCodegapminder |&gt; \n  head(0) |&gt; \n  glimpse()\n\nRows: 0\nColumns: 6\n$ country   &lt;fct&gt; \n$ continent &lt;fct&gt; \n$ year      &lt;int&gt; \n$ lifeExp   &lt;dbl&gt; \n$ pop       &lt;int&gt; \n$ gdpPercap &lt;dbl&gt; \n\nCode# glimpse(head(gapminder, 0))\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nThe schema of a dataframe/tibble is the list of column names and classes. The content of a dataframe is made of the rows. A dataframe may have null content\n\nCodegapminder |&gt; \n  filter(FALSE) |&gt; \n  glimpse()\n## Rows: 0\n## Columns: 6\n## $ country   &lt;fct&gt; \n## $ continent &lt;fct&gt; \n## $ year      &lt;int&gt; \n## $ lifeExp   &lt;dbl&gt; \n## $ pop       &lt;int&gt; \n## $ gdpPercap &lt;dbl&gt;"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#get-a-feeling-of-the-dataset",
    "href": "core/labs-solutions/lab-gapminder.html#get-a-feeling-of-the-dataset",
    "title": "Data visualization",
    "section": "Get a feeling of the dataset",
    "text": "Get a feeling of the dataset\n\n\n\n\n\n\nQuestion\n\n\n\nPick two random rows for each continent using slice_sample()\n\n\n\n\n\n\n\n\nsolution\n\n\n\nTo pick a slice at random, we can use function slice_sample. We can even perform sampling within groups defined by the value of a column.\n\nCodegapminder |&gt; \n  slice_sample(n=2, by=continent)\n\n# A tibble: 10 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Israel      Asia       2002    79.7  6029529    21906.\n 2 Nepal       Asia       1982    49.6 15796314      718.\n 3 Albania     Europe     1972    67.7  2263554     3313.\n 4 Hungary     Europe     1952    64.0  9504000     5264.\n 5 Congo, Rep. Africa     1987    57.5  2064095     4201.\n 6 Djibouti    Africa     1992    51.6   384156     2377.\n 7 Guatemala   Americas   1987    60.8  7326406     4246.\n 8 Haiti       Americas   1987    53.6  5756203     1823.\n 9 Australia   Oceania    1952    69.1  8691212    10040.\n10 New Zealand Oceania    2007    80.2  4115771    25185.\n\nCode#&lt; or equivalently \n# gapminder |&gt; \n#   group_by(continent) |&gt; \n#   slice_sample(n=2)\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat makes a table tidy?\n\n\n\n\n\n\n\n\nTip\n\n\n\nHave a look at Data tidying in R for Data Science (2nd ed.)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs the gapminder table redundant?\n\n\n\n\n\n\n\n\nsolution\n\n\n\ngapminder is redundant: column country completely determines the content of column continent. In database parlance, we have a functional dependancy: country → continent whereas the key of the table is made of columns country, year.\nTable gapminder is not in Boyce-Codd Normal Form (BCNF), not even in Third Normal Form (3NF)."
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#gapminder-tibble-extract",
    "href": "core/labs-solutions/lab-gapminder.html#gapminder-tibble-extract",
    "title": "Data visualization",
    "section": "Gapminder tibble (extract)",
    "text": "Gapminder tibble (extract)\n\n\n\n\n\n\nQuestion\n\n\n\nExtract/filter a subset of rows using dplyr::filter(...)\n\nAll rows concerning a given country\nAll rows concerning a year\nAll rows concerning a given continnent and a year\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCode# q: in gapminder table extract all raws concerning France\n\ngapminder |&gt; \n  filter(country=='France') |&gt; \n  head()\n\n# A tibble: 6 × 6\n  country continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 France  Europe     1952    67.4 42459667     7030.\n2 France  Europe     1957    68.9 44310863     8663.\n3 France  Europe     1962    70.5 47124000    10560.\n4 France  Europe     1967    71.6 49569000    13000.\n5 France  Europe     1972    72.4 51732000    16107.\n6 France  Europe     1977    73.8 53165019    18293.\n\n\n\n\n\n\n\n\n\n\nEquality testing is performed using ==, not = (which is used to implement assignment)"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#filtering-selection-σ-from-database-theory-picking-one-year-of-data",
    "href": "core/labs-solutions/lab-gapminder.html#filtering-selection-σ-from-database-theory-picking-one-year-of-data",
    "title": "Data visualization",
    "section": "Filtering (selection \\(σ\\) from database theory) : Picking one year of data",
    "text": "Filtering (selection \\(σ\\) from database theory) : Picking one year of data\nThere is simple way to filter rows satisfying some condition. It consists in mimicking indexation in a matrix, leaving the colum index empty, replacing the row index by a condition statement (a logical expression) also called a mask.\n\nCode# q: in gapminder table extract all raws concerning year 2002\n\ngapminder_2002 &lt;- gapminder |&gt;\n  filter(year==2002)  # \n\ngapminder_2002 &lt;- gapminder[gapminder$year==2002,]\n\n\nHave a look at\n\nCodegapminder$year==2002\n\n\nWhat is the type/class of this expression?\nThis is possible in base R and very often convenient.\nNevertheless, this way of performing row filtering does not emphasize the connection between the dataframe and the condition. Any logical vector with the right length could be used as a mask. Moreover, this way of performing filtering is not very functional.\n\n\n\n\n\n\nIn the parlance of Relational Algebra, filter performs a selection of rows. Relational expression\n\\[σ_{\\text{condition}}(\\text{Table})\\]\ntranslates to\n\nCodefilter(Table, condition)\n\n\nwhere \\(\\text{condition}\\) is a boolean expression that can be evaluated on each row of \\(\\text{Table}\\). In SQL, the relational expression would translate into\n\nCodeSELECT \n  *\nFROM \n  Table\nWHERE \n  condition\n\n\nCheck Package dplyr docs\nThe posit cheatsheet on dplyr is an unvaluable resource for table manipulation.\n\n\n\nUse dplyr::filter() to perform row filtering\n\n\n\n\n\n\nsolution\n\n\n\n\nCode# filter(gapminder, year==2002)\n\ngapminder |&gt; \n  filter(year==2002)\n\n# A tibble: 142 × 6\n   country     continent  year lifeExp       pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       2002    42.1  25268405      727.\n 2 Albania     Europe     2002    75.7   3508512     4604.\n 3 Algeria     Africa     2002    71.0  31287142     5288.\n 4 Angola      Africa     2002    41.0  10866106     2773.\n 5 Argentina   Americas   2002    74.3  38331121     8798.\n 6 Australia   Oceania    2002    80.4  19546792    30688.\n 7 Austria     Europe     2002    79.0   8148312    32418.\n 8 Bahrain     Asia       2002    74.8    656397    23404.\n 9 Bangladesh  Asia       2002    62.0 135656790     1136.\n10 Belgium     Europe     2002    78.3  10311970    30486.\n# ℹ 132 more rows\n\n\n\n\n\n\n\n\n\n\nData masking\n\n\n\nNote that in stating the condition, we simply write year==2002 even though year is not the name of an object in our current session. This is possible because filter( ) uses data masking, year is meant to denote a column in gapminder. SQL interpreters use the same mechanism.\nThe ability to use data masking is one of the great strengths of the R programming language."
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#static-plotting-first-attempt",
    "href": "core/labs-solutions/lab-gapminder.html#static-plotting-first-attempt",
    "title": "Data visualization",
    "section": "Static plotting: First attempt",
    "text": "Static plotting: First attempt\n\n\n\n\n\n\nQuestion\n\n\n\nDefine a plot with respect to gapminder_2002 along the lines suggested by Rosling’s presentation.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodep &lt;- gapminder_2002 |&gt;\n  ggplot() \n\n\n\n\n\n\n\n\n\n\nYou should define a ggplot object with data layer gapminder_2022 and call this object p for further reuse.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMap variables gdpPercap and lifeExp to axes x and y. Define the axes. In ggplot2 parlance, this is called aesthetic mapping. Use aes().\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCode# q: Map variables gdpPercap and lifeExp to axes x and y. Define the axes.\np &lt;- p +\n  aes(x=gdpPercap, y=lifeExp)\n\np \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse ggplot object p and add a global aesthetic mapping gdpPercap and lifeExp to axes x and y (using + from ggplot2) .\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nFor each row, draw a point at coordinates defined by the mapping. You need to add a geom_ layer to your ggplot object, in this case geom_point() will do.\n\n\n\n\n\n\n\n\nsolution\n\n\n\nWe add another layer to our graphical object.\n\nCodep &lt;- p +\n  geom_point()\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s up?\n\n\n\nWe are building a graphical object (a ggplot object) around a data frame (gapminder)\nWe supply aesthetic mappings (aes()) that can be either global or specifically bound to some geometries (geom_point()) or statistics\nThe global aesthetic mapping defines which columns (variables) are\n\nmapped to position (which columns are mapped to axes),\npossibly mapped to colours, linetypes, shapes, …\n\nGeometries and Statistics describe the building blocks of graphics\n\n\nWhat’s missing here?\nwhen comparing to the Gapminder demonstration, we can spot that\n\ncolors are missing\nbubble sizes are all the same. They should reflect the population size of the country\ntitles and legends are missing. This means the graphic object is useless.\n\nWe will add other layers to the graphical object to complete the plot"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#second-attempt-display-more-information",
    "href": "core/labs-solutions/lab-gapminder.html#second-attempt-display-more-information",
    "title": "Data visualization",
    "section": "Second attempt: display more information",
    "text": "Second attempt: display more information\n\n\n\n\n\n\nQuestion\n\n\n\n\nMap continent to color (use aes())\nMap pop to bubble size (use aes())\nMake point transparent by tuning alpha (inside geom_point() avoid overplotting)\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodep &lt;- p +\n  aes(color=continent, size=pop) +\n  geom_point(alpha=.5) \n\np\n\n\n\n\n\n\n\nNote that we only use global aesthetic mappings. This makes sense since we do not need to taylor aesthetics to specific geometries. Indeed we only have one geometry in our graphical object.\n\n\n\n\n\n\n\n\nsolution\n\n\n\nIn this enrichment of the graphical object, guides have been automatically added for two aesthetics: color and size. Those two guides are deemed necessary since the reader has no way to guess the mapping from the five levels of continent to color (the color scale), and the reader needs help to connect population size and bubble size.\nggplot2 provides us with helpers to fine tune guides.\nThe scalings on the x and y axis do not deserve guides: the ticks along the coordinate axes provide enough information."
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#scaling",
    "href": "core/labs-solutions/lab-gapminder.html#scaling",
    "title": "Data visualization",
    "section": "Scaling",
    "text": "Scaling\nTo pay tribute to Hans Rosling, we need to take care of two scaling issues:\n\nthe gdp per capita axis should be logarithmic scale_x_log10()\n\nthe area of the point should be proportional to the population scale_size_area()\n\n\n\n\n\n\n\n\nComplete the graphical object accordingly\n\n\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCode# q: use logarithmic scale for both axes\np &lt;- p +\n  scale_x_log10() +\n##  scale_size_area() +\n  ggtitle(\"Gapminder 2002, scaled\")\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMotivate the proposed scalings.\n\nWhy is it important to use logarithmic scaling for gdp per capita?\nWhen is it important to use logarithmic scaling on some axis (in other contexts)?\nWhy is it important to specify scale_size_area() ?\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nTo see why using scale_size_area() is important, we can check what happens when we use scale_size() instead.\n\nCodepop_range &lt;- c(0, max(gapminder_2002$pop))\n\np +\n  scale_radius(limits = pop_range) + \n  ggtitle(\"scale_radius\")\n\n\n\n\n\n\n\nWith scale_size_area(), the area of the point is proportional to the value of the variable mapped to size. With scale_size(), the radius of the point is proportional to the value of the variable mapped to size, so the area is proportional to the square of the value of the variable. This tends to exaggerate the differences between the sizes of the points. This is a way of lying with statistics.\n\n\n\n\n\n\n\n\nsolution\n\n\n\nWe use package patchwork to collect and present several graphical objects.\n\nCodeptchwrk &lt;- (\n  p + \n  scale_size(limits = pop_range) + \n  ggtitle(\"scale_size\")) + \n  (p + \n  scale_radius(limits = pop_range) + \n  ggtitle(\"scale_radius\")) \n\nptchwrk + plot_annotation(\n  title='Comparing scale_size and scale_radius', \n  caption='In the current setting, scale_size() should be favored'\n)\n\n\n\n\n\n\n\nAccording to the documentation, scale_size_area() ensures that a value of \\(0\\) is mapped to a size of \\(0\\). This is not the case with scale_size().\n\nCodeptchwrk &lt;- (\n  p + \n  scale_size(limits = pop_range) + \n  ggtitle(\"scale_size\")) + \n  (p + \n  scale_size_area() + \n  ggtitle(\"scale_size_area\")) \n\nptchwrk + plot_annotation(\n  title='Comparing scale_size and scale_size_area', \n  caption='In the current setting, scale_size_area() should be favored'\n)\n\n\n\n\n\n\n\n\nCodep &lt;- p + \n  scale_size_area()"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#in-perspective",
    "href": "core/labs-solutions/lab-gapminder.html#in-perspective",
    "title": "Data visualization",
    "section": "In perspective",
    "text": "In perspective\n\n\n\n\n\n\nQuestion\n\n\n\nUsing copilots completions, we can summarize the construcion of the graphical object in a series of questions.\n# q: Define a plot with respect to table gapminder_2002 along the lines suggested by Rosling's TED presentation\n# q: Map variables gdpPercap and lifeExp to axes x and y. Define the axes. \n# q: For each row, draw a point at coordinates defined by the mapping.\n# q: Map continent to color\n# q: Map pop to bubble size\n# q: Make point transparent by tuning alpha (inside geom_point() avoid overplotting)\n# q: Add a plot title\n# q: Make axes titles explicit and readable\n# q: Use labs(...)  \n# q: Use scale_x_log10() and scale_size_area()\n# q: Fine tune the guides: replace pop by Population and titlecase continent\n# q: Use theme_minimal()\n# q: Use scale_color_manual(...) to fine tune the color aesthetic mapping.\n# q: Use facet_zoom() from package ggforce\n# q: Add labels to points. This can be done by aesthetic mapping. Use aes(label=..)\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodeyoi &lt;- 2002\n\np &lt;-  p + \n  labs(\n    title=glue('The world in year {yoi}'),\n    x=\"Gross Domestic Product per capita (US$ 2009, corrected for PPP)\",\n    y=\"Life expectancy at birth\"\n  )\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nWe should also fine tune the guides: replace pop by Population and titlecase continent.\n\nCode# q: fine tune the guides: replace `pop` by `Population` and titlecase `continent`.\np &lt;- p +\n  guides(color = guide_legend(title = \"Continent\",\n                              override.aes = list(size = 5),\n                              order = 1),\n         size = guide_legend(title = \"Population\",\n                             order = 2))\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat should be the respective purposes of Title, Subtitle, Caption, … ?\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nThe title should be explicit and concise. It should summarize the content of the graphic object. Our title here “The world in year 2002” is concise but not explicit enough. The world may signify widely different things. Here, we mean world countries\nThe subtitle should provide additional information: “Public health does not boil down to GDP per capita”\nThe caption should provide additional information. Here we could explain the meaning of the axes, the color scale, the size scale, … provided guides are not enough. Here we could spot the source(s) of the data: UNO, WHO, World Bank, …, Gapminder foundation.\n\n\nCodep &lt;- p +\n  labs(\n    subtitle=\"Public health does not boil down to GDP per capita\",\n    caption=\"Source: Gapminder Foundation through Gapminder package\"\n  )\np"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#theming-using-ggthemes-or-not",
    "href": "core/labs-solutions/lab-gapminder.html#theming-using-ggthemes-or-not",
    "title": "Data visualization",
    "section": "Theming using ggthemes (or not)",
    "text": "Theming using ggthemes (or not)\n\nCodestopifnot(\n  require(\"ggthemes\")\n)\n\n\nA theme defines the look and feel of plots\nWithin a single document, we should use only one theme\nSee Getting the theme for a gallery of available themes\n\nCodep +\n  theme_economist()"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#tuning-scales",
    "href": "core/labs-solutions/lab-gapminder.html#tuning-scales",
    "title": "Data visualization",
    "section": "Tuning scales",
    "text": "Tuning scales\n\n\n\n\n\n\nQuestion\n\n\n\nUse scale_color_manual(...) to fine tune the color aesthetic mapping.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCode```{r}\n#| label: theme_scale\nneat_color_scale &lt;-\n      c(\"Africa\" = \"#01d4e5\",\n        \"Americas\" = \"#7dea01\" ,\n        \"Asia\" = \"#fc5173\",\n        \"Europe\" = \"#fde803\",\n        \"Oceania\" = \"#536227\")\n```\n\n\n\nCodep &lt;- p +\n  scale_size_area(max_size = 15) + #&lt;&lt;\n  scale_color_manual(values = neat_color_scale) #&lt;&lt;\n\nScale for size is already present.\nAdding another scale for size, which will replace the existing scale.\n\nCodep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nChoosing a color scale is a difficult task\nviridis is often a good pick.\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nMimnimalist themes are often a good pick.\n\nCodeold_theme &lt;- theme_set(theme_minimal())\n\n\n\nCodep &lt;- p +\n   scale_size_area(max_size = 15,\n                  labels= scales::label_number(scale=1/1e6,\n                                               suffix=\" M\")) +\n   scale_color_manual(values = neat_color_scale) +\n    labs(title= glue(\"Gapminder  {min(gapminder$year)}-{max(gapminder$year)}\"),\n         x = \"Yearly Income per Capita\",\n         y = \"Life Expectancy\",\n       caption=\"From sick  and poor (bottom left) to healthy and rich (top right)\")   \n\nScale for size is already present.\nAdding another scale for size, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\nCodep + theme(legend.position = \"none\")"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#zooming-on-a-continent",
    "href": "core/labs-solutions/lab-gapminder.html#zooming-on-a-continent",
    "title": "Data visualization",
    "section": "Zooming on a continent",
    "text": "Zooming on a continent\n\nCodezoom_continent &lt;- 'Europe'  # choose another continent at your convenience \n\n\n\n\n\n\n\n\nUse facet_zoom() from package ggforce\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodestopifnot(\n  require(\"ggforce\") #&lt;&lt;\n)\n\np_zoom_continent &lt;- p + \n  facet_zoom( #&lt;&lt;\n    xy= continent==zoom_continent, #&lt;&lt;\n    zoom.data= continent==zoom_continent #&lt;&lt;\n    ) #&lt;&lt;\n\np_zoom_continent"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#adding-labels",
    "href": "core/labs-solutions/lab-gapminder.html#adding-labels",
    "title": "Data visualization",
    "section": "Adding labels",
    "text": "Adding labels\n\n\n\n\n\n\nQuestion\n\n\n\nAdd labels to points. This can be done by aesthetic mapping. Use aes(label=..)\nTo avoid text cluttering, package ggrepel offers interesting tools.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodestopifnot(\nrequire(ggrepel) #&lt;&lt;\n)\n\np +\n   aes(label=country) + #&lt;&lt;\n   ggrepel::geom_label_repel(max.overlaps = 5) + #&lt;&lt;\n   scale_size_area(max_size = 15,\n                  labels= scales::label_number(scale=1/1e6,\n                                               suffix=\" M\"))  #+\n\n\n\nGapminder 2002 layer by layer\n\n\nCode  # scale_color_manual(values = neat_color_scale) +\n  # theme(legend.position = \"none\") +\n    # labs(title= glue(\"Gapminder  {min(gapminder$year)}-{max(gapminder$year)}\"),\n    #      x = \"Yearly Income per Capita\",\n    #      y = \"Life Expectancy\",\n    #    caption=\"From sick  and poor (bottom left) to healthy and rich (top right)\")"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#facetting",
    "href": "core/labs-solutions/lab-gapminder.html#facetting",
    "title": "Data visualization",
    "section": "Facetting",
    "text": "Facetting\nSo far we have only presented one year of data (2002)\nRosling used an animation to display the flow of time\nIf we have to deliver a printable report, we cannot rely on animation, but we can rely on facetting\nFacets are collections of small plots constructed in the same way on subsets of the data\n\n\n\n\n\n\nQuestion\n\n\n\nAdd a layer to the graphical object using facet_wrap()\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodep &lt;- p +\n  aes(text=country) +\n  guides(color = guide_legend(title = \"Continent\",\n                              override.aes = list(size = 5),\n                              order = 1),\n         size = guide_legend(title = \"Population\",\n                             order = 2)) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1)) +\n  facet_wrap(vars(year), ncol=6) +\n  ggtitle(\"Gapminder 1952-2007\")\n\np\n\n\n\n\n\n\n\n\n\n\nAs all rows in gapminder_2002 are all related to year 2002, we need to rebuild the graphical object along the same lines (using the same graphical pipeline) but starting from the whole gapminder dataset.\nShould we do this using cut and paste?\n No!!!\n\n\n\n\n\n\n\nDon’t Repeat Yoursel (DRY)\n\n\n\n\nAbide to the DRY principle using operator %+%: the ggplot2 object p can be fed with another dataframe and all you need is proper facetting.\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodep %+% gapminder"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#animate-for-free-with-plotly",
    "href": "core/labs-solutions/lab-gapminder.html#animate-for-free-with-plotly",
    "title": "Data visualization",
    "section": "Animate for free with plotly\n",
    "text": "Animate for free with plotly\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse plotly::ggplotly() to create a Rosling like animation.\nUse frame aesthetics.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCode```{r}\n#| label: animate\n#| eval: !expr knitr::is_html_output()\n#| code-annotations: hover\n\nq &lt;- filter(gapminder, FALSE) |&gt;\n   ggplot() +\n   aes(x = gdpPercap) +\n   aes(y = lifeExp) +\n   aes(size = pop) +\n   aes(text = country) +                   #\n   aes(fill = continent) +\n   # aes(frame = year) +                     #\n  geom_point(alpha=.5, colour='black') +\n  scale_x_log10() +\n  scale_size_area(max_size = 15,\n                  labels= scales::label_number(scale=1/1e6,\n                                               suffix=\" M\")) +\n  scale_fill_manual(values = neat_color_scale) +\n  theme(legend.position = \"none\") +\n  labs(title= glue(\"Gapminder  {min(gapminder$year)}-{max(gapminder$year)}\"),\n       x = \"Yearly Income per Capita\",\n       y = \"Life Expectancy\",\n       caption=\"From sick  and poor (bottom left) to healthy and rich (top right)\")\n\n\n(q %+% gapminder) |&gt;\n  plotly::ggplotly(height = 500, width=750)   \n```\n\n\n\n\n\n\n\ntext will be used while hovering\n\n\nframe is used by plotly to drive the animation. One frame per year\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCode```{r}\n#| eval: !expr knitr::is_html_output()\n\n(p %+% gapminder +\n facet_null() +\n aes(frame=year)) |&gt;\n plotly::ggplotly(height = 500, width=750)\n```"
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#suggestions",
    "href": "core/labs-solutions/lab-gapminder.html#suggestions",
    "title": "Data visualization",
    "section": "Suggestions",
    "text": "Suggestions\nThink about ways to visualize specific aspects of the gapminder data.\n\nHow could you overlay the world in 1952 and 2007?\nHow could you visualize the evolution of life expectancy and population across the different countries?\nVisualize the evolution of former colonies and their colonizers.\nVisualize the evolution of countries from the former Soviet Union, Warsaw Pact, and Yugoslavia.\nVisualize the evolution of countries from the former British Empire."
  },
  {
    "objectID": "core/labs-solutions/lab-gapminder.html#more-material",
    "href": "core/labs-solutions/lab-gapminder.html#more-material",
    "title": "Data visualization",
    "section": "More material",
    "text": "More material\n\nRead Visualization in R for Data Science"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-categorical.html#counting",
    "href": "core/labs-solutions/lab-univariate-categorical.html#counting",
    "title": "Univariate Categorical Analysis",
    "section": "Counting",
    "text": "Counting\n\n\n\n\n\n\nQuestion\n\n\n\nUse table, prop.table from base R to compute the frequencies and proportions of the different levels. In statistics, the result of table() is a (one-way) contingency table.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodedf |&gt;\n  pull(REV_FOYER) |&gt;\n  table()\n\n\n      [0-5000)    [5000-7500)   [7500-10000)  [10000-12500)  [12500-15000) \n             9              5              5              9              7 \n [15000-17500)  [17500-20000)  [20000-25000)  [25000-30000)  [30000-35000) \n            19             26             38             30             35 \n [35000-40000)  [40000-50000)  [50000-60000)  [60000-75000)  [75000-1e+05) \n            61             70             71             89             77 \n[1e+05-150000) \n            48 \n\n\n\nCodedf |&gt;\n  count(REV_FOYER)\n\n# A tibble: 16 × 2\n   REV_FOYER          n\n   &lt;fct&gt;          &lt;int&gt;\n 1 [0-5000)           9\n 2 [5000-7500)        5\n 3 [7500-10000)       5\n 4 [10000-12500)      9\n 5 [12500-15000)      7\n 6 [15000-17500)     19\n 7 [17500-20000)     26\n 8 [20000-25000)     38\n 9 [25000-30000)     30\n10 [30000-35000)     35\n11 [35000-40000)     61\n12 [40000-50000)     70\n13 [50000-60000)     71\n14 [60000-75000)     89\n15 [75000-1e+05)     77\n16 [1e+05-150000)    48\n\n\n\n\n\nWhat is the class of the object generated by table? Is it a vector, a list, a matrix, an array ?\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodeta &lt;- df %&gt;% \n  pull(REV_FOYER) %&gt;% \n  table() \n\nl &lt;- list(\n  is.vector=is.vector, \n  is.list=is.list, \n  is.matrix=is.matrix, \n  is.array=is.array\n)\n\nmap_lgl(l, ~ .x(ta))\n\nis.vector   is.list is.matrix  is.array \n    FALSE     FALSE     FALSE      TRUE \n\n\n\n\n\n\n\n\n\n\n\nas.data.frame() (or as_tibble) can transform a table object into a dataframe.\n\nCodeta &lt;-  rename(as.data.frame(ta), REV_FOYER=`.`)\n\nta\n\n        REV_FOYER Freq\n1        [0-5000)    9\n2     [5000-7500)    5\n3    [7500-10000)    5\n4   [10000-12500)    9\n5   [12500-15000)    7\n6   [15000-17500)   19\n7   [17500-20000)   26\n8   [20000-25000)   38\n9   [25000-30000)   30\n10  [30000-35000)   35\n11  [35000-40000)   61\n12  [40000-50000)   70\n13  [50000-60000)   71\n14  [60000-75000)   89\n15  [75000-1e+05)   77\n16 [1e+05-150000)   48\n\n\n\n\n\nYou may use knitr::kabble(), possibly knitr::kable(., format=\"markdown\") to tweak the output.\nIf you are more ambitious, use gt::....\nIn order to feed ggplot with a contingency table, it is useful to build contingency tables as dataframes. Use dplyr::count() to do this.\n\n\n\n\n\n\nskimr::skim() allows us to perform univariate categorical analysis all at once.\n\nCodedf %&gt;% \n  skimr::skim(where(is.factor)) %&gt;% \n  print(n=50)\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Piped data\nNumber of rows             599       \nNumber of columns          11        \n_______________________              \nColumn type frequency:               \n  factor                   9         \n________________________             \nGroup variables            None      \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 SEXE                  0             1 FALSE          2\n2 REGION                0             1 FALSE          4\n3 STAT_MARI             0             1 FALSE          5\n4 SYNDICAT              0             1 FALSE          2\n5 CATEGORIE             0             1 FALSE         10\n6 NIV_ETUDES            0             1 FALSE         15\n7 NB_PERS               0             1 FALSE          9\n8 NB_ENF                0             1 FALSE          7\n9 REV_FOYER             0             1 FALSE         16\n  top_counts                           \n1 M: 302, F: 297                       \n2 S: 200, W: 148, NE: 129, NW: 122     \n3 M: 325, C: 193, D: 61, S: 14         \n4 non: 496, oui: 103                   \n5 Lib: 133, Ser: 125, Adm: 94, Sel: 48 \n6 12 : 187, Col: 148, Bac: 114, Ass: 45\n7 2: 196, 4: 130, 3: 122, 1: 63        \n8 0: 413, 1: 86, 2: 76, 3: 18          \n9 [60: 89, [75: 77, [50: 71, [40: 70   \n\n\nThe output can be tailored to your specific objectives and fed to functions that are geared to displaying large tables (see packages knitr, DT, and gt)"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-categorical.html#using-a-for-loop",
    "href": "core/labs-solutions/lab-univariate-categorical.html#using-a-for-loop",
    "title": "Univariate Categorical Analysis",
    "section": "Using a for loop",
    "text": "Using a for loop\nWe have to build a barplot for each categorical variable. Here, we just have nine of them. We could do this using cut and paste, and some editing. In doing so, we would not comply with the DRY (Don’t Repeat Yourself) principle.\nIn order to remain DRY, we will attempt to abstract the recipe we used to build our first barplot.\nThis recipe is pretty simple:\n\nBuild a ggplot object with df as the data layer.\nAdd an aesthetic mapping a categorical column to axis x\n\nAdd a geometry using geom_bar\n\nAdd labels explaining the reader which column is under scrutiny\n\nWe first need to gather the names of the categorical columns. The following chunk does this in a simple way.\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodecol_names &lt;- df %&gt;% \n  select(where(is.factor))%&gt;%\n  names()\n\n\n\n\n\nIn the next chunk, we shall build a named list of ggplot objects consisting of barplots. The for loop body is almost obtained by cutting and pasting the recipe for the first barplot.\n\n\n\n\n\n\nNote an important difference: instead of something aes(x=col) where col denotes a column in the dataframe, we shall write aes(x=.data[[col]]) where col is a string that matches a column name. Writing aes(x=col) would not work.\nThe loop variable col iterates over the column names, not over the columns themselves.\nWhen using ggplot in interactive computations, we write aes(x=col), and, under the hood, the interpreter uses the tidy evaluation mechanism that underpins R to map df$col to the x axis.\nggplot functions like aes() use data masking to alleviate the burden of the working Statistician.\nWithin the context of ggplot programming, pronoun .data refers to the data layer of the graphical object.\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodelist_plots &lt;- list()\n\nfor (col in col_names){\n  p &lt;- df %&gt;% \n  ggplot() +\n  aes(x=.data[[col]]) +     # mind the .data pronoun\n  geom_bar() +\n  labs(\n    title=\"Census data\",\n    subtitle = col\n  )\n\n  list_plots[[col]] &lt;- p  # add the ggplot object to the list\n} \n\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nInspect the individual plots.\n\nCodep_temp &lt;- list_plots[[\"REV_FOYER\"]] +\n  aes(x=forcats::fct_infreq(.data[[\"REV_FOYER\"]]))\n\np_temp\n\n\n\n\n\n\n\n\n\n\nIf the labels on the x-axis are not readable, we need to tweak them. This amounts to modifying the theme layer in the ggplot object, and more specifically the axis.text.x attribute.\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodep_temp +\n  theme(axis.text.x = element_text(angle = 45))"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-categorical.html#using-functional-programming-lapply-purrr...",
    "href": "core/labs-solutions/lab-univariate-categorical.html#using-functional-programming-lapply-purrr...",
    "title": "Univariate Categorical Analysis",
    "section": "Using functional programming (lapply, purrr::...)",
    "text": "Using functional programming (lapply, purrr::...)\nAnother way to compute the list of graphical objects replaces the for loop by calling a functional programming tool. This mechanism relies on the fact that in R, functions are first-class objects.\n\n\n\n\n\n\nPackage purrr offers a large range of tools with a clean API. Base R offers lapply().\n\n\n\nWe shall first define a function that takes as arguments a datafame, a column name, and a title. We do not perform any defensive programming. Call your function foo.\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodefoo &lt;- function(df, col, .title= \"WE NEED A TITLE!!!\"){\n  p &lt;- df %&gt;% \n  ggplot() +\n  aes(x=fct_infreq(.data[[col]])) +\n  geom_bar() +\n  labs(\n    title=.title,\n    subtitle = col\n  ) +\n  theme(axis.text.x = element_text(angle = 45)) \n  return(p)\n}\n\n\n\n\n\nFunctional programmming makes code easier to understand.\nUse foo, lapply or purrr::map() to build the list of graphical objects.\nWith purrr::map(), you may use either a formula or an anonymous function. With lapply use an anonymous function.\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodell &lt;- map(col_names, ~ foo(df, .x, \"Census data\"))\n\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodemap(col_names, \\(x) foo(df, x, \"Census data\"))\n\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodelapply(col_names, \\(x)  foo(df, x, \"Census data\"))\n\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nThis is essentially like executing\n\nCodell &lt;- list()\n\nfor (.x in col_names){\n  ll[[.x]] &lt;- foo(df, .x, \"Census data\")\n}\n\n\n\n\n\nPackage patchwork offers functions for displaying collections of related plots.\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodepatchwork::wrap_plots(ll, ncol=3)"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#packages",
    "href": "core/labs-solutions/lab-r-intro.html#packages",
    "title": "R language: a tour",
    "section": "Packages",
    "text": "Packages\nBase R can do a lot. But the full power of R comes from a fast growing collection of packages.\nPackages are first installed (that is downloaded from cran and copied somewhere on the hard drive), and if needed, loaded during a session.\n\nInstallation can usually be performed using command install.packages(). In some circumstances, ad hoc installation commands (often from packages devtools) are needed\nPackage pak offers an interesting alternative to base R install.packages()\n\nOnce a package has been installed/downloaded on your hard drive\n\nif you want all objects exported by the package to be available in your session, you should load the package, using library() or require() (what’s the difference?). Technically, this loads the NameSpace defined by the package.\nif you just want to pick some objects exported from the package, you can use qualified names like package_name::object_name to access the object (function, dataset, …).\n\n\n\nFor example, when we write\ngapminder &lt;- gapminder::gapminder\nwe assign dataframe/tibble gapminder from package gapminder to identifier \"gapminder\" in global environment .\nFunction p_load() from pacman (package manager) blends installation and loading: if the package named in the argument of p_load() is not installed (not among the installed.packages()), p_load() attempts to install the package. If installation is successful, the package is loaded.\n\nif (! require(pak)){\n  install.packages(\"pak\")\n}\n\n\nstopifnot(\n  require(\"tidyverse\"), \n  require(\"lobstr\"),\n  require(\"ggforce\"),\n  require(\"nycflights13\"),\n  require(\"patchwork\"), \n  require(\"viridis\"),\n  require(\"MASS\"),\n  require(\"gapminder\"),\n  require(\"pryr\"),\n  require(\"pak\")\n)\n\n\n\n\n\n\n\nOptional arguments\n\n\n\nA very nice feature of R is that functions from base R as well as from packages have optional arguments with sensible default values. Look for example at documentation of require() using expression ?require.\nOptional settings may concern individual functions or the collection of functions exported by some packages. In the next chunk, we reset the default color scales used by graphical functions from ggplot2.\n\nopts &lt;- options()  # save old options\n\noptions(ggplot2.discrete.colour=\"viridis\")\noptions(ggplot2.continuous.colour=\"viridis\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou shall not confuse installing (on your hard-drive) and loading (in session) a package.\n\n\n\n\n\n\n\n\nQuestion for Pythonistas\n\n\n\n\nIn  what is the analogue of install.packages()?\nIn  what is the analogue of require()/library()?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nIn , you can install a package pck using pip install pck or conda install pck (for example).\nIn , the analogue of require(pck) could be\nfrom pck import *\nNote that in R, once a package in installed on the hard drive, you do not need to write something like\nimport pck\nto be able to use objects exported by pck using qualified names (like pck.ze_object), you just need to use R qualified names:\npck::ze_object"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#vector-creation-and-assignment",
    "href": "core/labs-solutions/lab-r-intro.html#vector-creation-and-assignment",
    "title": "R language: a tour",
    "section": "Vector creation and assignment",
    "text": "Vector creation and assignment\nThe next three lines create three numerical atomic vectors.\nIn IDE Rstudio, have a look at the environment pane on the right before running the chunk, and after.\nUse ls() to investigate the environment before and after the execution of the three assignments.\n\nls()\nx &lt;- c(1, 2, 12)\ny &lt;- 5:7\nz &lt;- 10:1\nx ; y ; z \nls()\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat are the identifiers known in the global environment before execution of lines 2-4?\nWhat are the identifiers known in the global environment after execution of lines 2-4?\nWhich objects are attached to identifiers x, y, and z?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nls()\n\n[1] \"opts\"\n\nx &lt;- c(1, 2, 12)\ny &lt;- 5:7\nz &lt;- 10:1\nx ; y ; z \n\n[1]  1  2 12\n\n\n[1] 5 6 7\n\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\nls()\n\n[1] \"opts\" \"x\"    \"y\"    \"z\"   \n\n\nThe chunks adds three identifiers x,y,z to the global environment. Identifiers are bound to R objects which turn out to be numerical vectors.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does the next chunk?\n\nls()\nw &lt;- y\nls()\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe chunk inserts a new identifier w in the global environment. This identifier is associated with the same object as y.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nIs the content of object denoted by y copied to a new object bound to w?\nInterpret the result of w == y.\nInterpret the result of identical(w,y) (use help(\"identical\") if needed).\n\n\nw == y \nidentical(w,y)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPackage lobstr lets us explore low-level aspects of R (and much more). Function lobstr::obj_addr() returns the address of the object denoted by the argument.\n\nlobstr::obj_addr(w)\n\n[1] \"0x62ef53d1d5a0\"\n\nlobstr::obj_addr(y)\n\n[1] \"0x62ef53d1d5a0\"\n\n\nNow, if we modify either y or w\n\ny &lt;- y + 1\nidentical(y, w)\n\n[1] FALSE\n\nc(lobstr::obj_addr(w), lobstr::obj_addr(y))\n\n[1] \"0x62ef53d1d5a0\" \"0x62ef53b16478\"\n\n\nThe address associated with y has changed!\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe meaning of assignment in R differs from its counterpart in Python. In Python, assignment is shallow. In R, assignment creates a new identifier bound to the same object as the right-hand side of the assignment. If either side of the assignment is modified, it is copied to a new object before modification. This is called copy-on-modify."
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#indexation-slicing-modification",
    "href": "core/labs-solutions/lab-r-intro.html#indexation-slicing-modification",
    "title": "R language: a tour",
    "section": "Indexation, slicing, modification",
    "text": "Indexation, slicing, modification\nSlicing a vector can be done in two ways:\n\nproviding a vector of indices to be selected. Indices need not be consecutive.\nproviding a Boolean mask, that is a logical vector to select a set of positions.\n\n\nx &lt;- c(1, 2, 12) ; y &lt;- 5:7 ; z &lt;- 10:1\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next lines\n\nz[1]   # slice of length 1\nz[0]   # What did you expect?\nz[x]   # slice of length ??? index error ?\nz[y]\nz[x %% 2]   # what happens with x[0] ?\nz[0 == (x %% 2)] # masking\nz[c(2, 1, 1)]\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nIndices start at 1 (not like in C, Java, or Python)\n\n z[0] does not return an Error message. It returns an empty vector with the same basetype as x\n\n\nz[x] returns a vector made of z[x[1]], z[x[2]] and z[x[3]]==z[12]. Note again that z[12] does not raise an exception. It is simply not available (NA).\n\nx %% 2 returns 1 0 0 as %% stands for mod. z[x %% 2] returns the same thing as z[1]\n\n\nc( ) stands for combine, or concatenate.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIf the length of mask and and the length of the sliced vector do not coincide, what happens?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNo error is signaled, the returned sequence is as long as the number of truthies in the mask.\nOut of bound truthies show up as NA\n\nz[rep(c(TRUE, FALSE), 6)]\n\n[1] 10  8  6  4  2 NA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA scalar is just a vector of length \\(1\\)!\n\nclass(z)\n\n[1] \"integer\"\n\nclass(z[1])\n\n[1] \"integer\"\n\nclass(z[c(2,1)])\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next lines\n\ny[2:3] &lt;- z[2:3]\ny == z[-10]\n\nz[-11]\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can assign a slice of a vector to a slice of identical size of another vector.\nWhat is the result of z[-11], z[-c(11:7)]?\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next line\n\nz[-(1:5)]\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe pick all positions in z but the ones in 1:5, that is 6, 7, 8, 9, 10\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you select the last element from a vector (say z)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nz[length(z)]\n\n[1] 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n is not  (reminder)!\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nReverse the entries of a vector. Find two ways to do that.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nz[seq(length(z), 1, by=-1)]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nz[length(z):1]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nrev(z)   # the simplest way, once you know rev()\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\nIn statistics, machine learning, we are often faced with the task of building grids of regularly spaced elements (these elements can be numeric or not). R offers a collection of tools to perform this. The most basic tool is rep().\n\n\n\n\n\n\nQuestion\n\n\n\n\nRepeat a vector \\(2\\) times\nRepeat each element of a vector twice\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nw &lt;- c(1, 7, 9)\nrep(w, 2)\n\n[1] 1 7 9 1 7 9\n\nrep(w, rep(2, length(w)))\n\n[1] 1 1 7 7 9 9\n\n\nNow, we can try something more fancy.\n\nrep(w, 1:3)\n\n[1] 1 7 7 9 9 9\n\n\nWhat are the requirements on the second (times) argument?\n\n\n\nLet us remove objects from the global environment.\n\nrm(w, x, y ,z)"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#numbers",
    "href": "core/labs-solutions/lab-r-intro.html#numbers",
    "title": "R language: a tour",
    "section": "Numbers",
    "text": "Numbers\nSo far, we told about numeric vectors. Numeric vectors are vectors of floating point numbers. R distinguishes several kinds of numbers.\n\nIntegers\nFloating point numbers (double)\n\nTo check whether a vector is made of numeric or of integer, use is.numeric() or is.integer(). Use as.integer, as.numeric() to enforce type conversion.\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the outcome of the next chunks\n\nclass(113L) ; class(113) ; class(113L + 113) ; class(2 * 113L) ; class(pi) ; as.integer(pi)\n\n[1] \"integer\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] 3\n\n\n\nfloor(pi) ; class(floor(pi)) # mind the floor\n\n[1] 3\n\n\n[1] \"numeric\""
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#integer-arithmetic",
    "href": "core/labs-solutions/lab-r-intro.html#integer-arithmetic",
    "title": "R language: a tour",
    "section": "Integer arithmetic",
    "text": "Integer arithmetic\n\n29L * 31L ; 899L %/% 32L ; 899L %% 30L\n\n[1] 899\n\n\n[1] 28\n\n\n[1] 29\n\n\n\n\n\n\n\n\nR integers are not the natural numbers from Mathematics\nR numerics are not the real numbers from Mathematics\n\n.Machine$double.eps\n\n[1] 2.220446e-16\n\n.Machine$double.xmax\n\n[1] 1.797693e+308\n\n.Machine$sizeof.longlong\n\n[1] 8\n\nu &lt;- double(19L)\nv &lt;- numeric(5L)\nw &lt;- integer(7L)\nlapply(list(u, v, w), typeof)\n\n[[1]]\n[1] \"double\"\n\n[[2]]\n[1] \"double\"\n\n[[3]]\n[1] \"integer\"\n\nlength(c(u, v, w))\n\n[1] 31\n\ntypeof(c(u, v, w))\n\n[1] \"double\"\n\n\n\n\n\nR is (sometimes) able to make sensible use of Infinite.\n\nlog(0)\n\n[1] -Inf\n\nlog(Inf)\n\n[1] Inf\n\n1/0\n\n[1] Inf\n\n0/0\n\n[1] NaN\n\nmax(c( 0/0,1,10))\n\n[1] NaN\n\nmax(c(NA,1,10))\n\n[1] NA\n\nmax(c(-Inf,1,10))\n\n[1] 10\n\nis.finite(c(-Inf,1,10))\n\n[1] FALSE  TRUE  TRUE\n\nis.na(c(NA,1,10))\n\n[1]  TRUE FALSE FALSE\n\nis.nan(c(NaN,1,10))\n\n[1]  TRUE FALSE FALSE"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#computing-with-vectors",
    "href": "core/labs-solutions/lab-r-intro.html#computing-with-vectors",
    "title": "R language: a tour",
    "section": "Computing with vectors",
    "text": "Computing with vectors\nSumming, scalar multiplication\n\nx &lt;- 1:3\ny &lt;- 9:7\n\nsum(x) ; prod(x)\n\n[1] 6\n\n\n[1] 6\n\nz &lt;- cumsum(1:3)\nw &lt;- cumprod(3:5)\n\nx + y\n\n[1] 10 10 10\n\nx + z\n\n[1] 2 5 9\n\n2 * w\n\n[1]   6  24 120\n\n2 + w\n\n[1]  5 14 62\n\nw / 2\n\n[1]  1.5  6.0 30.0\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute a factorial?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn &lt;- 10\ncumprod(1:n)\n\n [1]       1       2       6      24     120     720    5040   40320  362880\n[10] 3628800\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nApproximate \\(\\sum_{n=1}^\\infty 1/n^2\\) within \\(10^{-3}\\)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\\sum_{n &gt; N} \\frac{1}{n^2} &lt; \\sum_{n &gt; N} \\frac{1}{n(n-1)} = \\sum_{n &gt; N} \\left(\\frac{1}{n-1}-\\frac{1}{n}\\right) = \\frac{1}{N}\\] So we may pick \\(N=1000\\).\n\nsum(x*y) # inner product\n\n[1] 46\n\nprod(1:5) # factorial(n) as prod(1:n)\n\n[1] 120\n\nN &lt;- 1000L\nsum(1/((1:N)^2)) ; pi^2/6 # grand truth\n\n[1] 1.643935\n\n\n[1] 1.644934\n\n(pi^2/6 - sum(1/((1:N)^2))) &lt; 1e-3\n\n[1] TRUE\n\n# N &lt;- 999L\n# (pi^2/6 - sum(1/((1:N)^2))) &lt; 1e-3\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute the inner product between two (atomic numeric) vectors?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nInner product between two vectors can be computed as a matrix product between a row vector and a column vector using %*%. Is this a good idea?\n\nmatrix(w, ncol=3) %*% matrix(y, nrow=3) == sum(w * y)\n\n     [,1]\n[1,] TRUE\n\n\n\n\n\n\n\n\n\n\n\nWhat we have called vectors so far are indeed atomic vectors.\n\nRead Chapter on Vectors in R advanced Programming\n\nKeep an eye on package vctrs for getting insights into the R vectors."
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#creation-transposition-and-reshaping",
    "href": "core/labs-solutions/lab-r-intro.html#creation-transposition-and-reshaping",
    "title": "R language: a tour",
    "section": "Creation, transposition and reshaping",
    "text": "Creation, transposition and reshaping\nA vector can be turned into a column matrix.\n\nv &lt;- as.matrix(1:5)\nv\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[4,]    4\n[5,]    5\n\n\nA matrix can be transposed\n\nt(v)  # transpose \n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n\ncat(dim(v), ' ', dim(t(v)), '\\n')\n\n5 1   1 5 \n\n\n\nA &lt;- matrix(1, nrow=5, ncol=2) ; A\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n[3,]    1    1\n[4,]    1    1\n[5,]    1    1\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nlobstr::mem_used() allows us to keep track of the amount of memory used by our R session. lobstr::obj_size() tells us the amount of memory used by the representation of an object.\nComment the next chunk\n\nm1 &lt;-lobstr::mem_used()\nA &lt;- matrix(rnorm(100000L), nrow=1000L)\nm2 &lt;- lobstr::mem_used()\nlobstr::obj_size(A)\n\n800.22 kB\n\nB &lt;- t(A)\nlobstr::obj_size(B)\n\n800.22 kB\n\nm3 &lt;- lobstr::mem_used()\nm2-m1 ; m3-m2\n\n804.94 kB\n\n\n1.09 MB\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nIs there a difference between the next two assignments?\nHow would you assign value to all entries of a matrix?\n\n\nA &lt;- matrix(rnorm(16), nrow=4)\nA[] &lt;- 0 ; A\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n\nA   &lt;- 0 ; A\n\n[1] 0\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere is!\nThe first assignment assigns 0 to every entry in A.\nThe second assignment binds 0 to name A\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the final shape of A?\n\nA &lt;- matrix(1, nrow=5, ncol=2) \nA\nA[] &lt;- 1:15 \nA\n\n\n\nWe can easily generate diagonal matrices and constant matrices.\n\ndiag(1, 3)  # building identity matrix\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\nmatrix(0, 3, 3) # building null matrix\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs there any difference between the next two assignments?\n\nB &lt;- A[]\nB ; A\n\n[1] 0\n\n\n[1] 0\n\nlobstr::obj_addr(B) ; lobstr::obj_addr(A)\n\n[1] \"0x62ef535a4118\"\n\n\n[1] \"0x62ef54131aa8\"\n\nB &lt;- A\nlobstr::obj_addr(B) ; lobstr::obj_addr(A)\n\n[1] \"0x62ef54131aa8\"\n\n\n[1] \"0x62ef54131aa8\""
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#indexation-slicing-modification-1",
    "href": "core/labs-solutions/lab-r-intro.html#indexation-slicing-modification-1",
    "title": "R language: a tour",
    "section": "Indexation, slicing, modification",
    "text": "Indexation, slicing, modification\nIndexation consists in getting one item from a vector/list/matrix/array/dataframe.\nSlicing and subsetting consists in picking a substructure:\n\nsubsetting a vector returns a vector\nsubsetting a list returns a list\nsubsetting a matrix/array returns a matrix/array (beware of implicit simplifications and dimension dropping)\nsubsetting a dataframe returns a dataframe or a vector (again, beware of implicit simplifications).\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next results\n\nA &lt;- matrix(1, nrow=5, ncol=2)\n\ndim(A[sample(5, 3), -1])\ndim(A[sample(5, 3), 1])\nlength(A[sample(5, 3), 1])\nis.vector(A[sample(5, 3), 1])\nA[10:15]\nA[60]\ndim(A[])\n\n\n\n\n\n\n\n\n\n\n\nNULL\n\n\nNULL\n\n\n[1] 3\n\n\n[1] TRUE\n\n\n[1]  1 NA NA NA NA NA\n\n\n[1] NA\n\n\n[1] 5 2\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you create a fresh copy of a matrix?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nA &lt;- matrix(rnorm(10), ncol=2L)\nB &lt;- matrix(0, nrow=5L, ncol=2L)\n\nB[] &lt;- A\nall(B==A) ; identical(A, B) ; lobstr::obj_addrs(list(A, B))\n\n[1] TRUE\n\n\n[1] TRUE\n\n\n[1] \"0x62ef52ccf8b8\" \"0x62ef52cd00f8\""
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#computing-with-matrices",
    "href": "core/labs-solutions/lab-r-intro.html#computing-with-matrices",
    "title": "R language: a tour",
    "section": "Computing with matrices",
    "text": "Computing with matrices\n\n\n* versus %*%\n\n\n%*% stands for matrix multiplication. In order to use it, the two matrices should have conformant dimensions.\n\n\n\nt(v) %*% A\n\n         [,1]      [,2]\n[1,] 16.80234 -3.524731\n\n\nThere are a variety of reasonable products around. Some of them are available in R.\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute the Hilbert-Schmidt inner product between two matrices?\n\\[\\langle A, B\\rangle_{\\text{HS}} = \\text{Trace} \\big(A \\times B^\\top\\big)\\]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn R, trace() does not return the trace of a matrix! Function is used for debugging.\nJust remember that the trace of a matrix is the sum of its diagonal elements.\n\nA &lt;- matrix(runif(6), 2, 3)\nB &lt;- matrix(runif(6), 2, 3)\nfoo &lt;- sum(diag(A %*% t(B)))\nbar &lt;- sum(A * B)\nfoo ; bar\n\n[1] 0.5090224\n\n\n[1] 0.5090224\n\n\nAre you surprised?\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow can you invert a square (invertible) matrix?\n\n\nUse solve(A) which is a shorthand for solve(A, diag(1, nrow(3)))."
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#handling-three-valued-logic",
    "href": "core/labs-solutions/lab-r-intro.html#handling-three-valued-logic",
    "title": "R language: a tour",
    "section": "Handling three-valued logic",
    "text": "Handling three-valued logic\n\n\n\n\n\n\nQuestion\n\n\n\n\nTRUE &  (1&gt; (0/0))\n(1&gt; (0/0)) | TRUE\n(1&gt; (0/0)) | FALSE\nTRUE || (1&gt; (0/0))\nTRUE |  (1&gt; (0/0))\nTRUE || stopifnot(4&lt;3)\n# TRUE |  stopifnot(4&lt;3)  \nFALSE && stopifnot(4&lt;3)\n# FALSE & stopifnot(4&lt;3)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nTRUE &  (1&gt; (0/0))\n\n[1] NA\n\n(1&gt; (0/0)) | TRUE\n\n[1] TRUE\n\n(1&gt; (0/0)) | FALSE\n\n[1] NA\n\nTRUE || (1&gt; (0/0))\n\n[1] TRUE\n\nTRUE |  (1&gt; (0/0))\n\n[1] TRUE\n\nTRUE || stopifnot(4&lt;3)\n\n[1] TRUE\n\n# TRUE |  stopifnot(4&lt;3)  \nFALSE && stopifnot(4&lt;3)\n\n[1] FALSE\n\n# FALSE & stopifnot(4&lt;3)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the difference between logical operators || and | ?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n|| is lazy. It does not evaluate its second argument if the first one evaluates to TRUE.\n&& is also lazy.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemark: favor &, | over &&, ||."
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#all-and-any",
    "href": "core/labs-solutions/lab-r-intro.html#all-and-any",
    "title": "R language: a tour",
    "section": "\nall and any\n",
    "text": "all and any\n\nLook at the definition of all and any.\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow would you check that a square matrix is symmetric?\nHow would you check that a matrix is diagonal?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA square matrix is symmetric iff it is equal to its transpose. Recall that t(A) denotes the transpose of matrix A.\n\nA &lt;- matrix(rnorm(9), nrow=3, ncol=3) # a.s. non-symmetric\nall(A == t(A))\n\n[1] FALSE\n\nA &lt;- A %*% t(A)  # build a symmetric matrix, A + t(A) would work also\nall(A == t(A))\n\n[1] TRUE\n\n\nA == t(A) returns a matrix a logical matrix, whose entries are all TRUE iff A is symmetric.\nall() works for matrices as well as for vectors. This is sensible as matrices can be considered as vectors with some additional structure."
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#if-then-else",
    "href": "core/labs-solutions/lab-r-intro.html#if-then-else",
    "title": "R language: a tour",
    "section": "If () then {} else {}",
    "text": "If () then {} else {}\nIf expressions yes_expr and no_expr are complicated it makes sense to use the if (...) {...} else {...} construct\nThere is also a conditional statement with an optional else {}\n#| label: if-else\n#| eval: false\n#| collapse: false\nif (condition) {\n  ...\n} else {\n  ...\n}\n\n\n\n\n\n\nQuestion\n\n\n\nIs there an elif construct in R?\n\n\nNope!\n R also offers a switch\n#| label: switch\nswitch (object,\n  case1 = {action1}, \n  case2 = {action2}, \n  ...\n)\n\n\n\n\n\n\nThere exists a selection function ifelse(test, yes_expr, no_expr).\n\nifelse(test, yes, no)\n\nNote that ifelse(...) is vectorized.\n\nx &lt;-  1L:6L\ny &lt;-  rep(\"odd\", 6)\nz &lt;- rep(\"even\", 6)\n\nifelse(x %% 2L, y, z)\n\n[1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\n This is a vectorized function"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#iterations-for-it-in-iterable-...",
    "href": "core/labs-solutions/lab-r-intro.html#iterations-for-it-in-iterable-...",
    "title": "R language: a tour",
    "section": "Iterations for (it in iterable) {...}\n",
    "text": "Iterations for (it in iterable) {...}\n\nHave a look at Iteration section in R for Data Science\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a lower triangular matrix which represents the 5 first lines of the Pascal triangle.\n\n\nRecall\n\\[\\binom{n}{k} = \\binom{n-1}{k-1} + \\binom{n-1}{k}\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nT &lt;- matrix(0L, nrow=6, ncol=6)\nT[1,1] &lt;- 1L\n\nfor (i in 2:ncol(T))\n  T[i, 1:i] &lt;- c(0L, T[i-1, 2:i-1]) + T[i-1, 1:i]\n\ncolnames(T) &lt;- 0L:5L\nrownames(T) &lt;- 0L:5L\n\nT\n\n  0 1  2  3 4 5\n0 1 0  0  0 0 0\n1 1 1  0  0 0 0\n2 1 2  1  0 0 0\n3 1 3  3  1 0 0\n4 1 4  6  4 1 0\n5 1 5 10 10 5 1\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nLocate the smallest element in a numerical vector\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nv &lt;- sample(1:100, 100)\nv[1:10]\n\n [1] 31 92 62 94 86 39 64 72 79 99\n\npmin &lt;- 1\n\n# q: what is the purpose of the following loop?\n# for (i in 2:length(v)) {\n#   if (v[i]&lt;v[pmin]) {\n#     pmin &lt;- i\n#   }\n# }\nfor (i in seq_along(v)) {\n  if (v[i]&lt;v[pmin]) {\n    pmin &lt;- i\n  }\n}\n\nprint(stringr::str_c('minimum is at ', pmin, ', it is equal to ', v[pmin]))\n\n[1] \"minimum is at 18, it is equal to 1\"\n\n\nThere are some redundant braces {}"
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#while-condition",
    "href": "core/labs-solutions/lab-r-intro.html#while-condition",
    "title": "R language: a tour",
    "section": "While (condition) {…}",
    "text": "While (condition) {…}\n\n\n\n\n\n\nQuestion\n\n\n\nFind the location of the minimum in a vector v\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nv &lt;- sample(100, 100)\n\npmin &lt;- 1   # Minimum in v[1:1]\ni &lt;- 2\n\n# q: find le location of the minimum in  vector v\n\nwhile (i &lt;= length(v)) {\n  # loop invariant: v[pmin] == min(v[1:i])\n  if (v[i]&lt;v[pmin]) {\n    pmin &lt;- i\n  }\n  i &lt;- i + 1\n}\n\nprint(stringr::str_c('minimum is at ', pmin, ', it is equal to ', v[pmin]))\n\n[1] \"minimum is at 91, it is equal to 1\"\n\nwhich.min(v); v[which.min(v)]\n\n[1] 91\n\n\n[1] 1\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a loop that checks whether vector v is non-decreasing.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nresult &lt;- TRUE\n\nfor (i in 2:length(v))\n  if (v[i] &lt; v[i-1]) {\n    result &lt;- FALSE\n    break\n  }\n\nif (result) {\n  print(\"non-decreasing\")\n} else {\n  print(\"not non-decreasing\")\n}\n\n[1] \"not non-decreasing\""
  },
  {
    "objectID": "core/labs-solutions/lab-r-intro.html#operators-purrrmap_",
    "href": "core/labs-solutions/lab-r-intro.html#operators-purrrmap_",
    "title": "R language: a tour",
    "section": "Operators purrr::map_???\n",
    "text": "Operators purrr::map_???\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite truth tables for &, |, &&, ||, ! and xor\nHint: use purrr::map, function outer()\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nvals &lt;- c(TRUE, FALSE, NA)\nops &lt;- c(`&`, `|`, `xor`)\n\ntruth &lt;- purrr::map(ops, \\(x) outer(vals,vals, x))\n\nnames(truth) &lt;- (ops)\ntruth\n\n$`.Primitive(\"&\")`\n      [,1]  [,2]  [,3]\n[1,]  TRUE FALSE    NA\n[2,] FALSE FALSE FALSE\n[3,]    NA FALSE    NA\n\n$`.Primitive(\"|\")`\n     [,1]  [,2] [,3]\n[1,] TRUE  TRUE TRUE\n[2,] TRUE FALSE   NA\n[3,] TRUE    NA   NA\n\n$`function (x, y) \\n{\\n    (x | y) & !(x & y)\\n}`\n      [,1]  [,2] [,3]\n[1,] FALSE  TRUE   NA\n[2,]  TRUE FALSE   NA\n[3,]    NA    NA   NA\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a function that takes as input a square matrix and returns TRUE if it is lower triangular.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlt &lt;- function(A){\n  n &lt;- nrow(A)\n  all(purrr::map_lgl(1:(n-1), \\(x) all(0 == A[x, (x+1):n])))\n}\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse map , choose and proper use of pronouns to deliver the n first lines of the Pascal triangle using one line of code.\nAs far as the total number of operations is concerned, would you recommend this way of computing the Pascal triangle?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn &lt;- 5\n\ntp5 &lt;- matrix(unlist(map(0:n,\n           \\(x) c(choose(x, 0:x), rep(0L, n-x)))),\n       nrow=n+1,\n       byrow=T)\n\nrownames(tp5) &lt;- 0:n\n\ncolnames(tp5) &lt;- 0:n\n\ntp5\n\n  0 1  2  3 4 5\n0 1 0  0  0 0 0\n1 1 1  0  0 0 0\n2 1 2  1  0 0 0\n3 1 3  3  1 0 0\n4 1 4  6  4 1 0\n5 1 5 10 10 5 1\n\n\nNo. Using map and choose, we do not reuse previous computations. The total number of arithmetic operations is \\(\\Omega(n^3)\\), it should be \\(O(n^2)\\).\n\n\n\n\n\n\n\n\n\nRead Chapter on Functional Programming in Advanced R"
  },
  {
    "objectID": "core/labs-solutions/lab-linear-regression-whiteside.html#introduction",
    "href": "core/labs-solutions/lab-linear-regression-whiteside.html#introduction",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "Introduction",
    "text": "Introduction\nThe purpose of this lab is to introduce linear regression using base R and the tidyverse. We work on a dataset provided by the MASS package. This dataset is investigated in the book by Venables and Ripley. This discusssion is worth being read. Our aim is to relate regression as a tool for data exploration with regression as a method in statistical inference. To perform regression, we will rely on the base R function lm() and on the eponymous S3 class lm. We will spend time understanding how the formula argument can be used to construct a design matrix from a dataframe representing a dataset."
  },
  {
    "objectID": "core/labs-solutions/lab-linear-regression-whiteside.html#packages-installation-and-loading-again",
    "href": "core/labs-solutions/lab-linear-regression-whiteside.html#packages-installation-and-loading-again",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "Packages installation and loading (again)",
    "text": "Packages installation and loading (again)\n\n\nCode\n# We will use the following packages. \n# If needed, install them : pak::pkg_install(). \nstopifnot(\n  require(\"magrittr\"),\n  require(\"lobstr\"),\n  require(\"ggforce\"),\n  require(\"patchwork\"), \n  require(\"gt\"),\n  require(\"glue\"),\n  require(\"skimr\"),\n  require(\"corrr\"),\n  require(\"GGally\"),\n  require(\"broom\"),\n  require(\"tidyverse\"),\n  require(\"ggfortify\"),\n  require(\"autoplotly\")\n\n)\n\n\nBesides the tidyverse, we rely on skimr to perform univariate analysis, GGally::ggpairs to perform pairwise (bivariate) analysis. Package corrr provide graphical tools to explore correlation matrices. At some point, we will showcase the exposing pipe %$% and the classical pipe %&gt;% of magrittr. We use gt to display handy tables, patchwork to compose graphical objects. glue provides a kind of formatted strings. Package broom proves very useful when milking lienar models produced by lm() (and many other objects produced by estimators, tests, …)"
  },
  {
    "objectID": "core/labs-solutions/lab-linear-regression-whiteside.html#s3-classes-in-r",
    "href": "core/labs-solutions/lab-linear-regression-whiteside.html#s3-classes-in-r",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "S3 classes in R",
    "text": "S3 classes in R"
  },
  {
    "objectID": "core/labs-solutions/lab-linear-regression-whiteside.html#generic-functions-for-s3-classes",
    "href": "core/labs-solutions/lab-linear-regression-whiteside.html#generic-functions-for-s3-classes",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "Generic functions for S3 classes",
    "text": "Generic functions for S3 classes\nmethods(autoplot) lists the S3 classes for which an autoplot method is defined. Some methods are defined in ggplot2, others like autoplot.lm are defined in extension packages like ggfortify."
  },
  {
    "objectID": "core/labs-solutions/lab-linear-regression-whiteside.html#s4-classes-in-r",
    "href": "core/labs-solutions/lab-linear-regression-whiteside.html#s4-classes-in-r",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "S4 classes in R",
    "text": "S4 classes in R\nThe output of autoplot.lm is an instance of S4 class"
  },
  {
    "objectID": "core/labs-solutions/lab-linear-regression-whiteside.html#tibbles-with-list-columns",
    "href": "core/labs-solutions/lab-linear-regression-whiteside.html#tibbles-with-list-columns",
    "title": "LAB: Linear Regression on Whiteside data",
    "section": "tibbles with list columns",
    "text": "tibbles with list columns"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html#numerical-summary",
    "href": "core/labs-solutions/lab-univariate-numeric.html#numerical-summary",
    "title": "LAB: Univariate analysis",
    "section": "Numerical summary",
    "text": "Numerical summary\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodedf %&gt;% \n    pull(AGE) %&gt;% \n    summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  16.00   29.00   42.00   41.85   53.50   80.00 \n\nCodesd(df$AGE) ; IQR(df$AGE) ; mad(df$AGE)\n\n[1] 14.11648\n\n\n[1] 24.5\n\n\n[1] 17.7912\n\n\n\n\n\nUse skimr::skim()\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodedf %&gt;% \n    pull(AGE) %&gt;% \n    skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n599\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\ndata\n0\n1\n41.85\n14.12\n16\n29\n42\n53.5\n80\n▆▇▇▆▁\n\n\n\n\n\nCodeskm &lt;- df %&gt;% \n  skimr::skim(AGE)\n\nclass(skm)\n\n[1] \"skim_df\"    \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nCodeattributes(skm)\n\n$class\n[1] \"skim_df\"    \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n[1] 1\n\n$names\n [1] \"skim_type\"     \"skim_variable\" \"n_missing\"     \"complete_rate\"\n [5] \"numeric.mean\"  \"numeric.sd\"    \"numeric.p0\"    \"numeric.p25\"  \n [9] \"numeric.p50\"   \"numeric.p75\"   \"numeric.p100\"  \"numeric.hist\" \n\n$data_rows\n[1] 599\n\n$data_cols\n[1] 11\n\n$df_name\n[1] \"`.`\"\n\n$dt_key\n[1] NA\n\n$groups\ncharacter(0)\n\n$base_skimmers\n[1] \"n_missing\"     \"complete_rate\"\n\n$skimmers_used\n$skimmers_used$numeric\n[1] \"mean\" \"sd\"   \"p0\"   \"p25\"  \"p50\"  \"p75\"  \"p100\" \"hist\"\n\nCodetibble::as_tibble(skm)\n\n# A tibble: 1 × 12\n  skim_type skim_variable n_missing complete_rate numeric.mean numeric.sd\n  &lt;chr&gt;     &lt;chr&gt;             &lt;int&gt;         &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 numeric   AGE                   0             1         41.8       14.1\n# ℹ 6 more variables: numeric.p0 &lt;dbl&gt;, numeric.p25 &lt;dbl&gt;, numeric.p50 &lt;dbl&gt;,\n#   numeric.p75 &lt;dbl&gt;, numeric.p100 &lt;dbl&gt;, numeric.hist &lt;chr&gt;\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCompare mean and median, sd and IQR.\nAre mean and median systematically related?\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nAsk chatgpt.\nThere is at least one relation between median and mean for square-integrable distributoins: \\[|\\text{Median} - \\text{Mean}| \\leq \\text{sd}\\] Lévy’s inequality.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre standard deviation and IQR systematically related ?\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nAsk chatgpt.\nYes."
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html#boxplots",
    "href": "core/labs-solutions/lab-univariate-numeric.html#boxplots",
    "title": "LAB: Univariate analysis",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n\n\n\n\nQuestion\n\n\n\nDraw a boxplot of the Age distribution\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodedf |&gt; \n    ggplot() +\n    aes(x=1L, y=AGE) +\n    geom_boxplot() +\n    labs(\n        title=\"Age distribution\",\n        subtitle = \"Census data\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you get rid of the useless ticks on the x-axis?\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nAsk chatgpt.\nYes."
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html#histograms",
    "href": "core/labs-solutions/lab-univariate-numeric.html#histograms",
    "title": "LAB: Univariate analysis",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nQuestion\n\n\n\nPlot a histogram of the empirical distribution of the AGE column\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodep &lt;- df |&gt; \n  ggplot() +\n  aes(x=AGE) +\n  labs(\n    title = \"Age distribution\",\n    subtitle = \"Census data\",\n    x = \"Age (Years)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\np +\n  geom_histogram(aes(y=after_stat(density)),\n                 bins=15,\n                 fill=\"white\",\n                 color=\"black\") +\n  labs(\n    caption = \"Histogram\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nTry different values for the bins parameter of geom_histogram()"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html#density-estimates",
    "href": "core/labs-solutions/lab-univariate-numeric.html#density-estimates",
    "title": "LAB: Univariate analysis",
    "section": "Density estimates",
    "text": "Density estimates\n\n\n\n\n\n\nQuestion\n\n\n\nPlot a density estimate of the AGE column (use stat_density.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodep +\n  stat_density(\n              fill=\"white\",\n              color=\"black\") +\n  labs(\n    caption = \"Kernel Density Estimate\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPlay with parameters bw, kernel and adjust.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nOverlay the two plots (histogram and density).\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodep +\n  stat_density(\n              fill=\"white\",\n              color=\"black\") +\n  geom_histogram(aes(y=after_stat(density)),\n                 bins=15,\n                 fill=\"white\",\n                 color=\"black\",\n                 alpha=.5) +\n  labs(\n    caption = \"Overlayed Density Estimates\"\n  )"
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html#ecdf",
    "href": "core/labs-solutions/lab-univariate-numeric.html#ecdf",
    "title": "LAB: Univariate analysis",
    "section": "ECDF",
    "text": "ECDF\n\n\n\n\n\n\nQuestion\n\n\n\nPlot the Empirical CDF of the AGE distribution\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCodep +\n    stat_ecdf()  +\n    labs(\n        caption = \"Empirical CDF\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you read the quartiles from the ECDF pplot?\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\nOf course. Yes, we can."
  },
  {
    "objectID": "core/labs-solutions/lab-univariate-numeric.html#quantile-function",
    "href": "core/labs-solutions/lab-univariate-numeric.html#quantile-function",
    "title": "LAB: Univariate analysis",
    "section": "Quantile function",
    "text": "Quantile function\n\n\n\n\n\n\nQuestion\n\n\n\nPlot the quantile function of the AGE distribution."
  }
]